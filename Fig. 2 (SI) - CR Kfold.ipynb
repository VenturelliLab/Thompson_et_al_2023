{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-j3yniibd because the default path (/home/jaron/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n",
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "\n",
    "from cr.mcr import *\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# set plot parameters\n",
    "params = {'legend.fontsize': 18,\n",
    "          'figure.figsize': (16, 12),\n",
    "          'lines.linewidth': 4,\n",
    "          'axes.labelsize': 24,\n",
    "          'axes.titlesize':24,\n",
    "          'axes.linewidth':5,\n",
    "          'xtick.labelsize':20,\n",
    "          'ytick.labelsize':20}\n",
    "plt.rcParams.update(params)\n",
    "plt.style.use('seaborn-colorblind')\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set parameters of script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define k for k-fold (n_splits = k)\n",
    "n_splits = 20\n",
    "\n",
    "# number of times to run k-fold x-validation \n",
    "# each trial uses different random partitioning of data\n",
    "n_trials = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experiments</th>\n",
       "      <th>Time</th>\n",
       "      <th>PC_OD</th>\n",
       "      <th>PJ_OD</th>\n",
       "      <th>BV_OD</th>\n",
       "      <th>BF_OD</th>\n",
       "      <th>BO_OD</th>\n",
       "      <th>BT_OD</th>\n",
       "      <th>BC_OD</th>\n",
       "      <th>BY_OD</th>\n",
       "      <th>...</th>\n",
       "      <th>CG_OD</th>\n",
       "      <th>ER_OD</th>\n",
       "      <th>RI_OD</th>\n",
       "      <th>CC_OD</th>\n",
       "      <th>DL_OD</th>\n",
       "      <th>DF_OD</th>\n",
       "      <th>Butyrate</th>\n",
       "      <th>Acetate</th>\n",
       "      <th>Lactate</th>\n",
       "      <th>Succinate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PC-BV-BT-BC-BP-EL-FP-CH-AC-BH-CG-ER-RI-DF</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PC-BV-BT-BC-BP-EL-FP-CH-AC-BH-CG-ER-RI-DF</td>\n",
       "      <td>16</td>\n",
       "      <td>0.465116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249717</td>\n",
       "      <td>0.500651</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024339</td>\n",
       "      <td>0.327601</td>\n",
       "      <td>0.001460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.392830</td>\n",
       "      <td>23.092697</td>\n",
       "      <td>47.849302</td>\n",
       "      <td>18.910852</td>\n",
       "      <td>26.141885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PC-BV-BT-BC-BP-EL-FP-CH-AC-BH-CG-ER-RI-DF</td>\n",
       "      <td>32</td>\n",
       "      <td>0.104523</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027928</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.220107</td>\n",
       "      <td>0.380210</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020739</td>\n",
       "      <td>0.293384</td>\n",
       "      <td>0.000830</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.280111</td>\n",
       "      <td>23.996267</td>\n",
       "      <td>38.915218</td>\n",
       "      <td>17.977137</td>\n",
       "      <td>26.884748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PC-BV-BT-BC-BP-EL-FP-CH-AC-BH-CG-ER-RI-DF</td>\n",
       "      <td>48</td>\n",
       "      <td>0.124852</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012194</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.268268</td>\n",
       "      <td>0.333970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023660</td>\n",
       "      <td>0.188188</td>\n",
       "      <td>0.002366</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.224952</td>\n",
       "      <td>24.839219</td>\n",
       "      <td>34.325914</td>\n",
       "      <td>19.406971</td>\n",
       "      <td>31.628061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BV-BF-BO-BT-BU-DP-BL-BA-BP-EL-FP-CH-AC-BH-CG-E...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Experiments  Time     PC_OD  PJ_OD   \n",
       "0          PC-BV-BT-BC-BP-EL-FP-CH-AC-BH-CG-ER-RI-DF     0  0.000471    0.0  \\\n",
       "1          PC-BV-BT-BC-BP-EL-FP-CH-AC-BH-CG-ER-RI-DF    16  0.465116    0.0   \n",
       "2          PC-BV-BT-BC-BP-EL-FP-CH-AC-BH-CG-ER-RI-DF    32  0.104523    0.0   \n",
       "3          PC-BV-BT-BC-BP-EL-FP-CH-AC-BH-CG-ER-RI-DF    48  0.124852    0.0   \n",
       "4  BV-BF-BO-BT-BU-DP-BL-BA-BP-EL-FP-CH-AC-BH-CG-E...     0  0.000000    0.0   \n",
       "\n",
       "      BV_OD     BF_OD     BO_OD     BT_OD     BC_OD  BY_OD  ...     CG_OD   \n",
       "0  0.000471  0.000000  0.000000  0.000471  0.000471    0.0  ...  0.000471  \\\n",
       "1  0.029207  0.000000  0.000000  0.249717  0.500651    0.0  ...  0.024339   \n",
       "2  0.027928  0.000000  0.000000  0.220107  0.380210    0.0  ...  0.020739   \n",
       "3  0.012194  0.000000  0.000000  0.268268  0.333970    0.0  ...  0.023660   \n",
       "4  0.000388  0.000388  0.000388  0.000388  0.000000    0.0  ...  0.000388   \n",
       "\n",
       "      ER_OD     RI_OD  CC_OD  DL_OD     DF_OD   Butyrate    Acetate   \n",
       "0  0.000471  0.000471    0.0    0.0  0.000471   0.000000   0.000000  \\\n",
       "1  0.327601  0.001460    0.0    0.0  0.392830  23.092697  47.849302   \n",
       "2  0.293384  0.000830    0.0    0.0  0.280111  23.996267  38.915218   \n",
       "3  0.188188  0.002366    0.0    0.0  0.224952  24.839219  34.325914   \n",
       "4  0.000388  0.000388    0.0    0.0  0.000000   0.000000   0.000000   \n",
       "\n",
       "     Lactate  Succinate  \n",
       "0  28.000000   0.000000  \n",
       "1  18.910852  26.141885  \n",
       "2  17.977137  26.884748  \n",
       "3  19.406971  31.628061  \n",
       "4  28.000000   0.000000  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Data/2021_02_19_MultifunctionalDynamicData.csv\")\n",
    "\n",
    "all_experiments = df.Experiments.values\n",
    "unique_experiments = np.unique(all_experiments)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define which variables are species and which are metabolites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify species and metabolite names \n",
    "species = df.columns.values[2:-4]\n",
    "metabolites =  df.columns.values[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 380.03038, Residuals: -4.49660, Convergence:   inf\n",
      "Epoch: 1, Loss: 354.27073, Residuals: -4.37526, Convergence: 0.072712\n",
      "Epoch: 2, Loss: 333.20754, Residuals: -4.20953, Convergence: 0.063213\n",
      "Epoch: 3, Loss: 317.17474, Residuals: -4.04488, Convergence: 0.050549\n",
      "Epoch: 4, Loss: 304.94271, Residuals: -3.90037, Convergence: 0.040113\n",
      "Epoch: 5, Loss: 295.24367, Residuals: -3.77320, Convergence: 0.032851\n",
      "Epoch: 6, Loss: 287.37163, Residuals: -3.66282, Convergence: 0.027393\n",
      "Epoch: 7, Loss: 280.83761, Residuals: -3.56855, Convergence: 0.023266\n",
      "Epoch: 8, Loss: 275.27863, Residuals: -3.48814, Convergence: 0.020194\n",
      "Epoch: 9, Loss: 270.43761, Residuals: -3.41916, Convergence: 0.017901\n",
      "Epoch: 10, Loss: 266.13117, Residuals: -3.35948, Convergence: 0.016182\n",
      "Epoch: 11, Loss: 262.22610, Residuals: -3.30730, Convergence: 0.014892\n",
      "Epoch: 12, Loss: 258.62479, Residuals: -3.26111, Convergence: 0.013925\n",
      "Epoch: 13, Loss: 255.25621, Residuals: -3.21964, Convergence: 0.013197\n",
      "Epoch: 14, Loss: 252.06953, Residuals: -3.18178, Convergence: 0.012642\n",
      "Epoch: 15, Loss: 249.03021, Residuals: -3.14661, Convergence: 0.012205\n",
      "Epoch: 16, Loss: 246.11758, Residuals: -3.11347, Convergence: 0.011834\n",
      "Epoch: 17, Loss: 243.31975, Residuals: -3.08194, Convergence: 0.011499\n",
      "Epoch: 18, Loss: 240.62249, Residuals: -3.05170, Convergence: 0.011210\n",
      "Epoch: 19, Loss: 238.00165, Residuals: -3.02234, Convergence: 0.011012\n",
      "Epoch: 20, Loss: 235.42633, Residuals: -2.99335, Convergence: 0.010939\n",
      "Epoch: 21, Loss: 232.86724, Residuals: -2.96422, Convergence: 0.010989\n",
      "Epoch: 22, Loss: 230.30280, Residuals: -2.93456, Convergence: 0.011135\n",
      "Epoch: 23, Loss: 227.71540, Residuals: -2.90412, Convergence: 0.011362\n",
      "Epoch: 24, Loss: 225.07688, Residuals: -2.87257, Convergence: 0.011723\n",
      "Epoch: 25, Loss: 222.33974, Residuals: -2.83932, Convergence: 0.012311\n",
      "Epoch: 26, Loss: 219.46197, Residuals: -2.80382, Convergence: 0.013113\n",
      "Epoch: 27, Loss: 216.48758, Residuals: -2.76644, Convergence: 0.013739\n",
      "Epoch: 28, Loss: 213.54685, Residuals: -2.72857, Convergence: 0.013771\n",
      "Epoch: 29, Loss: 210.71141, Residuals: -2.69112, Convergence: 0.013456\n",
      "Epoch: 30, Loss: 207.98369, Residuals: -2.65420, Convergence: 0.013115\n",
      "Epoch: 31, Loss: 205.34913, Residuals: -2.61774, Convergence: 0.012830\n",
      "Epoch: 32, Loss: 202.79467, Residuals: -2.58164, Convergence: 0.012596\n",
      "Epoch: 33, Loss: 200.31143, Residuals: -2.54585, Convergence: 0.012397\n",
      "Epoch: 34, Loss: 197.89405, Residuals: -2.51034, Convergence: 0.012216\n",
      "Epoch: 35, Loss: 195.53953, Residuals: -2.47512, Convergence: 0.012041\n",
      "Epoch: 36, Loss: 193.24632, Residuals: -2.44020, Convergence: 0.011867\n",
      "Epoch: 37, Loss: 191.01362, Residuals: -2.40559, Convergence: 0.011689\n",
      "Epoch: 38, Loss: 188.84096, Residuals: -2.37131, Convergence: 0.011505\n",
      "Epoch: 39, Loss: 186.72794, Residuals: -2.33737, Convergence: 0.011316\n",
      "Epoch: 40, Loss: 184.67408, Residuals: -2.30377, Convergence: 0.011122\n",
      "Epoch: 41, Loss: 182.67879, Residuals: -2.27050, Convergence: 0.010922\n",
      "Epoch: 42, Loss: 180.74129, Residuals: -2.23757, Convergence: 0.010720\n",
      "Epoch: 43, Loss: 178.86066, Residuals: -2.20498, Convergence: 0.010514\n",
      "Epoch: 44, Loss: 177.03578, Residuals: -2.17272, Convergence: 0.010308\n",
      "Epoch: 45, Loss: 175.26536, Residuals: -2.14080, Convergence: 0.010101\n",
      "Epoch: 46, Loss: 173.54799, Residuals: -2.10920, Convergence: 0.009896\n",
      "Epoch: 47, Loss: 171.88217, Residuals: -2.07793, Convergence: 0.009692\n",
      "Epoch: 48, Loss: 170.26657, Residuals: -2.04698, Convergence: 0.009489\n",
      "Epoch: 49, Loss: 168.70029, Residuals: -2.01635, Convergence: 0.009284\n",
      "Epoch: 50, Loss: 167.18302, Residuals: -1.98604, Convergence: 0.009075\n",
      "Epoch: 51, Loss: 165.71529, Residuals: -1.95606, Convergence: 0.008857\n",
      "Epoch: 52, Loss: 164.29827, Residuals: -1.92644, Convergence: 0.008625\n",
      "Epoch: 53, Loss: 162.93357, Residuals: -1.89722, Convergence: 0.008376\n",
      "Epoch: 54, Loss: 161.62280, Residuals: -1.86843, Convergence: 0.008110\n",
      "Epoch: 55, Loss: 160.36725, Residuals: -1.84014, Convergence: 0.007829\n",
      "Epoch: 56, Loss: 159.16769, Residuals: -1.81239, Convergence: 0.007536\n",
      "Epoch: 57, Loss: 158.02427, Residuals: -1.78523, Convergence: 0.007236\n",
      "Epoch: 58, Loss: 156.93654, Residuals: -1.75870, Convergence: 0.006931\n",
      "Epoch: 59, Loss: 155.90354, Residuals: -1.73285, Convergence: 0.006626\n",
      "Epoch: 60, Loss: 154.92389, Residuals: -1.70770, Convergence: 0.006323\n",
      "Epoch: 61, Loss: 153.99583, Residuals: -1.68329, Convergence: 0.006026\n",
      "Epoch: 62, Loss: 153.11734, Residuals: -1.65962, Convergence: 0.005737\n",
      "Epoch: 63, Loss: 152.28616, Residuals: -1.63670, Convergence: 0.005458\n",
      "Epoch: 64, Loss: 151.49994, Residuals: -1.61454, Convergence: 0.005190\n",
      "Epoch: 65, Loss: 150.75625, Residuals: -1.59313, Convergence: 0.004933\n",
      "Epoch: 66, Loss: 150.05274, Residuals: -1.57247, Convergence: 0.004688\n",
      "Epoch: 67, Loss: 149.38713, Residuals: -1.55254, Convergence: 0.004456\n",
      "Epoch: 68, Loss: 148.75734, Residuals: -1.53332, Convergence: 0.004234\n",
      "Epoch: 69, Loss: 148.16145, Residuals: -1.51481, Convergence: 0.004022\n",
      "Epoch: 70, Loss: 147.59775, Residuals: -1.49699, Convergence: 0.003819\n",
      "Epoch: 71, Loss: 147.06465, Residuals: -1.47984, Convergence: 0.003625\n",
      "Epoch: 72, Loss: 146.56075, Residuals: -1.46335, Convergence: 0.003438\n",
      "Epoch: 73, Loss: 146.08472, Residuals: -1.44751, Convergence: 0.003259\n",
      "Epoch: 74, Loss: 145.63535, Residuals: -1.43230, Convergence: 0.003086\n",
      "Epoch: 75, Loss: 145.21147, Residuals: -1.41772, Convergence: 0.002919\n",
      "Epoch: 76, Loss: 144.81196, Residuals: -1.40375, Convergence: 0.002759\n",
      "Epoch: 77, Loss: 144.43575, Residuals: -1.39038, Convergence: 0.002605\n",
      "Epoch: 78, Loss: 144.08180, Residuals: -1.37759, Convergence: 0.002457\n",
      "Epoch: 79, Loss: 143.74910, Residuals: -1.36537, Convergence: 0.002314\n",
      "Epoch: 80, Loss: 143.43667, Residuals: -1.35370, Convergence: 0.002178\n",
      "Epoch: 81, Loss: 143.14355, Residuals: -1.34258, Convergence: 0.002048\n",
      "Epoch: 82, Loss: 142.86877, Residuals: -1.33198, Convergence: 0.001923\n",
      "Epoch: 83, Loss: 142.61138, Residuals: -1.32191, Convergence: 0.001805\n",
      "Epoch: 84, Loss: 142.37041, Residuals: -1.31233, Convergence: 0.001693\n",
      "Epoch: 85, Loss: 142.14484, Residuals: -1.30324, Convergence: 0.001587\n",
      "Epoch: 86, Loss: 141.93359, Residuals: -1.29463, Convergence: 0.001488\n",
      "Epoch: 87, Loss: 141.73555, Residuals: -1.28647, Convergence: 0.001397\n",
      "Epoch: 88, Loss: 141.54946, Residuals: -1.27875, Convergence: 0.001315\n",
      "Epoch: 89, Loss: 141.37400, Residuals: -1.27144, Convergence: 0.001241\n",
      "Epoch: 90, Loss: 141.20777, Residuals: -1.26453, Convergence: 0.001177\n",
      "Epoch: 91, Loss: 141.04935, Residuals: -1.25798, Convergence: 0.001123\n",
      "Epoch: 92, Loss: 140.89728, Residuals: -1.25177, Convergence: 0.001079\n",
      "Epoch: 93, Loss: 140.75025, Residuals: -1.24585, Convergence: 0.001045\n",
      "Epoch: 94, Loss: 140.60706, Residuals: -1.24020, Convergence: 0.001018\n",
      "Epoch: 95, Loss: 140.46670, Residuals: -1.23478, Convergence: 0.000999\n",
      "Evidence -181.750\n",
      "\n",
      "Epoch: 95, Evidence: -181.74994, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.24e-01\n",
      "Epoch: 95, Loss: 1355.75315, Residuals: -1.23478, Convergence:   inf\n",
      "Epoch: 96, Loss: 1293.97011, Residuals: -1.26220, Convergence: 0.047747\n",
      "Epoch: 97, Loss: 1245.75250, Residuals: -1.28463, Convergence: 0.038706\n",
      "Epoch: 98, Loss: 1208.83145, Residuals: -1.30105, Convergence: 0.030543\n",
      "Epoch: 99, Loss: 1180.12333, Residuals: -1.31234, Convergence: 0.024326\n",
      "Epoch: 100, Loss: 1157.07805, Residuals: -1.32019, Convergence: 0.019917\n",
      "Epoch: 101, Loss: 1138.10523, Residuals: -1.32569, Convergence: 0.016671\n",
      "Epoch: 102, Loss: 1122.21077, Residuals: -1.32944, Convergence: 0.014164\n",
      "Epoch: 103, Loss: 1108.71409, Residuals: -1.33180, Convergence: 0.012173\n",
      "Epoch: 104, Loss: 1097.11507, Residuals: -1.33297, Convergence: 0.010572\n",
      "Epoch: 105, Loss: 1087.02742, Residuals: -1.33312, Convergence: 0.009280\n",
      "Epoch: 106, Loss: 1078.14259, Residuals: -1.33236, Convergence: 0.008241\n",
      "Epoch: 107, Loss: 1070.20597, Residuals: -1.33076, Convergence: 0.007416\n",
      "Epoch: 108, Loss: 1063.00183, Residuals: -1.32839, Convergence: 0.006777\n",
      "Epoch: 109, Loss: 1056.34186, Residuals: -1.32530, Convergence: 0.006305\n",
      "Epoch: 110, Loss: 1050.05726, Residuals: -1.32148, Convergence: 0.005985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 111, Loss: 1043.99831, Residuals: -1.31693, Convergence: 0.005804\n",
      "Epoch: 112, Loss: 1038.03641, Residuals: -1.31167, Convergence: 0.005743\n",
      "Epoch: 113, Loss: 1032.07751, Residuals: -1.30568, Convergence: 0.005774\n",
      "Epoch: 114, Loss: 1026.07593, Residuals: -1.29902, Convergence: 0.005849\n",
      "Epoch: 115, Loss: 1020.04860, Residuals: -1.29176, Convergence: 0.005909\n",
      "Epoch: 116, Loss: 1014.07087, Residuals: -1.28399, Convergence: 0.005895\n",
      "Epoch: 117, Loss: 1008.25040, Residuals: -1.27583, Convergence: 0.005773\n",
      "Epoch: 118, Loss: 1002.68634, Residuals: -1.26738, Convergence: 0.005549\n",
      "Epoch: 119, Loss: 997.44151, Residuals: -1.25875, Convergence: 0.005258\n",
      "Epoch: 120, Loss: 992.53734, Residuals: -1.25004, Convergence: 0.004941\n",
      "Epoch: 121, Loss: 987.96639, Residuals: -1.24132, Convergence: 0.004627\n",
      "Epoch: 122, Loss: 983.70526, Residuals: -1.23266, Convergence: 0.004332\n",
      "Epoch: 123, Loss: 979.72579, Residuals: -1.22410, Convergence: 0.004062\n",
      "Epoch: 124, Loss: 975.99997, Residuals: -1.21570, Convergence: 0.003817\n",
      "Epoch: 125, Loss: 972.50263, Residuals: -1.20748, Convergence: 0.003596\n",
      "Epoch: 126, Loss: 969.21137, Residuals: -1.19947, Convergence: 0.003396\n",
      "Epoch: 127, Loss: 966.10771, Residuals: -1.19169, Convergence: 0.003213\n",
      "Epoch: 128, Loss: 963.17518, Residuals: -1.18416, Convergence: 0.003045\n",
      "Epoch: 129, Loss: 960.40003, Residuals: -1.17687, Convergence: 0.002890\n",
      "Epoch: 130, Loss: 957.77039, Residuals: -1.16985, Convergence: 0.002746\n",
      "Epoch: 131, Loss: 955.27560, Residuals: -1.16310, Convergence: 0.002612\n",
      "Epoch: 132, Loss: 952.90621, Residuals: -1.15661, Convergence: 0.002486\n",
      "Epoch: 133, Loss: 950.65362, Residuals: -1.15038, Convergence: 0.002370\n",
      "Epoch: 134, Loss: 948.50996, Residuals: -1.14441, Convergence: 0.002260\n",
      "Epoch: 135, Loss: 946.46775, Residuals: -1.13870, Convergence: 0.002158\n",
      "Epoch: 136, Loss: 944.52016, Residuals: -1.13324, Convergence: 0.002062\n",
      "Epoch: 137, Loss: 942.66150, Residuals: -1.12803, Convergence: 0.001972\n",
      "Epoch: 138, Loss: 940.88556, Residuals: -1.12304, Convergence: 0.001888\n",
      "Epoch: 139, Loss: 939.18693, Residuals: -1.11829, Convergence: 0.001809\n",
      "Epoch: 140, Loss: 937.56103, Residuals: -1.11375, Convergence: 0.001734\n",
      "Epoch: 141, Loss: 936.00244, Residuals: -1.10941, Convergence: 0.001665\n",
      "Epoch: 142, Loss: 934.50750, Residuals: -1.10528, Convergence: 0.001600\n",
      "Epoch: 143, Loss: 933.07172, Residuals: -1.10133, Convergence: 0.001539\n",
      "Epoch: 144, Loss: 931.69147, Residuals: -1.09756, Convergence: 0.001481\n",
      "Epoch: 145, Loss: 930.36294, Residuals: -1.09396, Convergence: 0.001428\n",
      "Epoch: 146, Loss: 929.08270, Residuals: -1.09052, Convergence: 0.001378\n",
      "Epoch: 147, Loss: 927.84759, Residuals: -1.08723, Convergence: 0.001331\n",
      "Epoch: 148, Loss: 926.65463, Residuals: -1.08408, Convergence: 0.001287\n",
      "Epoch: 149, Loss: 925.50049, Residuals: -1.08107, Convergence: 0.001247\n",
      "Epoch: 150, Loss: 924.38238, Residuals: -1.07818, Convergence: 0.001210\n",
      "Epoch: 151, Loss: 923.29762, Residuals: -1.07540, Convergence: 0.001175\n",
      "Epoch: 152, Loss: 922.24321, Residuals: -1.07274, Convergence: 0.001143\n",
      "Epoch: 153, Loss: 921.21651, Residuals: -1.07017, Convergence: 0.001115\n",
      "Epoch: 154, Loss: 920.21408, Residuals: -1.06770, Convergence: 0.001089\n",
      "Epoch: 155, Loss: 919.23362, Residuals: -1.06531, Convergence: 0.001067\n",
      "Epoch: 156, Loss: 918.27184, Residuals: -1.06299, Convergence: 0.001047\n",
      "Epoch: 157, Loss: 917.32520, Residuals: -1.06074, Convergence: 0.001032\n",
      "Epoch: 158, Loss: 916.39106, Residuals: -1.05855, Convergence: 0.001019\n",
      "Epoch: 159, Loss: 915.46592, Residuals: -1.05640, Convergence: 0.001011\n",
      "Epoch: 160, Loss: 914.54665, Residuals: -1.05430, Convergence: 0.001005\n",
      "Epoch: 161, Loss: 913.63083, Residuals: -1.05223, Convergence: 0.001002\n",
      "Epoch: 162, Loss: 912.71631, Residuals: -1.05018, Convergence: 0.001002\n",
      "Epoch: 163, Loss: 911.80202, Residuals: -1.04815, Convergence: 0.001003\n",
      "Epoch: 164, Loss: 910.88802, Residuals: -1.04614, Convergence: 0.001003\n",
      "Epoch: 165, Loss: 909.97626, Residuals: -1.04414, Convergence: 0.001002\n",
      "Epoch: 166, Loss: 909.06931, Residuals: -1.04216, Convergence: 0.000998\n",
      "Evidence 11044.665\n",
      "\n",
      "Epoch: 166, Evidence: 11044.66504, Convergence: 1.016456\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.77e-01\n",
      "Epoch: 166, Loss: 2333.66512, Residuals: -1.04216, Convergence:   inf\n",
      "Epoch: 167, Loss: 2292.28686, Residuals: -1.05026, Convergence: 0.018051\n",
      "Epoch: 168, Loss: 2264.69173, Residuals: -1.04895, Convergence: 0.012185\n",
      "Epoch: 169, Loss: 2241.53615, Residuals: -1.04691, Convergence: 0.010330\n",
      "Epoch: 170, Loss: 2221.92401, Residuals: -1.04457, Convergence: 0.008827\n",
      "Epoch: 171, Loss: 2205.20306, Residuals: -1.04207, Convergence: 0.007583\n",
      "Epoch: 172, Loss: 2190.84857, Residuals: -1.03946, Convergence: 0.006552\n",
      "Epoch: 173, Loss: 2178.42561, Residuals: -1.03678, Convergence: 0.005703\n",
      "Epoch: 174, Loss: 2167.56936, Residuals: -1.03403, Convergence: 0.005008\n",
      "Epoch: 175, Loss: 2157.96916, Residuals: -1.03121, Convergence: 0.004449\n",
      "Epoch: 176, Loss: 2149.37099, Residuals: -1.02830, Convergence: 0.004000\n",
      "Epoch: 177, Loss: 2141.57901, Residuals: -1.02528, Convergence: 0.003638\n",
      "Epoch: 178, Loss: 2134.45661, Residuals: -1.02217, Convergence: 0.003337\n",
      "Epoch: 179, Loss: 2127.91990, Residuals: -1.01898, Convergence: 0.003072\n",
      "Epoch: 180, Loss: 2121.91664, Residuals: -1.01576, Convergence: 0.002829\n",
      "Epoch: 181, Loss: 2116.40920, Residuals: -1.01255, Convergence: 0.002602\n",
      "Epoch: 182, Loss: 2111.35742, Residuals: -1.00939, Convergence: 0.002393\n",
      "Epoch: 183, Loss: 2106.71889, Residuals: -1.00631, Convergence: 0.002202\n",
      "Epoch: 184, Loss: 2102.44938, Residuals: -1.00333, Convergence: 0.002031\n",
      "Epoch: 185, Loss: 2098.50524, Residuals: -1.00045, Convergence: 0.001880\n",
      "Epoch: 186, Loss: 2094.84463, Residuals: -0.99768, Convergence: 0.001747\n",
      "Epoch: 187, Loss: 2091.43092, Residuals: -0.99503, Convergence: 0.001632\n",
      "Epoch: 188, Loss: 2088.23328, Residuals: -0.99249, Convergence: 0.001531\n",
      "Epoch: 189, Loss: 2085.22438, Residuals: -0.99006, Convergence: 0.001443\n",
      "Epoch: 190, Loss: 2082.38452, Residuals: -0.98773, Convergence: 0.001364\n",
      "Epoch: 191, Loss: 2079.69672, Residuals: -0.98552, Convergence: 0.001292\n",
      "Epoch: 192, Loss: 2077.14980, Residuals: -0.98342, Convergence: 0.001226\n",
      "Epoch: 193, Loss: 2074.73510, Residuals: -0.98143, Convergence: 0.001164\n",
      "Epoch: 194, Loss: 2072.44561, Residuals: -0.97955, Convergence: 0.001105\n",
      "Epoch: 195, Loss: 2070.27611, Residuals: -0.97776, Convergence: 0.001048\n",
      "Epoch: 196, Loss: 2068.22171, Residuals: -0.97608, Convergence: 0.000993\n",
      "Evidence 14288.341\n",
      "\n",
      "Epoch: 196, Evidence: 14288.34082, Convergence: 0.227016\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.40e-01\n",
      "Epoch: 196, Loss: 2464.30844, Residuals: -0.97608, Convergence:   inf\n",
      "Epoch: 197, Loss: 2449.63402, Residuals: -0.97305, Convergence: 0.005990\n",
      "Epoch: 198, Loss: 2437.47019, Residuals: -0.96970, Convergence: 0.004990\n",
      "Epoch: 199, Loss: 2426.89586, Residuals: -0.96640, Convergence: 0.004357\n",
      "Epoch: 200, Loss: 2417.66993, Residuals: -0.96324, Convergence: 0.003816\n",
      "Epoch: 201, Loss: 2409.59041, Residuals: -0.96026, Convergence: 0.003353\n",
      "Epoch: 202, Loss: 2402.48979, Residuals: -0.95749, Convergence: 0.002956\n",
      "Epoch: 203, Loss: 2396.22729, Residuals: -0.95494, Convergence: 0.002613\n",
      "Epoch: 204, Loss: 2390.68434, Residuals: -0.95261, Convergence: 0.002319\n",
      "Epoch: 205, Loss: 2385.75973, Residuals: -0.95047, Convergence: 0.002064\n",
      "Epoch: 206, Loss: 2381.36699, Residuals: -0.94853, Convergence: 0.001845\n",
      "Epoch: 207, Loss: 2377.43176, Residuals: -0.94675, Convergence: 0.001655\n",
      "Epoch: 208, Loss: 2373.89047, Residuals: -0.94513, Convergence: 0.001492\n",
      "Epoch: 209, Loss: 2370.69009, Residuals: -0.94366, Convergence: 0.001350\n",
      "Epoch: 210, Loss: 2367.78337, Residuals: -0.94232, Convergence: 0.001228\n",
      "Epoch: 211, Loss: 2365.13301, Residuals: -0.94110, Convergence: 0.001121\n",
      "Epoch: 212, Loss: 2362.70492, Residuals: -0.93999, Convergence: 0.001028\n",
      "Epoch: 213, Loss: 2360.47215, Residuals: -0.93899, Convergence: 0.000946\n",
      "Evidence 14711.525\n",
      "\n",
      "Epoch: 213, Evidence: 14711.52539, Convergence: 0.028766\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.36e-01\n",
      "Epoch: 213, Loss: 2468.74841, Residuals: -0.93899, Convergence:   inf\n",
      "Epoch: 214, Loss: 2461.69193, Residuals: -0.93574, Convergence: 0.002867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 215, Loss: 2455.79429, Residuals: -0.93282, Convergence: 0.002402\n",
      "Epoch: 216, Loss: 2450.77819, Residuals: -0.93027, Convergence: 0.002047\n",
      "Epoch: 217, Loss: 2446.46767, Residuals: -0.92808, Convergence: 0.001762\n",
      "Epoch: 218, Loss: 2442.72463, Residuals: -0.92621, Convergence: 0.001532\n",
      "Epoch: 219, Loss: 2439.43965, Residuals: -0.92463, Convergence: 0.001347\n",
      "Epoch: 220, Loss: 2436.52854, Residuals: -0.92330, Convergence: 0.001195\n",
      "Epoch: 221, Loss: 2433.92323, Residuals: -0.92219, Convergence: 0.001070\n",
      "Epoch: 222, Loss: 2431.57157, Residuals: -0.92127, Convergence: 0.000967\n",
      "Evidence 14807.212\n",
      "\n",
      "Epoch: 222, Evidence: 14807.21191, Convergence: 0.006462\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.64e-01\n",
      "Epoch: 222, Loss: 2470.24032, Residuals: -0.92127, Convergence:   inf\n",
      "Epoch: 223, Loss: 2466.14171, Residuals: -0.91883, Convergence: 0.001662\n",
      "Epoch: 224, Loss: 2462.71069, Residuals: -0.91681, Convergence: 0.001393\n",
      "Epoch: 225, Loss: 2459.77662, Residuals: -0.91515, Convergence: 0.001193\n",
      "Epoch: 226, Loss: 2457.22624, Residuals: -0.91382, Convergence: 0.001038\n",
      "Epoch: 227, Loss: 2454.97491, Residuals: -0.91277, Convergence: 0.000917\n",
      "Evidence 14840.453\n",
      "\n",
      "Epoch: 227, Evidence: 14840.45312, Convergence: 0.002240\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.13e-01\n",
      "Epoch: 227, Loss: 2471.12631, Residuals: -0.91277, Convergence:   inf\n",
      "Epoch: 228, Loss: 2468.24598, Residuals: -0.91091, Convergence: 0.001167\n",
      "Epoch: 229, Loss: 2465.82423, Residuals: -0.90941, Convergence: 0.000982\n",
      "Evidence 14853.529\n",
      "\n",
      "Epoch: 229, Evidence: 14853.52930, Convergence: 0.000880\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.78e-01\n",
      "Epoch: 229, Loss: 2471.79342, Residuals: -0.90941, Convergence:   inf\n",
      "Epoch: 230, Loss: 2467.28745, Residuals: -0.90707, Convergence: 0.001826\n",
      "Epoch: 231, Loss: 2463.84979, Residuals: -0.90519, Convergence: 0.001395\n",
      "Epoch: 232, Loss: 2461.07257, Residuals: -0.90418, Convergence: 0.001128\n",
      "Epoch: 233, Loss: 2458.72087, Residuals: -0.90384, Convergence: 0.000956\n",
      "Evidence 14871.439\n",
      "\n",
      "Epoch: 233, Evidence: 14871.43945, Convergence: 0.002084\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.47e-01\n",
      "Epoch: 233, Loss: 2472.01857, Residuals: -0.90384, Convergence:   inf\n",
      "Epoch: 234, Loss: 2469.09408, Residuals: -0.90159, Convergence: 0.001184\n",
      "Epoch: 235, Loss: 2466.78574, Residuals: -0.90060, Convergence: 0.000936\n",
      "Evidence 14882.340\n",
      "\n",
      "Epoch: 235, Evidence: 14882.33984, Convergence: 0.000732\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.24e-01\n",
      "Epoch: 235, Loss: 2472.21251, Residuals: -0.90060, Convergence:   inf\n",
      "Epoch: 236, Loss: 2467.99702, Residuals: -0.89750, Convergence: 0.001708\n",
      "Epoch: 237, Loss: 2464.98596, Residuals: -0.89952, Convergence: 0.001222\n",
      "Epoch: 238, Loss: 2462.40689, Residuals: -0.90105, Convergence: 0.001047\n",
      "Epoch: 239, Loss: 2460.25021, Residuals: -0.90429, Convergence: 0.000877\n",
      "Evidence 14897.625\n",
      "\n",
      "Epoch: 239, Evidence: 14897.62500, Convergence: 0.001758\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.10e-01\n",
      "Epoch: 239, Loss: 2471.54081, Residuals: -0.90429, Convergence:   inf\n",
      "Epoch: 240, Loss: 2469.75585, Residuals: -0.90233, Convergence: 0.000723\n",
      "Evidence 14904.264\n",
      "\n",
      "Epoch: 240, Evidence: 14904.26367, Convergence: 0.000445\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.99e-02\n",
      "Epoch: 240, Loss: 2472.40557, Residuals: -0.90233, Convergence:   inf\n",
      "Epoch: 241, Loss: 2513.71210, Residuals: -0.93337, Convergence: -0.016432\n",
      "Epoch: 241, Loss: 2470.45944, Residuals: -0.89927, Convergence: 0.000788\n",
      "Evidence 14908.221\n",
      "\n",
      "Epoch: 241, Evidence: 14908.22070, Convergence: 0.000711\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.20e-02\n",
      "Epoch: 241, Loss: 2471.90880, Residuals: -0.89927, Convergence:   inf\n",
      "Epoch: 242, Loss: 2476.30331, Residuals: -0.89858, Convergence: -0.001775\n",
      "Epoch: 242, Loss: 2471.91894, Residuals: -0.89717, Convergence: -0.000004\n",
      "Evidence 14909.774\n",
      "\n",
      "Epoch: 242, Evidence: 14909.77441, Convergence: 0.000815\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 384.16175, Residuals: -4.53057, Convergence:   inf\n",
      "Epoch: 1, Loss: 358.51060, Residuals: -4.41076, Convergence: 0.071549\n",
      "Epoch: 2, Loss: 337.52496, Residuals: -4.24733, Convergence: 0.062175\n",
      "Epoch: 3, Loss: 321.47585, Residuals: -4.08306, Convergence: 0.049923\n",
      "Epoch: 4, Loss: 309.23492, Residuals: -3.93810, Convergence: 0.039585\n",
      "Epoch: 5, Loss: 299.52594, Residuals: -3.80967, Convergence: 0.032414\n",
      "Epoch: 6, Loss: 291.64250, Residuals: -3.69758, Convergence: 0.027031\n",
      "Epoch: 7, Loss: 285.09398, Residuals: -3.60148, Convergence: 0.022970\n",
      "Epoch: 8, Loss: 279.51678, Residuals: -3.51939, Convergence: 0.019953\n",
      "Epoch: 9, Loss: 274.65438, Residuals: -3.44904, Convergence: 0.017704\n",
      "Epoch: 10, Loss: 270.32497, Residuals: -3.38835, Convergence: 0.016016\n",
      "Epoch: 11, Loss: 266.39756, Residuals: -3.33552, Convergence: 0.014743\n",
      "Epoch: 12, Loss: 262.77708, Residuals: -3.28903, Convergence: 0.013778\n",
      "Epoch: 13, Loss: 259.39474, Residuals: -3.24752, Convergence: 0.013039\n",
      "Epoch: 14, Loss: 256.20113, Residuals: -3.20981, Convergence: 0.012465\n",
      "Epoch: 15, Loss: 253.16225, Residuals: -3.17490, Convergence: 0.012004\n",
      "Epoch: 16, Loss: 250.25764, Residuals: -3.14206, Convergence: 0.011606\n",
      "Epoch: 17, Loss: 247.47398, Residuals: -3.11082, Convergence: 0.011248\n",
      "Epoch: 18, Loss: 244.79284, Residuals: -3.08080, Convergence: 0.010953\n",
      "Epoch: 19, Loss: 242.18522, Residuals: -3.05151, Convergence: 0.010767\n",
      "Epoch: 20, Loss: 239.61739, Residuals: -3.02240, Convergence: 0.010716\n",
      "Epoch: 21, Loss: 237.06018, Residuals: -2.99302, Convergence: 0.010787\n",
      "Epoch: 22, Loss: 234.49359, Residuals: -2.96306, Convergence: 0.010945\n",
      "Epoch: 23, Loss: 231.89798, Residuals: -2.93232, Convergence: 0.011193\n",
      "Epoch: 24, Loss: 229.23710, Residuals: -2.90041, Convergence: 0.011608\n",
      "Epoch: 25, Loss: 226.45595, Residuals: -2.86667, Convergence: 0.012281\n",
      "Epoch: 26, Loss: 223.52317, Residuals: -2.83064, Convergence: 0.013121\n",
      "Epoch: 27, Loss: 220.51529, Residuals: -2.79305, Convergence: 0.013640\n",
      "Epoch: 28, Loss: 217.55943, Residuals: -2.75529, Convergence: 0.013586\n",
      "Epoch: 29, Loss: 214.70114, Residuals: -2.71798, Convergence: 0.013313\n",
      "Epoch: 30, Loss: 211.93366, Residuals: -2.68115, Convergence: 0.013058\n",
      "Epoch: 31, Loss: 209.24271, Residuals: -2.64469, Convergence: 0.012860\n",
      "Epoch: 32, Loss: 206.61785, Residuals: -2.60851, Convergence: 0.012704\n",
      "Epoch: 33, Loss: 204.05325, Residuals: -2.57257, Convergence: 0.012568\n",
      "Epoch: 34, Loss: 201.54643, Residuals: -2.53685, Convergence: 0.012438\n",
      "Epoch: 35, Loss: 199.09706, Residuals: -2.50134, Convergence: 0.012302\n",
      "Epoch: 36, Loss: 196.70604, Residuals: -2.46606, Convergence: 0.012155\n",
      "Epoch: 37, Loss: 194.37489, Residuals: -2.43102, Convergence: 0.011993\n",
      "Epoch: 38, Loss: 192.10529, Residuals: -2.39624, Convergence: 0.011814\n",
      "Epoch: 39, Loss: 189.89885, Residuals: -2.36172, Convergence: 0.011619\n",
      "Epoch: 40, Loss: 187.75696, Residuals: -2.32749, Convergence: 0.011408\n",
      "Epoch: 41, Loss: 185.68070, Residuals: -2.29356, Convergence: 0.011182\n",
      "Epoch: 42, Loss: 183.67084, Residuals: -2.25995, Convergence: 0.010943\n",
      "Epoch: 43, Loss: 181.72784, Residuals: -2.22669, Convergence: 0.010692\n",
      "Epoch: 44, Loss: 179.85181, Residuals: -2.19381, Convergence: 0.010431\n",
      "Epoch: 45, Loss: 178.04259, Residuals: -2.16131, Convergence: 0.010162\n",
      "Epoch: 46, Loss: 176.29973, Residuals: -2.12923, Convergence: 0.009886\n",
      "Epoch: 47, Loss: 174.62250, Residuals: -2.09759, Convergence: 0.009605\n",
      "Epoch: 48, Loss: 173.00994, Residuals: -2.06641, Convergence: 0.009321\n",
      "Epoch: 49, Loss: 171.46095, Residuals: -2.03572, Convergence: 0.009034\n",
      "Epoch: 50, Loss: 169.97432, Residuals: -2.00553, Convergence: 0.008746\n",
      "Epoch: 51, Loss: 168.54891, Residuals: -1.97587, Convergence: 0.008457\n",
      "Epoch: 52, Loss: 167.18370, Residuals: -1.94676, Convergence: 0.008166\n",
      "Epoch: 53, Loss: 165.87783, Residuals: -1.91824, Convergence: 0.007872\n",
      "Epoch: 54, Loss: 164.63055, Residuals: -1.89032, Convergence: 0.007576\n",
      "Epoch: 55, Loss: 163.44111, Residuals: -1.86304, Convergence: 0.007278\n",
      "Epoch: 56, Loss: 162.30851, Residuals: -1.83643, Convergence: 0.006978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57, Loss: 161.23138, Residuals: -1.81051, Convergence: 0.006681\n",
      "Epoch: 58, Loss: 160.20800, Residuals: -1.78530, Convergence: 0.006388\n",
      "Epoch: 59, Loss: 159.23623, Residuals: -1.76080, Convergence: 0.006103\n",
      "Epoch: 60, Loss: 158.31371, Residuals: -1.73701, Convergence: 0.005827\n",
      "Epoch: 61, Loss: 157.43790, Residuals: -1.71394, Convergence: 0.005563\n",
      "Epoch: 62, Loss: 156.60617, Residuals: -1.69158, Convergence: 0.005311\n",
      "Epoch: 63, Loss: 155.81587, Residuals: -1.66992, Convergence: 0.005072\n",
      "Epoch: 64, Loss: 155.06435, Residuals: -1.64894, Convergence: 0.004846\n",
      "Epoch: 65, Loss: 154.34901, Residuals: -1.62864, Convergence: 0.004635\n",
      "Epoch: 66, Loss: 153.66730, Residuals: -1.60898, Convergence: 0.004436\n",
      "Epoch: 67, Loss: 153.01683, Residuals: -1.58996, Convergence: 0.004251\n",
      "Epoch: 68, Loss: 152.39544, Residuals: -1.57154, Convergence: 0.004078\n",
      "Epoch: 69, Loss: 151.80115, Residuals: -1.55372, Convergence: 0.003915\n",
      "Epoch: 70, Loss: 151.23227, Residuals: -1.53646, Convergence: 0.003762\n",
      "Epoch: 71, Loss: 150.68734, Residuals: -1.51975, Convergence: 0.003616\n",
      "Epoch: 72, Loss: 150.16513, Residuals: -1.50358, Convergence: 0.003478\n",
      "Epoch: 73, Loss: 149.66462, Residuals: -1.48794, Convergence: 0.003344\n",
      "Epoch: 74, Loss: 149.18487, Residuals: -1.47280, Convergence: 0.003216\n",
      "Epoch: 75, Loss: 148.72512, Residuals: -1.45816, Convergence: 0.003091\n",
      "Epoch: 76, Loss: 148.28466, Residuals: -1.44402, Convergence: 0.002970\n",
      "Epoch: 77, Loss: 147.86284, Residuals: -1.43036, Convergence: 0.002853\n",
      "Epoch: 78, Loss: 147.45905, Residuals: -1.41717, Convergence: 0.002738\n",
      "Epoch: 79, Loss: 147.07267, Residuals: -1.40445, Convergence: 0.002627\n",
      "Epoch: 80, Loss: 146.70315, Residuals: -1.39218, Convergence: 0.002519\n",
      "Epoch: 81, Loss: 146.34989, Residuals: -1.38035, Convergence: 0.002414\n",
      "Epoch: 82, Loss: 146.01234, Residuals: -1.36896, Convergence: 0.002312\n",
      "Epoch: 83, Loss: 145.68992, Residuals: -1.35798, Convergence: 0.002213\n",
      "Epoch: 84, Loss: 145.38207, Residuals: -1.34742, Convergence: 0.002118\n",
      "Epoch: 85, Loss: 145.08826, Residuals: -1.33726, Convergence: 0.002025\n",
      "Epoch: 86, Loss: 144.80793, Residuals: -1.32749, Convergence: 0.001936\n",
      "Epoch: 87, Loss: 144.54056, Residuals: -1.31809, Convergence: 0.001850\n",
      "Epoch: 88, Loss: 144.28566, Residuals: -1.30905, Convergence: 0.001767\n",
      "Epoch: 89, Loss: 144.04273, Residuals: -1.30037, Convergence: 0.001686\n",
      "Epoch: 90, Loss: 143.81132, Residuals: -1.29203, Convergence: 0.001609\n",
      "Epoch: 91, Loss: 143.59099, Residuals: -1.28401, Convergence: 0.001534\n",
      "Epoch: 92, Loss: 143.38132, Residuals: -1.27632, Convergence: 0.001462\n",
      "Epoch: 93, Loss: 143.18193, Residuals: -1.26893, Convergence: 0.001393\n",
      "Epoch: 94, Loss: 142.99247, Residuals: -1.26184, Convergence: 0.001325\n",
      "Epoch: 95, Loss: 142.81259, Residuals: -1.25503, Convergence: 0.001260\n",
      "Epoch: 96, Loss: 142.64198, Residuals: -1.24851, Convergence: 0.001196\n",
      "Epoch: 97, Loss: 142.48035, Residuals: -1.24226, Convergence: 0.001134\n",
      "Epoch: 98, Loss: 142.32740, Residuals: -1.23628, Convergence: 0.001075\n",
      "Epoch: 99, Loss: 142.18286, Residuals: -1.23055, Convergence: 0.001017\n",
      "Epoch: 100, Loss: 142.04642, Residuals: -1.22509, Convergence: 0.000961\n",
      "Evidence -183.296\n",
      "\n",
      "Epoch: 100, Evidence: -183.29555, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.24e-01\n",
      "Epoch: 100, Loss: 1376.48790, Residuals: -1.22509, Convergence:   inf\n",
      "Epoch: 101, Loss: 1314.83049, Residuals: -1.25541, Convergence: 0.046894\n",
      "Epoch: 102, Loss: 1268.08390, Residuals: -1.27958, Convergence: 0.036864\n",
      "Epoch: 103, Loss: 1232.81653, Residuals: -1.29726, Convergence: 0.028607\n",
      "Epoch: 104, Loss: 1205.38341, Residuals: -1.30987, Convergence: 0.022759\n",
      "Epoch: 105, Loss: 1183.19734, Residuals: -1.31922, Convergence: 0.018751\n",
      "Epoch: 106, Loss: 1164.77645, Residuals: -1.32634, Convergence: 0.015815\n",
      "Epoch: 107, Loss: 1149.22866, Residuals: -1.33170, Convergence: 0.013529\n",
      "Epoch: 108, Loss: 1135.94942, Residuals: -1.33558, Convergence: 0.011690\n",
      "Epoch: 109, Loss: 1124.49331, Residuals: -1.33815, Convergence: 0.010188\n",
      "Epoch: 110, Loss: 1114.51509, Residuals: -1.33955, Convergence: 0.008953\n",
      "Epoch: 111, Loss: 1105.73946, Residuals: -1.33991, Convergence: 0.007936\n",
      "Epoch: 112, Loss: 1097.94221, Residuals: -1.33934, Convergence: 0.007102\n",
      "Epoch: 113, Loss: 1090.93765, Residuals: -1.33794, Convergence: 0.006421\n",
      "Epoch: 114, Loss: 1084.56924, Residuals: -1.33580, Convergence: 0.005872\n",
      "Epoch: 115, Loss: 1078.70233, Residuals: -1.33298, Convergence: 0.005439\n",
      "Epoch: 116, Loss: 1073.22026, Residuals: -1.32953, Convergence: 0.005108\n",
      "Epoch: 117, Loss: 1068.01875, Residuals: -1.32550, Convergence: 0.004870\n",
      "Epoch: 118, Loss: 1063.00589, Residuals: -1.32091, Convergence: 0.004716\n",
      "Epoch: 119, Loss: 1058.10120, Residuals: -1.31578, Convergence: 0.004635\n",
      "Epoch: 120, Loss: 1053.23649, Residuals: -1.31010, Convergence: 0.004619\n",
      "Epoch: 121, Loss: 1048.35769, Residuals: -1.30390, Convergence: 0.004654\n",
      "Epoch: 122, Loss: 1043.43113, Residuals: -1.29719, Convergence: 0.004722\n",
      "Epoch: 123, Loss: 1038.44870, Residuals: -1.29002, Convergence: 0.004798\n",
      "Epoch: 124, Loss: 1033.43598, Residuals: -1.28244, Convergence: 0.004851\n",
      "Epoch: 125, Loss: 1028.45222, Residuals: -1.27456, Convergence: 0.004846\n",
      "Epoch: 126, Loss: 1023.57713, Residuals: -1.26646, Convergence: 0.004763\n",
      "Epoch: 127, Loss: 1018.88762, Residuals: -1.25822, Convergence: 0.004603\n",
      "Epoch: 128, Loss: 1014.43688, Residuals: -1.24993, Convergence: 0.004387\n",
      "Epoch: 129, Loss: 1010.24937, Residuals: -1.24165, Convergence: 0.004145\n",
      "Epoch: 130, Loss: 1006.32533, Residuals: -1.23343, Convergence: 0.003899\n",
      "Epoch: 131, Loss: 1002.65086, Residuals: -1.22532, Convergence: 0.003665\n",
      "Epoch: 132, Loss: 999.20559, Residuals: -1.21736, Convergence: 0.003448\n",
      "Epoch: 133, Loss: 995.96912, Residuals: -1.20957, Convergence: 0.003250\n",
      "Epoch: 134, Loss: 992.92117, Residuals: -1.20199, Convergence: 0.003070\n",
      "Epoch: 135, Loss: 990.04485, Residuals: -1.19461, Convergence: 0.002905\n",
      "Epoch: 136, Loss: 987.32537, Residuals: -1.18748, Convergence: 0.002754\n",
      "Epoch: 137, Loss: 984.75021, Residuals: -1.18059, Convergence: 0.002615\n",
      "Epoch: 138, Loss: 982.30848, Residuals: -1.17395, Convergence: 0.002486\n",
      "Epoch: 139, Loss: 979.99098, Residuals: -1.16757, Convergence: 0.002365\n",
      "Epoch: 140, Loss: 977.78940, Residuals: -1.16145, Convergence: 0.002252\n",
      "Epoch: 141, Loss: 975.69605, Residuals: -1.15560, Convergence: 0.002145\n",
      "Epoch: 142, Loss: 973.70432, Residuals: -1.15001, Convergence: 0.002046\n",
      "Epoch: 143, Loss: 971.80708, Residuals: -1.14467, Convergence: 0.001952\n",
      "Epoch: 144, Loss: 969.99820, Residuals: -1.13958, Convergence: 0.001865\n",
      "Epoch: 145, Loss: 968.27204, Residuals: -1.13474, Convergence: 0.001783\n",
      "Epoch: 146, Loss: 966.62240, Residuals: -1.13013, Convergence: 0.001707\n",
      "Epoch: 147, Loss: 965.04425, Residuals: -1.12575, Convergence: 0.001635\n",
      "Epoch: 148, Loss: 963.53223, Residuals: -1.12158, Convergence: 0.001569\n",
      "Epoch: 149, Loss: 962.08214, Residuals: -1.11762, Convergence: 0.001507\n",
      "Epoch: 150, Loss: 960.68892, Residuals: -1.11386, Convergence: 0.001450\n",
      "Epoch: 151, Loss: 959.34908, Residuals: -1.11028, Convergence: 0.001397\n",
      "Epoch: 152, Loss: 958.05888, Residuals: -1.10687, Convergence: 0.001347\n",
      "Epoch: 153, Loss: 956.81515, Residuals: -1.10362, Convergence: 0.001300\n",
      "Epoch: 154, Loss: 955.61533, Residuals: -1.10053, Convergence: 0.001256\n",
      "Epoch: 155, Loss: 954.45656, Residuals: -1.09758, Convergence: 0.001214\n",
      "Epoch: 156, Loss: 953.33702, Residuals: -1.09477, Convergence: 0.001174\n",
      "Epoch: 157, Loss: 952.25422, Residuals: -1.09208, Convergence: 0.001137\n",
      "Epoch: 158, Loss: 951.20691, Residuals: -1.08951, Convergence: 0.001101\n",
      "Epoch: 159, Loss: 950.19303, Residuals: -1.08705, Convergence: 0.001067\n",
      "Epoch: 160, Loss: 949.21112, Residuals: -1.08470, Convergence: 0.001034\n",
      "Epoch: 161, Loss: 948.25931, Residuals: -1.08244, Convergence: 0.001004\n",
      "Epoch: 162, Loss: 947.33601, Residuals: -1.08028, Convergence: 0.000975\n",
      "Evidence 11143.727\n",
      "\n",
      "Epoch: 162, Evidence: 11143.72656, Convergence: 1.016448\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.77e-01\n",
      "Epoch: 162, Loss: 2351.28494, Residuals: -1.08028, Convergence:   inf\n",
      "Epoch: 163, Loss: 2311.56957, Residuals: -1.08960, Convergence: 0.017181\n",
      "Epoch: 164, Loss: 2283.77773, Residuals: -1.08918, Convergence: 0.012169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 165, Loss: 2260.57717, Residuals: -1.08755, Convergence: 0.010263\n",
      "Epoch: 166, Loss: 2240.92416, Residuals: -1.08549, Convergence: 0.008770\n",
      "Epoch: 167, Loss: 2224.11838, Residuals: -1.08314, Convergence: 0.007556\n",
      "Epoch: 168, Loss: 2209.62185, Residuals: -1.08055, Convergence: 0.006561\n",
      "Epoch: 169, Loss: 2197.00356, Residuals: -1.07775, Convergence: 0.005743\n",
      "Epoch: 170, Loss: 2185.91591, Residuals: -1.07479, Convergence: 0.005072\n",
      "Epoch: 171, Loss: 2176.07480, Residuals: -1.07168, Convergence: 0.004522\n",
      "Epoch: 172, Loss: 2167.25118, Residuals: -1.06845, Convergence: 0.004071\n",
      "Epoch: 173, Loss: 2159.25668, Residuals: -1.06509, Convergence: 0.003702\n",
      "Epoch: 174, Loss: 2151.94152, Residuals: -1.06164, Convergence: 0.003399\n",
      "Epoch: 175, Loss: 2145.19043, Residuals: -1.05808, Convergence: 0.003147\n",
      "Epoch: 176, Loss: 2138.92544, Residuals: -1.05445, Convergence: 0.002929\n",
      "Epoch: 177, Loss: 2133.09558, Residuals: -1.05076, Convergence: 0.002733\n",
      "Epoch: 178, Loss: 2127.67102, Residuals: -1.04707, Convergence: 0.002550\n",
      "Epoch: 179, Loss: 2122.63122, Residuals: -1.04340, Convergence: 0.002374\n",
      "Epoch: 180, Loss: 2117.95327, Residuals: -1.03980, Convergence: 0.002209\n",
      "Epoch: 181, Loss: 2113.61391, Residuals: -1.03629, Convergence: 0.002053\n",
      "Epoch: 182, Loss: 2109.58702, Residuals: -1.03290, Convergence: 0.001909\n",
      "Epoch: 183, Loss: 2105.84514, Residuals: -1.02965, Convergence: 0.001777\n",
      "Epoch: 184, Loss: 2102.36265, Residuals: -1.02653, Convergence: 0.001656\n",
      "Epoch: 185, Loss: 2099.11467, Residuals: -1.02357, Convergence: 0.001547\n",
      "Epoch: 186, Loss: 2096.07872, Residuals: -1.02074, Convergence: 0.001448\n",
      "Epoch: 187, Loss: 2093.23579, Residuals: -1.01807, Convergence: 0.001358\n",
      "Epoch: 188, Loss: 2090.56806, Residuals: -1.01553, Convergence: 0.001276\n",
      "Epoch: 189, Loss: 2088.06089, Residuals: -1.01314, Convergence: 0.001201\n",
      "Epoch: 190, Loss: 2085.70086, Residuals: -1.01087, Convergence: 0.001132\n",
      "Epoch: 191, Loss: 2083.47792, Residuals: -1.00874, Convergence: 0.001067\n",
      "Epoch: 192, Loss: 2081.38096, Residuals: -1.00672, Convergence: 0.001007\n",
      "Epoch: 193, Loss: 2079.40195, Residuals: -1.00482, Convergence: 0.000952\n",
      "Evidence 14280.184\n",
      "\n",
      "Epoch: 193, Evidence: 14280.18359, Convergence: 0.219637\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.39e-01\n",
      "Epoch: 193, Loss: 2476.74060, Residuals: -1.00482, Convergence:   inf\n",
      "Epoch: 194, Loss: 2462.32726, Residuals: -1.00153, Convergence: 0.005854\n",
      "Epoch: 195, Loss: 2450.69742, Residuals: -0.99762, Convergence: 0.004746\n",
      "Epoch: 196, Loss: 2440.74890, Residuals: -0.99379, Convergence: 0.004076\n",
      "Epoch: 197, Loss: 2432.14505, Residuals: -0.99016, Convergence: 0.003538\n",
      "Epoch: 198, Loss: 2424.64141, Residuals: -0.98676, Convergence: 0.003095\n",
      "Epoch: 199, Loss: 2418.05209, Residuals: -0.98360, Convergence: 0.002725\n",
      "Epoch: 200, Loss: 2412.23057, Residuals: -0.98067, Convergence: 0.002413\n",
      "Epoch: 201, Loss: 2407.05976, Residuals: -0.97794, Convergence: 0.002148\n",
      "Epoch: 202, Loss: 2402.44143, Residuals: -0.97541, Convergence: 0.001922\n",
      "Epoch: 203, Loss: 2398.29490, Residuals: -0.97306, Convergence: 0.001729\n",
      "Epoch: 204, Loss: 2394.55297, Residuals: -0.97087, Convergence: 0.001563\n",
      "Epoch: 205, Loss: 2391.15752, Residuals: -0.96884, Convergence: 0.001420\n",
      "Epoch: 206, Loss: 2388.06171, Residuals: -0.96695, Convergence: 0.001296\n",
      "Epoch: 207, Loss: 2385.22530, Residuals: -0.96519, Convergence: 0.001189\n",
      "Epoch: 208, Loss: 2382.61454, Residuals: -0.96355, Convergence: 0.001096\n",
      "Epoch: 209, Loss: 2380.20046, Residuals: -0.96203, Convergence: 0.001014\n",
      "Epoch: 210, Loss: 2377.96114, Residuals: -0.96062, Convergence: 0.000942\n",
      "Evidence 14694.141\n",
      "\n",
      "Epoch: 210, Evidence: 14694.14062, Convergence: 0.028172\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.36e-01\n",
      "Epoch: 210, Loss: 2482.06325, Residuals: -0.96062, Convergence:   inf\n",
      "Epoch: 211, Loss: 2475.35285, Residuals: -0.95703, Convergence: 0.002711\n",
      "Epoch: 212, Loss: 2469.86703, Residuals: -0.95381, Convergence: 0.002221\n",
      "Epoch: 213, Loss: 2465.21361, Residuals: -0.95101, Convergence: 0.001888\n",
      "Epoch: 214, Loss: 2461.19106, Residuals: -0.94857, Convergence: 0.001634\n",
      "Epoch: 215, Loss: 2457.66543, Residuals: -0.94643, Convergence: 0.001435\n",
      "Epoch: 216, Loss: 2454.53961, Residuals: -0.94455, Convergence: 0.001273\n",
      "Epoch: 217, Loss: 2451.73810, Residuals: -0.94289, Convergence: 0.001143\n",
      "Epoch: 218, Loss: 2449.20535, Residuals: -0.94143, Convergence: 0.001034\n",
      "Epoch: 219, Loss: 2446.89421, Residuals: -0.94015, Convergence: 0.000945\n",
      "Evidence 14782.090\n",
      "\n",
      "Epoch: 219, Evidence: 14782.08984, Convergence: 0.005950\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.64e-01\n",
      "Epoch: 219, Loss: 2483.73141, Residuals: -0.94015, Convergence:   inf\n",
      "Epoch: 220, Loss: 2479.78008, Residuals: -0.93751, Convergence: 0.001593\n",
      "Epoch: 221, Loss: 2476.49678, Residuals: -0.93533, Convergence: 0.001326\n",
      "Epoch: 222, Loss: 2473.66370, Residuals: -0.93350, Convergence: 0.001145\n",
      "Epoch: 223, Loss: 2471.17030, Residuals: -0.93194, Convergence: 0.001009\n",
      "Epoch: 224, Loss: 2468.94275, Residuals: -0.93060, Convergence: 0.000902\n",
      "Evidence 14813.438\n",
      "\n",
      "Epoch: 224, Evidence: 14813.43750, Convergence: 0.002116\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.12e-01\n",
      "Epoch: 224, Loss: 2484.72795, Residuals: -0.93060, Convergence:   inf\n",
      "Epoch: 225, Loss: 2481.89570, Residuals: -0.92857, Convergence: 0.001141\n",
      "Epoch: 226, Loss: 2479.49521, Residuals: -0.92690, Convergence: 0.000968\n",
      "Evidence 14826.040\n",
      "\n",
      "Epoch: 226, Evidence: 14826.04004, Convergence: 0.000850\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.76e-01\n",
      "Epoch: 226, Loss: 2485.50030, Residuals: -0.92690, Convergence:   inf\n",
      "Epoch: 227, Loss: 2480.97713, Residuals: -0.92395, Convergence: 0.001823\n",
      "Epoch: 228, Loss: 2477.42665, Residuals: -0.92165, Convergence: 0.001433\n",
      "Epoch: 229, Loss: 2474.51125, Residuals: -0.92005, Convergence: 0.001178\n",
      "Epoch: 230, Loss: 2472.02559, Residuals: -0.91909, Convergence: 0.001006\n",
      "Epoch: 231, Loss: 2469.84143, Residuals: -0.91856, Convergence: 0.000884\n",
      "Evidence 14846.352\n",
      "\n",
      "Epoch: 231, Evidence: 14846.35156, Convergence: 0.002217\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.46e-01\n",
      "Epoch: 231, Loss: 2485.64710, Residuals: -0.91856, Convergence:   inf\n",
      "Epoch: 232, Loss: 2482.68460, Residuals: -0.91605, Convergence: 0.001193\n",
      "Epoch: 233, Loss: 2480.29776, Residuals: -0.91471, Convergence: 0.000962\n",
      "Evidence 14857.849\n",
      "\n",
      "Epoch: 233, Evidence: 14857.84863, Convergence: 0.000774\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.24e-01\n",
      "Epoch: 233, Loss: 2485.92520, Residuals: -0.91471, Convergence:   inf\n",
      "Epoch: 234, Loss: 2481.49933, Residuals: -0.91128, Convergence: 0.001784\n",
      "Epoch: 235, Loss: 2478.24674, Residuals: -0.91227, Convergence: 0.001312\n",
      "Epoch: 236, Loss: 2475.58247, Residuals: -0.91316, Convergence: 0.001076\n",
      "Epoch: 237, Loss: 2473.30390, Residuals: -0.91578, Convergence: 0.000921\n",
      "Evidence 14873.614\n",
      "\n",
      "Epoch: 237, Evidence: 14873.61426, Convergence: 0.001833\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.10e-01\n",
      "Epoch: 237, Loss: 2485.37535, Residuals: -0.91578, Convergence:   inf\n",
      "Epoch: 238, Loss: 2483.72240, Residuals: -0.91463, Convergence: 0.000666\n",
      "Evidence 14880.127\n",
      "\n",
      "Epoch: 238, Evidence: 14880.12695, Convergence: 0.000438\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.96e-02\n",
      "Epoch: 238, Loss: 2486.39243, Residuals: -0.91463, Convergence:   inf\n",
      "Epoch: 239, Loss: 2535.98795, Residuals: -0.95927, Convergence: -0.019557\n",
      "Epoch: 239, Loss: 2483.87137, Residuals: -0.91215, Convergence: 0.001015\n",
      "Epoch: 240, Loss: 2484.23787, Residuals: -0.91445, Convergence: -0.000148\n",
      "Evidence 14884.596\n",
      "\n",
      "Epoch: 240, Evidence: 14884.59570, Convergence: 0.000738\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.43e-02\n",
      "Epoch: 240, Loss: 2486.33191, Residuals: -0.91445, Convergence:   inf\n",
      "Epoch: 241, Loss: 2557.00080, Residuals: -0.97050, Convergence: -0.027637\n",
      "Epoch: 241, Loss: 2483.49290, Residuals: -0.91176, Convergence: 0.001143\n",
      "Epoch: 242, Loss: 2483.00497, Residuals: -0.91465, Convergence: 0.000197\n",
      "Evidence 14890.323\n",
      "\n",
      "Epoch: 242, Evidence: 14890.32324, Convergence: 0.001122\n",
      "Updating hyper-parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 182, Updated regularization: 6.16e-02\n",
      "Epoch: 242, Loss: 2486.25076, Residuals: -0.91465, Convergence:   inf\n",
      "Epoch: 243, Loss: 2483.97511, Residuals: -0.91131, Convergence: 0.000916\n",
      "Evidence 14894.848\n",
      "\n",
      "Epoch: 243, Evidence: 14894.84766, Convergence: 0.000304\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.80e-02\n",
      "Epoch: 243, Loss: 2485.96494, Residuals: -0.91131, Convergence:   inf\n",
      "Epoch: 244, Loss: 2490.27099, Residuals: -0.91701, Convergence: -0.001729\n",
      "Epoch: 244, Loss: 2486.31346, Residuals: -0.91031, Convergence: -0.000140\n",
      "Evidence 14895.620\n",
      "\n",
      "Epoch: 244, Evidence: 14895.62012, Convergence: 0.000356\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.07e-02\n",
      "Epoch: 244, Loss: 2486.41507, Residuals: -0.91031, Convergence:   inf\n",
      "Epoch: 245, Loss: 2553.96834, Residuals: -0.96277, Convergence: -0.026450\n",
      "Epoch: 245, Loss: 2484.33225, Residuals: -0.90903, Convergence: 0.000838\n",
      "Evidence 14899.035\n",
      "\n",
      "Epoch: 245, Evidence: 14899.03516, Convergence: 0.000585\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 381.52605, Residuals: -4.50437, Convergence:   inf\n",
      "Epoch: 1, Loss: 355.85613, Residuals: -4.38556, Convergence: 0.072136\n",
      "Epoch: 2, Loss: 334.88750, Residuals: -4.22354, Convergence: 0.062614\n",
      "Epoch: 3, Loss: 318.88940, Residuals: -4.06098, Convergence: 0.050168\n",
      "Epoch: 4, Loss: 306.68499, Residuals: -3.91768, Convergence: 0.039795\n",
      "Epoch: 5, Loss: 297.00273, Residuals: -3.79093, Convergence: 0.032600\n",
      "Epoch: 6, Loss: 289.14087, Residuals: -3.68052, Convergence: 0.027190\n",
      "Epoch: 7, Loss: 282.61220, Residuals: -3.58598, Convergence: 0.023101\n",
      "Epoch: 8, Loss: 277.05585, Residuals: -3.50523, Convergence: 0.020055\n",
      "Epoch: 9, Loss: 272.21692, Residuals: -3.43594, Convergence: 0.017776\n",
      "Epoch: 10, Loss: 267.91389, Residuals: -3.37600, Convergence: 0.016061\n",
      "Epoch: 11, Loss: 264.01494, Residuals: -3.32363, Convergence: 0.014768\n",
      "Epoch: 12, Loss: 260.42360, Residuals: -3.27730, Convergence: 0.013790\n",
      "Epoch: 13, Loss: 257.06987, Residuals: -3.23570, Convergence: 0.013046\n",
      "Epoch: 14, Loss: 253.90497, Residuals: -3.19770, Convergence: 0.012465\n",
      "Epoch: 15, Loss: 250.89935, Residuals: -3.16240, Convergence: 0.011979\n",
      "Epoch: 16, Loss: 248.03991, Residuals: -3.12925, Convergence: 0.011528\n",
      "Epoch: 17, Loss: 245.31772, Residuals: -3.09794, Convergence: 0.011097\n",
      "Epoch: 18, Loss: 242.71304, Residuals: -3.06809, Convergence: 0.010731\n",
      "Epoch: 19, Loss: 240.19380, Residuals: -3.03920, Convergence: 0.010488\n",
      "Epoch: 20, Loss: 237.72373, Residuals: -3.01067, Convergence: 0.010391\n",
      "Epoch: 21, Loss: 235.27089, Residuals: -2.98197, Convergence: 0.010426\n",
      "Epoch: 22, Loss: 232.81151, Residuals: -2.95271, Convergence: 0.010564\n",
      "Epoch: 23, Loss: 230.32259, Residuals: -2.92261, Convergence: 0.010806\n",
      "Epoch: 24, Loss: 227.76573, Residuals: -2.89121, Convergence: 0.011226\n",
      "Epoch: 25, Loss: 225.08518, Residuals: -2.85787, Convergence: 0.011909\n",
      "Epoch: 26, Loss: 222.26030, Residuals: -2.82224, Convergence: 0.012710\n",
      "Epoch: 27, Loss: 219.39158, Residuals: -2.78535, Convergence: 0.013076\n",
      "Epoch: 28, Loss: 216.60553, Residuals: -2.74866, Convergence: 0.012862\n",
      "Epoch: 29, Loss: 213.93369, Residuals: -2.71270, Convergence: 0.012489\n",
      "Epoch: 30, Loss: 211.36134, Residuals: -2.67743, Convergence: 0.012170\n",
      "Epoch: 31, Loss: 208.86888, Residuals: -2.64273, Convergence: 0.011933\n",
      "Epoch: 32, Loss: 206.44106, Residuals: -2.60848, Convergence: 0.011760\n",
      "Epoch: 33, Loss: 204.06742, Residuals: -2.57458, Convergence: 0.011632\n",
      "Epoch: 34, Loss: 201.74128, Residuals: -2.54098, Convergence: 0.011530\n",
      "Epoch: 35, Loss: 199.45863, Residuals: -2.50760, Convergence: 0.011444\n",
      "Epoch: 36, Loss: 197.21718, Residuals: -2.47443, Convergence: 0.011365\n",
      "Epoch: 37, Loss: 195.01583, Residuals: -2.44142, Convergence: 0.011288\n",
      "Epoch: 38, Loss: 192.85424, Residuals: -2.40853, Convergence: 0.011208\n",
      "Epoch: 39, Loss: 190.73269, Residuals: -2.37574, Convergence: 0.011123\n",
      "Epoch: 40, Loss: 188.65228, Residuals: -2.34303, Convergence: 0.011028\n",
      "Epoch: 41, Loss: 186.61512, Residuals: -2.31037, Convergence: 0.010916\n",
      "Epoch: 42, Loss: 184.62448, Residuals: -2.27779, Convergence: 0.010782\n",
      "Epoch: 43, Loss: 182.68475, Residuals: -2.24530, Convergence: 0.010618\n",
      "Epoch: 44, Loss: 180.80105, Residuals: -2.21297, Convergence: 0.010419\n",
      "Epoch: 45, Loss: 178.97861, Residuals: -2.18087, Convergence: 0.010182\n",
      "Epoch: 46, Loss: 177.22209, Residuals: -2.14908, Convergence: 0.009911\n",
      "Epoch: 47, Loss: 175.53501, Residuals: -2.11771, Convergence: 0.009611\n",
      "Epoch: 48, Loss: 173.91951, Residuals: -2.08681, Convergence: 0.009289\n",
      "Epoch: 49, Loss: 172.37629, Residuals: -2.05646, Convergence: 0.008953\n",
      "Epoch: 50, Loss: 170.90475, Residuals: -2.02670, Convergence: 0.008610\n",
      "Epoch: 51, Loss: 169.50320, Residuals: -1.99758, Convergence: 0.008269\n",
      "Epoch: 52, Loss: 168.16914, Residuals: -1.96912, Convergence: 0.007933\n",
      "Epoch: 53, Loss: 166.89950, Residuals: -1.94133, Convergence: 0.007607\n",
      "Epoch: 54, Loss: 165.69090, Residuals: -1.91421, Convergence: 0.007294\n",
      "Epoch: 55, Loss: 164.53986, Residuals: -1.88777, Convergence: 0.006996\n",
      "Epoch: 56, Loss: 163.44291, Residuals: -1.86198, Convergence: 0.006711\n",
      "Epoch: 57, Loss: 162.39674, Residuals: -1.83685, Convergence: 0.006442\n",
      "Epoch: 58, Loss: 161.39824, Residuals: -1.81236, Convergence: 0.006187\n",
      "Epoch: 59, Loss: 160.44451, Residuals: -1.78851, Convergence: 0.005944\n",
      "Epoch: 60, Loss: 159.53286, Residuals: -1.76528, Convergence: 0.005714\n",
      "Epoch: 61, Loss: 158.66080, Residuals: -1.74267, Convergence: 0.005496\n",
      "Epoch: 62, Loss: 157.82606, Residuals: -1.72066, Convergence: 0.005289\n",
      "Epoch: 63, Loss: 157.02647, Residuals: -1.69926, Convergence: 0.005092\n",
      "Epoch: 64, Loss: 156.26010, Residuals: -1.67844, Convergence: 0.004904\n",
      "Epoch: 65, Loss: 155.52512, Residuals: -1.65821, Convergence: 0.004726\n",
      "Epoch: 66, Loss: 154.81986, Residuals: -1.63855, Convergence: 0.004555\n",
      "Epoch: 67, Loss: 154.14278, Residuals: -1.61945, Convergence: 0.004393\n",
      "Epoch: 68, Loss: 153.49248, Residuals: -1.60092, Convergence: 0.004237\n",
      "Epoch: 69, Loss: 152.86767, Residuals: -1.58293, Convergence: 0.004087\n",
      "Epoch: 70, Loss: 152.26719, Residuals: -1.56548, Convergence: 0.003944\n",
      "Epoch: 71, Loss: 151.68999, Residuals: -1.54856, Convergence: 0.003805\n",
      "Epoch: 72, Loss: 151.13510, Residuals: -1.53217, Convergence: 0.003671\n",
      "Epoch: 73, Loss: 150.60166, Residuals: -1.51629, Convergence: 0.003542\n",
      "Epoch: 74, Loss: 150.08889, Residuals: -1.50092, Convergence: 0.003416\n",
      "Epoch: 75, Loss: 149.59607, Residuals: -1.48605, Convergence: 0.003294\n",
      "Epoch: 76, Loss: 149.12256, Residuals: -1.47167, Convergence: 0.003175\n",
      "Epoch: 77, Loss: 148.66776, Residuals: -1.45777, Convergence: 0.003059\n",
      "Epoch: 78, Loss: 148.23109, Residuals: -1.44435, Convergence: 0.002946\n",
      "Epoch: 79, Loss: 147.81203, Residuals: -1.43139, Convergence: 0.002835\n",
      "Epoch: 80, Loss: 147.41005, Residuals: -1.41889, Convergence: 0.002727\n",
      "Epoch: 81, Loss: 147.02466, Residuals: -1.40684, Convergence: 0.002621\n",
      "Epoch: 82, Loss: 146.65536, Residuals: -1.39522, Convergence: 0.002518\n",
      "Epoch: 83, Loss: 146.30165, Residuals: -1.38404, Convergence: 0.002418\n",
      "Epoch: 84, Loss: 145.96303, Residuals: -1.37327, Convergence: 0.002320\n",
      "Epoch: 85, Loss: 145.63898, Residuals: -1.36291, Convergence: 0.002225\n",
      "Epoch: 86, Loss: 145.32900, Residuals: -1.35294, Convergence: 0.002133\n",
      "Epoch: 87, Loss: 145.03256, Residuals: -1.34336, Convergence: 0.002044\n",
      "Epoch: 88, Loss: 144.74914, Residuals: -1.33416, Convergence: 0.001958\n",
      "Epoch: 89, Loss: 144.47823, Residuals: -1.32532, Convergence: 0.001875\n",
      "Epoch: 90, Loss: 144.21933, Residuals: -1.31683, Convergence: 0.001795\n",
      "Epoch: 91, Loss: 143.97192, Residuals: -1.30869, Convergence: 0.001718\n",
      "Epoch: 92, Loss: 143.73553, Residuals: -1.30087, Convergence: 0.001645\n",
      "Epoch: 93, Loss: 143.50968, Residuals: -1.29337, Convergence: 0.001574\n",
      "Epoch: 94, Loss: 143.29392, Residuals: -1.28617, Convergence: 0.001506\n",
      "Epoch: 95, Loss: 143.08785, Residuals: -1.27927, Convergence: 0.001440\n",
      "Epoch: 96, Loss: 142.89104, Residuals: -1.27265, Convergence: 0.001377\n",
      "Epoch: 97, Loss: 142.70316, Residuals: -1.26631, Convergence: 0.001317\n",
      "Epoch: 98, Loss: 142.52385, Residuals: -1.26022, Convergence: 0.001258\n",
      "Epoch: 99, Loss: 142.35278, Residuals: -1.25438, Convergence: 0.001202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Loss: 142.18969, Residuals: -1.24879, Convergence: 0.001147\n",
      "Epoch: 101, Loss: 142.03433, Residuals: -1.24343, Convergence: 0.001094\n",
      "Epoch: 102, Loss: 141.88645, Residuals: -1.23829, Convergence: 0.001042\n",
      "Epoch: 103, Loss: 141.74585, Residuals: -1.23336, Convergence: 0.000992\n",
      "Evidence -183.199\n",
      "\n",
      "Epoch: 103, Evidence: -183.19879, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.25e-01\n",
      "Epoch: 103, Loss: 1380.69541, Residuals: -1.23336, Convergence:   inf\n",
      "Epoch: 104, Loss: 1317.51572, Residuals: -1.26493, Convergence: 0.047954\n",
      "Epoch: 105, Loss: 1268.89963, Residuals: -1.28965, Convergence: 0.038314\n",
      "Epoch: 106, Loss: 1231.85523, Residuals: -1.30740, Convergence: 0.030072\n",
      "Epoch: 107, Loss: 1202.98225, Residuals: -1.31985, Convergence: 0.024001\n",
      "Epoch: 108, Loss: 1179.65873, Residuals: -1.32903, Convergence: 0.019771\n",
      "Epoch: 109, Loss: 1160.30322, Residuals: -1.33605, Convergence: 0.016681\n",
      "Epoch: 110, Loss: 1143.94688, Residuals: -1.34138, Convergence: 0.014298\n",
      "Epoch: 111, Loss: 1129.92891, Residuals: -1.34525, Convergence: 0.012406\n",
      "Epoch: 112, Loss: 1117.75843, Residuals: -1.34780, Convergence: 0.010888\n",
      "Epoch: 113, Loss: 1107.05143, Residuals: -1.34913, Convergence: 0.009672\n",
      "Epoch: 114, Loss: 1097.49965, Residuals: -1.34933, Convergence: 0.008703\n",
      "Epoch: 115, Loss: 1088.85290, Residuals: -1.34848, Convergence: 0.007941\n",
      "Epoch: 116, Loss: 1080.90597, Residuals: -1.34664, Convergence: 0.007352\n",
      "Epoch: 117, Loss: 1073.49391, Residuals: -1.34387, Convergence: 0.006905\n",
      "Epoch: 118, Loss: 1066.48205, Residuals: -1.34022, Convergence: 0.006575\n",
      "Epoch: 119, Loss: 1059.76218, Residuals: -1.33573, Convergence: 0.006341\n",
      "Epoch: 120, Loss: 1053.24634, Residuals: -1.33045, Convergence: 0.006186\n",
      "Epoch: 121, Loss: 1046.87096, Residuals: -1.32446, Convergence: 0.006090\n",
      "Epoch: 122, Loss: 1040.60362, Residuals: -1.31783, Convergence: 0.006023\n",
      "Epoch: 123, Loss: 1034.45165, Residuals: -1.31069, Convergence: 0.005947\n",
      "Epoch: 124, Loss: 1028.45999, Residuals: -1.30314, Convergence: 0.005826\n",
      "Epoch: 125, Loss: 1022.69463, Residuals: -1.29529, Convergence: 0.005637\n",
      "Epoch: 126, Loss: 1017.21518, Residuals: -1.28725, Convergence: 0.005387\n",
      "Epoch: 127, Loss: 1012.05635, Residuals: -1.27908, Convergence: 0.005097\n",
      "Epoch: 128, Loss: 1007.22497, Residuals: -1.27086, Convergence: 0.004797\n",
      "Epoch: 129, Loss: 1002.70577, Residuals: -1.26263, Convergence: 0.004507\n",
      "Epoch: 130, Loss: 998.47332, Residuals: -1.25446, Convergence: 0.004239\n",
      "Epoch: 131, Loss: 994.49744, Residuals: -1.24636, Convergence: 0.003998\n",
      "Epoch: 132, Loss: 990.75007, Residuals: -1.23839, Convergence: 0.003782\n",
      "Epoch: 133, Loss: 987.20613, Residuals: -1.23056, Convergence: 0.003590\n",
      "Epoch: 134, Loss: 983.84383, Residuals: -1.22289, Convergence: 0.003418\n",
      "Epoch: 135, Loss: 980.64643, Residuals: -1.21541, Convergence: 0.003260\n",
      "Epoch: 136, Loss: 977.59938, Residuals: -1.20813, Convergence: 0.003117\n",
      "Epoch: 137, Loss: 974.69123, Residuals: -1.20107, Convergence: 0.002984\n",
      "Epoch: 138, Loss: 971.91317, Residuals: -1.19422, Convergence: 0.002858\n",
      "Epoch: 139, Loss: 969.25726, Residuals: -1.18761, Convergence: 0.002740\n",
      "Epoch: 140, Loss: 966.71711, Residuals: -1.18123, Convergence: 0.002628\n",
      "Epoch: 141, Loss: 964.28609, Residuals: -1.17509, Convergence: 0.002521\n",
      "Epoch: 142, Loss: 961.95909, Residuals: -1.16918, Convergence: 0.002419\n",
      "Epoch: 143, Loss: 959.73020, Residuals: -1.16350, Convergence: 0.002322\n",
      "Epoch: 144, Loss: 957.59393, Residuals: -1.15805, Convergence: 0.002231\n",
      "Epoch: 145, Loss: 955.54463, Residuals: -1.15281, Convergence: 0.002145\n",
      "Epoch: 146, Loss: 953.57623, Residuals: -1.14779, Convergence: 0.002064\n",
      "Epoch: 147, Loss: 951.68270, Residuals: -1.14296, Convergence: 0.001990\n",
      "Epoch: 148, Loss: 949.85712, Residuals: -1.13832, Convergence: 0.001922\n",
      "Epoch: 149, Loss: 948.09326, Residuals: -1.13386, Convergence: 0.001860\n",
      "Epoch: 150, Loss: 946.38341, Residuals: -1.12956, Convergence: 0.001807\n",
      "Epoch: 151, Loss: 944.72046, Residuals: -1.12541, Convergence: 0.001760\n",
      "Epoch: 152, Loss: 943.09758, Residuals: -1.12140, Convergence: 0.001721\n",
      "Epoch: 153, Loss: 941.50855, Residuals: -1.11750, Convergence: 0.001688\n",
      "Epoch: 154, Loss: 939.94781, Residuals: -1.11371, Convergence: 0.001660\n",
      "Epoch: 155, Loss: 938.41284, Residuals: -1.11002, Convergence: 0.001636\n",
      "Epoch: 156, Loss: 936.90235, Residuals: -1.10641, Convergence: 0.001612\n",
      "Epoch: 157, Loss: 935.41838, Residuals: -1.10290, Convergence: 0.001586\n",
      "Epoch: 158, Loss: 933.96512, Residuals: -1.09949, Convergence: 0.001556\n",
      "Epoch: 159, Loss: 932.54749, Residuals: -1.09617, Convergence: 0.001520\n",
      "Epoch: 160, Loss: 931.17123, Residuals: -1.09296, Convergence: 0.001478\n",
      "Epoch: 161, Loss: 929.84092, Residuals: -1.08987, Convergence: 0.001431\n",
      "Epoch: 162, Loss: 928.56040, Residuals: -1.08689, Convergence: 0.001379\n",
      "Epoch: 163, Loss: 927.33145, Residuals: -1.08404, Convergence: 0.001325\n",
      "Epoch: 164, Loss: 926.15446, Residuals: -1.08130, Convergence: 0.001271\n",
      "Epoch: 165, Loss: 925.02887, Residuals: -1.07868, Convergence: 0.001217\n",
      "Epoch: 166, Loss: 923.95297, Residuals: -1.07617, Convergence: 0.001164\n",
      "Epoch: 167, Loss: 922.92435, Residuals: -1.07378, Convergence: 0.001115\n",
      "Epoch: 168, Loss: 921.94028, Residuals: -1.07148, Convergence: 0.001067\n",
      "Epoch: 169, Loss: 920.99835, Residuals: -1.06928, Convergence: 0.001023\n",
      "Epoch: 170, Loss: 920.09591, Residuals: -1.06718, Convergence: 0.000981\n",
      "Evidence 11286.654\n",
      "\n",
      "Epoch: 170, Evidence: 11286.65430, Convergence: 1.016232\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.75e-01\n",
      "Epoch: 170, Loss: 2355.09096, Residuals: -1.06718, Convergence:   inf\n",
      "Epoch: 171, Loss: 2314.33330, Residuals: -1.07686, Convergence: 0.017611\n",
      "Epoch: 172, Loss: 2287.01431, Residuals: -1.07655, Convergence: 0.011945\n",
      "Epoch: 173, Loss: 2264.43446, Residuals: -1.07523, Convergence: 0.009972\n",
      "Epoch: 174, Loss: 2245.41702, Residuals: -1.07356, Convergence: 0.008469\n",
      "Epoch: 175, Loss: 2229.23241, Residuals: -1.07165, Convergence: 0.007260\n",
      "Epoch: 176, Loss: 2215.32539, Residuals: -1.06957, Convergence: 0.006278\n",
      "Epoch: 177, Loss: 2203.24970, Residuals: -1.06733, Convergence: 0.005481\n",
      "Epoch: 178, Loss: 2192.64119, Residuals: -1.06495, Convergence: 0.004838\n",
      "Epoch: 179, Loss: 2183.20402, Residuals: -1.06242, Convergence: 0.004323\n",
      "Epoch: 180, Loss: 2174.70598, Residuals: -1.05974, Convergence: 0.003908\n",
      "Epoch: 181, Loss: 2166.97588, Residuals: -1.05692, Convergence: 0.003567\n",
      "Epoch: 182, Loss: 2159.89598, Residuals: -1.05398, Convergence: 0.003278\n",
      "Epoch: 183, Loss: 2153.38524, Residuals: -1.05096, Convergence: 0.003023\n",
      "Epoch: 184, Loss: 2147.38699, Residuals: -1.04790, Convergence: 0.002793\n",
      "Epoch: 185, Loss: 2141.85409, Residuals: -1.04485, Convergence: 0.002583\n",
      "Epoch: 186, Loss: 2136.74635, Residuals: -1.04185, Convergence: 0.002390\n",
      "Epoch: 187, Loss: 2132.02382, Residuals: -1.03892, Convergence: 0.002215\n",
      "Epoch: 188, Loss: 2127.65372, Residuals: -1.03608, Convergence: 0.002054\n",
      "Epoch: 189, Loss: 2123.60335, Residuals: -1.03335, Convergence: 0.001907\n",
      "Epoch: 190, Loss: 2119.84587, Residuals: -1.03073, Convergence: 0.001773\n",
      "Epoch: 191, Loss: 2116.35633, Residuals: -1.02823, Convergence: 0.001649\n",
      "Epoch: 192, Loss: 2113.11389, Residuals: -1.02585, Convergence: 0.001534\n",
      "Epoch: 193, Loss: 2110.09840, Residuals: -1.02358, Convergence: 0.001429\n",
      "Epoch: 194, Loss: 2107.29187, Residuals: -1.02142, Convergence: 0.001332\n",
      "Epoch: 195, Loss: 2104.67809, Residuals: -1.01937, Convergence: 0.001242\n",
      "Epoch: 196, Loss: 2102.24141, Residuals: -1.01742, Convergence: 0.001159\n",
      "Epoch: 197, Loss: 2099.96780, Residuals: -1.01557, Convergence: 0.001083\n",
      "Epoch: 198, Loss: 2097.84395, Residuals: -1.01380, Convergence: 0.001012\n",
      "Epoch: 199, Loss: 2095.85817, Residuals: -1.01211, Convergence: 0.000947\n",
      "Evidence 14419.565\n",
      "\n",
      "Epoch: 199, Evidence: 14419.56543, Convergence: 0.217268\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.38e-01\n",
      "Epoch: 199, Loss: 2473.91059, Residuals: -1.01211, Convergence:   inf\n",
      "Epoch: 200, Loss: 2460.55901, Residuals: -1.00971, Convergence: 0.005426\n",
      "Epoch: 201, Loss: 2449.68607, Residuals: -1.00669, Convergence: 0.004439\n",
      "Epoch: 202, Loss: 2440.30416, Residuals: -1.00373, Convergence: 0.003845\n",
      "Epoch: 203, Loss: 2432.16165, Residuals: -1.00096, Convergence: 0.003348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 204, Loss: 2425.06880, Residuals: -0.99842, Convergence: 0.002925\n",
      "Epoch: 205, Loss: 2418.86612, Residuals: -0.99610, Convergence: 0.002564\n",
      "Epoch: 206, Loss: 2413.41875, Residuals: -0.99399, Convergence: 0.002257\n",
      "Epoch: 207, Loss: 2408.61093, Residuals: -0.99205, Convergence: 0.001996\n",
      "Epoch: 208, Loss: 2404.34465, Residuals: -0.99029, Convergence: 0.001774\n",
      "Epoch: 209, Loss: 2400.53589, Residuals: -0.98867, Convergence: 0.001587\n",
      "Epoch: 210, Loss: 2397.11698, Residuals: -0.98719, Convergence: 0.001426\n",
      "Epoch: 211, Loss: 2394.03008, Residuals: -0.98582, Convergence: 0.001289\n",
      "Epoch: 212, Loss: 2391.22795, Residuals: -0.98457, Convergence: 0.001172\n",
      "Epoch: 213, Loss: 2388.66930, Residuals: -0.98342, Convergence: 0.001071\n",
      "Epoch: 214, Loss: 2386.32128, Residuals: -0.98236, Convergence: 0.000984\n",
      "Evidence 14790.720\n",
      "\n",
      "Epoch: 214, Evidence: 14790.71973, Convergence: 0.025094\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.37e-01\n",
      "Epoch: 214, Loss: 2479.06255, Residuals: -0.98236, Convergence:   inf\n",
      "Epoch: 215, Loss: 2472.56434, Residuals: -0.97929, Convergence: 0.002628\n",
      "Epoch: 216, Loss: 2467.18830, Residuals: -0.97662, Convergence: 0.002179\n",
      "Epoch: 217, Loss: 2462.62323, Residuals: -0.97438, Convergence: 0.001854\n",
      "Epoch: 218, Loss: 2458.69663, Residuals: -0.97251, Convergence: 0.001597\n",
      "Epoch: 219, Loss: 2455.27795, Residuals: -0.97094, Convergence: 0.001392\n",
      "Epoch: 220, Loss: 2452.26694, Residuals: -0.96962, Convergence: 0.001228\n",
      "Epoch: 221, Loss: 2449.58454, Residuals: -0.96851, Convergence: 0.001095\n",
      "Epoch: 222, Loss: 2447.17158, Residuals: -0.96757, Convergence: 0.000986\n",
      "Evidence 14870.924\n",
      "\n",
      "Epoch: 222, Evidence: 14870.92383, Convergence: 0.005393\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.65e-01\n",
      "Epoch: 222, Loss: 2480.60299, Residuals: -0.96757, Convergence:   inf\n",
      "Epoch: 223, Loss: 2476.61464, Residuals: -0.96525, Convergence: 0.001610\n",
      "Epoch: 224, Loss: 2473.29735, Residuals: -0.96344, Convergence: 0.001341\n",
      "Epoch: 225, Loss: 2470.45968, Residuals: -0.96203, Convergence: 0.001149\n",
      "Epoch: 226, Loss: 2467.98791, Residuals: -0.96093, Convergence: 0.001002\n",
      "Epoch: 227, Loss: 2465.80041, Residuals: -0.96005, Convergence: 0.000887\n",
      "Evidence 14901.633\n",
      "\n",
      "Epoch: 227, Evidence: 14901.63281, Convergence: 0.002061\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.13e-01\n",
      "Epoch: 227, Loss: 2481.45417, Residuals: -0.96005, Convergence:   inf\n",
      "Epoch: 228, Loss: 2478.60303, Residuals: -0.95834, Convergence: 0.001150\n",
      "Epoch: 229, Loss: 2476.20694, Residuals: -0.95709, Convergence: 0.000968\n",
      "Evidence 14914.479\n",
      "\n",
      "Epoch: 229, Evidence: 14914.47852, Convergence: 0.000861\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.77e-01\n",
      "Epoch: 229, Loss: 2482.11756, Residuals: -0.95709, Convergence:   inf\n",
      "Epoch: 230, Loss: 2477.58999, Residuals: -0.95497, Convergence: 0.001827\n",
      "Epoch: 231, Loss: 2474.14766, Residuals: -0.95379, Convergence: 0.001391\n",
      "Epoch: 232, Loss: 2471.36073, Residuals: -0.95309, Convergence: 0.001128\n",
      "Epoch: 233, Loss: 2469.00198, Residuals: -0.95280, Convergence: 0.000955\n",
      "Evidence 14932.375\n",
      "\n",
      "Epoch: 233, Evidence: 14932.37500, Convergence: 0.002059\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.47e-01\n",
      "Epoch: 233, Loss: 2482.33725, Residuals: -0.95280, Convergence:   inf\n",
      "Epoch: 234, Loss: 2479.30578, Residuals: -0.95099, Convergence: 0.001223\n",
      "Epoch: 235, Loss: 2476.90908, Residuals: -0.95027, Convergence: 0.000968\n",
      "Evidence 14943.493\n",
      "\n",
      "Epoch: 235, Evidence: 14943.49316, Convergence: 0.000744\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.24e-01\n",
      "Epoch: 235, Loss: 2482.54756, Residuals: -0.95027, Convergence:   inf\n",
      "Epoch: 236, Loss: 2478.11448, Residuals: -0.94823, Convergence: 0.001789\n",
      "Epoch: 237, Loss: 2474.91112, Residuals: -0.94957, Convergence: 0.001294\n",
      "Epoch: 238, Loss: 2472.27899, Residuals: -0.94961, Convergence: 0.001065\n",
      "Epoch: 239, Loss: 2469.98013, Residuals: -0.95186, Convergence: 0.000931\n",
      "Evidence 14959.412\n",
      "\n",
      "Epoch: 239, Evidence: 14959.41211, Convergence: 0.001807\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.10e-01\n",
      "Epoch: 239, Loss: 2481.90273, Residuals: -0.95186, Convergence:   inf\n",
      "Epoch: 240, Loss: 2479.93683, Residuals: -0.94951, Convergence: 0.000793\n",
      "Evidence 14966.296\n",
      "\n",
      "Epoch: 240, Evidence: 14966.29590, Convergence: 0.000460\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 9.02e-02\n",
      "Epoch: 240, Loss: 2482.80799, Residuals: -0.94951, Convergence:   inf\n",
      "Epoch: 241, Loss: 2528.24033, Residuals: -0.98526, Convergence: -0.017970\n",
      "Epoch: 241, Loss: 2480.45956, Residuals: -0.94814, Convergence: 0.000947\n",
      "Evidence 14970.658\n",
      "\n",
      "Epoch: 241, Evidence: 14970.65820, Convergence: 0.000751\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.25e-02\n",
      "Epoch: 241, Loss: 2482.29079, Residuals: -0.94814, Convergence:   inf\n",
      "Epoch: 242, Loss: 2485.86425, Residuals: -0.94959, Convergence: -0.001438\n",
      "Epoch: 242, Loss: 2481.85798, Residuals: -0.94661, Convergence: 0.000174\n",
      "Evidence 14972.723\n",
      "\n",
      "Epoch: 242, Evidence: 14972.72266, Convergence: 0.000889\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 381.39978, Residuals: -4.52469, Convergence:   inf\n",
      "Epoch: 1, Loss: 355.60640, Residuals: -4.40448, Convergence: 0.072534\n",
      "Epoch: 2, Loss: 334.45550, Residuals: -4.23945, Convergence: 0.063240\n",
      "Epoch: 3, Loss: 318.31078, Residuals: -4.07440, Convergence: 0.050720\n",
      "Epoch: 4, Loss: 305.99627, Residuals: -3.92907, Convergence: 0.040244\n",
      "Epoch: 5, Loss: 296.22420, Residuals: -3.80058, Convergence: 0.032989\n",
      "Epoch: 6, Loss: 288.28641, Residuals: -3.68861, Convergence: 0.027534\n",
      "Epoch: 7, Loss: 281.69385, Residuals: -3.59271, Convergence: 0.023403\n",
      "Epoch: 8, Loss: 276.08493, Residuals: -3.51082, Convergence: 0.020316\n",
      "Epoch: 9, Loss: 271.20445, Residuals: -3.44059, Convergence: 0.017996\n",
      "Epoch: 10, Loss: 266.87098, Residuals: -3.37992, Convergence: 0.016238\n",
      "Epoch: 11, Loss: 262.95307, Residuals: -3.32700, Convergence: 0.014900\n",
      "Epoch: 12, Loss: 259.35480, Residuals: -3.28033, Convergence: 0.013874\n",
      "Epoch: 13, Loss: 256.00638, Residuals: -3.23861, Convergence: 0.013079\n",
      "Epoch: 14, Loss: 252.85748, Residuals: -3.20068, Convergence: 0.012453\n",
      "Epoch: 15, Loss: 249.87346, Residuals: -3.16558, Convergence: 0.011942\n",
      "Epoch: 16, Loss: 247.03364, Residuals: -3.13262, Convergence: 0.011496\n",
      "Epoch: 17, Loss: 244.32495, Residuals: -3.10137, Convergence: 0.011086\n",
      "Epoch: 18, Loss: 241.72940, Residuals: -3.07147, Convergence: 0.010737\n",
      "Epoch: 19, Loss: 239.21815, Residuals: -3.04247, Convergence: 0.010498\n",
      "Epoch: 20, Loss: 236.75650, Residuals: -3.01381, Convergence: 0.010397\n",
      "Epoch: 21, Loss: 234.31192, Residuals: -2.98496, Convergence: 0.010433\n",
      "Epoch: 22, Loss: 231.85909, Residuals: -2.95552, Convergence: 0.010579\n",
      "Epoch: 23, Loss: 229.37556, Residuals: -2.92521, Convergence: 0.010827\n",
      "Epoch: 24, Loss: 226.82685, Residuals: -2.89364, Convergence: 0.011236\n",
      "Epoch: 25, Loss: 224.15874, Residuals: -2.86017, Convergence: 0.011903\n",
      "Epoch: 26, Loss: 221.33456, Residuals: -2.82432, Convergence: 0.012760\n",
      "Epoch: 27, Loss: 218.42931, Residuals: -2.78682, Convergence: 0.013301\n",
      "Epoch: 28, Loss: 215.58318, Residuals: -2.74923, Convergence: 0.013202\n",
      "Epoch: 29, Loss: 212.84922, Residuals: -2.71229, Convergence: 0.012845\n",
      "Epoch: 30, Loss: 210.21897, Residuals: -2.67607, Convergence: 0.012512\n",
      "Epoch: 31, Loss: 207.67371, Residuals: -2.64043, Convergence: 0.012256\n",
      "Epoch: 32, Loss: 205.19811, Residuals: -2.60526, Convergence: 0.012064\n",
      "Epoch: 33, Loss: 202.78174, Residuals: -2.57046, Convergence: 0.011916\n",
      "Epoch: 34, Loss: 200.41825, Residuals: -2.53595, Convergence: 0.011793\n",
      "Epoch: 35, Loss: 198.10432, Residuals: -2.50169, Convergence: 0.011680\n",
      "Epoch: 36, Loss: 195.83875, Residuals: -2.46765, Convergence: 0.011569\n",
      "Epoch: 37, Loss: 193.62167, Residuals: -2.43381, Convergence: 0.011451\n",
      "Epoch: 38, Loss: 191.45400, Residuals: -2.40016, Convergence: 0.011322\n",
      "Epoch: 39, Loss: 189.33693, Residuals: -2.36672, Convergence: 0.011181\n",
      "Epoch: 40, Loss: 187.27164, Residuals: -2.33350, Convergence: 0.011028\n",
      "Epoch: 41, Loss: 185.25913, Residuals: -2.30051, Convergence: 0.010863\n",
      "Epoch: 42, Loss: 183.30006, Residuals: -2.26779, Convergence: 0.010688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43, Loss: 181.39485, Residuals: -2.23533, Convergence: 0.010503\n",
      "Epoch: 44, Loss: 179.54379, Residuals: -2.20317, Convergence: 0.010310\n",
      "Epoch: 45, Loss: 177.74729, Residuals: -2.17130, Convergence: 0.010107\n",
      "Epoch: 46, Loss: 176.00612, Residuals: -2.13974, Convergence: 0.009893\n",
      "Epoch: 47, Loss: 174.32152, Residuals: -2.10851, Convergence: 0.009664\n",
      "Epoch: 48, Loss: 172.69513, Residuals: -2.07764, Convergence: 0.009418\n",
      "Epoch: 49, Loss: 171.12868, Residuals: -2.04716, Convergence: 0.009154\n",
      "Epoch: 50, Loss: 169.62374, Residuals: -2.01713, Convergence: 0.008872\n",
      "Epoch: 51, Loss: 168.18130, Residuals: -1.98759, Convergence: 0.008577\n",
      "Epoch: 52, Loss: 166.80166, Residuals: -1.95860, Convergence: 0.008271\n",
      "Epoch: 53, Loss: 165.48430, Residuals: -1.93018, Convergence: 0.007961\n",
      "Epoch: 54, Loss: 164.22795, Residuals: -1.90238, Convergence: 0.007650\n",
      "Epoch: 55, Loss: 163.03068, Residuals: -1.87522, Convergence: 0.007344\n",
      "Epoch: 56, Loss: 161.89004, Residuals: -1.84872, Convergence: 0.007046\n",
      "Epoch: 57, Loss: 160.80324, Residuals: -1.82289, Convergence: 0.006759\n",
      "Epoch: 58, Loss: 159.76727, Residuals: -1.79772, Convergence: 0.006484\n",
      "Epoch: 59, Loss: 158.77916, Residuals: -1.77321, Convergence: 0.006223\n",
      "Epoch: 60, Loss: 157.83598, Residuals: -1.74936, Convergence: 0.005976\n",
      "Epoch: 61, Loss: 156.93503, Residuals: -1.72614, Convergence: 0.005741\n",
      "Epoch: 62, Loss: 156.07386, Residuals: -1.70355, Convergence: 0.005518\n",
      "Epoch: 63, Loss: 155.25030, Residuals: -1.68159, Convergence: 0.005305\n",
      "Epoch: 64, Loss: 154.46245, Residuals: -1.66023, Convergence: 0.005101\n",
      "Epoch: 65, Loss: 153.70865, Residuals: -1.63948, Convergence: 0.004904\n",
      "Epoch: 66, Loss: 152.98742, Residuals: -1.61933, Convergence: 0.004714\n",
      "Epoch: 67, Loss: 152.29746, Residuals: -1.59978, Convergence: 0.004530\n",
      "Epoch: 68, Loss: 151.63758, Residuals: -1.58081, Convergence: 0.004352\n",
      "Epoch: 69, Loss: 151.00671, Residuals: -1.56244, Convergence: 0.004178\n",
      "Epoch: 70, Loss: 150.40382, Residuals: -1.54465, Convergence: 0.004008\n",
      "Epoch: 71, Loss: 149.82795, Residuals: -1.52743, Convergence: 0.003844\n",
      "Epoch: 72, Loss: 149.27816, Residuals: -1.51079, Convergence: 0.003683\n",
      "Epoch: 73, Loss: 148.75354, Residuals: -1.49471, Convergence: 0.003527\n",
      "Epoch: 74, Loss: 148.25320, Residuals: -1.47919, Convergence: 0.003375\n",
      "Epoch: 75, Loss: 147.77623, Residuals: -1.46421, Convergence: 0.003228\n",
      "Epoch: 76, Loss: 147.32177, Residuals: -1.44978, Convergence: 0.003085\n",
      "Epoch: 77, Loss: 146.88892, Residuals: -1.43587, Convergence: 0.002947\n",
      "Epoch: 78, Loss: 146.47681, Residuals: -1.42248, Convergence: 0.002813\n",
      "Epoch: 79, Loss: 146.08456, Residuals: -1.40960, Convergence: 0.002685\n",
      "Epoch: 80, Loss: 145.71130, Residuals: -1.39721, Convergence: 0.002562\n",
      "Epoch: 81, Loss: 145.35618, Residuals: -1.38530, Convergence: 0.002443\n",
      "Epoch: 82, Loss: 145.01836, Residuals: -1.37385, Convergence: 0.002329\n",
      "Epoch: 83, Loss: 144.69703, Residuals: -1.36285, Convergence: 0.002221\n",
      "Epoch: 84, Loss: 144.39140, Residuals: -1.35229, Convergence: 0.002117\n",
      "Epoch: 85, Loss: 144.10070, Residuals: -1.34216, Convergence: 0.002017\n",
      "Epoch: 86, Loss: 143.82419, Residuals: -1.33242, Convergence: 0.001923\n",
      "Epoch: 87, Loss: 143.56119, Residuals: -1.32308, Convergence: 0.001832\n",
      "Epoch: 88, Loss: 143.31102, Residuals: -1.31412, Convergence: 0.001746\n",
      "Epoch: 89, Loss: 143.07309, Residuals: -1.30553, Convergence: 0.001663\n",
      "Epoch: 90, Loss: 142.84680, Residuals: -1.29728, Convergence: 0.001584\n",
      "Epoch: 91, Loss: 142.63161, Residuals: -1.28936, Convergence: 0.001509\n",
      "Epoch: 92, Loss: 142.42703, Residuals: -1.28177, Convergence: 0.001436\n",
      "Epoch: 93, Loss: 142.23259, Residuals: -1.27449, Convergence: 0.001367\n",
      "Epoch: 94, Loss: 142.04786, Residuals: -1.26750, Convergence: 0.001300\n",
      "Epoch: 95, Loss: 141.87247, Residuals: -1.26080, Convergence: 0.001236\n",
      "Epoch: 96, Loss: 141.70605, Residuals: -1.25437, Convergence: 0.001174\n",
      "Epoch: 97, Loss: 141.54829, Residuals: -1.24821, Convergence: 0.001115\n",
      "Epoch: 98, Loss: 141.39891, Residuals: -1.24230, Convergence: 0.001056\n",
      "Epoch: 99, Loss: 141.25764, Residuals: -1.23664, Convergence: 0.001000\n",
      "Epoch: 100, Loss: 141.12426, Residuals: -1.23121, Convergence: 0.000945\n",
      "Evidence -182.683\n",
      "\n",
      "Epoch: 100, Evidence: -182.68301, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.25e-01\n",
      "Epoch: 100, Loss: 1370.15960, Residuals: -1.23121, Convergence:   inf\n",
      "Epoch: 101, Loss: 1308.10274, Residuals: -1.26191, Convergence: 0.047440\n",
      "Epoch: 102, Loss: 1260.46829, Residuals: -1.28574, Convergence: 0.037791\n",
      "Epoch: 103, Loss: 1224.18174, Residuals: -1.30280, Convergence: 0.029641\n",
      "Epoch: 104, Loss: 1195.89035, Residuals: -1.31478, Convergence: 0.023657\n",
      "Epoch: 105, Loss: 1173.04796, Residuals: -1.32360, Convergence: 0.019473\n",
      "Epoch: 106, Loss: 1154.12006, Residuals: -1.33033, Convergence: 0.016400\n",
      "Epoch: 107, Loss: 1138.16643, Residuals: -1.33543, Convergence: 0.014017\n",
      "Epoch: 108, Loss: 1124.54713, Residuals: -1.33914, Convergence: 0.012111\n",
      "Epoch: 109, Loss: 1112.78903, Residuals: -1.34160, Convergence: 0.010566\n",
      "Epoch: 110, Loss: 1102.52381, Residuals: -1.34293, Convergence: 0.009311\n",
      "Epoch: 111, Loss: 1093.45539, Residuals: -1.34323, Convergence: 0.008293\n",
      "Epoch: 112, Loss: 1085.33976, Residuals: -1.34258, Convergence: 0.007478\n",
      "Epoch: 113, Loss: 1077.97243, Residuals: -1.34105, Convergence: 0.006834\n",
      "Epoch: 114, Loss: 1071.17982, Residuals: -1.33869, Convergence: 0.006341\n",
      "Epoch: 115, Loss: 1064.81377, Residuals: -1.33555, Convergence: 0.005979\n",
      "Epoch: 116, Loss: 1058.74769, Residuals: -1.33165, Convergence: 0.005729\n",
      "Epoch: 117, Loss: 1052.87464, Residuals: -1.32702, Convergence: 0.005578\n",
      "Epoch: 118, Loss: 1047.10647, Residuals: -1.32168, Convergence: 0.005509\n",
      "Epoch: 119, Loss: 1041.37599, Residuals: -1.31567, Convergence: 0.005503\n",
      "Epoch: 120, Loss: 1035.64555, Residuals: -1.30905, Convergence: 0.005533\n",
      "Epoch: 121, Loss: 1029.91806, Residuals: -1.30190, Convergence: 0.005561\n",
      "Epoch: 122, Loss: 1024.24110, Residuals: -1.29432, Convergence: 0.005543\n",
      "Epoch: 123, Loss: 1018.69646, Residuals: -1.28643, Convergence: 0.005443\n",
      "Epoch: 124, Loss: 1013.37036, Residuals: -1.27831, Convergence: 0.005256\n",
      "Epoch: 125, Loss: 1008.32578, Residuals: -1.27006, Convergence: 0.005003\n",
      "Epoch: 126, Loss: 1003.59128, Residuals: -1.26175, Convergence: 0.004718\n",
      "Epoch: 127, Loss: 999.16488, Residuals: -1.25343, Convergence: 0.004430\n",
      "Epoch: 128, Loss: 995.02855, Residuals: -1.24516, Convergence: 0.004157\n",
      "Epoch: 129, Loss: 991.15535, Residuals: -1.23698, Convergence: 0.003908\n",
      "Epoch: 130, Loss: 987.51838, Residuals: -1.22893, Convergence: 0.003683\n",
      "Epoch: 131, Loss: 984.09282, Residuals: -1.22104, Convergence: 0.003481\n",
      "Epoch: 132, Loss: 980.85740, Residuals: -1.21332, Convergence: 0.003299\n",
      "Epoch: 133, Loss: 977.79506, Residuals: -1.20581, Convergence: 0.003132\n",
      "Epoch: 134, Loss: 974.89154, Residuals: -1.19852, Convergence: 0.002978\n",
      "Epoch: 135, Loss: 972.13593, Residuals: -1.19147, Convergence: 0.002835\n",
      "Epoch: 136, Loss: 969.51920, Residuals: -1.18467, Convergence: 0.002699\n",
      "Epoch: 137, Loss: 967.03352, Residuals: -1.17813, Convergence: 0.002570\n",
      "Epoch: 138, Loss: 964.67218, Residuals: -1.17186, Convergence: 0.002448\n",
      "Epoch: 139, Loss: 962.42863, Residuals: -1.16585, Convergence: 0.002331\n",
      "Epoch: 140, Loss: 960.29658, Residuals: -1.16011, Convergence: 0.002220\n",
      "Epoch: 141, Loss: 958.27024, Residuals: -1.15464, Convergence: 0.002115\n",
      "Epoch: 142, Loss: 956.34334, Residuals: -1.14942, Convergence: 0.002015\n",
      "Epoch: 143, Loss: 954.51004, Residuals: -1.14446, Convergence: 0.001921\n",
      "Epoch: 144, Loss: 952.76348, Residuals: -1.13974, Convergence: 0.001833\n",
      "Epoch: 145, Loss: 951.09797, Residuals: -1.13525, Convergence: 0.001751\n",
      "Epoch: 146, Loss: 949.50769, Residuals: -1.13098, Convergence: 0.001675\n",
      "Epoch: 147, Loss: 947.98661, Residuals: -1.12692, Convergence: 0.001605\n",
      "Epoch: 148, Loss: 946.52907, Residuals: -1.12305, Convergence: 0.001540\n",
      "Epoch: 149, Loss: 945.12966, Residuals: -1.11937, Convergence: 0.001481\n",
      "Epoch: 150, Loss: 943.78281, Residuals: -1.11587, Convergence: 0.001427\n",
      "Epoch: 151, Loss: 942.48389, Residuals: -1.11252, Convergence: 0.001378\n",
      "Epoch: 152, Loss: 941.22736, Residuals: -1.10932, Convergence: 0.001335\n",
      "Epoch: 153, Loss: 940.00913, Residuals: -1.10626, Convergence: 0.001296\n",
      "Epoch: 154, Loss: 938.82412, Residuals: -1.10332, Convergence: 0.001262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 155, Loss: 937.66769, Residuals: -1.10049, Convergence: 0.001233\n",
      "Epoch: 156, Loss: 936.53596, Residuals: -1.09777, Convergence: 0.001208\n",
      "Epoch: 157, Loss: 935.42458, Residuals: -1.09513, Convergence: 0.001188\n",
      "Epoch: 158, Loss: 934.32953, Residuals: -1.09256, Convergence: 0.001172\n",
      "Epoch: 159, Loss: 933.24693, Residuals: -1.09006, Convergence: 0.001160\n",
      "Epoch: 160, Loss: 932.17388, Residuals: -1.08761, Convergence: 0.001151\n",
      "Epoch: 161, Loss: 931.10783, Residuals: -1.08521, Convergence: 0.001145\n",
      "Epoch: 162, Loss: 930.04710, Residuals: -1.08284, Convergence: 0.001141\n",
      "Epoch: 163, Loss: 928.99221, Residuals: -1.08051, Convergence: 0.001136\n",
      "Epoch: 164, Loss: 927.94381, Residuals: -1.07820, Convergence: 0.001130\n",
      "Epoch: 165, Loss: 926.90513, Residuals: -1.07593, Convergence: 0.001121\n",
      "Epoch: 166, Loss: 925.87967, Residuals: -1.07369, Convergence: 0.001108\n",
      "Epoch: 167, Loss: 924.87181, Residuals: -1.07150, Convergence: 0.001090\n",
      "Epoch: 168, Loss: 923.88545, Residuals: -1.06935, Convergence: 0.001068\n",
      "Epoch: 169, Loss: 922.92510, Residuals: -1.06726, Convergence: 0.001041\n",
      "Epoch: 170, Loss: 921.99363, Residuals: -1.06523, Convergence: 0.001010\n",
      "Epoch: 171, Loss: 921.09364, Residuals: -1.06326, Convergence: 0.000977\n",
      "Evidence 11141.769\n",
      "\n",
      "Epoch: 171, Evidence: 11141.76855, Convergence: 1.016396\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.74e-01\n",
      "Epoch: 171, Loss: 2346.15818, Residuals: -1.06326, Convergence:   inf\n",
      "Epoch: 172, Loss: 2306.59102, Residuals: -1.07143, Convergence: 0.017154\n",
      "Epoch: 173, Loss: 2278.96242, Residuals: -1.06952, Convergence: 0.012123\n",
      "Epoch: 174, Loss: 2256.02981, Residuals: -1.06673, Convergence: 0.010165\n",
      "Epoch: 175, Loss: 2236.72132, Residuals: -1.06377, Convergence: 0.008632\n",
      "Epoch: 176, Loss: 2220.32673, Residuals: -1.06076, Convergence: 0.007384\n",
      "Epoch: 177, Loss: 2206.29683, Residuals: -1.05773, Convergence: 0.006359\n",
      "Epoch: 178, Loss: 2194.17758, Residuals: -1.05468, Convergence: 0.005523\n",
      "Epoch: 179, Loss: 2183.59169, Residuals: -1.05162, Convergence: 0.004848\n",
      "Epoch: 180, Loss: 2174.22313, Residuals: -1.04850, Convergence: 0.004309\n",
      "Epoch: 181, Loss: 2165.81909, Residuals: -1.04533, Convergence: 0.003880\n",
      "Epoch: 182, Loss: 2158.19086, Residuals: -1.04207, Convergence: 0.003535\n",
      "Epoch: 183, Loss: 2151.21165, Residuals: -1.03874, Convergence: 0.003244\n",
      "Epoch: 184, Loss: 2144.80412, Residuals: -1.03536, Convergence: 0.002987\n",
      "Epoch: 185, Loss: 2138.91983, Residuals: -1.03197, Convergence: 0.002751\n",
      "Epoch: 186, Loss: 2133.51851, Residuals: -1.02863, Convergence: 0.002532\n",
      "Epoch: 187, Loss: 2128.56145, Residuals: -1.02535, Convergence: 0.002329\n",
      "Epoch: 188, Loss: 2124.00620, Residuals: -1.02217, Convergence: 0.002145\n",
      "Epoch: 189, Loss: 2119.81097, Residuals: -1.01911, Convergence: 0.001979\n",
      "Epoch: 190, Loss: 2115.93421, Residuals: -1.01616, Convergence: 0.001832\n",
      "Epoch: 191, Loss: 2112.33936, Residuals: -1.01334, Convergence: 0.001702\n",
      "Epoch: 192, Loss: 2108.99119, Residuals: -1.01064, Convergence: 0.001588\n",
      "Epoch: 193, Loss: 2105.86183, Residuals: -1.00807, Convergence: 0.001486\n",
      "Epoch: 194, Loss: 2102.92562, Residuals: -1.00561, Convergence: 0.001396\n",
      "Epoch: 195, Loss: 2100.16262, Residuals: -1.00328, Convergence: 0.001316\n",
      "Epoch: 196, Loss: 2097.55546, Residuals: -1.00107, Convergence: 0.001243\n",
      "Epoch: 197, Loss: 2095.09111, Residuals: -0.99897, Convergence: 0.001176\n",
      "Epoch: 198, Loss: 2092.75747, Residuals: -0.99699, Convergence: 0.001115\n",
      "Epoch: 199, Loss: 2090.54640, Residuals: -0.99511, Convergence: 0.001058\n",
      "Epoch: 200, Loss: 2088.44944, Residuals: -0.99334, Convergence: 0.001004\n",
      "Epoch: 201, Loss: 2086.46057, Residuals: -0.99167, Convergence: 0.000953\n",
      "Evidence 14315.502\n",
      "\n",
      "Epoch: 201, Evidence: 14315.50195, Convergence: 0.221699\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.35e-01\n",
      "Epoch: 201, Loss: 2470.55599, Residuals: -0.99167, Convergence:   inf\n",
      "Epoch: 202, Loss: 2456.96259, Residuals: -0.98836, Convergence: 0.005533\n",
      "Epoch: 203, Loss: 2445.79780, Residuals: -0.98470, Convergence: 0.004565\n",
      "Epoch: 204, Loss: 2436.15668, Residuals: -0.98131, Convergence: 0.003958\n",
      "Epoch: 205, Loss: 2427.78604, Residuals: -0.97823, Convergence: 0.003448\n",
      "Epoch: 206, Loss: 2420.48460, Residuals: -0.97546, Convergence: 0.003017\n",
      "Epoch: 207, Loss: 2414.08911, Residuals: -0.97301, Convergence: 0.002649\n",
      "Epoch: 208, Loss: 2408.45937, Residuals: -0.97083, Convergence: 0.002337\n",
      "Epoch: 209, Loss: 2403.47986, Residuals: -0.96891, Convergence: 0.002072\n",
      "Epoch: 210, Loss: 2399.05209, Residuals: -0.96721, Convergence: 0.001846\n",
      "Epoch: 211, Loss: 2395.09299, Residuals: -0.96571, Convergence: 0.001653\n",
      "Epoch: 212, Loss: 2391.53487, Residuals: -0.96437, Convergence: 0.001488\n",
      "Epoch: 213, Loss: 2388.31979, Residuals: -0.96319, Convergence: 0.001346\n",
      "Epoch: 214, Loss: 2385.40026, Residuals: -0.96213, Convergence: 0.001224\n",
      "Epoch: 215, Loss: 2382.73686, Residuals: -0.96118, Convergence: 0.001118\n",
      "Epoch: 216, Loss: 2380.29627, Residuals: -0.96033, Convergence: 0.001025\n",
      "Epoch: 217, Loss: 2378.04955, Residuals: -0.95956, Convergence: 0.000945\n",
      "Evidence 14700.331\n",
      "\n",
      "Epoch: 217, Evidence: 14700.33105, Convergence: 0.026178\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.30e-01\n",
      "Epoch: 217, Loss: 2476.00315, Residuals: -0.95956, Convergence:   inf\n",
      "Epoch: 218, Loss: 2469.48531, Residuals: -0.95661, Convergence: 0.002639\n",
      "Epoch: 219, Loss: 2464.09151, Residuals: -0.95420, Convergence: 0.002189\n",
      "Epoch: 220, Loss: 2459.51834, Residuals: -0.95227, Convergence: 0.001859\n",
      "Epoch: 221, Loss: 2455.58394, Residuals: -0.95071, Convergence: 0.001602\n",
      "Epoch: 222, Loss: 2452.15531, Residuals: -0.94946, Convergence: 0.001398\n",
      "Epoch: 223, Loss: 2449.12939, Residuals: -0.94844, Convergence: 0.001236\n",
      "Epoch: 224, Loss: 2446.42922, Residuals: -0.94761, Convergence: 0.001104\n",
      "Epoch: 225, Loss: 2443.99704, Residuals: -0.94693, Convergence: 0.000995\n",
      "Evidence 14782.757\n",
      "\n",
      "Epoch: 225, Evidence: 14782.75684, Convergence: 0.005576\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.57e-01\n",
      "Epoch: 225, Loss: 2477.85057, Residuals: -0.94693, Convergence:   inf\n",
      "Epoch: 226, Loss: 2473.95803, Residuals: -0.94481, Convergence: 0.001573\n",
      "Epoch: 227, Loss: 2470.70975, Residuals: -0.94322, Convergence: 0.001315\n",
      "Epoch: 228, Loss: 2467.91693, Residuals: -0.94201, Convergence: 0.001132\n",
      "Epoch: 229, Loss: 2465.46913, Residuals: -0.94108, Convergence: 0.000993\n",
      "Evidence 14810.508\n",
      "\n",
      "Epoch: 229, Evidence: 14810.50781, Convergence: 0.001874\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.07e-01\n",
      "Epoch: 229, Loss: 2478.92932, Residuals: -0.94108, Convergence:   inf\n",
      "Epoch: 230, Loss: 2476.04572, Residuals: -0.93944, Convergence: 0.001165\n",
      "Epoch: 231, Loss: 2473.61626, Residuals: -0.93824, Convergence: 0.000982\n",
      "Evidence 14822.493\n",
      "\n",
      "Epoch: 231, Evidence: 14822.49316, Convergence: 0.000809\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.71e-01\n",
      "Epoch: 231, Loss: 2479.67521, Residuals: -0.93824, Convergence:   inf\n",
      "Epoch: 232, Loss: 2475.14696, Residuals: -0.93615, Convergence: 0.001829\n",
      "Epoch: 233, Loss: 2471.65671, Residuals: -0.93482, Convergence: 0.001412\n",
      "Epoch: 234, Loss: 2468.80863, Residuals: -0.93392, Convergence: 0.001154\n",
      "Epoch: 235, Loss: 2466.38281, Residuals: -0.93345, Convergence: 0.000984\n",
      "Evidence 14840.193\n",
      "\n",
      "Epoch: 235, Evidence: 14840.19336, Convergence: 0.002000\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.41e-01\n",
      "Epoch: 235, Loss: 2479.84044, Residuals: -0.93345, Convergence:   inf\n",
      "Epoch: 236, Loss: 2476.84267, Residuals: -0.93138, Convergence: 0.001210\n",
      "Epoch: 237, Loss: 2474.44131, Residuals: -0.93040, Convergence: 0.000970\n",
      "Evidence 14850.949\n",
      "\n",
      "Epoch: 237, Evidence: 14850.94922, Convergence: 0.000724\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.20e-01\n",
      "Epoch: 237, Loss: 2480.01603, Residuals: -0.93040, Convergence:   inf\n",
      "Epoch: 238, Loss: 2475.59553, Residuals: -0.92769, Convergence: 0.001786\n",
      "Epoch: 239, Loss: 2472.39224, Residuals: -0.92874, Convergence: 0.001296\n",
      "Epoch: 240, Loss: 2469.74817, Residuals: -0.92943, Convergence: 0.001071\n",
      "Epoch: 241, Loss: 2467.39215, Residuals: -0.93226, Convergence: 0.000955\n",
      "Evidence 14866.715\n",
      "\n",
      "Epoch: 241, Evidence: 14866.71484, Convergence: 0.001784\n",
      "Updating hyper-parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 182, Updated regularization: 1.07e-01\n",
      "Epoch: 241, Loss: 2479.31941, Residuals: -0.93226, Convergence:   inf\n",
      "Epoch: 242, Loss: 2477.43662, Residuals: -0.93020, Convergence: 0.000760\n",
      "Evidence 14873.347\n",
      "\n",
      "Epoch: 242, Evidence: 14873.34668, Convergence: 0.000446\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.74e-02\n",
      "Epoch: 242, Loss: 2480.26392, Residuals: -0.93020, Convergence:   inf\n",
      "Epoch: 243, Loss: 2521.68742, Residuals: -0.97885, Convergence: -0.016427\n",
      "Epoch: 243, Loss: 2477.99164, Residuals: -0.92971, Convergence: 0.000917\n",
      "Evidence 14877.561\n",
      "\n",
      "Epoch: 243, Evidence: 14877.56055, Convergence: 0.000729\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.09e-02\n",
      "Epoch: 243, Loss: 2479.67342, Residuals: -0.92971, Convergence:   inf\n",
      "Epoch: 244, Loss: 2483.72298, Residuals: -0.93375, Convergence: -0.001630\n",
      "Epoch: 244, Loss: 2479.39241, Residuals: -0.92932, Convergence: 0.000113\n",
      "Evidence 14879.492\n",
      "\n",
      "Epoch: 244, Evidence: 14879.49219, Convergence: 0.000859\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 385.63746, Residuals: -4.56591, Convergence:   inf\n",
      "Epoch: 1, Loss: 359.93510, Residuals: -4.44549, Convergence: 0.071408\n",
      "Epoch: 2, Loss: 338.90796, Residuals: -4.28183, Convergence: 0.062044\n",
      "Epoch: 3, Loss: 322.86094, Residuals: -4.11770, Convergence: 0.049703\n",
      "Epoch: 4, Loss: 310.61332, Residuals: -3.97308, Convergence: 0.039430\n",
      "Epoch: 5, Loss: 300.89328, Residuals: -3.84514, Convergence: 0.032304\n",
      "Epoch: 6, Loss: 292.99494, Residuals: -3.73354, Convergence: 0.026957\n",
      "Epoch: 7, Loss: 286.43051, Residuals: -3.63781, Convergence: 0.022918\n",
      "Epoch: 8, Loss: 280.83901, Residuals: -3.55594, Convergence: 0.019910\n",
      "Epoch: 9, Loss: 275.96526, Residuals: -3.48559, Convergence: 0.017661\n",
      "Epoch: 10, Loss: 271.62760, Residuals: -3.42465, Convergence: 0.015969\n",
      "Epoch: 11, Loss: 267.69412, Residuals: -3.37134, Convergence: 0.014694\n",
      "Epoch: 12, Loss: 264.06831, Residuals: -3.32413, Convergence: 0.013731\n",
      "Epoch: 13, Loss: 260.68015, Residuals: -3.28174, Convergence: 0.012997\n",
      "Epoch: 14, Loss: 257.48045, Residuals: -3.24305, Convergence: 0.012427\n",
      "Epoch: 15, Loss: 254.43801, Residuals: -3.20713, Convergence: 0.011957\n",
      "Epoch: 16, Loss: 251.53702, Residuals: -3.17334, Convergence: 0.011533\n",
      "Epoch: 17, Loss: 248.76721, Residuals: -3.14132, Convergence: 0.011134\n",
      "Epoch: 18, Loss: 246.10931, Residuals: -3.11068, Convergence: 0.010800\n",
      "Epoch: 19, Loss: 243.53147, Residuals: -3.08091, Convergence: 0.010585\n",
      "Epoch: 20, Loss: 240.99669, Residuals: -3.05142, Convergence: 0.010518\n",
      "Epoch: 21, Loss: 238.47209, Residuals: -3.02168, Convergence: 0.010587\n",
      "Epoch: 22, Loss: 235.93208, Residuals: -2.99130, Convergence: 0.010766\n",
      "Epoch: 23, Loss: 233.34747, Residuals: -2.95992, Convergence: 0.011076\n",
      "Epoch: 24, Loss: 230.67002, Residuals: -2.92698, Convergence: 0.011607\n",
      "Epoch: 25, Loss: 227.84875, Residuals: -2.89186, Convergence: 0.012382\n",
      "Epoch: 26, Loss: 224.91461, Residuals: -2.85478, Convergence: 0.013046\n",
      "Epoch: 27, Loss: 222.00040, Residuals: -2.81717, Convergence: 0.013127\n",
      "Epoch: 28, Loss: 219.18372, Residuals: -2.78001, Convergence: 0.012851\n",
      "Epoch: 29, Loss: 216.46459, Residuals: -2.74347, Convergence: 0.012562\n",
      "Epoch: 30, Loss: 213.82401, Residuals: -2.70744, Convergence: 0.012349\n",
      "Epoch: 31, Loss: 211.24492, Residuals: -2.67178, Convergence: 0.012209\n",
      "Epoch: 32, Loss: 208.71540, Residuals: -2.63639, Convergence: 0.012119\n",
      "Epoch: 33, Loss: 206.22805, Residuals: -2.60117, Convergence: 0.012061\n",
      "Epoch: 34, Loss: 203.77890, Residuals: -2.56608, Convergence: 0.012019\n",
      "Epoch: 35, Loss: 201.36643, Residuals: -2.53107, Convergence: 0.011980\n",
      "Epoch: 36, Loss: 198.99079, Residuals: -2.49611, Convergence: 0.011938\n",
      "Epoch: 37, Loss: 196.65320, Residuals: -2.46118, Convergence: 0.011887\n",
      "Epoch: 38, Loss: 194.35547, Residuals: -2.42628, Convergence: 0.011822\n",
      "Epoch: 39, Loss: 192.09967, Residuals: -2.39141, Convergence: 0.011743\n",
      "Epoch: 40, Loss: 189.88797, Residuals: -2.35657, Convergence: 0.011647\n",
      "Epoch: 41, Loss: 187.72251, Residuals: -2.32177, Convergence: 0.011535\n",
      "Epoch: 42, Loss: 185.60544, Residuals: -2.28703, Convergence: 0.011406\n",
      "Epoch: 43, Loss: 183.53904, Residuals: -2.25238, Convergence: 0.011259\n",
      "Epoch: 44, Loss: 181.52591, Residuals: -2.21783, Convergence: 0.011090\n",
      "Epoch: 45, Loss: 179.56900, Residuals: -2.18345, Convergence: 0.010898\n",
      "Epoch: 46, Loss: 177.67157, Residuals: -2.14927, Convergence: 0.010679\n",
      "Epoch: 47, Loss: 175.83688, Residuals: -2.11536, Convergence: 0.010434\n",
      "Epoch: 48, Loss: 174.06775, Residuals: -2.08179, Convergence: 0.010163\n",
      "Epoch: 49, Loss: 172.36627, Residuals: -2.04863, Convergence: 0.009871\n",
      "Epoch: 50, Loss: 170.73355, Residuals: -2.01593, Convergence: 0.009563\n",
      "Epoch: 51, Loss: 169.16976, Residuals: -1.98376, Convergence: 0.009244\n",
      "Epoch: 52, Loss: 167.67424, Residuals: -1.95217, Convergence: 0.008919\n",
      "Epoch: 53, Loss: 166.24567, Residuals: -1.92119, Convergence: 0.008593\n",
      "Epoch: 54, Loss: 164.88226, Residuals: -1.89085, Convergence: 0.008269\n",
      "Epoch: 55, Loss: 163.58194, Residuals: -1.86120, Convergence: 0.007949\n",
      "Epoch: 56, Loss: 162.34245, Residuals: -1.83224, Convergence: 0.007635\n",
      "Epoch: 57, Loss: 161.16146, Residuals: -1.80399, Convergence: 0.007328\n",
      "Epoch: 58, Loss: 160.03667, Residuals: -1.77648, Convergence: 0.007028\n",
      "Epoch: 59, Loss: 158.96582, Residuals: -1.74970, Convergence: 0.006736\n",
      "Epoch: 60, Loss: 157.94673, Residuals: -1.72368, Convergence: 0.006452\n",
      "Epoch: 61, Loss: 156.97735, Residuals: -1.69842, Convergence: 0.006175\n",
      "Epoch: 62, Loss: 156.05575, Residuals: -1.67392, Convergence: 0.005906\n",
      "Epoch: 63, Loss: 155.18012, Residuals: -1.65019, Convergence: 0.005643\n",
      "Epoch: 64, Loss: 154.34875, Residuals: -1.62724, Convergence: 0.005386\n",
      "Epoch: 65, Loss: 153.56006, Residuals: -1.60506, Convergence: 0.005136\n",
      "Epoch: 66, Loss: 152.81257, Residuals: -1.58365, Convergence: 0.004892\n",
      "Epoch: 67, Loss: 152.10484, Residuals: -1.56303, Convergence: 0.004653\n",
      "Epoch: 68, Loss: 151.43551, Residuals: -1.54319, Convergence: 0.004420\n",
      "Epoch: 69, Loss: 150.80322, Residuals: -1.52412, Convergence: 0.004193\n",
      "Epoch: 70, Loss: 150.20662, Residuals: -1.50582, Convergence: 0.003972\n",
      "Epoch: 71, Loss: 149.64429, Residuals: -1.48830, Convergence: 0.003758\n",
      "Epoch: 72, Loss: 149.11476, Residuals: -1.47154, Convergence: 0.003551\n",
      "Epoch: 73, Loss: 148.61644, Residuals: -1.45552, Convergence: 0.003353\n",
      "Epoch: 74, Loss: 148.14761, Residuals: -1.44025, Convergence: 0.003165\n",
      "Epoch: 75, Loss: 147.70640, Residuals: -1.42569, Convergence: 0.002987\n",
      "Epoch: 76, Loss: 147.29079, Residuals: -1.41182, Convergence: 0.002822\n",
      "Epoch: 77, Loss: 146.89873, Residuals: -1.39861, Convergence: 0.002669\n",
      "Epoch: 78, Loss: 146.52810, Residuals: -1.38603, Convergence: 0.002529\n",
      "Epoch: 79, Loss: 146.17689, Residuals: -1.37403, Convergence: 0.002403\n",
      "Epoch: 80, Loss: 145.84323, Residuals: -1.36258, Convergence: 0.002288\n",
      "Epoch: 81, Loss: 145.52551, Residuals: -1.35165, Convergence: 0.002183\n",
      "Epoch: 82, Loss: 145.22236, Residuals: -1.34119, Convergence: 0.002087\n",
      "Epoch: 83, Loss: 144.93265, Residuals: -1.33117, Convergence: 0.001999\n",
      "Epoch: 84, Loss: 144.65550, Residuals: -1.32157, Convergence: 0.001916\n",
      "Epoch: 85, Loss: 144.39021, Residuals: -1.31237, Convergence: 0.001837\n",
      "Epoch: 86, Loss: 144.13620, Residuals: -1.30355, Convergence: 0.001762\n",
      "Epoch: 87, Loss: 143.89303, Residuals: -1.29508, Convergence: 0.001690\n",
      "Epoch: 88, Loss: 143.66032, Residuals: -1.28696, Convergence: 0.001620\n",
      "Epoch: 89, Loss: 143.43772, Residuals: -1.27916, Convergence: 0.001552\n",
      "Epoch: 90, Loss: 143.22492, Residuals: -1.27168, Convergence: 0.001486\n",
      "Epoch: 91, Loss: 143.02165, Residuals: -1.26450, Convergence: 0.001421\n",
      "Epoch: 92, Loss: 142.82762, Residuals: -1.25762, Convergence: 0.001358\n",
      "Epoch: 93, Loss: 142.64258, Residuals: -1.25101, Convergence: 0.001297\n",
      "Epoch: 94, Loss: 142.46627, Residuals: -1.24466, Convergence: 0.001238\n",
      "Epoch: 95, Loss: 142.29849, Residuals: -1.23858, Convergence: 0.001179\n",
      "Epoch: 96, Loss: 142.13897, Residuals: -1.23275, Convergence: 0.001122\n",
      "Epoch: 97, Loss: 141.98753, Residuals: -1.22715, Convergence: 0.001067\n",
      "Epoch: 98, Loss: 141.84396, Residuals: -1.22178, Convergence: 0.001012\n",
      "Epoch: 99, Loss: 141.70807, Residuals: -1.21664, Convergence: 0.000959\n",
      "Evidence -183.997\n",
      "\n",
      "Epoch: 99, Evidence: -183.99696, Convergence:   inf\n",
      "Updating hyper-parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 182, Updated regularization: 7.25e-01\n",
      "Epoch: 99, Loss: 1355.18294, Residuals: -1.21664, Convergence:   inf\n",
      "Epoch: 100, Loss: 1291.39275, Residuals: -1.24979, Convergence: 0.049396\n",
      "Epoch: 101, Loss: 1243.66338, Residuals: -1.27542, Convergence: 0.038378\n",
      "Epoch: 102, Loss: 1208.12973, Residuals: -1.29326, Convergence: 0.029412\n",
      "Epoch: 103, Loss: 1180.65568, Residuals: -1.30546, Convergence: 0.023270\n",
      "Epoch: 104, Loss: 1158.47200, Residuals: -1.31434, Convergence: 0.019149\n",
      "Epoch: 105, Loss: 1140.07237, Residuals: -1.32106, Convergence: 0.016139\n",
      "Epoch: 106, Loss: 1124.56538, Residuals: -1.32609, Convergence: 0.013789\n",
      "Epoch: 107, Loss: 1111.34381, Residuals: -1.32970, Convergence: 0.011897\n",
      "Epoch: 108, Loss: 1099.95414, Residuals: -1.33205, Convergence: 0.010355\n",
      "Epoch: 109, Loss: 1090.04382, Residuals: -1.33330, Convergence: 0.009092\n",
      "Epoch: 110, Loss: 1081.32817, Residuals: -1.33357, Convergence: 0.008060\n",
      "Epoch: 111, Loss: 1073.57424, Residuals: -1.33295, Convergence: 0.007223\n",
      "Epoch: 112, Loss: 1066.58753, Residuals: -1.33154, Convergence: 0.006551\n",
      "Epoch: 113, Loss: 1060.20197, Residuals: -1.32940, Convergence: 0.006023\n",
      "Epoch: 114, Loss: 1054.27337, Residuals: -1.32657, Convergence: 0.005623\n",
      "Epoch: 115, Loss: 1048.67309, Residuals: -1.32310, Convergence: 0.005340\n",
      "Epoch: 116, Loss: 1043.28529, Residuals: -1.31900, Convergence: 0.005164\n",
      "Epoch: 117, Loss: 1038.00858, Residuals: -1.31427, Convergence: 0.005083\n",
      "Epoch: 118, Loss: 1032.75850, Residuals: -1.30893, Convergence: 0.005084\n",
      "Epoch: 119, Loss: 1027.48117, Residuals: -1.30302, Convergence: 0.005136\n",
      "Epoch: 120, Loss: 1022.16656, Residuals: -1.29658, Convergence: 0.005199\n",
      "Epoch: 121, Loss: 1016.85470, Residuals: -1.28970, Convergence: 0.005224\n",
      "Epoch: 122, Loss: 1011.62752, Residuals: -1.28247, Convergence: 0.005167\n",
      "Epoch: 123, Loss: 1006.57727, Residuals: -1.27498, Convergence: 0.005017\n",
      "Epoch: 124, Loss: 1001.77817, Residuals: -1.26731, Convergence: 0.004791\n",
      "Epoch: 125, Loss: 997.26981, Residuals: -1.25954, Convergence: 0.004521\n",
      "Epoch: 126, Loss: 993.06019, Residuals: -1.25174, Convergence: 0.004239\n",
      "Epoch: 127, Loss: 989.13694, Residuals: -1.24397, Convergence: 0.003966\n",
      "Epoch: 128, Loss: 985.47755, Residuals: -1.23626, Convergence: 0.003713\n",
      "Epoch: 129, Loss: 982.05604, Residuals: -1.22867, Convergence: 0.003484\n",
      "Epoch: 130, Loss: 978.84836, Residuals: -1.22121, Convergence: 0.003277\n",
      "Epoch: 131, Loss: 975.83270, Residuals: -1.21392, Convergence: 0.003090\n",
      "Epoch: 132, Loss: 972.99024, Residuals: -1.20682, Convergence: 0.002921\n",
      "Epoch: 133, Loss: 970.30580, Residuals: -1.19992, Convergence: 0.002767\n",
      "Epoch: 134, Loss: 967.76641, Residuals: -1.19324, Convergence: 0.002624\n",
      "Epoch: 135, Loss: 965.36169, Residuals: -1.18679, Convergence: 0.002491\n",
      "Epoch: 136, Loss: 963.08238, Residuals: -1.18058, Convergence: 0.002367\n",
      "Epoch: 137, Loss: 960.92082, Residuals: -1.17461, Convergence: 0.002249\n",
      "Epoch: 138, Loss: 958.86942, Residuals: -1.16889, Convergence: 0.002139\n",
      "Epoch: 139, Loss: 956.92198, Residuals: -1.16342, Convergence: 0.002035\n",
      "Epoch: 140, Loss: 955.07218, Residuals: -1.15819, Convergence: 0.001937\n",
      "Epoch: 141, Loss: 953.31370, Residuals: -1.15321, Convergence: 0.001845\n",
      "Epoch: 142, Loss: 951.64073, Residuals: -1.14847, Convergence: 0.001758\n",
      "Epoch: 143, Loss: 950.04712, Residuals: -1.14395, Convergence: 0.001677\n",
      "Epoch: 144, Loss: 948.52720, Residuals: -1.13966, Convergence: 0.001602\n",
      "Epoch: 145, Loss: 947.07569, Residuals: -1.13558, Convergence: 0.001533\n",
      "Epoch: 146, Loss: 945.68683, Residuals: -1.13171, Convergence: 0.001469\n",
      "Epoch: 147, Loss: 944.35584, Residuals: -1.12802, Convergence: 0.001409\n",
      "Epoch: 148, Loss: 943.07730, Residuals: -1.12453, Convergence: 0.001356\n",
      "Epoch: 149, Loss: 941.84732, Residuals: -1.12120, Convergence: 0.001306\n",
      "Epoch: 150, Loss: 940.66115, Residuals: -1.11803, Convergence: 0.001261\n",
      "Epoch: 151, Loss: 939.51483, Residuals: -1.11502, Convergence: 0.001220\n",
      "Epoch: 152, Loss: 938.40496, Residuals: -1.11215, Convergence: 0.001183\n",
      "Epoch: 153, Loss: 937.32787, Residuals: -1.10941, Convergence: 0.001149\n",
      "Epoch: 154, Loss: 936.28074, Residuals: -1.10679, Convergence: 0.001118\n",
      "Epoch: 155, Loss: 935.26061, Residuals: -1.10429, Convergence: 0.001091\n",
      "Epoch: 156, Loss: 934.26478, Residuals: -1.10189, Convergence: 0.001066\n",
      "Epoch: 157, Loss: 933.29083, Residuals: -1.09958, Convergence: 0.001044\n",
      "Epoch: 158, Loss: 932.33686, Residuals: -1.09737, Convergence: 0.001023\n",
      "Epoch: 159, Loss: 931.40025, Residuals: -1.09523, Convergence: 0.001006\n",
      "Epoch: 160, Loss: 930.47922, Residuals: -1.09317, Convergence: 0.000990\n",
      "Evidence 11043.924\n",
      "\n",
      "Epoch: 160, Evidence: 11043.92383, Convergence: 1.016660\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.78e-01\n",
      "Epoch: 160, Loss: 2336.58497, Residuals: -1.09317, Convergence:   inf\n",
      "Epoch: 161, Loss: 2296.95490, Residuals: -1.10267, Convergence: 0.017253\n",
      "Epoch: 162, Loss: 2268.84173, Residuals: -1.10247, Convergence: 0.012391\n",
      "Epoch: 163, Loss: 2245.16762, Residuals: -1.10099, Convergence: 0.010544\n",
      "Epoch: 164, Loss: 2224.90299, Residuals: -1.09902, Convergence: 0.009108\n",
      "Epoch: 165, Loss: 2207.40686, Residuals: -1.09671, Convergence: 0.007926\n",
      "Epoch: 166, Loss: 2192.19386, Residuals: -1.09414, Convergence: 0.006940\n",
      "Epoch: 167, Loss: 2178.87401, Residuals: -1.09136, Convergence: 0.006113\n",
      "Epoch: 168, Loss: 2167.12689, Residuals: -1.08842, Convergence: 0.005421\n",
      "Epoch: 169, Loss: 2156.68392, Residuals: -1.08536, Convergence: 0.004842\n",
      "Epoch: 170, Loss: 2147.31585, Residuals: -1.08220, Convergence: 0.004363\n",
      "Epoch: 171, Loss: 2138.82629, Residuals: -1.07895, Convergence: 0.003969\n",
      "Epoch: 172, Loss: 2131.04812, Residuals: -1.07562, Convergence: 0.003650\n",
      "Epoch: 173, Loss: 2123.85002, Residuals: -1.07221, Convergence: 0.003389\n",
      "Epoch: 174, Loss: 2117.13996, Residuals: -1.06872, Convergence: 0.003169\n",
      "Epoch: 175, Loss: 2110.86725, Residuals: -1.06518, Convergence: 0.002972\n",
      "Epoch: 176, Loss: 2105.00966, Residuals: -1.06162, Convergence: 0.002783\n",
      "Epoch: 177, Loss: 2099.55739, Residuals: -1.05809, Convergence: 0.002597\n",
      "Epoch: 178, Loss: 2094.50286, Residuals: -1.05464, Convergence: 0.002413\n",
      "Epoch: 179, Loss: 2089.83104, Residuals: -1.05129, Convergence: 0.002236\n",
      "Epoch: 180, Loss: 2085.52265, Residuals: -1.04808, Convergence: 0.002066\n",
      "Epoch: 181, Loss: 2081.55305, Residuals: -1.04503, Convergence: 0.001907\n",
      "Epoch: 182, Loss: 2077.89697, Residuals: -1.04214, Convergence: 0.001760\n",
      "Epoch: 183, Loss: 2074.52738, Residuals: -1.03941, Convergence: 0.001624\n",
      "Epoch: 184, Loss: 2071.41968, Residuals: -1.03686, Convergence: 0.001500\n",
      "Epoch: 185, Loss: 2068.54990, Residuals: -1.03447, Convergence: 0.001387\n",
      "Epoch: 186, Loss: 2065.89621, Residuals: -1.03223, Convergence: 0.001285\n",
      "Epoch: 187, Loss: 2063.43789, Residuals: -1.03014, Convergence: 0.001191\n",
      "Epoch: 188, Loss: 2061.15716, Residuals: -1.02818, Convergence: 0.001107\n",
      "Epoch: 189, Loss: 2059.03724, Residuals: -1.02635, Convergence: 0.001030\n",
      "Epoch: 190, Loss: 2057.06259, Residuals: -1.02464, Convergence: 0.000960\n",
      "Evidence 14218.964\n",
      "\n",
      "Epoch: 190, Evidence: 14218.96387, Convergence: 0.223296\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.40e-01\n",
      "Epoch: 190, Loss: 2465.09168, Residuals: -1.02464, Convergence:   inf\n",
      "Epoch: 191, Loss: 2450.38511, Residuals: -1.02194, Convergence: 0.006002\n",
      "Epoch: 192, Loss: 2438.46309, Residuals: -1.01851, Convergence: 0.004889\n",
      "Epoch: 193, Loss: 2428.27745, Residuals: -1.01516, Convergence: 0.004195\n",
      "Epoch: 194, Loss: 2419.50465, Residuals: -1.01198, Convergence: 0.003626\n",
      "Epoch: 195, Loss: 2411.90483, Residuals: -1.00902, Convergence: 0.003151\n",
      "Epoch: 196, Loss: 2405.28433, Residuals: -1.00627, Convergence: 0.002752\n",
      "Epoch: 197, Loss: 2399.48461, Residuals: -1.00374, Convergence: 0.002417\n",
      "Epoch: 198, Loss: 2394.37409, Residuals: -1.00140, Convergence: 0.002134\n",
      "Epoch: 199, Loss: 2389.84272, Residuals: -0.99925, Convergence: 0.001896\n",
      "Epoch: 200, Loss: 2385.79883, Residuals: -0.99726, Convergence: 0.001695\n",
      "Epoch: 201, Loss: 2382.16806, Residuals: -0.99543, Convergence: 0.001524\n",
      "Epoch: 202, Loss: 2378.88704, Residuals: -0.99373, Convergence: 0.001379\n",
      "Epoch: 203, Loss: 2375.90498, Residuals: -0.99216, Convergence: 0.001255\n",
      "Epoch: 204, Loss: 2373.17817, Residuals: -0.99070, Convergence: 0.001149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 205, Loss: 2370.67307, Residuals: -0.98934, Convergence: 0.001057\n",
      "Epoch: 206, Loss: 2368.36014, Residuals: -0.98807, Convergence: 0.000977\n",
      "Evidence 14648.306\n",
      "\n",
      "Epoch: 206, Evidence: 14648.30566, Convergence: 0.029310\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.36e-01\n",
      "Epoch: 206, Loss: 2470.55955, Residuals: -0.98807, Convergence:   inf\n",
      "Epoch: 207, Loss: 2463.78351, Residuals: -0.98488, Convergence: 0.002750\n",
      "Epoch: 208, Loss: 2458.20568, Residuals: -0.98193, Convergence: 0.002269\n",
      "Epoch: 209, Loss: 2453.47907, Residuals: -0.97933, Convergence: 0.001926\n",
      "Epoch: 210, Loss: 2449.41222, Residuals: -0.97706, Convergence: 0.001660\n",
      "Epoch: 211, Loss: 2445.86813, Residuals: -0.97506, Convergence: 0.001449\n",
      "Epoch: 212, Loss: 2442.73988, Residuals: -0.97330, Convergence: 0.001281\n",
      "Epoch: 213, Loss: 2439.94727, Residuals: -0.97175, Convergence: 0.001145\n",
      "Epoch: 214, Loss: 2437.42817, Residuals: -0.97037, Convergence: 0.001034\n",
      "Epoch: 215, Loss: 2435.13477, Residuals: -0.96915, Convergence: 0.000942\n",
      "Evidence 14736.363\n",
      "\n",
      "Epoch: 215, Evidence: 14736.36328, Convergence: 0.005976\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.63e-01\n",
      "Epoch: 215, Loss: 2471.99680, Residuals: -0.96915, Convergence:   inf\n",
      "Epoch: 216, Loss: 2468.01786, Residuals: -0.96660, Convergence: 0.001612\n",
      "Epoch: 217, Loss: 2464.71276, Residuals: -0.96447, Convergence: 0.001341\n",
      "Epoch: 218, Loss: 2461.87720, Residuals: -0.96271, Convergence: 0.001152\n",
      "Epoch: 219, Loss: 2459.39478, Residuals: -0.96122, Convergence: 0.001009\n",
      "Epoch: 220, Loss: 2457.18658, Residuals: -0.95997, Convergence: 0.000899\n",
      "Evidence 14768.188\n",
      "\n",
      "Epoch: 220, Evidence: 14768.18848, Convergence: 0.002155\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.12e-01\n",
      "Epoch: 220, Loss: 2472.87110, Residuals: -0.95997, Convergence:   inf\n",
      "Epoch: 221, Loss: 2470.02803, Residuals: -0.95797, Convergence: 0.001151\n",
      "Epoch: 222, Loss: 2467.63593, Residuals: -0.95637, Convergence: 0.000969\n",
      "Evidence 14780.909\n",
      "\n",
      "Epoch: 222, Evidence: 14780.90918, Convergence: 0.000861\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.76e-01\n",
      "Epoch: 222, Loss: 2473.55697, Residuals: -0.95637, Convergence:   inf\n",
      "Epoch: 223, Loss: 2469.03339, Residuals: -0.95364, Convergence: 0.001832\n",
      "Epoch: 224, Loss: 2465.58740, Residuals: -0.95174, Convergence: 0.001398\n",
      "Epoch: 225, Loss: 2462.77919, Residuals: -0.95046, Convergence: 0.001140\n",
      "Epoch: 226, Loss: 2460.38770, Residuals: -0.94972, Convergence: 0.000972\n",
      "Evidence 14798.741\n",
      "\n",
      "Epoch: 226, Evidence: 14798.74121, Convergence: 0.002065\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.45e-01\n",
      "Epoch: 226, Loss: 2473.76544, Residuals: -0.94972, Convergence:   inf\n",
      "Epoch: 227, Loss: 2470.79514, Residuals: -0.94746, Convergence: 0.001202\n",
      "Epoch: 228, Loss: 2468.43909, Residuals: -0.94634, Convergence: 0.000954\n",
      "Evidence 14809.596\n",
      "\n",
      "Epoch: 228, Evidence: 14809.59570, Convergence: 0.000733\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.23e-01\n",
      "Epoch: 228, Loss: 2473.99338, Residuals: -0.94634, Convergence:   inf\n",
      "Epoch: 229, Loss: 2469.71740, Residuals: -0.94329, Convergence: 0.001731\n",
      "Epoch: 230, Loss: 2466.59182, Residuals: -0.94442, Convergence: 0.001267\n",
      "Epoch: 231, Loss: 2463.95262, Residuals: -0.94497, Convergence: 0.001071\n",
      "Epoch: 232, Loss: 2461.67402, Residuals: -0.94750, Convergence: 0.000926\n",
      "Evidence 14825.115\n",
      "\n",
      "Epoch: 232, Evidence: 14825.11523, Convergence: 0.001779\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.09e-01\n",
      "Epoch: 232, Loss: 2473.32186, Residuals: -0.94750, Convergence:   inf\n",
      "Epoch: 233, Loss: 2471.74086, Residuals: -0.94570, Convergence: 0.000640\n",
      "Evidence 14831.550\n",
      "\n",
      "Epoch: 233, Evidence: 14831.54980, Convergence: 0.000434\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.85e-02\n",
      "Epoch: 233, Loss: 2474.29387, Residuals: -0.94570, Convergence:   inf\n",
      "Epoch: 234, Loss: 2515.90492, Residuals: -0.98443, Convergence: -0.016539\n",
      "Epoch: 234, Loss: 2472.21576, Residuals: -0.94335, Convergence: 0.000841\n",
      "Evidence 14835.673\n",
      "\n",
      "Epoch: 234, Evidence: 14835.67285, Convergence: 0.000712\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.12e-02\n",
      "Epoch: 234, Loss: 2473.72261, Residuals: -0.94335, Convergence:   inf\n",
      "Epoch: 235, Loss: 2478.68355, Residuals: -0.94541, Convergence: -0.002001\n",
      "Epoch: 235, Loss: 2473.76376, Residuals: -0.94198, Convergence: -0.000017\n",
      "Evidence 14837.193\n",
      "\n",
      "Epoch: 235, Evidence: 14837.19336, Convergence: 0.000814\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 381.74263, Residuals: -4.51383, Convergence:   inf\n",
      "Epoch: 1, Loss: 356.04520, Residuals: -4.39362, Convergence: 0.072175\n",
      "Epoch: 2, Loss: 334.98105, Residuals: -4.22948, Convergence: 0.062882\n",
      "Epoch: 3, Loss: 318.84320, Residuals: -4.06422, Convergence: 0.050614\n",
      "Epoch: 4, Loss: 306.54790, Residuals: -3.91881, Convergence: 0.040109\n",
      "Epoch: 5, Loss: 296.80266, Residuals: -3.79035, Convergence: 0.032834\n",
      "Epoch: 6, Loss: 288.89576, Residuals: -3.67836, Convergence: 0.027369\n",
      "Epoch: 7, Loss: 282.33629, Residuals: -3.58239, Convergence: 0.023233\n",
      "Epoch: 8, Loss: 276.75959, Residuals: -3.50044, Convergence: 0.020150\n",
      "Epoch: 9, Loss: 271.90782, Residuals: -3.43019, Convergence: 0.017843\n",
      "Epoch: 10, Loss: 267.59781, Residuals: -3.36955, Convergence: 0.016106\n",
      "Epoch: 11, Loss: 263.69710, Residuals: -3.31672, Convergence: 0.014792\n",
      "Epoch: 12, Loss: 260.10912, Residuals: -3.27018, Convergence: 0.013794\n",
      "Epoch: 13, Loss: 256.76371, Residuals: -3.22861, Convergence: 0.013029\n",
      "Epoch: 14, Loss: 253.61076, Residuals: -3.19084, Convergence: 0.012432\n",
      "Epoch: 15, Loss: 250.61725, Residuals: -3.15590, Convergence: 0.011945\n",
      "Epoch: 16, Loss: 247.76543, Residuals: -3.12311, Convergence: 0.011510\n",
      "Epoch: 17, Loss: 245.04425, Residuals: -3.09203, Convergence: 0.011105\n",
      "Epoch: 18, Loss: 242.43489, Residuals: -3.06228, Convergence: 0.010763\n",
      "Epoch: 19, Loss: 239.90629, Residuals: -3.03335, Convergence: 0.010540\n",
      "Epoch: 20, Loss: 237.42188, Residuals: -3.00468, Convergence: 0.010464\n",
      "Epoch: 21, Loss: 234.94854, Residuals: -2.97578, Convergence: 0.010527\n",
      "Epoch: 22, Loss: 232.46151, Residuals: -2.94628, Convergence: 0.010699\n",
      "Epoch: 23, Loss: 229.93788, Residuals: -2.91593, Convergence: 0.010975\n",
      "Epoch: 24, Loss: 227.33980, Residuals: -2.88432, Convergence: 0.011428\n",
      "Epoch: 25, Loss: 224.61020, Residuals: -2.85076, Convergence: 0.012153\n",
      "Epoch: 26, Loss: 221.71930, Residuals: -2.81484, Convergence: 0.013039\n",
      "Epoch: 27, Loss: 218.75712, Residuals: -2.77740, Convergence: 0.013541\n",
      "Epoch: 28, Loss: 215.85809, Residuals: -2.73995, Convergence: 0.013430\n",
      "Epoch: 29, Loss: 213.06279, Residuals: -2.70306, Convergence: 0.013120\n",
      "Epoch: 30, Loss: 210.35968, Residuals: -2.66671, Convergence: 0.012850\n",
      "Epoch: 31, Loss: 207.73133, Residuals: -2.63075, Convergence: 0.012653\n",
      "Epoch: 32, Loss: 205.16522, Residuals: -2.59506, Convergence: 0.012508\n",
      "Epoch: 33, Loss: 202.65430, Residuals: -2.55955, Convergence: 0.012390\n",
      "Epoch: 34, Loss: 200.19566, Residuals: -2.52418, Convergence: 0.012281\n",
      "Epoch: 35, Loss: 197.78914, Residuals: -2.48892, Convergence: 0.012167\n",
      "Epoch: 36, Loss: 195.43611, Residuals: -2.45376, Convergence: 0.012040\n",
      "Epoch: 37, Loss: 193.13864, Residuals: -2.41874, Convergence: 0.011895\n",
      "Epoch: 38, Loss: 190.89888, Residuals: -2.38385, Convergence: 0.011733\n",
      "Epoch: 39, Loss: 188.71879, Residuals: -2.34913, Convergence: 0.011552\n",
      "Epoch: 40, Loss: 186.59992, Residuals: -2.31461, Convergence: 0.011355\n",
      "Epoch: 41, Loss: 184.54352, Residuals: -2.28031, Convergence: 0.011143\n",
      "Epoch: 42, Loss: 182.55056, Residuals: -2.24626, Convergence: 0.010917\n",
      "Epoch: 43, Loss: 180.62192, Residuals: -2.21249, Convergence: 0.010678\n",
      "Epoch: 44, Loss: 178.75849, Residuals: -2.17904, Convergence: 0.010424\n",
      "Epoch: 45, Loss: 176.96128, Residuals: -2.14594, Convergence: 0.010156\n",
      "Epoch: 46, Loss: 175.23137, Residuals: -2.11324, Convergence: 0.009872\n",
      "Epoch: 47, Loss: 173.56982, Residuals: -2.08098, Convergence: 0.009573\n",
      "Epoch: 48, Loss: 171.97747, Residuals: -2.04922, Convergence: 0.009259\n",
      "Epoch: 49, Loss: 170.45481, Residuals: -2.01801, Convergence: 0.008933\n",
      "Epoch: 50, Loss: 169.00180, Residuals: -1.98738, Convergence: 0.008598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51, Loss: 167.61786, Residuals: -1.95739, Convergence: 0.008257\n",
      "Epoch: 52, Loss: 166.30185, Residuals: -1.92807, Convergence: 0.007913\n",
      "Epoch: 53, Loss: 165.05216, Residuals: -1.89944, Convergence: 0.007571\n",
      "Epoch: 54, Loss: 163.86671, Residuals: -1.87154, Convergence: 0.007234\n",
      "Epoch: 55, Loss: 162.74305, Residuals: -1.84439, Convergence: 0.006905\n",
      "Epoch: 56, Loss: 161.67841, Residuals: -1.81799, Convergence: 0.006585\n",
      "Epoch: 57, Loss: 160.66978, Residuals: -1.79236, Convergence: 0.006278\n",
      "Epoch: 58, Loss: 159.71397, Residuals: -1.76748, Convergence: 0.005984\n",
      "Epoch: 59, Loss: 158.80769, Residuals: -1.74336, Convergence: 0.005707\n",
      "Epoch: 60, Loss: 157.94763, Residuals: -1.71999, Convergence: 0.005445\n",
      "Epoch: 61, Loss: 157.13054, Residuals: -1.69734, Convergence: 0.005200\n",
      "Epoch: 62, Loss: 156.35327, Residuals: -1.67540, Convergence: 0.004971\n",
      "Epoch: 63, Loss: 155.61282, Residuals: -1.65416, Convergence: 0.004758\n",
      "Epoch: 64, Loss: 154.90632, Residuals: -1.63360, Convergence: 0.004561\n",
      "Epoch: 65, Loss: 154.23105, Residuals: -1.61368, Convergence: 0.004378\n",
      "Epoch: 66, Loss: 153.58443, Residuals: -1.59440, Convergence: 0.004210\n",
      "Epoch: 67, Loss: 152.96404, Residuals: -1.57573, Convergence: 0.004056\n",
      "Epoch: 68, Loss: 152.36771, Residuals: -1.55764, Convergence: 0.003914\n",
      "Epoch: 69, Loss: 151.79351, Residuals: -1.54011, Convergence: 0.003783\n",
      "Epoch: 70, Loss: 151.23983, Residuals: -1.52311, Convergence: 0.003661\n",
      "Epoch: 71, Loss: 150.70536, Residuals: -1.50663, Convergence: 0.003546\n",
      "Epoch: 72, Loss: 150.18914, Residuals: -1.49064, Convergence: 0.003437\n",
      "Epoch: 73, Loss: 149.69044, Residuals: -1.47514, Convergence: 0.003332\n",
      "Epoch: 74, Loss: 149.20875, Residuals: -1.46011, Convergence: 0.003228\n",
      "Epoch: 75, Loss: 148.74375, Residuals: -1.44555, Convergence: 0.003126\n",
      "Epoch: 76, Loss: 148.29517, Residuals: -1.43145, Convergence: 0.003025\n",
      "Epoch: 77, Loss: 147.86284, Residuals: -1.41781, Convergence: 0.002924\n",
      "Epoch: 78, Loss: 147.44660, Residuals: -1.40461, Convergence: 0.002823\n",
      "Epoch: 79, Loss: 147.04630, Residuals: -1.39186, Convergence: 0.002722\n",
      "Epoch: 80, Loss: 146.66177, Residuals: -1.37954, Convergence: 0.002622\n",
      "Epoch: 81, Loss: 146.29283, Residuals: -1.36766, Convergence: 0.002522\n",
      "Epoch: 82, Loss: 145.93927, Residuals: -1.35621, Convergence: 0.002423\n",
      "Epoch: 83, Loss: 145.60083, Residuals: -1.34518, Convergence: 0.002324\n",
      "Epoch: 84, Loss: 145.27724, Residuals: -1.33457, Convergence: 0.002227\n",
      "Epoch: 85, Loss: 144.96822, Residuals: -1.32436, Convergence: 0.002132\n",
      "Epoch: 86, Loss: 144.67345, Residuals: -1.31455, Convergence: 0.002038\n",
      "Epoch: 87, Loss: 144.39258, Residuals: -1.30513, Convergence: 0.001945\n",
      "Epoch: 88, Loss: 144.12527, Residuals: -1.29609, Convergence: 0.001855\n",
      "Epoch: 89, Loss: 143.87116, Residuals: -1.28743, Convergence: 0.001766\n",
      "Epoch: 90, Loss: 143.62988, Residuals: -1.27913, Convergence: 0.001680\n",
      "Epoch: 91, Loss: 143.40107, Residuals: -1.27119, Convergence: 0.001596\n",
      "Epoch: 92, Loss: 143.18436, Residuals: -1.26361, Convergence: 0.001514\n",
      "Epoch: 93, Loss: 142.97937, Residuals: -1.25636, Convergence: 0.001434\n",
      "Epoch: 94, Loss: 142.78572, Residuals: -1.24946, Convergence: 0.001356\n",
      "Epoch: 95, Loss: 142.60306, Residuals: -1.24289, Convergence: 0.001281\n",
      "Epoch: 96, Loss: 142.43095, Residuals: -1.23664, Convergence: 0.001208\n",
      "Epoch: 97, Loss: 142.26898, Residuals: -1.23072, Convergence: 0.001139\n",
      "Epoch: 98, Loss: 142.11665, Residuals: -1.22512, Convergence: 0.001072\n",
      "Epoch: 99, Loss: 141.97341, Residuals: -1.21983, Convergence: 0.001009\n",
      "Epoch: 100, Loss: 141.83862, Residuals: -1.21485, Convergence: 0.000950\n",
      "Evidence -183.642\n",
      "\n",
      "Epoch: 100, Evidence: -183.64189, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.25e-01\n",
      "Epoch: 100, Loss: 1376.18636, Residuals: -1.21485, Convergence:   inf\n",
      "Epoch: 101, Loss: 1312.69538, Residuals: -1.24310, Convergence: 0.048367\n",
      "Epoch: 102, Loss: 1264.25680, Residuals: -1.26564, Convergence: 0.038314\n",
      "Epoch: 103, Loss: 1227.58767, Residuals: -1.28209, Convergence: 0.029871\n",
      "Epoch: 104, Loss: 1199.02913, Residuals: -1.29381, Convergence: 0.023818\n",
      "Epoch: 105, Loss: 1175.94726, Residuals: -1.30257, Convergence: 0.019628\n",
      "Epoch: 106, Loss: 1156.82153, Residuals: -1.30934, Convergence: 0.016533\n",
      "Epoch: 107, Loss: 1140.72256, Residuals: -1.31455, Convergence: 0.014113\n",
      "Epoch: 108, Loss: 1127.01089, Residuals: -1.31843, Convergence: 0.012166\n",
      "Epoch: 109, Loss: 1115.20929, Residuals: -1.32115, Convergence: 0.010582\n",
      "Epoch: 110, Loss: 1104.94443, Residuals: -1.32282, Convergence: 0.009290\n",
      "Epoch: 111, Loss: 1095.91541, Residuals: -1.32354, Convergence: 0.008239\n",
      "Epoch: 112, Loss: 1087.87493, Residuals: -1.32339, Convergence: 0.007391\n",
      "Epoch: 113, Loss: 1080.61357, Residuals: -1.32243, Convergence: 0.006720\n",
      "Epoch: 114, Loss: 1073.94971, Residuals: -1.32072, Convergence: 0.006205\n",
      "Epoch: 115, Loss: 1067.72190, Residuals: -1.31827, Convergence: 0.005833\n",
      "Epoch: 116, Loss: 1061.78315, Residuals: -1.31510, Convergence: 0.005593\n",
      "Epoch: 117, Loss: 1056.00236, Residuals: -1.31122, Convergence: 0.005474\n",
      "Epoch: 118, Loss: 1050.27325, Residuals: -1.30666, Convergence: 0.005455\n",
      "Epoch: 119, Loss: 1044.53173, Residuals: -1.30146, Convergence: 0.005497\n",
      "Epoch: 120, Loss: 1038.77341, Residuals: -1.29569, Convergence: 0.005543\n",
      "Epoch: 121, Loss: 1033.05646, Residuals: -1.28945, Convergence: 0.005534\n",
      "Epoch: 122, Loss: 1027.47647, Residuals: -1.28282, Convergence: 0.005431\n",
      "Epoch: 123, Loss: 1022.12604, Residuals: -1.27590, Convergence: 0.005235\n",
      "Epoch: 124, Loss: 1017.06670, Residuals: -1.26876, Convergence: 0.004974\n",
      "Epoch: 125, Loss: 1012.32280, Residuals: -1.26147, Convergence: 0.004686\n",
      "Epoch: 126, Loss: 1007.89061, Residuals: -1.25410, Convergence: 0.004397\n",
      "Epoch: 127, Loss: 1003.75138, Residuals: -1.24671, Convergence: 0.004124\n",
      "Epoch: 128, Loss: 999.87903, Residuals: -1.23936, Convergence: 0.003873\n",
      "Epoch: 129, Loss: 996.24681, Residuals: -1.23207, Convergence: 0.003646\n",
      "Epoch: 130, Loss: 992.82928, Residuals: -1.22489, Convergence: 0.003442\n",
      "Epoch: 131, Loss: 989.60366, Residuals: -1.21784, Convergence: 0.003260\n",
      "Epoch: 132, Loss: 986.54981, Residuals: -1.21094, Convergence: 0.003095\n",
      "Epoch: 133, Loss: 983.65107, Residuals: -1.20421, Convergence: 0.002947\n",
      "Epoch: 134, Loss: 980.89250, Residuals: -1.19766, Convergence: 0.002812\n",
      "Epoch: 135, Loss: 978.26241, Residuals: -1.19130, Convergence: 0.002689\n",
      "Epoch: 136, Loss: 975.75028, Residuals: -1.18514, Convergence: 0.002575\n",
      "Epoch: 137, Loss: 973.34743, Residuals: -1.17918, Convergence: 0.002469\n",
      "Epoch: 138, Loss: 971.04600, Residuals: -1.17342, Convergence: 0.002370\n",
      "Epoch: 139, Loss: 968.84064, Residuals: -1.16787, Convergence: 0.002276\n",
      "Epoch: 140, Loss: 966.72499, Residuals: -1.16252, Convergence: 0.002188\n",
      "Epoch: 141, Loss: 964.69480, Residuals: -1.15738, Convergence: 0.002104\n",
      "Epoch: 142, Loss: 962.74556, Residuals: -1.15244, Convergence: 0.002025\n",
      "Epoch: 143, Loss: 960.87369, Residuals: -1.14769, Convergence: 0.001948\n",
      "Epoch: 144, Loss: 959.07569, Residuals: -1.14314, Convergence: 0.001875\n",
      "Epoch: 145, Loss: 957.34909, Residuals: -1.13877, Convergence: 0.001804\n",
      "Epoch: 146, Loss: 955.69030, Residuals: -1.13459, Convergence: 0.001736\n",
      "Epoch: 147, Loss: 954.09721, Residuals: -1.13059, Convergence: 0.001670\n",
      "Epoch: 148, Loss: 952.56710, Residuals: -1.12676, Convergence: 0.001606\n",
      "Epoch: 149, Loss: 951.09732, Residuals: -1.12310, Convergence: 0.001545\n",
      "Epoch: 150, Loss: 949.68573, Residuals: -1.11960, Convergence: 0.001486\n",
      "Epoch: 151, Loss: 948.32940, Residuals: -1.11625, Convergence: 0.001430\n",
      "Epoch: 152, Loss: 947.02542, Residuals: -1.11304, Convergence: 0.001377\n",
      "Epoch: 153, Loss: 945.77145, Residuals: -1.10998, Convergence: 0.001326\n",
      "Epoch: 154, Loss: 944.56413, Residuals: -1.10705, Convergence: 0.001278\n",
      "Epoch: 155, Loss: 943.40031, Residuals: -1.10424, Convergence: 0.001234\n",
      "Epoch: 156, Loss: 942.27671, Residuals: -1.10156, Convergence: 0.001192\n",
      "Epoch: 157, Loss: 941.18985, Residuals: -1.09897, Convergence: 0.001155\n",
      "Epoch: 158, Loss: 940.13581, Residuals: -1.09650, Convergence: 0.001121\n",
      "Epoch: 159, Loss: 939.11115, Residuals: -1.09411, Convergence: 0.001091\n",
      "Epoch: 160, Loss: 938.11173, Residuals: -1.09180, Convergence: 0.001065\n",
      "Epoch: 161, Loss: 937.13303, Residuals: -1.08957, Convergence: 0.001044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 162, Loss: 936.17103, Residuals: -1.08739, Convergence: 0.001028\n",
      "Epoch: 163, Loss: 935.22174, Residuals: -1.08527, Convergence: 0.001015\n",
      "Epoch: 164, Loss: 934.28131, Residuals: -1.08319, Convergence: 0.001007\n",
      "Epoch: 165, Loss: 933.34633, Residuals: -1.08114, Convergence: 0.001002\n",
      "Epoch: 166, Loss: 932.41404, Residuals: -1.07912, Convergence: 0.001000\n",
      "Evidence 11194.988\n",
      "\n",
      "Epoch: 166, Evidence: 11194.98828, Convergence: 1.016404\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.77e-01\n",
      "Epoch: 166, Loss: 2352.89647, Residuals: -1.07912, Convergence:   inf\n",
      "Epoch: 167, Loss: 2314.81053, Residuals: -1.08687, Convergence: 0.016453\n",
      "Epoch: 168, Loss: 2288.43085, Residuals: -1.08583, Convergence: 0.011527\n",
      "Epoch: 169, Loss: 2266.32044, Residuals: -1.08386, Convergence: 0.009756\n",
      "Epoch: 170, Loss: 2247.55853, Residuals: -1.08150, Convergence: 0.008348\n",
      "Epoch: 171, Loss: 2231.49209, Residuals: -1.07889, Convergence: 0.007200\n",
      "Epoch: 172, Loss: 2217.61712, Residuals: -1.07611, Convergence: 0.006257\n",
      "Epoch: 173, Loss: 2205.52490, Residuals: -1.07321, Convergence: 0.005483\n",
      "Epoch: 174, Loss: 2194.87726, Residuals: -1.07020, Convergence: 0.004851\n",
      "Epoch: 175, Loss: 2185.39513, Residuals: -1.06708, Convergence: 0.004339\n",
      "Epoch: 176, Loss: 2176.85337, Residuals: -1.06385, Convergence: 0.003924\n",
      "Epoch: 177, Loss: 2169.08345, Residuals: -1.06051, Convergence: 0.003582\n",
      "Epoch: 178, Loss: 2161.96881, Residuals: -1.05707, Convergence: 0.003291\n",
      "Epoch: 179, Loss: 2155.43174, Residuals: -1.05357, Convergence: 0.003033\n",
      "Epoch: 180, Loss: 2149.41724, Residuals: -1.05006, Convergence: 0.002798\n",
      "Epoch: 181, Loss: 2143.88008, Residuals: -1.04657, Convergence: 0.002583\n",
      "Epoch: 182, Loss: 2138.77670, Residuals: -1.04315, Convergence: 0.002386\n",
      "Epoch: 183, Loss: 2134.06529, Residuals: -1.03982, Convergence: 0.002208\n",
      "Epoch: 184, Loss: 2129.70584, Residuals: -1.03660, Convergence: 0.002047\n",
      "Epoch: 185, Loss: 2125.66235, Residuals: -1.03351, Convergence: 0.001902\n",
      "Epoch: 186, Loss: 2121.90175, Residuals: -1.03054, Convergence: 0.001772\n",
      "Epoch: 187, Loss: 2118.39673, Residuals: -1.02771, Convergence: 0.001655\n",
      "Epoch: 188, Loss: 2115.12358, Residuals: -1.02502, Convergence: 0.001547\n",
      "Epoch: 189, Loss: 2112.06268, Residuals: -1.02247, Convergence: 0.001449\n",
      "Epoch: 190, Loss: 2109.19694, Residuals: -1.02005, Convergence: 0.001359\n",
      "Epoch: 191, Loss: 2106.51129, Residuals: -1.01776, Convergence: 0.001275\n",
      "Epoch: 192, Loss: 2103.99312, Residuals: -1.01559, Convergence: 0.001197\n",
      "Epoch: 193, Loss: 2101.63049, Residuals: -1.01353, Convergence: 0.001124\n",
      "Epoch: 194, Loss: 2099.41177, Residuals: -1.01159, Convergence: 0.001057\n",
      "Epoch: 195, Loss: 2097.32752, Residuals: -1.00974, Convergence: 0.000994\n",
      "Evidence 14321.018\n",
      "\n",
      "Epoch: 195, Evidence: 14321.01758, Convergence: 0.218283\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.41e-01\n",
      "Epoch: 195, Loss: 2476.21988, Residuals: -1.00974, Convergence:   inf\n",
      "Epoch: 196, Loss: 2462.66622, Residuals: -1.00666, Convergence: 0.005504\n",
      "Epoch: 197, Loss: 2451.63796, Residuals: -1.00321, Convergence: 0.004498\n",
      "Epoch: 198, Loss: 2442.15204, Residuals: -0.99982, Convergence: 0.003884\n",
      "Epoch: 199, Loss: 2433.92560, Residuals: -0.99662, Convergence: 0.003380\n",
      "Epoch: 200, Loss: 2426.75040, Residuals: -0.99364, Convergence: 0.002957\n",
      "Epoch: 201, Loss: 2420.46200, Residuals: -0.99088, Convergence: 0.002598\n",
      "Epoch: 202, Loss: 2414.92282, Residuals: -0.98834, Convergence: 0.002294\n",
      "Epoch: 203, Loss: 2410.01875, Residuals: -0.98599, Convergence: 0.002035\n",
      "Epoch: 204, Loss: 2405.65193, Residuals: -0.98382, Convergence: 0.001815\n",
      "Epoch: 205, Loss: 2401.74193, Residuals: -0.98181, Convergence: 0.001628\n",
      "Epoch: 206, Loss: 2398.22111, Residuals: -0.97996, Convergence: 0.001468\n",
      "Epoch: 207, Loss: 2395.03340, Residuals: -0.97824, Convergence: 0.001331\n",
      "Epoch: 208, Loss: 2392.13115, Residuals: -0.97664, Convergence: 0.001213\n",
      "Epoch: 209, Loss: 2389.47561, Residuals: -0.97515, Convergence: 0.001111\n",
      "Epoch: 210, Loss: 2387.03530, Residuals: -0.97376, Convergence: 0.001022\n",
      "Epoch: 211, Loss: 2384.78178, Residuals: -0.97246, Convergence: 0.000945\n",
      "Evidence 14697.834\n",
      "\n",
      "Epoch: 211, Evidence: 14697.83398, Convergence: 0.025638\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.38e-01\n",
      "Epoch: 211, Loss: 2481.48389, Residuals: -0.97246, Convergence:   inf\n",
      "Epoch: 212, Loss: 2474.90662, Residuals: -0.96929, Convergence: 0.002658\n",
      "Epoch: 213, Loss: 2469.46931, Residuals: -0.96647, Convergence: 0.002202\n",
      "Epoch: 214, Loss: 2464.85157, Residuals: -0.96404, Convergence: 0.001873\n",
      "Epoch: 215, Loss: 2460.87674, Residuals: -0.96194, Convergence: 0.001615\n",
      "Epoch: 216, Loss: 2457.41196, Residuals: -0.96013, Convergence: 0.001410\n",
      "Epoch: 217, Loss: 2454.35743, Residuals: -0.95855, Convergence: 0.001245\n",
      "Epoch: 218, Loss: 2451.63414, Residuals: -0.95716, Convergence: 0.001111\n",
      "Epoch: 219, Loss: 2449.18197, Residuals: -0.95595, Convergence: 0.001001\n",
      "Epoch: 220, Loss: 2446.95337, Residuals: -0.95487, Convergence: 0.000911\n",
      "Evidence 14783.301\n",
      "\n",
      "Epoch: 220, Evidence: 14783.30078, Convergence: 0.005781\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.65e-01\n",
      "Epoch: 220, Loss: 2483.09192, Residuals: -0.95487, Convergence:   inf\n",
      "Epoch: 221, Loss: 2479.16767, Residuals: -0.95252, Convergence: 0.001583\n",
      "Epoch: 222, Loss: 2475.90632, Residuals: -0.95064, Convergence: 0.001317\n",
      "Epoch: 223, Loss: 2473.11622, Residuals: -0.94914, Convergence: 0.001128\n",
      "Epoch: 224, Loss: 2470.68307, Residuals: -0.94791, Convergence: 0.000985\n",
      "Evidence 14812.789\n",
      "\n",
      "Epoch: 224, Evidence: 14812.78906, Convergence: 0.001991\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.14e-01\n",
      "Epoch: 224, Loss: 2484.21423, Residuals: -0.94791, Convergence:   inf\n",
      "Epoch: 225, Loss: 2481.24039, Residuals: -0.94615, Convergence: 0.001199\n",
      "Epoch: 226, Loss: 2478.75793, Residuals: -0.94483, Convergence: 0.001001\n",
      "Epoch: 227, Loss: 2476.61644, Residuals: -0.94379, Convergence: 0.000865\n",
      "Evidence 14827.673\n",
      "\n",
      "Epoch: 227, Evidence: 14827.67285, Convergence: 0.001004\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.76e-01\n",
      "Epoch: 227, Loss: 2484.97395, Residuals: -0.94379, Convergence:   inf\n",
      "Epoch: 228, Loss: 2482.59834, Residuals: -0.94245, Convergence: 0.000957\n",
      "Evidence 14834.347\n",
      "\n",
      "Epoch: 228, Evidence: 14834.34668, Convergence: 0.000450\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.50e-01\n",
      "Epoch: 228, Loss: 2485.53640, Residuals: -0.94245, Convergence:   inf\n",
      "Epoch: 229, Loss: 2481.46601, Residuals: -0.94095, Convergence: 0.001640\n",
      "Epoch: 230, Loss: 2478.38780, Residuals: -0.94001, Convergence: 0.001242\n",
      "Epoch: 231, Loss: 2475.87790, Residuals: -0.93934, Convergence: 0.001014\n",
      "Epoch: 232, Loss: 2473.74005, Residuals: -0.93901, Convergence: 0.000864\n",
      "Evidence 14849.717\n",
      "\n",
      "Epoch: 232, Evidence: 14849.71680, Convergence: 0.001484\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.26e-01\n",
      "Epoch: 232, Loss: 2485.80505, Residuals: -0.93901, Convergence:   inf\n",
      "Epoch: 233, Loss: 2483.10360, Residuals: -0.93786, Convergence: 0.001088\n",
      "Epoch: 234, Loss: 2480.94175, Residuals: -0.93742, Convergence: 0.000871\n",
      "Evidence 14859.363\n",
      "\n",
      "Epoch: 234, Evidence: 14859.36328, Convergence: 0.000649\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.08e-01\n",
      "Epoch: 234, Loss: 2485.97072, Residuals: -0.93742, Convergence:   inf\n",
      "Epoch: 235, Loss: 2481.95201, Residuals: -0.93660, Convergence: 0.001619\n",
      "Epoch: 236, Loss: 2479.00005, Residuals: -0.93789, Convergence: 0.001191\n",
      "Epoch: 237, Loss: 2476.57735, Residuals: -0.93784, Convergence: 0.000978\n",
      "Evidence 14871.716\n",
      "\n",
      "Epoch: 237, Evidence: 14871.71582, Convergence: 0.001479\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.93e-02\n",
      "Epoch: 237, Loss: 2485.98808, Residuals: -0.93784, Convergence:   inf\n",
      "Epoch: 238, Loss: 2483.31554, Residuals: -0.93767, Convergence: 0.001076\n",
      "Epoch: 239, Loss: 2482.23161, Residuals: -0.93992, Convergence: 0.000437\n",
      "Evidence 14879.077\n",
      "\n",
      "Epoch: 239, Evidence: 14879.07715, Convergence: 0.000495\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.35e-02\n",
      "Epoch: 239, Loss: 2486.00221, Residuals: -0.93992, Convergence:   inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 240, Loss: 2539.29186, Residuals: -0.98120, Convergence: -0.020986\n",
      "Epoch: 240, Loss: 2483.57963, Residuals: -0.93858, Convergence: 0.000975\n",
      "Evidence 14883.926\n",
      "\n",
      "Epoch: 240, Evidence: 14883.92578, Convergence: 0.000820\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 6.73e-02\n",
      "Epoch: 240, Loss: 2485.74541, Residuals: -0.93858, Convergence:   inf\n",
      "Epoch: 241, Loss: 2486.87831, Residuals: -0.94091, Convergence: -0.000456\n",
      "Evidence 14883.881\n",
      "\n",
      "Epoch: 241, Evidence: 14883.88086, Convergence: 0.000817\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 380.74305, Residuals: -4.50438, Convergence:   inf\n",
      "Epoch: 1, Loss: 355.01733, Residuals: -4.38392, Convergence: 0.072463\n",
      "Epoch: 2, Loss: 334.03017, Residuals: -4.22078, Convergence: 0.062830\n",
      "Epoch: 3, Loss: 317.95977, Residuals: -4.05680, Convergence: 0.050542\n",
      "Epoch: 4, Loss: 305.69748, Residuals: -3.91212, Convergence: 0.040112\n",
      "Epoch: 5, Loss: 295.96883, Residuals: -3.78415, Convergence: 0.032871\n",
      "Epoch: 6, Loss: 288.06877, Residuals: -3.67263, Convergence: 0.027424\n",
      "Epoch: 7, Loss: 281.50978, Residuals: -3.57712, Convergence: 0.023299\n",
      "Epoch: 8, Loss: 275.93040, Residuals: -3.49558, Convergence: 0.020220\n",
      "Epoch: 9, Loss: 271.07523, Residuals: -3.42569, Convergence: 0.017911\n",
      "Epoch: 10, Loss: 266.76265, Residuals: -3.36533, Convergence: 0.016166\n",
      "Epoch: 11, Loss: 262.86126, Residuals: -3.31274, Convergence: 0.014842\n",
      "Epoch: 12, Loss: 259.27558, Residuals: -3.26638, Convergence: 0.013830\n",
      "Epoch: 13, Loss: 255.93658, Residuals: -3.22495, Convergence: 0.013046\n",
      "Epoch: 14, Loss: 252.79516, Residuals: -3.18729, Convergence: 0.012427\n",
      "Epoch: 15, Loss: 249.81809, Residuals: -3.15243, Convergence: 0.011917\n",
      "Epoch: 16, Loss: 246.98606, Residuals: -3.11967, Convergence: 0.011466\n",
      "Epoch: 17, Loss: 244.28695, Residuals: -3.08858, Convergence: 0.011049\n",
      "Epoch: 18, Loss: 241.70303, Residuals: -3.05880, Convergence: 0.010690\n",
      "Epoch: 19, Loss: 239.20517, Residuals: -3.02988, Convergence: 0.010442\n",
      "Epoch: 20, Loss: 236.75835, Residuals: -3.00127, Convergence: 0.010335\n",
      "Epoch: 21, Loss: 234.33049, Residuals: -2.97246, Convergence: 0.010361\n",
      "Epoch: 22, Loss: 231.89809, Residuals: -2.94311, Convergence: 0.010489\n",
      "Epoch: 23, Loss: 229.44083, Residuals: -2.91297, Convergence: 0.010710\n",
      "Epoch: 24, Loss: 226.92581, Residuals: -2.88170, Convergence: 0.011083\n",
      "Epoch: 25, Loss: 224.30055, Residuals: -2.84870, Convergence: 0.011704\n",
      "Epoch: 26, Loss: 221.52506, Residuals: -2.81344, Convergence: 0.012529\n",
      "Epoch: 27, Loss: 218.65708, Residuals: -2.77647, Convergence: 0.013116\n",
      "Epoch: 28, Loss: 215.82734, Residuals: -2.73925, Convergence: 0.013111\n",
      "Epoch: 29, Loss: 213.09490, Residuals: -2.70256, Convergence: 0.012823\n",
      "Epoch: 30, Loss: 210.45639, Residuals: -2.66652, Convergence: 0.012537\n",
      "Epoch: 31, Loss: 207.89590, Residuals: -2.63100, Convergence: 0.012316\n",
      "Epoch: 32, Loss: 205.40020, Residuals: -2.59589, Convergence: 0.012150\n",
      "Epoch: 33, Loss: 202.96059, Residuals: -2.56108, Convergence: 0.012020\n",
      "Epoch: 34, Loss: 200.57218, Residuals: -2.52651, Convergence: 0.011908\n",
      "Epoch: 35, Loss: 198.23273, Residuals: -2.49210, Convergence: 0.011802\n",
      "Epoch: 36, Loss: 195.94184, Residuals: -2.45783, Convergence: 0.011692\n",
      "Epoch: 37, Loss: 193.70016, Residuals: -2.42367, Convergence: 0.011573\n",
      "Epoch: 38, Loss: 191.50895, Residuals: -2.38963, Convergence: 0.011442\n",
      "Epoch: 39, Loss: 189.36973, Residuals: -2.35569, Convergence: 0.011297\n",
      "Epoch: 40, Loss: 187.28415, Residuals: -2.32188, Convergence: 0.011136\n",
      "Epoch: 41, Loss: 185.25390, Residuals: -2.28821, Convergence: 0.010959\n",
      "Epoch: 42, Loss: 183.28075, Residuals: -2.25471, Convergence: 0.010766\n",
      "Epoch: 43, Loss: 181.36659, Residuals: -2.22143, Convergence: 0.010554\n",
      "Epoch: 44, Loss: 179.51353, Residuals: -2.18840, Convergence: 0.010323\n",
      "Epoch: 45, Loss: 177.72375, Residuals: -2.15568, Convergence: 0.010071\n",
      "Epoch: 46, Loss: 175.99946, Residuals: -2.12333, Convergence: 0.009797\n",
      "Epoch: 47, Loss: 174.34253, Residuals: -2.09142, Convergence: 0.009504\n",
      "Epoch: 48, Loss: 172.75430, Residuals: -2.06001, Convergence: 0.009194\n",
      "Epoch: 49, Loss: 171.23537, Residuals: -2.02916, Convergence: 0.008870\n",
      "Epoch: 50, Loss: 169.78550, Residuals: -1.99893, Convergence: 0.008539\n",
      "Epoch: 51, Loss: 168.40362, Residuals: -1.96935, Convergence: 0.008206\n",
      "Epoch: 52, Loss: 167.08787, Residuals: -1.94047, Convergence: 0.007875\n",
      "Epoch: 53, Loss: 165.83573, Residuals: -1.91230, Convergence: 0.007550\n",
      "Epoch: 54, Loss: 164.64416, Residuals: -1.88488, Convergence: 0.007237\n",
      "Epoch: 55, Loss: 163.50966, Residuals: -1.85819, Convergence: 0.006938\n",
      "Epoch: 56, Loss: 162.42850, Residuals: -1.83225, Convergence: 0.006656\n",
      "Epoch: 57, Loss: 161.39682, Residuals: -1.80702, Convergence: 0.006392\n",
      "Epoch: 58, Loss: 160.41076, Residuals: -1.78250, Convergence: 0.006147\n",
      "Epoch: 59, Loss: 159.46670, Residuals: -1.75866, Convergence: 0.005920\n",
      "Epoch: 60, Loss: 158.56130, Residuals: -1.73547, Convergence: 0.005710\n",
      "Epoch: 61, Loss: 157.69170, Residuals: -1.71290, Convergence: 0.005515\n",
      "Epoch: 62, Loss: 156.85549, Residuals: -1.69094, Convergence: 0.005331\n",
      "Epoch: 63, Loss: 156.05076, Residuals: -1.66955, Convergence: 0.005157\n",
      "Epoch: 64, Loss: 155.27601, Residuals: -1.64873, Convergence: 0.004989\n",
      "Epoch: 65, Loss: 154.53010, Residuals: -1.62847, Convergence: 0.004827\n",
      "Epoch: 66, Loss: 153.81212, Residuals: -1.60876, Convergence: 0.004668\n",
      "Epoch: 67, Loss: 153.12136, Residuals: -1.58961, Convergence: 0.004511\n",
      "Epoch: 68, Loss: 152.45719, Residuals: -1.57100, Convergence: 0.004356\n",
      "Epoch: 69, Loss: 151.81905, Residuals: -1.55293, Convergence: 0.004203\n",
      "Epoch: 70, Loss: 151.20638, Residuals: -1.53542, Convergence: 0.004052\n",
      "Epoch: 71, Loss: 150.61863, Residuals: -1.51844, Convergence: 0.003902\n",
      "Epoch: 72, Loss: 150.05521, Residuals: -1.50201, Convergence: 0.003755\n",
      "Epoch: 73, Loss: 149.51553, Residuals: -1.48611, Convergence: 0.003610\n",
      "Epoch: 74, Loss: 148.99894, Residuals: -1.47074, Convergence: 0.003467\n",
      "Epoch: 75, Loss: 148.50480, Residuals: -1.45589, Convergence: 0.003327\n",
      "Epoch: 76, Loss: 148.03242, Residuals: -1.44155, Convergence: 0.003191\n",
      "Epoch: 77, Loss: 147.58110, Residuals: -1.42772, Convergence: 0.003058\n",
      "Epoch: 78, Loss: 147.15013, Residuals: -1.41439, Convergence: 0.002929\n",
      "Epoch: 79, Loss: 146.73875, Residuals: -1.40153, Convergence: 0.002803\n",
      "Epoch: 80, Loss: 146.34624, Residuals: -1.38916, Convergence: 0.002682\n",
      "Epoch: 81, Loss: 145.97186, Residuals: -1.37724, Convergence: 0.002565\n",
      "Epoch: 82, Loss: 145.61485, Residuals: -1.36577, Convergence: 0.002452\n",
      "Epoch: 83, Loss: 145.27448, Residuals: -1.35474, Convergence: 0.002343\n",
      "Epoch: 84, Loss: 144.95002, Residuals: -1.34413, Convergence: 0.002238\n",
      "Epoch: 85, Loss: 144.64076, Residuals: -1.33393, Convergence: 0.002138\n",
      "Epoch: 86, Loss: 144.34601, Residuals: -1.32413, Convergence: 0.002042\n",
      "Epoch: 87, Loss: 144.06510, Residuals: -1.31471, Convergence: 0.001950\n",
      "Epoch: 88, Loss: 143.79738, Residuals: -1.30567, Convergence: 0.001862\n",
      "Epoch: 89, Loss: 143.54224, Residuals: -1.29698, Convergence: 0.001777\n",
      "Epoch: 90, Loss: 143.29911, Residuals: -1.28864, Convergence: 0.001697\n",
      "Epoch: 91, Loss: 143.06742, Residuals: -1.28063, Convergence: 0.001619\n",
      "Epoch: 92, Loss: 142.84669, Residuals: -1.27294, Convergence: 0.001545\n",
      "Epoch: 93, Loss: 142.63642, Residuals: -1.26556, Convergence: 0.001474\n",
      "Epoch: 94, Loss: 142.43620, Residuals: -1.25847, Convergence: 0.001406\n",
      "Epoch: 95, Loss: 142.24561, Residuals: -1.25168, Convergence: 0.001340\n",
      "Epoch: 96, Loss: 142.06430, Residuals: -1.24515, Convergence: 0.001276\n",
      "Epoch: 97, Loss: 141.89195, Residuals: -1.23890, Convergence: 0.001215\n",
      "Epoch: 98, Loss: 141.72825, Residuals: -1.23290, Convergence: 0.001155\n",
      "Epoch: 99, Loss: 141.57295, Residuals: -1.22716, Convergence: 0.001097\n",
      "Epoch: 100, Loss: 141.42580, Residuals: -1.22165, Convergence: 0.001040\n",
      "Epoch: 101, Loss: 141.28659, Residuals: -1.21638, Convergence: 0.000985\n",
      "Evidence -182.800\n",
      "\n",
      "Epoch: 101, Evidence: -182.80028, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.25e-01\n",
      "Epoch: 101, Loss: 1367.81385, Residuals: -1.21638, Convergence:   inf\n",
      "Epoch: 102, Loss: 1306.02406, Residuals: -1.24639, Convergence: 0.047311\n",
      "Epoch: 103, Loss: 1258.88541, Residuals: -1.27007, Convergence: 0.037445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 104, Loss: 1223.16801, Residuals: -1.28748, Convergence: 0.029201\n",
      "Epoch: 105, Loss: 1195.41813, Residuals: -1.30007, Convergence: 0.023214\n",
      "Epoch: 106, Loss: 1173.06166, Residuals: -1.30954, Convergence: 0.019058\n",
      "Epoch: 107, Loss: 1154.55831, Residuals: -1.31690, Convergence: 0.016026\n",
      "Epoch: 108, Loss: 1138.97442, Residuals: -1.32260, Convergence: 0.013682\n",
      "Epoch: 109, Loss: 1125.67812, Residuals: -1.32687, Convergence: 0.011812\n",
      "Epoch: 110, Loss: 1114.20326, Residuals: -1.32987, Convergence: 0.010299\n",
      "Epoch: 111, Loss: 1104.18641, Residuals: -1.33170, Convergence: 0.009072\n",
      "Epoch: 112, Loss: 1095.33493, Residuals: -1.33248, Convergence: 0.008081\n",
      "Epoch: 113, Loss: 1087.40763, Residuals: -1.33229, Convergence: 0.007290\n",
      "Epoch: 114, Loss: 1080.20088, Residuals: -1.33120, Convergence: 0.006672\n",
      "Epoch: 115, Loss: 1073.54016, Residuals: -1.32927, Convergence: 0.006204\n",
      "Epoch: 116, Loss: 1067.27415, Residuals: -1.32653, Convergence: 0.005871\n",
      "Epoch: 117, Loss: 1061.27034, Residuals: -1.32301, Convergence: 0.005657\n",
      "Epoch: 118, Loss: 1055.41552, Residuals: -1.31873, Convergence: 0.005547\n",
      "Epoch: 119, Loss: 1049.61950, Residuals: -1.31371, Convergence: 0.005522\n",
      "Epoch: 120, Loss: 1043.82280, Residuals: -1.30800, Convergence: 0.005553\n",
      "Epoch: 121, Loss: 1038.00856, Residuals: -1.30165, Convergence: 0.005601\n",
      "Epoch: 122, Loss: 1032.20973, Residuals: -1.29477, Convergence: 0.005618\n",
      "Epoch: 123, Loss: 1026.50161, Residuals: -1.28745, Convergence: 0.005561\n",
      "Epoch: 124, Loss: 1020.97640, Residuals: -1.27982, Convergence: 0.005412\n",
      "Epoch: 125, Loss: 1015.71214, Residuals: -1.27197, Convergence: 0.005183\n",
      "Epoch: 126, Loss: 1010.75420, Residuals: -1.26399, Convergence: 0.004905\n",
      "Epoch: 127, Loss: 1006.11499, Residuals: -1.25596, Convergence: 0.004611\n",
      "Epoch: 128, Loss: 1001.78367, Residuals: -1.24795, Convergence: 0.004324\n",
      "Epoch: 129, Loss: 997.73838, Residuals: -1.24001, Convergence: 0.004054\n",
      "Epoch: 130, Loss: 993.95212, Residuals: -1.23218, Convergence: 0.003809\n",
      "Epoch: 131, Loss: 990.39944, Residuals: -1.22450, Convergence: 0.003587\n",
      "Epoch: 132, Loss: 987.05618, Residuals: -1.21698, Convergence: 0.003387\n",
      "Epoch: 133, Loss: 983.90190, Residuals: -1.20966, Convergence: 0.003206\n",
      "Epoch: 134, Loss: 980.91826, Residuals: -1.20254, Convergence: 0.003042\n",
      "Epoch: 135, Loss: 978.09043, Residuals: -1.19563, Convergence: 0.002891\n",
      "Epoch: 136, Loss: 975.40549, Residuals: -1.18896, Convergence: 0.002753\n",
      "Epoch: 137, Loss: 972.85154, Residuals: -1.18251, Convergence: 0.002625\n",
      "Epoch: 138, Loss: 970.41972, Residuals: -1.17630, Convergence: 0.002506\n",
      "Epoch: 139, Loss: 968.10128, Residuals: -1.17033, Convergence: 0.002395\n",
      "Epoch: 140, Loss: 965.88890, Residuals: -1.16460, Convergence: 0.002291\n",
      "Epoch: 141, Loss: 963.77605, Residuals: -1.15910, Convergence: 0.002192\n",
      "Epoch: 142, Loss: 961.75613, Residuals: -1.15384, Convergence: 0.002100\n",
      "Epoch: 143, Loss: 959.82458, Residuals: -1.14880, Convergence: 0.002012\n",
      "Epoch: 144, Loss: 957.97598, Residuals: -1.14399, Convergence: 0.001930\n",
      "Epoch: 145, Loss: 956.20574, Residuals: -1.13939, Convergence: 0.001851\n",
      "Epoch: 146, Loss: 954.51013, Residuals: -1.13500, Convergence: 0.001776\n",
      "Epoch: 147, Loss: 952.88524, Residuals: -1.13081, Convergence: 0.001705\n",
      "Epoch: 148, Loss: 951.32746, Residuals: -1.12681, Convergence: 0.001637\n",
      "Epoch: 149, Loss: 949.83349, Residuals: -1.12299, Convergence: 0.001573\n",
      "Epoch: 150, Loss: 948.39986, Residuals: -1.11935, Convergence: 0.001512\n",
      "Epoch: 151, Loss: 947.02401, Residuals: -1.11587, Convergence: 0.001453\n",
      "Epoch: 152, Loss: 945.70264, Residuals: -1.11255, Convergence: 0.001397\n",
      "Epoch: 153, Loss: 944.43237, Residuals: -1.10938, Convergence: 0.001345\n",
      "Epoch: 154, Loss: 943.21035, Residuals: -1.10635, Convergence: 0.001296\n",
      "Epoch: 155, Loss: 942.03301, Residuals: -1.10345, Convergence: 0.001250\n",
      "Epoch: 156, Loss: 940.89710, Residuals: -1.10067, Convergence: 0.001207\n",
      "Epoch: 157, Loss: 939.79919, Residuals: -1.09801, Convergence: 0.001168\n",
      "Epoch: 158, Loss: 938.73524, Residuals: -1.09545, Convergence: 0.001133\n",
      "Epoch: 159, Loss: 937.70174, Residuals: -1.09299, Convergence: 0.001102\n",
      "Epoch: 160, Loss: 936.69464, Residuals: -1.09061, Convergence: 0.001075\n",
      "Epoch: 161, Loss: 935.70980, Residuals: -1.08831, Convergence: 0.001053\n",
      "Epoch: 162, Loss: 934.74297, Residuals: -1.08607, Convergence: 0.001034\n",
      "Epoch: 163, Loss: 933.79010, Residuals: -1.08389, Convergence: 0.001020\n",
      "Epoch: 164, Loss: 932.84726, Residuals: -1.08175, Convergence: 0.001011\n",
      "Epoch: 165, Loss: 931.91095, Residuals: -1.07966, Convergence: 0.001005\n",
      "Epoch: 166, Loss: 930.97871, Residuals: -1.07758, Convergence: 0.001001\n",
      "Epoch: 167, Loss: 930.04802, Residuals: -1.07553, Convergence: 0.001001\n",
      "Epoch: 168, Loss: 929.11958, Residuals: -1.07350, Convergence: 0.000999\n",
      "Evidence 11218.318\n",
      "\n",
      "Epoch: 168, Evidence: 11218.31836, Convergence: 1.016295\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.75e-01\n",
      "Epoch: 168, Loss: 2352.54129, Residuals: -1.07350, Convergence:   inf\n",
      "Epoch: 169, Loss: 2313.70669, Residuals: -1.08042, Convergence: 0.016785\n",
      "Epoch: 170, Loss: 2286.90650, Residuals: -1.07971, Convergence: 0.011719\n",
      "Epoch: 171, Loss: 2264.59416, Residuals: -1.07792, Convergence: 0.009853\n",
      "Epoch: 172, Loss: 2245.72485, Residuals: -1.07582, Convergence: 0.008402\n",
      "Epoch: 173, Loss: 2229.64590, Residuals: -1.07356, Convergence: 0.007211\n",
      "Epoch: 174, Loss: 2215.85973, Residuals: -1.07119, Convergence: 0.006222\n",
      "Epoch: 175, Loss: 2203.95851, Residuals: -1.06875, Convergence: 0.005400\n",
      "Epoch: 176, Loss: 2193.59810, Residuals: -1.06626, Convergence: 0.004723\n",
      "Epoch: 177, Loss: 2184.48674, Residuals: -1.06371, Convergence: 0.004171\n",
      "Epoch: 178, Loss: 2176.37721, Residuals: -1.06111, Convergence: 0.003726\n",
      "Epoch: 179, Loss: 2169.06735, Residuals: -1.05843, Convergence: 0.003370\n",
      "Epoch: 180, Loss: 2162.40406, Residuals: -1.05567, Convergence: 0.003081\n",
      "Epoch: 181, Loss: 2156.28207, Residuals: -1.05283, Convergence: 0.002839\n",
      "Epoch: 182, Loss: 2150.63775, Residuals: -1.04994, Convergence: 0.002624\n",
      "Epoch: 183, Loss: 2145.43255, Residuals: -1.04703, Convergence: 0.002426\n",
      "Epoch: 184, Loss: 2140.63864, Residuals: -1.04415, Convergence: 0.002239\n",
      "Epoch: 185, Loss: 2136.22824, Residuals: -1.04131, Convergence: 0.002065\n",
      "Epoch: 186, Loss: 2132.16860, Residuals: -1.03856, Convergence: 0.001904\n",
      "Epoch: 187, Loss: 2128.42806, Residuals: -1.03590, Convergence: 0.001757\n",
      "Epoch: 188, Loss: 2124.97033, Residuals: -1.03335, Convergence: 0.001627\n",
      "Epoch: 189, Loss: 2121.76323, Residuals: -1.03090, Convergence: 0.001512\n",
      "Epoch: 190, Loss: 2118.77495, Residuals: -1.02855, Convergence: 0.001410\n",
      "Epoch: 191, Loss: 2115.97749, Residuals: -1.02630, Convergence: 0.001322\n",
      "Epoch: 192, Loss: 2113.34479, Residuals: -1.02414, Convergence: 0.001246\n",
      "Epoch: 193, Loss: 2110.85610, Residuals: -1.02208, Convergence: 0.001179\n",
      "Epoch: 194, Loss: 2108.49291, Residuals: -1.02009, Convergence: 0.001121\n",
      "Epoch: 195, Loss: 2106.24055, Residuals: -1.01819, Convergence: 0.001069\n",
      "Epoch: 196, Loss: 2104.08756, Residuals: -1.01637, Convergence: 0.001023\n",
      "Epoch: 197, Loss: 2102.02365, Residuals: -1.01461, Convergence: 0.000982\n",
      "Evidence 14342.865\n",
      "\n",
      "Epoch: 197, Evidence: 14342.86523, Convergence: 0.217847\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.38e-01\n",
      "Epoch: 197, Loss: 2476.47890, Residuals: -1.01461, Convergence:   inf\n",
      "Epoch: 198, Loss: 2463.37327, Residuals: -1.01140, Convergence: 0.005320\n",
      "Epoch: 199, Loss: 2452.44696, Residuals: -1.00786, Convergence: 0.004455\n",
      "Epoch: 200, Loss: 2442.83153, Residuals: -1.00444, Convergence: 0.003936\n",
      "Epoch: 201, Loss: 2434.34627, Residuals: -1.00121, Convergence: 0.003486\n",
      "Epoch: 202, Loss: 2426.84276, Residuals: -0.99821, Convergence: 0.003092\n",
      "Epoch: 203, Loss: 2420.19085, Residuals: -0.99544, Convergence: 0.002749\n",
      "Epoch: 204, Loss: 2414.27621, Residuals: -0.99290, Convergence: 0.002450\n",
      "Epoch: 205, Loss: 2409.00061, Residuals: -0.99059, Convergence: 0.002190\n",
      "Epoch: 206, Loss: 2404.27653, Residuals: -0.98848, Convergence: 0.001965\n",
      "Epoch: 207, Loss: 2400.02949, Residuals: -0.98657, Convergence: 0.001770\n",
      "Epoch: 208, Loss: 2396.19643, Residuals: -0.98484, Convergence: 0.001600\n",
      "Epoch: 209, Loss: 2392.72207, Residuals: -0.98326, Convergence: 0.001452\n",
      "Epoch: 210, Loss: 2389.55880, Residuals: -0.98182, Convergence: 0.001324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 211, Loss: 2386.66817, Residuals: -0.98051, Convergence: 0.001211\n",
      "Epoch: 212, Loss: 2384.01624, Residuals: -0.97931, Convergence: 0.001112\n",
      "Epoch: 213, Loss: 2381.57304, Residuals: -0.97822, Convergence: 0.001026\n",
      "Epoch: 214, Loss: 2379.31499, Residuals: -0.97721, Convergence: 0.000949\n",
      "Evidence 14713.998\n",
      "\n",
      "Epoch: 214, Evidence: 14713.99805, Convergence: 0.025223\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.34e-01\n",
      "Epoch: 214, Loss: 2481.38544, Residuals: -0.97721, Convergence:   inf\n",
      "Epoch: 215, Loss: 2474.39126, Residuals: -0.97367, Convergence: 0.002827\n",
      "Epoch: 216, Loss: 2468.57690, Residuals: -0.97073, Convergence: 0.002355\n",
      "Epoch: 217, Loss: 2463.64600, Residuals: -0.96828, Convergence: 0.002001\n",
      "Epoch: 218, Loss: 2459.41114, Residuals: -0.96623, Convergence: 0.001722\n",
      "Epoch: 219, Loss: 2455.72578, Residuals: -0.96450, Convergence: 0.001501\n",
      "Epoch: 220, Loss: 2452.48069, Residuals: -0.96304, Convergence: 0.001323\n",
      "Epoch: 221, Loss: 2449.59133, Residuals: -0.96181, Convergence: 0.001180\n",
      "Epoch: 222, Loss: 2446.99393, Residuals: -0.96075, Convergence: 0.001061\n",
      "Epoch: 223, Loss: 2444.63823, Residuals: -0.95984, Convergence: 0.000964\n",
      "Evidence 14804.328\n",
      "\n",
      "Epoch: 223, Evidence: 14804.32812, Convergence: 0.006102\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.60e-01\n",
      "Epoch: 223, Loss: 2482.85892, Residuals: -0.95984, Convergence:   inf\n",
      "Epoch: 224, Loss: 2478.70984, Residuals: -0.95729, Convergence: 0.001674\n",
      "Epoch: 225, Loss: 2475.26678, Residuals: -0.95530, Convergence: 0.001391\n",
      "Epoch: 226, Loss: 2472.32576, Residuals: -0.95371, Convergence: 0.001190\n",
      "Epoch: 227, Loss: 2469.76270, Residuals: -0.95243, Convergence: 0.001038\n",
      "Epoch: 228, Loss: 2467.49135, Residuals: -0.95139, Convergence: 0.000921\n",
      "Evidence 14837.226\n",
      "\n",
      "Epoch: 228, Evidence: 14837.22559, Convergence: 0.002217\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.08e-01\n",
      "Epoch: 228, Loss: 2483.73548, Residuals: -0.95139, Convergence:   inf\n",
      "Epoch: 229, Loss: 2480.80716, Residuals: -0.94953, Convergence: 0.001180\n",
      "Epoch: 230, Loss: 2478.36336, Residuals: -0.94810, Convergence: 0.000986\n",
      "Evidence 14850.242\n",
      "\n",
      "Epoch: 230, Evidence: 14850.24219, Convergence: 0.000877\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.72e-01\n",
      "Epoch: 230, Loss: 2484.42884, Residuals: -0.94810, Convergence:   inf\n",
      "Epoch: 231, Loss: 2479.91895, Residuals: -0.94597, Convergence: 0.001819\n",
      "Epoch: 232, Loss: 2476.48017, Residuals: -0.94422, Convergence: 0.001389\n",
      "Epoch: 233, Loss: 2473.69278, Residuals: -0.94289, Convergence: 0.001127\n",
      "Epoch: 234, Loss: 2471.32615, Residuals: -0.94202, Convergence: 0.000958\n",
      "Evidence 14867.824\n",
      "\n",
      "Epoch: 234, Evidence: 14867.82422, Convergence: 0.002058\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.42e-01\n",
      "Epoch: 234, Loss: 2484.53220, Residuals: -0.94202, Convergence:   inf\n",
      "Epoch: 235, Loss: 2481.63366, Residuals: -0.93963, Convergence: 0.001168\n",
      "Epoch: 236, Loss: 2479.32759, Residuals: -0.93820, Convergence: 0.000930\n",
      "Evidence 14878.523\n",
      "\n",
      "Epoch: 236, Evidence: 14878.52344, Convergence: 0.000719\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.20e-01\n",
      "Epoch: 236, Loss: 2484.72312, Residuals: -0.93820, Convergence:   inf\n",
      "Epoch: 237, Loss: 2480.54609, Residuals: -0.93419, Convergence: 0.001684\n",
      "Epoch: 238, Loss: 2477.53575, Residuals: -0.93489, Convergence: 0.001215\n",
      "Epoch: 239, Loss: 2474.94066, Residuals: -0.93474, Convergence: 0.001049\n",
      "Epoch: 240, Loss: 2472.67770, Residuals: -0.93714, Convergence: 0.000915\n",
      "Evidence 14893.600\n",
      "\n",
      "Epoch: 240, Evidence: 14893.59961, Convergence: 0.001731\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.06e-01\n",
      "Epoch: 240, Loss: 2484.00648, Residuals: -0.93714, Convergence:   inf\n",
      "Epoch: 241, Loss: 2482.46798, Residuals: -0.93348, Convergence: 0.000620\n",
      "Evidence 14899.813\n",
      "\n",
      "Epoch: 241, Evidence: 14899.81348, Convergence: 0.000417\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.70e-02\n",
      "Epoch: 241, Loss: 2485.06198, Residuals: -0.93348, Convergence:   inf\n",
      "Epoch: 242, Loss: 2526.10382, Residuals: -0.96466, Convergence: -0.016247\n",
      "Epoch: 242, Loss: 2483.05012, Residuals: -0.93072, Convergence: 0.000810\n",
      "Evidence 14903.690\n",
      "\n",
      "Epoch: 242, Evidence: 14903.69043, Convergence: 0.000677\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.06e-02\n",
      "Epoch: 242, Loss: 2484.43940, Residuals: -0.93072, Convergence:   inf\n",
      "Epoch: 243, Loss: 2490.09367, Residuals: -0.93027, Convergence: -0.002271\n",
      "Epoch: 243, Loss: 2484.71407, Residuals: -0.92824, Convergence: -0.000111\n",
      "Evidence 14905.028\n",
      "\n",
      "Epoch: 243, Evidence: 14905.02832, Convergence: 0.000767\n",
      "Total samples: 183, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 383.45856, Residuals: -4.51088, Convergence:   inf\n",
      "Epoch: 1, Loss: 357.57056, Residuals: -4.39032, Convergence: 0.072400\n",
      "Epoch: 2, Loss: 336.35290, Residuals: -4.22543, Convergence: 0.063082\n",
      "Epoch: 3, Loss: 320.15670, Residuals: -4.06019, Convergence: 0.050588\n",
      "Epoch: 4, Loss: 307.81376, Residuals: -3.91478, Convergence: 0.040099\n",
      "Epoch: 5, Loss: 298.02896, Residuals: -3.78639, Convergence: 0.032832\n",
      "Epoch: 6, Loss: 290.08540, Residuals: -3.67456, Convergence: 0.027384\n",
      "Epoch: 7, Loss: 283.49136, Residuals: -3.57881, Convergence: 0.023260\n",
      "Epoch: 8, Loss: 277.88279, Residuals: -3.49708, Convergence: 0.020183\n",
      "Epoch: 9, Loss: 273.00190, Residuals: -3.42706, Convergence: 0.017879\n",
      "Epoch: 10, Loss: 268.66547, Residuals: -3.36662, Convergence: 0.016141\n",
      "Epoch: 11, Loss: 264.74112, Residuals: -3.31399, Convergence: 0.014823\n",
      "Epoch: 12, Loss: 261.13287, Residuals: -3.26768, Convergence: 0.013818\n",
      "Epoch: 13, Loss: 257.77165, Residuals: -3.22638, Convergence: 0.013040\n",
      "Epoch: 14, Loss: 254.60809, Residuals: -3.18896, Convergence: 0.012425\n",
      "Epoch: 15, Loss: 251.60741, Residuals: -3.15442, Convergence: 0.011926\n",
      "Epoch: 16, Loss: 248.74654, Residuals: -3.12200, Convergence: 0.011501\n",
      "Epoch: 17, Loss: 246.00914, Residuals: -3.09122, Convergence: 0.011127\n",
      "Epoch: 18, Loss: 243.37563, Residuals: -3.06169, Convergence: 0.010821\n",
      "Epoch: 19, Loss: 240.81709, Residuals: -3.03294, Convergence: 0.010624\n",
      "Epoch: 20, Loss: 238.29927, Residuals: -3.00442, Convergence: 0.010566\n",
      "Epoch: 21, Loss: 235.79123, Residuals: -2.97562, Convergence: 0.010637\n",
      "Epoch: 22, Loss: 233.27162, Residuals: -2.94617, Convergence: 0.010801\n",
      "Epoch: 23, Loss: 230.72337, Residuals: -2.91581, Convergence: 0.011045\n",
      "Epoch: 24, Loss: 228.11576, Residuals: -2.88421, Convergence: 0.011431\n",
      "Epoch: 25, Loss: 225.39624, Residuals: -2.85075, Convergence: 0.012065\n",
      "Epoch: 26, Loss: 222.52660, Residuals: -2.81494, Convergence: 0.012896\n",
      "Epoch: 27, Loss: 219.57719, Residuals: -2.77743, Convergence: 0.013432\n",
      "Epoch: 28, Loss: 216.68940, Residuals: -2.73980, Convergence: 0.013327\n",
      "Epoch: 29, Loss: 213.92014, Residuals: -2.70283, Convergence: 0.012945\n",
      "Epoch: 30, Loss: 211.26235, Residuals: -2.66661, Convergence: 0.012580\n",
      "Epoch: 31, Loss: 208.69732, Residuals: -2.63105, Convergence: 0.012291\n",
      "Epoch: 32, Loss: 206.20896, Residuals: -2.59605, Convergence: 0.012067\n",
      "Epoch: 33, Loss: 203.78560, Residuals: -2.56150, Convergence: 0.011892\n",
      "Epoch: 34, Loss: 201.41942, Residuals: -2.52732, Convergence: 0.011748\n",
      "Epoch: 35, Loss: 199.10568, Residuals: -2.49344, Convergence: 0.011621\n",
      "Epoch: 36, Loss: 196.84194, Residuals: -2.45980, Convergence: 0.011500\n",
      "Epoch: 37, Loss: 194.62753, Residuals: -2.42637, Convergence: 0.011378\n",
      "Epoch: 38, Loss: 192.46295, Residuals: -2.39311, Convergence: 0.011247\n",
      "Epoch: 39, Loss: 190.34939, Residuals: -2.36002, Convergence: 0.011104\n",
      "Epoch: 40, Loss: 188.28841, Residuals: -2.32711, Convergence: 0.010946\n",
      "Epoch: 41, Loss: 186.28156, Residuals: -2.29439, Convergence: 0.010773\n",
      "Epoch: 42, Loss: 184.33020, Residuals: -2.26188, Convergence: 0.010586\n",
      "Epoch: 43, Loss: 182.43534, Residuals: -2.22961, Convergence: 0.010386\n",
      "Epoch: 44, Loss: 180.59756, Residuals: -2.19762, Convergence: 0.010176\n",
      "Epoch: 45, Loss: 178.81705, Residuals: -2.16591, Convergence: 0.009957\n",
      "Epoch: 46, Loss: 177.09371, Residuals: -2.13453, Convergence: 0.009731\n",
      "Epoch: 47, Loss: 175.42731, Residuals: -2.10350, Convergence: 0.009499\n",
      "Epoch: 48, Loss: 173.81761, Residuals: -2.07284, Convergence: 0.009261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49, Loss: 172.26437, Residuals: -2.04259, Convergence: 0.009017\n",
      "Epoch: 50, Loss: 170.76724, Residuals: -2.01277, Convergence: 0.008767\n",
      "Epoch: 51, Loss: 169.32567, Residuals: -1.98342, Convergence: 0.008514\n",
      "Epoch: 52, Loss: 167.93873, Residuals: -1.95454, Convergence: 0.008259\n",
      "Epoch: 53, Loss: 166.60506, Residuals: -1.92618, Convergence: 0.008005\n",
      "Epoch: 54, Loss: 165.32300, Residuals: -1.89832, Convergence: 0.007755\n",
      "Epoch: 55, Loss: 164.09071, Residuals: -1.87099, Convergence: 0.007510\n",
      "Epoch: 56, Loss: 162.90632, Residuals: -1.84419, Convergence: 0.007270\n",
      "Epoch: 57, Loss: 161.76810, Residuals: -1.81792, Convergence: 0.007036\n",
      "Epoch: 58, Loss: 160.67447, Residuals: -1.79219, Convergence: 0.006806\n",
      "Epoch: 59, Loss: 159.62409, Residuals: -1.76701, Convergence: 0.006580\n",
      "Epoch: 60, Loss: 158.61574, Residuals: -1.74239, Convergence: 0.006357\n",
      "Epoch: 61, Loss: 157.64837, Residuals: -1.71835, Convergence: 0.006136\n",
      "Epoch: 62, Loss: 156.72100, Residuals: -1.69489, Convergence: 0.005917\n",
      "Epoch: 63, Loss: 155.83271, Residuals: -1.67204, Convergence: 0.005700\n",
      "Epoch: 64, Loss: 154.98258, Residuals: -1.64980, Convergence: 0.005485\n",
      "Epoch: 65, Loss: 154.16971, Residuals: -1.62819, Convergence: 0.005273\n",
      "Epoch: 66, Loss: 153.39316, Residuals: -1.60721, Convergence: 0.005063\n",
      "Epoch: 67, Loss: 152.65197, Residuals: -1.58688, Convergence: 0.004855\n",
      "Epoch: 68, Loss: 151.94516, Residuals: -1.56719, Convergence: 0.004652\n",
      "Epoch: 69, Loss: 151.27171, Residuals: -1.54816, Convergence: 0.004452\n",
      "Epoch: 70, Loss: 150.63056, Residuals: -1.52977, Convergence: 0.004256\n",
      "Epoch: 71, Loss: 150.02062, Residuals: -1.51204, Convergence: 0.004066\n",
      "Epoch: 72, Loss: 149.44076, Residuals: -1.49495, Convergence: 0.003880\n",
      "Epoch: 73, Loss: 148.88983, Residuals: -1.47849, Convergence: 0.003700\n",
      "Epoch: 74, Loss: 148.36668, Residuals: -1.46267, Convergence: 0.003526\n",
      "Epoch: 75, Loss: 147.87014, Residuals: -1.44746, Convergence: 0.003358\n",
      "Epoch: 76, Loss: 147.39902, Residuals: -1.43286, Convergence: 0.003196\n",
      "Epoch: 77, Loss: 146.95217, Residuals: -1.41884, Convergence: 0.003041\n",
      "Epoch: 78, Loss: 146.52843, Residuals: -1.40540, Convergence: 0.002892\n",
      "Epoch: 79, Loss: 146.12666, Residuals: -1.39252, Convergence: 0.002749\n",
      "Epoch: 80, Loss: 145.74578, Residuals: -1.38019, Convergence: 0.002613\n",
      "Epoch: 81, Loss: 145.38470, Residuals: -1.36837, Convergence: 0.002484\n",
      "Epoch: 82, Loss: 145.04239, Residuals: -1.35706, Convergence: 0.002360\n",
      "Epoch: 83, Loss: 144.71788, Residuals: -1.34624, Convergence: 0.002242\n",
      "Epoch: 84, Loss: 144.41021, Residuals: -1.33588, Convergence: 0.002131\n",
      "Epoch: 85, Loss: 144.11849, Residuals: -1.32597, Convergence: 0.002024\n",
      "Epoch: 86, Loss: 143.84188, Residuals: -1.31649, Convergence: 0.001923\n",
      "Epoch: 87, Loss: 143.57957, Residuals: -1.30743, Convergence: 0.001827\n",
      "Epoch: 88, Loss: 143.33081, Residuals: -1.29875, Convergence: 0.001736\n",
      "Epoch: 89, Loss: 143.09489, Residuals: -1.29046, Convergence: 0.001649\n",
      "Epoch: 90, Loss: 142.87115, Residuals: -1.28252, Convergence: 0.001566\n",
      "Epoch: 91, Loss: 142.65899, Residuals: -1.27493, Convergence: 0.001487\n",
      "Epoch: 92, Loss: 142.45783, Residuals: -1.26766, Convergence: 0.001412\n",
      "Epoch: 93, Loss: 142.26716, Residuals: -1.26071, Convergence: 0.001340\n",
      "Epoch: 94, Loss: 142.08647, Residuals: -1.25405, Convergence: 0.001272\n",
      "Epoch: 95, Loss: 141.91536, Residuals: -1.24768, Convergence: 0.001206\n",
      "Epoch: 96, Loss: 141.75339, Residuals: -1.24159, Convergence: 0.001143\n",
      "Epoch: 97, Loss: 141.60022, Residuals: -1.23576, Convergence: 0.001082\n",
      "Epoch: 98, Loss: 141.45550, Residuals: -1.23018, Convergence: 0.001023\n",
      "Epoch: 99, Loss: 141.31893, Residuals: -1.22484, Convergence: 0.000966\n",
      "Evidence -183.077\n",
      "\n",
      "Epoch: 99, Evidence: -183.07700, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.24e-01\n",
      "Epoch: 99, Loss: 1372.10577, Residuals: -1.22484, Convergence:   inf\n",
      "Epoch: 100, Loss: 1309.62881, Residuals: -1.25569, Convergence: 0.047706\n",
      "Epoch: 101, Loss: 1262.28060, Residuals: -1.27939, Convergence: 0.037510\n",
      "Epoch: 102, Loss: 1226.56868, Residuals: -1.29614, Convergence: 0.029115\n",
      "Epoch: 103, Loss: 1198.81814, Residuals: -1.30777, Convergence: 0.023148\n",
      "Epoch: 104, Loss: 1176.38974, Residuals: -1.31625, Convergence: 0.019065\n",
      "Epoch: 105, Loss: 1157.76068, Residuals: -1.32263, Convergence: 0.016091\n",
      "Epoch: 106, Loss: 1142.02130, Residuals: -1.32737, Convergence: 0.013782\n",
      "Epoch: 107, Loss: 1128.55799, Residuals: -1.33070, Convergence: 0.011930\n",
      "Epoch: 108, Loss: 1116.91702, Residuals: -1.33279, Convergence: 0.010422\n",
      "Epoch: 109, Loss: 1106.74569, Residuals: -1.33375, Convergence: 0.009190\n",
      "Epoch: 110, Loss: 1097.75976, Residuals: -1.33369, Convergence: 0.008186\n",
      "Epoch: 111, Loss: 1089.72594, Residuals: -1.33272, Convergence: 0.007372\n",
      "Epoch: 112, Loss: 1082.44937, Residuals: -1.33090, Convergence: 0.006722\n",
      "Epoch: 113, Loss: 1075.76377, Residuals: -1.32830, Convergence: 0.006215\n",
      "Epoch: 114, Loss: 1069.52782, Residuals: -1.32498, Convergence: 0.005831\n",
      "Epoch: 115, Loss: 1063.61713, Residuals: -1.32096, Convergence: 0.005557\n",
      "Epoch: 116, Loss: 1057.92525, Residuals: -1.31627, Convergence: 0.005380\n",
      "Epoch: 117, Loss: 1052.35911, Residuals: -1.31094, Convergence: 0.005289\n",
      "Epoch: 118, Loss: 1046.84168, Residuals: -1.30497, Convergence: 0.005271\n",
      "Epoch: 119, Loss: 1041.31725, Residuals: -1.29841, Convergence: 0.005305\n",
      "Epoch: 120, Loss: 1035.76318, Residuals: -1.29130, Convergence: 0.005362\n",
      "Epoch: 121, Loss: 1030.20426, Residuals: -1.28373, Convergence: 0.005396\n",
      "Epoch: 122, Loss: 1024.70763, Residuals: -1.27578, Convergence: 0.005364\n",
      "Epoch: 123, Loss: 1019.36537, Residuals: -1.26757, Convergence: 0.005241\n",
      "Epoch: 124, Loss: 1014.26067, Residuals: -1.25919, Convergence: 0.005033\n",
      "Epoch: 125, Loss: 1009.44551, Residuals: -1.25070, Convergence: 0.004770\n",
      "Epoch: 126, Loss: 1004.93749, Residuals: -1.24220, Convergence: 0.004486\n",
      "Epoch: 127, Loss: 1000.72892, Residuals: -1.23373, Convergence: 0.004206\n",
      "Epoch: 128, Loss: 996.79881, Residuals: -1.22535, Convergence: 0.003943\n",
      "Epoch: 129, Loss: 993.12158, Residuals: -1.21711, Convergence: 0.003703\n",
      "Epoch: 130, Loss: 989.67152, Residuals: -1.20902, Convergence: 0.003486\n",
      "Epoch: 131, Loss: 986.42629, Residuals: -1.20113, Convergence: 0.003290\n",
      "Epoch: 132, Loss: 983.36637, Residuals: -1.19345, Convergence: 0.003112\n",
      "Epoch: 133, Loss: 980.47539, Residuals: -1.18601, Convergence: 0.002949\n",
      "Epoch: 134, Loss: 977.74022, Residuals: -1.17882, Convergence: 0.002797\n",
      "Epoch: 135, Loss: 975.14914, Residuals: -1.17188, Convergence: 0.002657\n",
      "Epoch: 136, Loss: 972.69351, Residuals: -1.16522, Convergence: 0.002525\n",
      "Epoch: 137, Loss: 970.36375, Residuals: -1.15882, Convergence: 0.002401\n",
      "Epoch: 138, Loss: 968.15294, Residuals: -1.15270, Convergence: 0.002284\n",
      "Epoch: 139, Loss: 966.05354, Residuals: -1.14685, Convergence: 0.002173\n",
      "Epoch: 140, Loss: 964.05903, Residuals: -1.14127, Convergence: 0.002069\n",
      "Epoch: 141, Loss: 962.16238, Residuals: -1.13595, Convergence: 0.001971\n",
      "Epoch: 142, Loss: 960.35810, Residuals: -1.13088, Convergence: 0.001879\n",
      "Epoch: 143, Loss: 958.63934, Residuals: -1.12606, Convergence: 0.001793\n",
      "Epoch: 144, Loss: 957.00045, Residuals: -1.12148, Convergence: 0.001713\n",
      "Epoch: 145, Loss: 955.43628, Residuals: -1.11712, Convergence: 0.001637\n",
      "Epoch: 146, Loss: 953.94104, Residuals: -1.11297, Convergence: 0.001567\n",
      "Epoch: 147, Loss: 952.51025, Residuals: -1.10903, Convergence: 0.001502\n",
      "Epoch: 148, Loss: 951.13907, Residuals: -1.10528, Convergence: 0.001442\n",
      "Epoch: 149, Loss: 949.82330, Residuals: -1.10171, Convergence: 0.001385\n",
      "Epoch: 150, Loss: 948.55896, Residuals: -1.09831, Convergence: 0.001333\n",
      "Epoch: 151, Loss: 947.34241, Residuals: -1.09507, Convergence: 0.001284\n",
      "Epoch: 152, Loss: 946.17051, Residuals: -1.09198, Convergence: 0.001239\n",
      "Epoch: 153, Loss: 945.03994, Residuals: -1.08903, Convergence: 0.001196\n",
      "Epoch: 154, Loss: 943.94792, Residuals: -1.08621, Convergence: 0.001157\n",
      "Epoch: 155, Loss: 942.89146, Residuals: -1.08351, Convergence: 0.001120\n",
      "Epoch: 156, Loss: 941.86825, Residuals: -1.08093, Convergence: 0.001086\n",
      "Epoch: 157, Loss: 940.87547, Residuals: -1.07845, Convergence: 0.001055\n",
      "Epoch: 158, Loss: 939.91087, Residuals: -1.07607, Convergence: 0.001026\n",
      "Epoch: 159, Loss: 938.97172, Residuals: -1.07378, Convergence: 0.001000\n",
      "Epoch: 160, Loss: 938.05558, Residuals: -1.07157, Convergence: 0.000977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence 11178.017\n",
      "\n",
      "Epoch: 160, Evidence: 11178.01660, Convergence: 1.016378\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 5.77e-01\n",
      "Epoch: 160, Loss: 2352.92744, Residuals: -1.07157, Convergence:   inf\n",
      "Epoch: 161, Loss: 2314.65136, Residuals: -1.08009, Convergence: 0.016536\n",
      "Epoch: 162, Loss: 2287.02016, Residuals: -1.07975, Convergence: 0.012082\n",
      "Epoch: 163, Loss: 2263.81398, Residuals: -1.07815, Convergence: 0.010251\n",
      "Epoch: 164, Loss: 2244.02991, Residuals: -1.07605, Convergence: 0.008816\n",
      "Epoch: 165, Loss: 2227.00131, Residuals: -1.07362, Convergence: 0.007646\n",
      "Epoch: 166, Loss: 2212.22756, Residuals: -1.07092, Convergence: 0.006678\n",
      "Epoch: 167, Loss: 2199.31042, Residuals: -1.06800, Convergence: 0.005873\n",
      "Epoch: 168, Loss: 2187.92569, Residuals: -1.06492, Convergence: 0.005203\n",
      "Epoch: 169, Loss: 2177.80290, Residuals: -1.06169, Convergence: 0.004648\n",
      "Epoch: 170, Loss: 2168.71656, Residuals: -1.05836, Convergence: 0.004190\n",
      "Epoch: 171, Loss: 2160.47558, Residuals: -1.05492, Convergence: 0.003814\n",
      "Epoch: 172, Loss: 2152.92525, Residuals: -1.05139, Convergence: 0.003507\n",
      "Epoch: 173, Loss: 2145.94978, Residuals: -1.04777, Convergence: 0.003251\n",
      "Epoch: 174, Loss: 2139.47419, Residuals: -1.04408, Convergence: 0.003027\n",
      "Epoch: 175, Loss: 2133.45471, Residuals: -1.04036, Convergence: 0.002821\n",
      "Epoch: 176, Loss: 2127.87190, Residuals: -1.03665, Convergence: 0.002624\n",
      "Epoch: 177, Loss: 2122.71165, Residuals: -1.03300, Convergence: 0.002431\n",
      "Epoch: 178, Loss: 2117.95689, Residuals: -1.02944, Convergence: 0.002245\n",
      "Epoch: 179, Loss: 2113.58768, Residuals: -1.02600, Convergence: 0.002067\n",
      "Epoch: 180, Loss: 2109.57719, Residuals: -1.02271, Convergence: 0.001901\n",
      "Epoch: 181, Loss: 2105.89525, Residuals: -1.01958, Convergence: 0.001748\n",
      "Epoch: 182, Loss: 2102.51142, Residuals: -1.01661, Convergence: 0.001609\n",
      "Epoch: 183, Loss: 2099.39500, Residuals: -1.01380, Convergence: 0.001484\n",
      "Epoch: 184, Loss: 2096.51689, Residuals: -1.01115, Convergence: 0.001373\n",
      "Epoch: 185, Loss: 2093.84918, Residuals: -1.00865, Convergence: 0.001274\n",
      "Epoch: 186, Loss: 2091.36726, Residuals: -1.00629, Convergence: 0.001187\n",
      "Epoch: 187, Loss: 2089.04889, Residuals: -1.00408, Convergence: 0.001110\n",
      "Epoch: 188, Loss: 2086.87441, Residuals: -1.00199, Convergence: 0.001042\n",
      "Epoch: 189, Loss: 2084.82687, Residuals: -1.00003, Convergence: 0.000982\n",
      "Evidence 14343.460\n",
      "\n",
      "Epoch: 189, Evidence: 14343.45996, Convergence: 0.220689\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 4.39e-01\n",
      "Epoch: 189, Loss: 2481.46005, Residuals: -1.00003, Convergence:   inf\n",
      "Epoch: 190, Loss: 2467.31022, Residuals: -0.99651, Convergence: 0.005735\n",
      "Epoch: 191, Loss: 2455.78768, Residuals: -0.99255, Convergence: 0.004692\n",
      "Epoch: 192, Loss: 2445.90824, Residuals: -0.98873, Convergence: 0.004039\n",
      "Epoch: 193, Loss: 2437.36683, Residuals: -0.98514, Convergence: 0.003504\n",
      "Epoch: 194, Loss: 2429.92706, Residuals: -0.98180, Convergence: 0.003062\n",
      "Epoch: 195, Loss: 2423.40135, Residuals: -0.97873, Convergence: 0.002693\n",
      "Epoch: 196, Loss: 2417.63915, Residuals: -0.97592, Convergence: 0.002383\n",
      "Epoch: 197, Loss: 2412.51759, Residuals: -0.97335, Convergence: 0.002123\n",
      "Epoch: 198, Loss: 2407.93617, Residuals: -0.97101, Convergence: 0.001903\n",
      "Epoch: 199, Loss: 2403.81327, Residuals: -0.96888, Convergence: 0.001715\n",
      "Epoch: 200, Loss: 2400.08126, Residuals: -0.96695, Convergence: 0.001555\n",
      "Epoch: 201, Loss: 2396.68526, Residuals: -0.96520, Convergence: 0.001417\n",
      "Epoch: 202, Loss: 2393.58059, Residuals: -0.96361, Convergence: 0.001297\n",
      "Epoch: 203, Loss: 2390.72856, Residuals: -0.96217, Convergence: 0.001193\n",
      "Epoch: 204, Loss: 2388.09818, Residuals: -0.96086, Convergence: 0.001101\n",
      "Epoch: 205, Loss: 2385.66368, Residuals: -0.95968, Convergence: 0.001020\n",
      "Epoch: 206, Loss: 2383.40175, Residuals: -0.95860, Convergence: 0.000949\n",
      "Evidence 14749.695\n",
      "\n",
      "Epoch: 206, Evidence: 14749.69531, Convergence: 0.027542\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 3.33e-01\n",
      "Epoch: 206, Loss: 2486.66837, Residuals: -0.95860, Convergence:   inf\n",
      "Epoch: 207, Loss: 2480.12938, Residuals: -0.95518, Convergence: 0.002637\n",
      "Epoch: 208, Loss: 2474.73773, Residuals: -0.95218, Convergence: 0.002179\n",
      "Epoch: 209, Loss: 2470.16833, Residuals: -0.94963, Convergence: 0.001850\n",
      "Epoch: 210, Loss: 2466.22898, Residuals: -0.94747, Convergence: 0.001597\n",
      "Epoch: 211, Loss: 2462.77987, Residuals: -0.94565, Convergence: 0.001400\n",
      "Epoch: 212, Loss: 2459.71831, Residuals: -0.94411, Convergence: 0.001245\n",
      "Epoch: 213, Loss: 2456.96913, Residuals: -0.94281, Convergence: 0.001119\n",
      "Epoch: 214, Loss: 2454.47613, Residuals: -0.94170, Convergence: 0.001016\n",
      "Epoch: 215, Loss: 2452.19520, Residuals: -0.94076, Convergence: 0.000930\n",
      "Evidence 14835.811\n",
      "\n",
      "Epoch: 215, Evidence: 14835.81055, Convergence: 0.005805\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.59e-01\n",
      "Epoch: 215, Loss: 2488.42899, Residuals: -0.94076, Convergence:   inf\n",
      "Epoch: 216, Loss: 2484.58493, Residuals: -0.93819, Convergence: 0.001547\n",
      "Epoch: 217, Loss: 2481.38595, Residuals: -0.93615, Convergence: 0.001289\n",
      "Epoch: 218, Loss: 2478.62766, Residuals: -0.93453, Convergence: 0.001113\n",
      "Epoch: 219, Loss: 2476.19437, Residuals: -0.93322, Convergence: 0.000983\n",
      "Evidence 14864.255\n",
      "\n",
      "Epoch: 219, Evidence: 14864.25488, Convergence: 0.001914\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.07e-01\n",
      "Epoch: 219, Loss: 2489.58100, Residuals: -0.93322, Convergence:   inf\n",
      "Epoch: 220, Loss: 2486.68362, Residuals: -0.93115, Convergence: 0.001165\n",
      "Epoch: 221, Loss: 2484.24246, Residuals: -0.92956, Convergence: 0.000983\n",
      "Evidence 14876.258\n",
      "\n",
      "Epoch: 221, Evidence: 14876.25781, Convergence: 0.000807\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.72e-01\n",
      "Epoch: 221, Loss: 2490.41164, Residuals: -0.92956, Convergence:   inf\n",
      "Epoch: 222, Loss: 2485.77219, Residuals: -0.92664, Convergence: 0.001866\n",
      "Epoch: 223, Loss: 2482.23680, Residuals: -0.92478, Convergence: 0.001424\n",
      "Epoch: 224, Loss: 2479.34192, Residuals: -0.92359, Convergence: 0.001168\n",
      "Epoch: 225, Loss: 2476.86792, Residuals: -0.92294, Convergence: 0.000999\n",
      "Evidence 14894.235\n",
      "\n",
      "Epoch: 225, Evidence: 14894.23535, Convergence: 0.002013\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.41e-01\n",
      "Epoch: 225, Loss: 2490.63105, Residuals: -0.92294, Convergence:   inf\n",
      "Epoch: 226, Loss: 2487.55349, Residuals: -0.92045, Convergence: 0.001237\n",
      "Epoch: 227, Loss: 2485.10857, Residuals: -0.91925, Convergence: 0.000984\n",
      "Evidence 14905.169\n",
      "\n",
      "Epoch: 227, Evidence: 14905.16895, Convergence: 0.000734\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.19e-01\n",
      "Epoch: 227, Loss: 2490.91073, Residuals: -0.91925, Convergence:   inf\n",
      "Epoch: 228, Loss: 2486.44856, Residuals: -0.91569, Convergence: 0.001795\n",
      "Epoch: 229, Loss: 2483.21375, Residuals: -0.91700, Convergence: 0.001303\n",
      "Epoch: 230, Loss: 2480.48532, Residuals: -0.91803, Convergence: 0.001100\n",
      "Epoch: 231, Loss: 2478.10666, Residuals: -0.92099, Convergence: 0.000960\n",
      "Evidence 14920.979\n",
      "\n",
      "Epoch: 231, Evidence: 14920.97949, Convergence: 0.001792\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.06e-01\n",
      "Epoch: 231, Loss: 2490.25327, Residuals: -0.92099, Convergence:   inf\n",
      "Epoch: 232, Loss: 2488.59096, Residuals: -0.91939, Convergence: 0.000668\n",
      "Evidence 14927.581\n",
      "\n",
      "Epoch: 232, Evidence: 14927.58105, Convergence: 0.000442\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 8.65e-02\n",
      "Epoch: 232, Loss: 2491.35345, Residuals: -0.91939, Convergence:   inf\n",
      "Epoch: 233, Loss: 2537.46371, Residuals: -0.96295, Convergence: -0.018172\n",
      "Epoch: 233, Loss: 2489.23059, Residuals: -0.91680, Convergence: 0.000853\n",
      "Evidence 14931.590\n",
      "\n",
      "Epoch: 233, Evidence: 14931.58984, Convergence: 0.000711\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.99e-02\n",
      "Epoch: 233, Loss: 2490.72438, Residuals: -0.91680, Convergence:   inf\n",
      "Epoch: 234, Loss: 2495.92603, Residuals: -0.92083, Convergence: -0.002084\n",
      "Epoch: 234, Loss: 2490.79219, Residuals: -0.91629, Convergence: -0.000027\n",
      "Evidence 14933.136\n",
      "\n",
      "Epoch: 234, Evidence: 14933.13574, Convergence: 0.000814\n",
      "Total samples: 181, Number of parameters: 300, Initial regularization: 1.00e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 382.41755, Residuals: -4.54534, Convergence:   inf\n",
      "Epoch: 1, Loss: 356.67743, Residuals: -4.42536, Convergence: 0.072166\n",
      "Epoch: 2, Loss: 335.66273, Residuals: -4.26234, Convergence: 0.062607\n",
      "Epoch: 3, Loss: 319.61211, Residuals: -4.09881, Convergence: 0.050219\n",
      "Epoch: 4, Loss: 307.36605, Residuals: -3.95469, Convergence: 0.039842\n",
      "Epoch: 5, Loss: 297.65232, Residuals: -3.82726, Convergence: 0.032635\n",
      "Epoch: 6, Loss: 289.76710, Residuals: -3.71630, Convergence: 0.027212\n",
      "Epoch: 7, Loss: 283.22216, Residuals: -3.62131, Convergence: 0.023109\n",
      "Epoch: 8, Loss: 277.65593, Residuals: -3.54020, Convergence: 0.020047\n",
      "Epoch: 9, Loss: 272.81292, Residuals: -3.47059, Convergence: 0.017752\n",
      "Epoch: 10, Loss: 268.51108, Residuals: -3.41035, Convergence: 0.016021\n",
      "Epoch: 11, Loss: 264.61797, Residuals: -3.35766, Convergence: 0.014712\n",
      "Epoch: 12, Loss: 261.03630, Residuals: -3.31099, Convergence: 0.013721\n",
      "Epoch: 13, Loss: 257.69495, Residuals: -3.26900, Convergence: 0.012966\n",
      "Epoch: 14, Loss: 254.54364, Residuals: -3.23055, Convergence: 0.012380\n",
      "Epoch: 15, Loss: 251.55077, Residuals: -3.19473, Convergence: 0.011898\n",
      "Epoch: 16, Loss: 248.70090, Residuals: -3.16098, Convergence: 0.011459\n",
      "Epoch: 17, Loss: 245.98336, Residuals: -3.12897, Convergence: 0.011048\n",
      "Epoch: 18, Loss: 243.37748, Residuals: -3.09833, Convergence: 0.010707\n",
      "Epoch: 19, Loss: 240.85051, Residuals: -3.06853, Convergence: 0.010492\n",
      "Epoch: 20, Loss: 238.36571, Residuals: -3.03901, Convergence: 0.010424\n",
      "Epoch: 21, Loss: 235.89173, Residuals: -3.00923, Convergence: 0.010488\n",
      "Epoch: 22, Loss: 233.40626, Residuals: -2.97887, Convergence: 0.010649\n",
      "Epoch: 23, Loss: 230.88533, Residuals: -2.94764, Convergence: 0.010919\n",
      "Epoch: 24, Loss: 228.28533, Residuals: -2.91507, Convergence: 0.011389\n",
      "Epoch: 25, Loss: 225.54667, Residuals: -2.88043, Convergence: 0.012142\n",
      "Epoch: 26, Loss: 222.65929, Residuals: -2.84351, Convergence: 0.012968\n",
      "Epoch: 27, Loss: 219.73895, Residuals: -2.80549, Convergence: 0.013290\n",
      "Epoch: 28, Loss: 216.90215, Residuals: -2.76774, Convergence: 0.013079\n",
      "Epoch: 29, Loss: 214.17051, Residuals: -2.73066, Convergence: 0.012755\n",
      "Epoch: 30, Loss: 211.52855, Residuals: -2.69418, Convergence: 0.012490\n",
      "Epoch: 31, Loss: 208.95898, Residuals: -2.65814, Convergence: 0.012297\n",
      "Epoch: 32, Loss: 206.44953, Residuals: -2.62245, Convergence: 0.012155\n",
      "Epoch: 33, Loss: 203.99285, Residuals: -2.58700, Convergence: 0.012043\n",
      "Epoch: 34, Loss: 201.58537, Residuals: -2.55174, Convergence: 0.011943\n",
      "Epoch: 35, Loss: 199.22605, Residuals: -2.51664, Convergence: 0.011842\n",
      "Epoch: 36, Loss: 196.91549, Residuals: -2.48167, Convergence: 0.011734\n",
      "Epoch: 37, Loss: 194.65516, Residuals: -2.44685, Convergence: 0.011612\n",
      "Epoch: 38, Loss: 192.44694, Residuals: -2.41216, Convergence: 0.011474\n",
      "Epoch: 39, Loss: 190.29277, Residuals: -2.37763, Convergence: 0.011320\n",
      "Epoch: 40, Loss: 188.19439, Residuals: -2.34329, Convergence: 0.011150\n",
      "Epoch: 41, Loss: 186.15330, Residuals: -2.30914, Convergence: 0.010965\n",
      "Epoch: 42, Loss: 184.17070, Residuals: -2.27523, Convergence: 0.010765\n",
      "Epoch: 43, Loss: 182.24760, Residuals: -2.24158, Convergence: 0.010552\n",
      "Epoch: 44, Loss: 180.38495, Residuals: -2.20822, Convergence: 0.010326\n",
      "Epoch: 45, Loss: 178.58389, Residuals: -2.17520, Convergence: 0.010085\n",
      "Epoch: 46, Loss: 176.84581, Residuals: -2.14257, Convergence: 0.009828\n",
      "Epoch: 47, Loss: 175.17238, Residuals: -2.11038, Convergence: 0.009553\n",
      "Epoch: 48, Loss: 173.56523, Residuals: -2.07869, Convergence: 0.009260\n",
      "Epoch: 49, Loss: 172.02556, Residuals: -2.04755, Convergence: 0.008950\n",
      "Epoch: 50, Loss: 170.55384, Residuals: -2.01702, Convergence: 0.008629\n",
      "Epoch: 51, Loss: 169.14954, Residuals: -1.98715, Convergence: 0.008302\n",
      "Epoch: 52, Loss: 167.81119, Residuals: -1.95796, Convergence: 0.007975\n",
      "Epoch: 53, Loss: 166.53651, Residuals: -1.92948, Convergence: 0.007654\n",
      "Epoch: 54, Loss: 165.32267, Residuals: -1.90172, Convergence: 0.007342\n",
      "Epoch: 55, Loss: 164.16650, Residuals: -1.87466, Convergence: 0.007043\n",
      "Epoch: 56, Loss: 163.06478, Residuals: -1.84832, Convergence: 0.006756\n",
      "Epoch: 57, Loss: 162.01434, Residuals: -1.82269, Convergence: 0.006484\n",
      "Epoch: 58, Loss: 161.01224, Residuals: -1.79776, Convergence: 0.006224\n",
      "Epoch: 59, Loss: 160.05578, Residuals: -1.77351, Convergence: 0.005976\n",
      "Epoch: 60, Loss: 159.14257, Residuals: -1.74996, Convergence: 0.005738\n",
      "Epoch: 61, Loss: 158.27047, Residuals: -1.72709, Convergence: 0.005510\n",
      "Epoch: 62, Loss: 157.43759, Residuals: -1.70490, Convergence: 0.005290\n",
      "Epoch: 63, Loss: 156.64226, Residuals: -1.68339, Convergence: 0.005077\n",
      "Epoch: 64, Loss: 155.88292, Residuals: -1.66256, Convergence: 0.004871\n",
      "Epoch: 65, Loss: 155.15815, Residuals: -1.64242, Convergence: 0.004671\n",
      "Epoch: 66, Loss: 154.46655, Residuals: -1.62295, Convergence: 0.004477\n",
      "Epoch: 67, Loss: 153.80671, Residuals: -1.60416, Convergence: 0.004290\n",
      "Epoch: 68, Loss: 153.17720, Residuals: -1.58603, Convergence: 0.004110\n",
      "Epoch: 69, Loss: 152.57646, Residuals: -1.56856, Convergence: 0.003937\n",
      "Epoch: 70, Loss: 152.00286, Residuals: -1.55172, Convergence: 0.003774\n",
      "Epoch: 71, Loss: 151.45472, Residuals: -1.53551, Convergence: 0.003619\n",
      "Epoch: 72, Loss: 150.93033, Residuals: -1.51988, Convergence: 0.003474\n",
      "Epoch: 73, Loss: 150.42807, Residuals: -1.50482, Convergence: 0.003339\n",
      "Epoch: 74, Loss: 149.94645, Residuals: -1.49029, Convergence: 0.003212\n",
      "Epoch: 75, Loss: 149.48414, Residuals: -1.47627, Convergence: 0.003093\n",
      "Epoch: 76, Loss: 149.04007, Residuals: -1.46273, Convergence: 0.002980\n",
      "Epoch: 77, Loss: 148.61335, Residuals: -1.44966, Convergence: 0.002871\n",
      "Epoch: 78, Loss: 148.20328, Residuals: -1.43703, Convergence: 0.002767\n",
      "Epoch: 79, Loss: 147.80930, Residuals: -1.42483, Convergence: 0.002665\n",
      "Epoch: 80, Loss: 147.43096, Residuals: -1.41305, Convergence: 0.002566\n",
      "Epoch: 81, Loss: 147.06785, Residuals: -1.40167, Convergence: 0.002469\n",
      "Epoch: 82, Loss: 146.71959, Residuals: -1.39068, Convergence: 0.002374\n",
      "Epoch: 83, Loss: 146.38583, Residuals: -1.38009, Convergence: 0.002280\n",
      "Epoch: 84, Loss: 146.06620, Residuals: -1.36987, Convergence: 0.002188\n",
      "Epoch: 85, Loss: 145.76032, Residuals: -1.36002, Convergence: 0.002099\n",
      "Epoch: 86, Loss: 145.46778, Residuals: -1.35053, Convergence: 0.002011\n",
      "Epoch: 87, Loss: 145.18818, Residuals: -1.34140, Convergence: 0.001926\n",
      "Epoch: 88, Loss: 144.92109, Residuals: -1.33260, Convergence: 0.001843\n",
      "Epoch: 89, Loss: 144.66606, Residuals: -1.32414, Convergence: 0.001763\n",
      "Epoch: 90, Loss: 144.42266, Residuals: -1.31601, Convergence: 0.001685\n",
      "Epoch: 91, Loss: 144.19047, Residuals: -1.30818, Convergence: 0.001610\n",
      "Epoch: 92, Loss: 143.96905, Residuals: -1.30066, Convergence: 0.001538\n",
      "Epoch: 93, Loss: 143.75798, Residuals: -1.29344, Convergence: 0.001468\n",
      "Epoch: 94, Loss: 143.55688, Residuals: -1.28650, Convergence: 0.001401\n",
      "Epoch: 95, Loss: 143.36536, Residuals: -1.27984, Convergence: 0.001336\n",
      "Epoch: 96, Loss: 143.18308, Residuals: -1.27344, Convergence: 0.001273\n",
      "Epoch: 97, Loss: 143.00971, Residuals: -1.26730, Convergence: 0.001212\n",
      "Epoch: 98, Loss: 142.84495, Residuals: -1.26141, Convergence: 0.001153\n",
      "Epoch: 99, Loss: 142.68852, Residuals: -1.25576, Convergence: 0.001096\n",
      "Epoch: 100, Loss: 142.54019, Residuals: -1.25035, Convergence: 0.001041\n",
      "Epoch: 101, Loss: 142.39971, Residuals: -1.24517, Convergence: 0.000986\n",
      "Evidence -183.591\n",
      "\n",
      "Epoch: 101, Evidence: -183.59128, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 7.24e-01\n",
      "Epoch: 101, Loss: 1369.05174, Residuals: -1.24517, Convergence:   inf\n",
      "Epoch: 102, Loss: 1307.69854, Residuals: -1.27572, Convergence: 0.046917\n",
      "Epoch: 103, Loss: 1260.88930, Residuals: -1.29950, Convergence: 0.037124\n",
      "Epoch: 104, Loss: 1225.36906, Residuals: -1.31632, Convergence: 0.028987\n",
      "Epoch: 105, Loss: 1197.60138, Residuals: -1.32788, Convergence: 0.023186\n",
      "Epoch: 106, Loss: 1175.05130, Residuals: -1.33623, Convergence: 0.019191\n",
      "Epoch: 107, Loss: 1156.26534, Residuals: -1.34250, Convergence: 0.016247\n",
      "Epoch: 108, Loss: 1140.36725, Residuals: -1.34718, Convergence: 0.013941\n",
      "Epoch: 109, Loss: 1126.75965, Residuals: -1.35054, Convergence: 0.012077\n",
      "Epoch: 110, Loss: 1114.99423, Residuals: -1.35275, Convergence: 0.010552\n",
      "Epoch: 111, Loss: 1104.71840, Residuals: -1.35392, Convergence: 0.009302\n",
      "Epoch: 112, Loss: 1095.64646, Residuals: -1.35418, Convergence: 0.008280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 113, Loss: 1087.54146, Residuals: -1.35359, Convergence: 0.007453\n",
      "Epoch: 114, Loss: 1080.20516, Residuals: -1.35223, Convergence: 0.006792\n",
      "Epoch: 115, Loss: 1073.46846, Residuals: -1.35015, Convergence: 0.006276\n",
      "Epoch: 116, Loss: 1067.18698, Residuals: -1.34736, Convergence: 0.005886\n",
      "Epoch: 117, Loss: 1061.23661, Residuals: -1.34390, Convergence: 0.005607\n",
      "Epoch: 118, Loss: 1055.51012, Residuals: -1.33977, Convergence: 0.005425\n",
      "Epoch: 119, Loss: 1049.91826, Residuals: -1.33498, Convergence: 0.005326\n",
      "Epoch: 120, Loss: 1044.38957, Residuals: -1.32954, Convergence: 0.005294\n",
      "Epoch: 121, Loss: 1038.87721, Residuals: -1.32351, Convergence: 0.005306\n",
      "Epoch: 122, Loss: 1033.36987, Residuals: -1.31693, Convergence: 0.005330\n",
      "Epoch: 123, Loss: 1027.89912, Residuals: -1.30991, Convergence: 0.005322\n",
      "Epoch: 124, Loss: 1022.53435, Residuals: -1.30253, Convergence: 0.005247\n",
      "Epoch: 125, Loss: 1017.35931, Residuals: -1.29490, Convergence: 0.005087\n",
      "Epoch: 126, Loss: 1012.44360, Residuals: -1.28710, Convergence: 0.004855\n",
      "Epoch: 127, Loss: 1007.82492, Residuals: -1.27920, Convergence: 0.004583\n",
      "Epoch: 128, Loss: 1003.51029, Residuals: -1.27128, Convergence: 0.004300\n",
      "Epoch: 129, Loss: 999.48590, Residuals: -1.26338, Convergence: 0.004026\n",
      "Epoch: 130, Loss: 995.72832, Residuals: -1.25555, Convergence: 0.003774\n",
      "Epoch: 131, Loss: 992.21115, Residuals: -1.24783, Convergence: 0.003545\n",
      "Epoch: 132, Loss: 988.90992, Residuals: -1.24026, Convergence: 0.003338\n",
      "Epoch: 133, Loss: 985.80224, Residuals: -1.23285, Convergence: 0.003152\n",
      "Epoch: 134, Loss: 982.86993, Residuals: -1.22562, Convergence: 0.002983\n",
      "Epoch: 135, Loss: 980.09645, Residuals: -1.21861, Convergence: 0.002830\n",
      "Epoch: 136, Loss: 977.46924, Residuals: -1.21181, Convergence: 0.002688\n",
      "Epoch: 137, Loss: 974.97665, Residuals: -1.20524, Convergence: 0.002557\n",
      "Epoch: 138, Loss: 972.60927, Residuals: -1.19891, Convergence: 0.002434\n",
      "Epoch: 139, Loss: 970.35803, Residuals: -1.19282, Convergence: 0.002320\n",
      "Epoch: 140, Loss: 968.21577, Residuals: -1.18696, Convergence: 0.002213\n",
      "Epoch: 141, Loss: 966.17565, Residuals: -1.18134, Convergence: 0.002112\n",
      "Epoch: 142, Loss: 964.23097, Residuals: -1.17596, Convergence: 0.002017\n",
      "Epoch: 143, Loss: 962.37609, Residuals: -1.17082, Convergence: 0.001927\n",
      "Epoch: 144, Loss: 960.60489, Residuals: -1.16590, Convergence: 0.001844\n",
      "Epoch: 145, Loss: 958.91238, Residuals: -1.16119, Convergence: 0.001765\n",
      "Epoch: 146, Loss: 957.29307, Residuals: -1.15670, Convergence: 0.001692\n",
      "Epoch: 147, Loss: 955.74189, Residuals: -1.15241, Convergence: 0.001623\n",
      "Epoch: 148, Loss: 954.25399, Residuals: -1.14831, Convergence: 0.001559\n",
      "Epoch: 149, Loss: 952.82444, Residuals: -1.14440, Convergence: 0.001500\n",
      "Epoch: 150, Loss: 951.44819, Residuals: -1.14066, Convergence: 0.001446\n",
      "Epoch: 151, Loss: 950.12037, Residuals: -1.13708, Convergence: 0.001398\n",
      "Epoch: 152, Loss: 948.83615, Residuals: -1.13365, Convergence: 0.001353\n",
      "Epoch: 153, Loss: 947.59056, Residuals: -1.13037, Convergence: 0.001314\n",
      "Epoch: 154, Loss: 946.37823, Residuals: -1.12721, Convergence: 0.001281\n",
      "Epoch: 155, Loss: 945.19395, Residuals: -1.12416, Convergence: 0.001253\n",
      "Epoch: 156, Loss: 944.03290, Residuals: -1.12122, Convergence: 0.001230\n",
      "Epoch: 157, Loss: 942.88936, Residuals: -1.11837, Convergence: 0.001213\n",
      "Epoch: 158, Loss: 941.75814, Residuals: -1.11560, Convergence: 0.001201\n",
      "Epoch: 159, Loss: 940.63437, Residuals: -1.11289, Convergence: 0.001195\n",
      "Epoch: 160, Loss: 939.51393, Residuals: -1.11024, Convergence: 0.001193\n",
      "Epoch: 161, Loss: 938.39309, Residuals: -1.10762, Convergence: 0.001194\n",
      "Epoch: 162, Loss: 937.26942, Residuals: -1.10505, Convergence: 0.001199\n",
      "Epoch: 163, Loss: 936.14302, Residuals: -1.10250, Convergence: 0.001203\n",
      "Epoch: 164, Loss: 935.01382, Residuals: -1.09997, Convergence: 0.001208\n",
      "Epoch: 165, Loss: 933.88494, Residuals: -1.09748, Convergence: 0.001209\n",
      "Epoch: 166, Loss: 932.76007, Residuals: -1.09502, Convergence: 0.001206\n",
      "Epoch: 167, Loss: 931.64366, Residuals: -1.09260, Convergence: 0.001198\n",
      "Epoch: 168, Loss: 930.53998, Residuals: -1.09023, Convergence: 0.001186\n",
      "Epoch: 169, Loss: 929.45390, Residuals: -1.08792, Convergence: 0.001169\n",
      "Epoch: 170, Loss: 928.38884, Residuals: -1.08567, Convergence: 0.001147\n",
      "Epoch: 171, Loss: 927.34792, Residuals: -1.08348, Convergence: 0.001122\n",
      "Epoch: 172, Loss: 926.33322, Residuals: -1.08137, Convergence: 0.001095\n",
      "Epoch: 173, Loss: 925.34615, Residuals: -1.07932, Convergence: 0.001067\n",
      "Epoch: 174, Loss: 924.38721, Residuals: -1.07735, Convergence: 0.001037\n",
      "Epoch: 175, Loss: 923.45707, Residuals: -1.07546, Convergence: 0.001007\n",
      "Epoch: 176, Loss: 922.55541, Residuals: -1.07363, Convergence: 0.000977\n",
      "Evidence 11159.295\n",
      "\n",
      "Epoch: 176, Evidence: 11159.29492, Convergence: 1.016452\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 5.74e-01\n",
      "Epoch: 176, Loss: 2341.78964, Residuals: -1.07363, Convergence:   inf\n",
      "Epoch: 177, Loss: 2303.10699, Residuals: -1.08117, Convergence: 0.016796\n",
      "Epoch: 178, Loss: 2276.45796, Residuals: -1.07992, Convergence: 0.011706\n",
      "Epoch: 179, Loss: 2254.31851, Residuals: -1.07782, Convergence: 0.009821\n",
      "Epoch: 180, Loss: 2235.63914, Residuals: -1.07556, Convergence: 0.008355\n",
      "Epoch: 181, Loss: 2219.72752, Residuals: -1.07323, Convergence: 0.007168\n",
      "Epoch: 182, Loss: 2206.04486, Residuals: -1.07086, Convergence: 0.006202\n",
      "Epoch: 183, Loss: 2194.15209, Residuals: -1.06844, Convergence: 0.005420\n",
      "Epoch: 184, Loss: 2183.68684, Residuals: -1.06597, Convergence: 0.004792\n",
      "Epoch: 185, Loss: 2174.35770, Residuals: -1.06340, Convergence: 0.004291\n",
      "Epoch: 186, Loss: 2165.93677, Residuals: -1.06072, Convergence: 0.003888\n",
      "Epoch: 187, Loss: 2158.26594, Residuals: -1.05790, Convergence: 0.003554\n",
      "Epoch: 188, Loss: 2151.24336, Residuals: -1.05497, Convergence: 0.003264\n",
      "Epoch: 189, Loss: 2144.80510, Residuals: -1.05197, Convergence: 0.003002\n",
      "Epoch: 190, Loss: 2138.90492, Residuals: -1.04893, Convergence: 0.002759\n",
      "Epoch: 191, Loss: 2133.49866, Residuals: -1.04591, Convergence: 0.002534\n",
      "Epoch: 192, Loss: 2128.54463, Residuals: -1.04294, Convergence: 0.002327\n",
      "Epoch: 193, Loss: 2123.99959, Residuals: -1.04004, Convergence: 0.002140\n",
      "Epoch: 194, Loss: 2119.82533, Residuals: -1.03722, Convergence: 0.001969\n",
      "Epoch: 195, Loss: 2115.98526, Residuals: -1.03450, Convergence: 0.001815\n",
      "Epoch: 196, Loss: 2112.44750, Residuals: -1.03189, Convergence: 0.001675\n",
      "Epoch: 197, Loss: 2109.18408, Residuals: -1.02939, Convergence: 0.001547\n",
      "Epoch: 198, Loss: 2106.16878, Residuals: -1.02699, Convergence: 0.001432\n",
      "Epoch: 199, Loss: 2103.37841, Residuals: -1.02471, Convergence: 0.001327\n",
      "Epoch: 200, Loss: 2100.79207, Residuals: -1.02253, Convergence: 0.001231\n",
      "Epoch: 201, Loss: 2098.39007, Residuals: -1.02045, Convergence: 0.001145\n",
      "Epoch: 202, Loss: 2096.15495, Residuals: -1.01847, Convergence: 0.001066\n",
      "Epoch: 203, Loss: 2094.07002, Residuals: -1.01657, Convergence: 0.000996\n",
      "Evidence 14256.030\n",
      "\n",
      "Epoch: 203, Evidence: 14256.03027, Convergence: 0.217223\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 4.38e-01\n",
      "Epoch: 203, Loss: 2464.59735, Residuals: -1.01657, Convergence:   inf\n",
      "Epoch: 204, Loss: 2450.85249, Residuals: -1.01418, Convergence: 0.005608\n",
      "Epoch: 205, Loss: 2439.68632, Residuals: -1.01112, Convergence: 0.004577\n",
      "Epoch: 206, Loss: 2430.09624, Residuals: -1.00810, Convergence: 0.003946\n",
      "Epoch: 207, Loss: 2421.78830, Residuals: -1.00525, Convergence: 0.003430\n",
      "Epoch: 208, Loss: 2414.54932, Residuals: -1.00261, Convergence: 0.002998\n",
      "Epoch: 209, Loss: 2408.21315, Residuals: -1.00018, Convergence: 0.002631\n",
      "Epoch: 210, Loss: 2402.64000, Residuals: -0.99793, Convergence: 0.002320\n",
      "Epoch: 211, Loss: 2397.71486, Residuals: -0.99585, Convergence: 0.002054\n",
      "Epoch: 212, Loss: 2393.33735, Residuals: -0.99391, Convergence: 0.001829\n",
      "Epoch: 213, Loss: 2389.42537, Residuals: -0.99210, Convergence: 0.001637\n",
      "Epoch: 214, Loss: 2385.91008, Residuals: -0.99041, Convergence: 0.001473\n",
      "Epoch: 215, Loss: 2382.73193, Residuals: -0.98883, Convergence: 0.001334\n",
      "Epoch: 216, Loss: 2379.84323, Residuals: -0.98734, Convergence: 0.001214\n",
      "Epoch: 217, Loss: 2377.20179, Residuals: -0.98595, Convergence: 0.001111\n",
      "Epoch: 218, Loss: 2374.77568, Residuals: -0.98464, Convergence: 0.001022\n",
      "Epoch: 219, Loss: 2372.53669, Residuals: -0.98342, Convergence: 0.000944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence 14627.803\n",
      "\n",
      "Epoch: 219, Evidence: 14627.80273, Convergence: 0.025415\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 3.37e-01\n",
      "Epoch: 219, Loss: 2469.63545, Residuals: -0.98342, Convergence:   inf\n",
      "Epoch: 220, Loss: 2463.17389, Residuals: -0.98045, Convergence: 0.002623\n",
      "Epoch: 221, Loss: 2457.84889, Residuals: -0.97778, Convergence: 0.002167\n",
      "Epoch: 222, Loss: 2453.33382, Residuals: -0.97549, Convergence: 0.001840\n",
      "Epoch: 223, Loss: 2449.45263, Residuals: -0.97352, Convergence: 0.001585\n",
      "Epoch: 224, Loss: 2446.07591, Residuals: -0.97181, Convergence: 0.001380\n",
      "Epoch: 225, Loss: 2443.10207, Residuals: -0.97032, Convergence: 0.001217\n",
      "Epoch: 226, Loss: 2440.45379, Residuals: -0.96901, Convergence: 0.001085\n",
      "Epoch: 227, Loss: 2438.07012, Residuals: -0.96785, Convergence: 0.000978\n",
      "Evidence 14709.910\n",
      "\n",
      "Epoch: 227, Evidence: 14709.91016, Convergence: 0.005582\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 2.65e-01\n",
      "Epoch: 227, Loss: 2471.27156, Residuals: -0.96785, Convergence:   inf\n",
      "Epoch: 228, Loss: 2467.36771, Residuals: -0.96566, Convergence: 0.001582\n",
      "Epoch: 229, Loss: 2464.13361, Residuals: -0.96386, Convergence: 0.001312\n",
      "Epoch: 230, Loss: 2461.37161, Residuals: -0.96235, Convergence: 0.001122\n",
      "Epoch: 231, Loss: 2458.96642, Residuals: -0.96108, Convergence: 0.000978\n",
      "Evidence 14737.734\n",
      "\n",
      "Epoch: 231, Evidence: 14737.73438, Convergence: 0.001888\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 2.14e-01\n",
      "Epoch: 231, Loss: 2472.21978, Residuals: -0.96108, Convergence:   inf\n",
      "Epoch: 232, Loss: 2469.34147, Residuals: -0.95932, Convergence: 0.001166\n",
      "Epoch: 233, Loss: 2466.92483, Residuals: -0.95786, Convergence: 0.000980\n",
      "Evidence 14749.854\n",
      "\n",
      "Epoch: 233, Evidence: 14749.85449, Convergence: 0.000822\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.78e-01\n",
      "Epoch: 233, Loss: 2472.90958, Residuals: -0.95786, Convergence:   inf\n",
      "Epoch: 234, Loss: 2468.41215, Residuals: -0.95540, Convergence: 0.001822\n",
      "Epoch: 235, Loss: 2464.90612, Residuals: -0.95341, Convergence: 0.001422\n",
      "Epoch: 236, Loss: 2462.06040, Residuals: -0.95194, Convergence: 0.001156\n",
      "Epoch: 237, Loss: 2459.64893, Residuals: -0.95105, Convergence: 0.000980\n",
      "Evidence 14767.867\n",
      "\n",
      "Epoch: 237, Evidence: 14767.86719, Convergence: 0.002040\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.48e-01\n",
      "Epoch: 237, Loss: 2473.06183, Residuals: -0.95105, Convergence:   inf\n",
      "Epoch: 238, Loss: 2470.00013, Residuals: -0.94862, Convergence: 0.001240\n",
      "Epoch: 239, Loss: 2467.56378, Residuals: -0.94725, Convergence: 0.000987\n",
      "Evidence 14779.030\n",
      "\n",
      "Epoch: 239, Evidence: 14779.03027, Convergence: 0.000755\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.26e-01\n",
      "Epoch: 239, Loss: 2473.23219, Residuals: -0.94725, Convergence:   inf\n",
      "Epoch: 240, Loss: 2468.69198, Residuals: -0.94407, Convergence: 0.001839\n",
      "Epoch: 241, Loss: 2465.43732, Residuals: -0.94441, Convergence: 0.001320\n",
      "Epoch: 242, Loss: 2462.81120, Residuals: -0.94496, Convergence: 0.001066\n",
      "Epoch: 243, Loss: 2460.48140, Residuals: -0.94725, Convergence: 0.000947\n",
      "Evidence 14795.300\n",
      "\n",
      "Epoch: 243, Evidence: 14795.29980, Convergence: 0.001854\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.12e-01\n",
      "Epoch: 243, Loss: 2472.53066, Residuals: -0.94725, Convergence:   inf\n",
      "Epoch: 244, Loss: 2470.79286, Residuals: -0.94550, Convergence: 0.000703\n",
      "Evidence 14802.020\n",
      "\n",
      "Epoch: 244, Evidence: 14802.01953, Convergence: 0.000454\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 9.18e-02\n",
      "Epoch: 244, Loss: 2473.42701, Residuals: -0.94550, Convergence:   inf\n",
      "Epoch: 245, Loss: 2521.86903, Residuals: -0.99253, Convergence: -0.019209\n",
      "Epoch: 245, Loss: 2470.93146, Residuals: -0.94412, Convergence: 0.001010\n",
      "Epoch: 246, Loss: 2470.78880, Residuals: -0.94604, Convergence: 0.000058\n",
      "Evidence 14807.131\n",
      "\n",
      "Epoch: 246, Evidence: 14807.13086, Convergence: 0.000799\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 7.52e-02\n",
      "Epoch: 246, Loss: 2473.36012, Residuals: -0.94604, Convergence:   inf\n",
      "Epoch: 247, Loss: 2531.71185, Residuals: -0.99618, Convergence: -0.023048\n",
      "Epoch: 247, Loss: 2470.69766, Residuals: -0.94222, Convergence: 0.001078\n",
      "Epoch: 248, Loss: 2470.07427, Residuals: -0.94143, Convergence: 0.000252\n",
      "Evidence 14813.098\n",
      "\n",
      "Epoch: 248, Evidence: 14813.09766, Convergence: 0.001201\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 6.44e-02\n",
      "Epoch: 248, Loss: 2473.14197, Residuals: -0.94143, Convergence:   inf\n",
      "Epoch: 249, Loss: 2471.21629, Residuals: -0.93808, Convergence: 0.000779\n",
      "Evidence 14816.939\n",
      "\n",
      "Epoch: 249, Evidence: 14816.93945, Convergence: 0.000259\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 6.05e-02\n",
      "Epoch: 249, Loss: 2472.94336, Residuals: -0.93808, Convergence:   inf\n",
      "Epoch: 250, Loss: 2476.81758, Residuals: -0.93886, Convergence: -0.001564\n",
      "Epoch: 250, Loss: 2473.11695, Residuals: -0.93638, Convergence: -0.000070\n",
      "Evidence 14818.074\n",
      "\n",
      "Epoch: 250, Evidence: 14818.07422, Convergence: 0.000336\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 5.23e-02\n",
      "Epoch: 250, Loss: 2473.38512, Residuals: -0.93638, Convergence:   inf\n",
      "Epoch: 251, Loss: 2529.21924, Residuals: -0.99151, Convergence: -0.022076\n",
      "Epoch: 251, Loss: 2471.72908, Residuals: -0.93292, Convergence: 0.000670\n",
      "Evidence 14820.867\n",
      "\n",
      "Epoch: 251, Evidence: 14820.86719, Convergence: 0.000524\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 383.07745, Residuals: -4.54304, Convergence:   inf\n",
      "Epoch: 1, Loss: 357.26368, Residuals: -4.42099, Convergence: 0.072254\n",
      "Epoch: 2, Loss: 336.16631, Residuals: -4.25559, Convergence: 0.062759\n",
      "Epoch: 3, Loss: 320.03002, Residuals: -4.08984, Convergence: 0.050421\n",
      "Epoch: 4, Loss: 307.71576, Residuals: -3.94354, Convergence: 0.040018\n",
      "Epoch: 5, Loss: 297.94525, Residuals: -3.81414, Convergence: 0.032793\n",
      "Epoch: 6, Loss: 290.01180, Residuals: -3.70150, Convergence: 0.027356\n",
      "Epoch: 7, Loss: 283.42522, Residuals: -3.60517, Convergence: 0.023239\n",
      "Epoch: 8, Loss: 277.82357, Residuals: -3.52303, Convergence: 0.020163\n",
      "Epoch: 9, Loss: 272.95165, Residuals: -3.45269, Convergence: 0.017849\n",
      "Epoch: 10, Loss: 268.62788, Residuals: -3.39202, Convergence: 0.016096\n",
      "Epoch: 11, Loss: 264.72047, Residuals: -3.33918, Convergence: 0.014761\n",
      "Epoch: 12, Loss: 261.13294, Residuals: -3.29260, Convergence: 0.013738\n",
      "Epoch: 13, Loss: 257.79463, Residuals: -3.25095, Convergence: 0.012949\n",
      "Epoch: 14, Loss: 254.65407, Residuals: -3.21308, Convergence: 0.012333\n",
      "Epoch: 15, Loss: 251.67483, Residuals: -3.17802, Convergence: 0.011838\n",
      "Epoch: 16, Loss: 248.83356, Residuals: -3.14506, Convergence: 0.011418\n",
      "Epoch: 17, Loss: 246.11462, Residuals: -3.11374, Convergence: 0.011047\n",
      "Epoch: 18, Loss: 243.49869, Residuals: -3.08370, Convergence: 0.010743\n",
      "Epoch: 19, Loss: 240.95631, Residuals: -3.05446, Convergence: 0.010551\n",
      "Epoch: 20, Loss: 238.45284, Residuals: -3.02547, Convergence: 0.010499\n",
      "Epoch: 21, Loss: 235.95848, Residuals: -2.99622, Convergence: 0.010571\n",
      "Epoch: 22, Loss: 233.45342, Residuals: -2.96642, Convergence: 0.010730\n",
      "Epoch: 23, Loss: 230.91796, Residuals: -2.93582, Convergence: 0.010980\n",
      "Epoch: 24, Loss: 228.31425, Residuals: -2.90399, Convergence: 0.011404\n",
      "Epoch: 25, Loss: 225.58452, Residuals: -2.87023, Convergence: 0.012101\n",
      "Epoch: 26, Loss: 222.69753, Residuals: -2.83408, Convergence: 0.012964\n",
      "Epoch: 27, Loss: 219.73795, Residuals: -2.79633, Convergence: 0.013469\n",
      "Epoch: 28, Loss: 216.83805, Residuals: -2.75843, Convergence: 0.013374\n",
      "Epoch: 29, Loss: 214.04183, Residuals: -2.72099, Convergence: 0.013064\n",
      "Epoch: 30, Loss: 211.34090, Residuals: -2.68403, Convergence: 0.012780\n",
      "Epoch: 31, Loss: 208.71953, Residuals: -2.64747, Convergence: 0.012559\n",
      "Epoch: 32, Loss: 206.16564, Residuals: -2.61120, Convergence: 0.012388\n",
      "Epoch: 33, Loss: 203.67159, Residuals: -2.57519, Convergence: 0.012245\n",
      "Epoch: 34, Loss: 201.23320, Residuals: -2.53940, Convergence: 0.012117\n",
      "Epoch: 35, Loss: 198.84877, Residuals: -2.50382, Convergence: 0.011991\n",
      "Epoch: 36, Loss: 196.51829, Residuals: -2.46846, Convergence: 0.011859\n",
      "Epoch: 37, Loss: 194.24288, Residuals: -2.43333, Convergence: 0.011714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38, Loss: 192.02433, Residuals: -2.39843, Convergence: 0.011553\n",
      "Epoch: 39, Loss: 189.86479, Residuals: -2.36379, Convergence: 0.011374\n",
      "Epoch: 40, Loss: 187.76640, Residuals: -2.32943, Convergence: 0.011176\n",
      "Epoch: 41, Loss: 185.73119, Residuals: -2.29539, Convergence: 0.010958\n",
      "Epoch: 42, Loss: 183.76086, Residuals: -2.26169, Convergence: 0.010722\n",
      "Epoch: 43, Loss: 181.85664, Residuals: -2.22836, Convergence: 0.010471\n",
      "Epoch: 44, Loss: 180.01928, Residuals: -2.19545, Convergence: 0.010206\n",
      "Epoch: 45, Loss: 178.24895, Residuals: -2.16298, Convergence: 0.009932\n",
      "Epoch: 46, Loss: 176.54534, Residuals: -2.13097, Convergence: 0.009650\n",
      "Epoch: 47, Loss: 174.90767, Residuals: -2.09946, Convergence: 0.009363\n",
      "Epoch: 48, Loss: 173.33488, Residuals: -2.06847, Convergence: 0.009074\n",
      "Epoch: 49, Loss: 171.82578, Residuals: -2.03803, Convergence: 0.008783\n",
      "Epoch: 50, Loss: 170.37914, Residuals: -2.00816, Convergence: 0.008491\n",
      "Epoch: 51, Loss: 168.99383, Residuals: -1.97889, Convergence: 0.008197\n",
      "Epoch: 52, Loss: 167.66870, Residuals: -1.95024, Convergence: 0.007903\n",
      "Epoch: 53, Loss: 166.40252, Residuals: -1.92223, Convergence: 0.007609\n",
      "Epoch: 54, Loss: 165.19388, Residuals: -1.89490, Convergence: 0.007316\n",
      "Epoch: 55, Loss: 164.04106, Residuals: -1.86824, Convergence: 0.007028\n",
      "Epoch: 56, Loss: 162.94204, Residuals: -1.84227, Convergence: 0.006745\n",
      "Epoch: 57, Loss: 161.89459, Residuals: -1.81699, Convergence: 0.006470\n",
      "Epoch: 58, Loss: 160.89624, Residuals: -1.79241, Convergence: 0.006205\n",
      "Epoch: 59, Loss: 159.94438, Residuals: -1.76851, Convergence: 0.005951\n",
      "Epoch: 60, Loss: 159.03629, Residuals: -1.74529, Convergence: 0.005710\n",
      "Epoch: 61, Loss: 158.16917, Residuals: -1.72275, Convergence: 0.005482\n",
      "Epoch: 62, Loss: 157.34025, Residuals: -1.70086, Convergence: 0.005268\n",
      "Epoch: 63, Loss: 156.54685, Residuals: -1.67961, Convergence: 0.005068\n",
      "Epoch: 64, Loss: 155.78644, Residuals: -1.65898, Convergence: 0.004881\n",
      "Epoch: 65, Loss: 155.05679, Residuals: -1.63895, Convergence: 0.004706\n",
      "Epoch: 66, Loss: 154.35589, Residuals: -1.61951, Convergence: 0.004541\n",
      "Epoch: 67, Loss: 153.68205, Residuals: -1.60063, Convergence: 0.004385\n",
      "Epoch: 68, Loss: 153.03387, Residuals: -1.58231, Convergence: 0.004236\n",
      "Epoch: 69, Loss: 152.41020, Residuals: -1.56452, Convergence: 0.004092\n",
      "Epoch: 70, Loss: 151.81006, Residuals: -1.54727, Convergence: 0.003953\n",
      "Epoch: 71, Loss: 151.23266, Residuals: -1.53054, Convergence: 0.003818\n",
      "Epoch: 72, Loss: 150.67733, Residuals: -1.51433, Convergence: 0.003686\n",
      "Epoch: 73, Loss: 150.14346, Residuals: -1.49862, Convergence: 0.003556\n",
      "Epoch: 74, Loss: 149.63054, Residuals: -1.48342, Convergence: 0.003428\n",
      "Epoch: 75, Loss: 149.13806, Residuals: -1.46872, Convergence: 0.003302\n",
      "Epoch: 76, Loss: 148.66553, Residuals: -1.45451, Convergence: 0.003178\n",
      "Epoch: 77, Loss: 148.21250, Residuals: -1.44078, Convergence: 0.003057\n",
      "Epoch: 78, Loss: 147.77846, Residuals: -1.42753, Convergence: 0.002937\n",
      "Epoch: 79, Loss: 147.36293, Residuals: -1.41476, Convergence: 0.002820\n",
      "Epoch: 80, Loss: 146.96540, Residuals: -1.40244, Convergence: 0.002705\n",
      "Epoch: 81, Loss: 146.58534, Residuals: -1.39058, Convergence: 0.002593\n",
      "Epoch: 82, Loss: 146.22219, Residuals: -1.37917, Convergence: 0.002484\n",
      "Epoch: 83, Loss: 145.87539, Residuals: -1.36818, Convergence: 0.002377\n",
      "Epoch: 84, Loss: 145.54436, Residuals: -1.35763, Convergence: 0.002274\n",
      "Epoch: 85, Loss: 145.22850, Residuals: -1.34748, Convergence: 0.002175\n",
      "Epoch: 86, Loss: 144.92722, Residuals: -1.33774, Convergence: 0.002079\n",
      "Epoch: 87, Loss: 144.63990, Residuals: -1.32839, Convergence: 0.001986\n",
      "Epoch: 88, Loss: 144.36597, Residuals: -1.31942, Convergence: 0.001897\n",
      "Epoch: 89, Loss: 144.10481, Residuals: -1.31081, Convergence: 0.001812\n",
      "Epoch: 90, Loss: 143.85586, Residuals: -1.30256, Convergence: 0.001731\n",
      "Epoch: 91, Loss: 143.61857, Residuals: -1.29465, Convergence: 0.001652\n",
      "Epoch: 92, Loss: 143.39238, Residuals: -1.28706, Convergence: 0.001577\n",
      "Epoch: 93, Loss: 143.17681, Residuals: -1.27980, Convergence: 0.001506\n",
      "Epoch: 94, Loss: 142.97136, Residuals: -1.27283, Convergence: 0.001437\n",
      "Epoch: 95, Loss: 142.77558, Residuals: -1.26617, Convergence: 0.001371\n",
      "Epoch: 96, Loss: 142.58907, Residuals: -1.25978, Convergence: 0.001308\n",
      "Epoch: 97, Loss: 142.41143, Residuals: -1.25366, Convergence: 0.001247\n",
      "Epoch: 98, Loss: 142.24232, Residuals: -1.24781, Convergence: 0.001189\n",
      "Epoch: 99, Loss: 142.08141, Residuals: -1.24221, Convergence: 0.001132\n",
      "Epoch: 100, Loss: 141.92843, Residuals: -1.23685, Convergence: 0.001078\n",
      "Epoch: 101, Loss: 141.78312, Residuals: -1.23173, Convergence: 0.001025\n",
      "Epoch: 102, Loss: 141.64525, Residuals: -1.22683, Convergence: 0.000973\n",
      "Evidence -183.488\n",
      "\n",
      "Epoch: 102, Evidence: -183.48758, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.25e-01\n",
      "Epoch: 102, Loss: 1371.92587, Residuals: -1.22683, Convergence:   inf\n",
      "Epoch: 103, Loss: 1309.67785, Residuals: -1.25670, Convergence: 0.047529\n",
      "Epoch: 104, Loss: 1262.08255, Residuals: -1.28030, Convergence: 0.037712\n",
      "Epoch: 105, Loss: 1226.01092, Residuals: -1.29741, Convergence: 0.029422\n",
      "Epoch: 106, Loss: 1197.95598, Residuals: -1.30948, Convergence: 0.023419\n",
      "Epoch: 107, Loss: 1175.30610, Residuals: -1.31842, Convergence: 0.019271\n",
      "Epoch: 108, Loss: 1156.52413, Residuals: -1.32526, Convergence: 0.016240\n",
      "Epoch: 109, Loss: 1140.68147, Residuals: -1.33048, Convergence: 0.013889\n",
      "Epoch: 110, Loss: 1127.14886, Residuals: -1.33432, Convergence: 0.012006\n",
      "Epoch: 111, Loss: 1115.46133, Residuals: -1.33692, Convergence: 0.010478\n",
      "Epoch: 112, Loss: 1105.25757, Residuals: -1.33842, Convergence: 0.009232\n",
      "Epoch: 113, Loss: 1096.24774, Residuals: -1.33892, Convergence: 0.008219\n",
      "Epoch: 114, Loss: 1088.19458, Residuals: -1.33849, Convergence: 0.007400\n",
      "Epoch: 115, Loss: 1080.90013, Residuals: -1.33723, Convergence: 0.006748\n",
      "Epoch: 116, Loss: 1074.19698, Residuals: -1.33517, Convergence: 0.006240\n",
      "Epoch: 117, Loss: 1067.94105, Residuals: -1.33237, Convergence: 0.005858\n",
      "Epoch: 118, Loss: 1062.00681, Residuals: -1.32884, Convergence: 0.005588\n",
      "Epoch: 119, Loss: 1056.28389, Residuals: -1.32462, Convergence: 0.005418\n",
      "Epoch: 120, Loss: 1050.67762, Residuals: -1.31971, Convergence: 0.005336\n",
      "Epoch: 121, Loss: 1045.11192, Residuals: -1.31415, Convergence: 0.005325\n",
      "Epoch: 122, Loss: 1039.53877, Residuals: -1.30798, Convergence: 0.005361\n",
      "Epoch: 123, Loss: 1033.95197, Residuals: -1.30127, Convergence: 0.005403\n",
      "Epoch: 124, Loss: 1028.39400, Residuals: -1.29414, Convergence: 0.005405\n",
      "Epoch: 125, Loss: 1022.94662, Residuals: -1.28668, Convergence: 0.005325\n",
      "Epoch: 126, Loss: 1017.70023, Residuals: -1.27897, Convergence: 0.005155\n",
      "Epoch: 127, Loss: 1012.72450, Residuals: -1.27112, Convergence: 0.004913\n",
      "Epoch: 128, Loss: 1008.05251, Residuals: -1.26318, Convergence: 0.004635\n",
      "Epoch: 129, Loss: 1003.68576, Residuals: -1.25521, Convergence: 0.004351\n",
      "Epoch: 130, Loss: 999.60681, Residuals: -1.24728, Convergence: 0.004081\n",
      "Epoch: 131, Loss: 995.79001, Residuals: -1.23943, Convergence: 0.003833\n",
      "Epoch: 132, Loss: 992.20736, Residuals: -1.23169, Convergence: 0.003611\n",
      "Epoch: 133, Loss: 988.83412, Residuals: -1.22409, Convergence: 0.003411\n",
      "Epoch: 134, Loss: 985.64817, Residuals: -1.21667, Convergence: 0.003232\n",
      "Epoch: 135, Loss: 982.63144, Residuals: -1.20944, Convergence: 0.003070\n",
      "Epoch: 136, Loss: 979.76833, Residuals: -1.20241, Convergence: 0.002922\n",
      "Epoch: 137, Loss: 977.04631, Residuals: -1.19560, Convergence: 0.002786\n",
      "Epoch: 138, Loss: 974.45487, Residuals: -1.18903, Convergence: 0.002659\n",
      "Epoch: 139, Loss: 971.98498, Residuals: -1.18269, Convergence: 0.002541\n",
      "Epoch: 140, Loss: 969.62895, Residuals: -1.17658, Convergence: 0.002430\n",
      "Epoch: 141, Loss: 967.37972, Residuals: -1.17072, Convergence: 0.002325\n",
      "Epoch: 142, Loss: 965.23146, Residuals: -1.16509, Convergence: 0.002226\n",
      "Epoch: 143, Loss: 963.17826, Residuals: -1.15970, Convergence: 0.002132\n",
      "Epoch: 144, Loss: 961.21555, Residuals: -1.15453, Convergence: 0.002042\n",
      "Epoch: 145, Loss: 959.33844, Residuals: -1.14959, Convergence: 0.001957\n",
      "Epoch: 146, Loss: 957.54241, Residuals: -1.14486, Convergence: 0.001876\n",
      "Epoch: 147, Loss: 955.82371, Residuals: -1.14034, Convergence: 0.001798\n",
      "Epoch: 148, Loss: 954.17840, Residuals: -1.13602, Convergence: 0.001724\n",
      "Epoch: 149, Loss: 952.60268, Residuals: -1.13189, Convergence: 0.001654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 150, Loss: 951.09242, Residuals: -1.12794, Convergence: 0.001588\n",
      "Epoch: 151, Loss: 949.64459, Residuals: -1.12416, Convergence: 0.001525\n",
      "Epoch: 152, Loss: 948.25485, Residuals: -1.12055, Convergence: 0.001466\n",
      "Epoch: 153, Loss: 946.91965, Residuals: -1.11709, Convergence: 0.001410\n",
      "Epoch: 154, Loss: 945.63487, Residuals: -1.11378, Convergence: 0.001359\n",
      "Epoch: 155, Loss: 944.39662, Residuals: -1.11061, Convergence: 0.001311\n",
      "Epoch: 156, Loss: 943.20072, Residuals: -1.10756, Convergence: 0.001268\n",
      "Epoch: 157, Loss: 942.04311, Residuals: -1.10464, Convergence: 0.001229\n",
      "Epoch: 158, Loss: 940.91932, Residuals: -1.10182, Convergence: 0.001194\n",
      "Epoch: 159, Loss: 939.82467, Residuals: -1.09910, Convergence: 0.001165\n",
      "Epoch: 160, Loss: 938.75523, Residuals: -1.09647, Convergence: 0.001139\n",
      "Epoch: 161, Loss: 937.70574, Residuals: -1.09391, Convergence: 0.001119\n",
      "Epoch: 162, Loss: 936.67219, Residuals: -1.09142, Convergence: 0.001103\n",
      "Epoch: 163, Loss: 935.65027, Residuals: -1.08899, Convergence: 0.001092\n",
      "Epoch: 164, Loss: 934.63626, Residuals: -1.08661, Convergence: 0.001085\n",
      "Epoch: 165, Loss: 933.62713, Residuals: -1.08426, Convergence: 0.001081\n",
      "Epoch: 166, Loss: 932.62079, Residuals: -1.08195, Convergence: 0.001079\n",
      "Epoch: 167, Loss: 931.61712, Residuals: -1.07966, Convergence: 0.001077\n",
      "Epoch: 168, Loss: 930.61660, Residuals: -1.07740, Convergence: 0.001075\n",
      "Epoch: 169, Loss: 929.62192, Residuals: -1.07517, Convergence: 0.001070\n",
      "Epoch: 170, Loss: 928.63643, Residuals: -1.07297, Convergence: 0.001061\n",
      "Epoch: 171, Loss: 927.66473, Residuals: -1.07080, Convergence: 0.001047\n",
      "Epoch: 172, Loss: 926.71124, Residuals: -1.06869, Convergence: 0.001029\n",
      "Epoch: 173, Loss: 925.78012, Residuals: -1.06663, Convergence: 0.001006\n",
      "Epoch: 174, Loss: 924.87540, Residuals: -1.06462, Convergence: 0.000978\n",
      "Evidence 11213.170\n",
      "\n",
      "Epoch: 174, Evidence: 11213.16992, Convergence: 1.016364\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.75e-01\n",
      "Epoch: 174, Loss: 2346.63709, Residuals: -1.06462, Convergence:   inf\n",
      "Epoch: 175, Loss: 2309.99733, Residuals: -1.07340, Convergence: 0.015861\n",
      "Epoch: 176, Loss: 2284.42331, Residuals: -1.07233, Convergence: 0.011195\n",
      "Epoch: 177, Loss: 2263.11241, Residuals: -1.07036, Convergence: 0.009417\n",
      "Epoch: 178, Loss: 2245.11230, Residuals: -1.06814, Convergence: 0.008017\n",
      "Epoch: 179, Loss: 2229.75633, Residuals: -1.06579, Convergence: 0.006887\n",
      "Epoch: 180, Loss: 2216.52520, Residuals: -1.06332, Convergence: 0.005969\n",
      "Epoch: 181, Loss: 2204.99297, Residuals: -1.06076, Convergence: 0.005230\n",
      "Epoch: 182, Loss: 2194.81186, Residuals: -1.05809, Convergence: 0.004639\n",
      "Epoch: 183, Loss: 2185.69985, Residuals: -1.05531, Convergence: 0.004169\n",
      "Epoch: 184, Loss: 2177.44436, Residuals: -1.05240, Convergence: 0.003791\n",
      "Epoch: 185, Loss: 2169.89783, Residuals: -1.04939, Convergence: 0.003478\n",
      "Epoch: 186, Loss: 2162.96604, Residuals: -1.04629, Convergence: 0.003205\n",
      "Epoch: 187, Loss: 2156.58798, Residuals: -1.04316, Convergence: 0.002957\n",
      "Epoch: 188, Loss: 2150.71797, Residuals: -1.04004, Convergence: 0.002729\n",
      "Epoch: 189, Loss: 2145.31463, Residuals: -1.03697, Convergence: 0.002519\n",
      "Epoch: 190, Loss: 2140.33891, Residuals: -1.03398, Convergence: 0.002325\n",
      "Epoch: 191, Loss: 2135.75275, Residuals: -1.03108, Convergence: 0.002147\n",
      "Epoch: 192, Loss: 2131.52051, Residuals: -1.02829, Convergence: 0.001986\n",
      "Epoch: 193, Loss: 2127.61087, Residuals: -1.02562, Convergence: 0.001838\n",
      "Epoch: 194, Loss: 2123.99457, Residuals: -1.02307, Convergence: 0.001703\n",
      "Epoch: 195, Loss: 2120.64572, Residuals: -1.02063, Convergence: 0.001579\n",
      "Epoch: 196, Loss: 2117.54051, Residuals: -1.01832, Convergence: 0.001466\n",
      "Epoch: 197, Loss: 2114.65907, Residuals: -1.01611, Convergence: 0.001363\n",
      "Epoch: 198, Loss: 2111.98083, Residuals: -1.01401, Convergence: 0.001268\n",
      "Epoch: 199, Loss: 2109.48792, Residuals: -1.01201, Convergence: 0.001182\n",
      "Epoch: 200, Loss: 2107.16502, Residuals: -1.01011, Convergence: 0.001102\n",
      "Epoch: 201, Loss: 2104.99576, Residuals: -1.00829, Convergence: 0.001031\n",
      "Epoch: 202, Loss: 2102.96751, Residuals: -1.00656, Convergence: 0.000964\n",
      "Evidence 14327.335\n",
      "\n",
      "Epoch: 202, Evidence: 14327.33496, Convergence: 0.217358\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.36e-01\n",
      "Epoch: 202, Loss: 2470.65541, Residuals: -1.00656, Convergence:   inf\n",
      "Epoch: 203, Loss: 2457.65582, Residuals: -1.00377, Convergence: 0.005289\n",
      "Epoch: 204, Loss: 2447.00838, Residuals: -1.00047, Convergence: 0.004351\n",
      "Epoch: 205, Loss: 2437.84751, Residuals: -0.99731, Convergence: 0.003758\n",
      "Epoch: 206, Loss: 2429.92523, Residuals: -0.99437, Convergence: 0.003260\n",
      "Epoch: 207, Loss: 2423.04087, Residuals: -0.99168, Convergence: 0.002841\n",
      "Epoch: 208, Loss: 2417.02777, Residuals: -0.98923, Convergence: 0.002488\n",
      "Epoch: 209, Loss: 2411.74693, Residuals: -0.98702, Convergence: 0.002190\n",
      "Epoch: 210, Loss: 2407.08050, Residuals: -0.98503, Convergence: 0.001939\n",
      "Epoch: 211, Loss: 2402.93255, Residuals: -0.98323, Convergence: 0.001726\n",
      "Epoch: 212, Loss: 2399.22201, Residuals: -0.98160, Convergence: 0.001547\n",
      "Epoch: 213, Loss: 2395.88172, Residuals: -0.98013, Convergence: 0.001394\n",
      "Epoch: 214, Loss: 2392.85787, Residuals: -0.97881, Convergence: 0.001264\n",
      "Epoch: 215, Loss: 2390.10401, Residuals: -0.97761, Convergence: 0.001152\n",
      "Epoch: 216, Loss: 2387.58313, Residuals: -0.97652, Convergence: 0.001056\n",
      "Epoch: 217, Loss: 2385.26476, Residuals: -0.97553, Convergence: 0.000972\n",
      "Evidence 14678.005\n",
      "\n",
      "Epoch: 217, Evidence: 14678.00488, Convergence: 0.023891\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.33e-01\n",
      "Epoch: 217, Loss: 2476.22586, Residuals: -0.97553, Convergence:   inf\n",
      "Epoch: 218, Loss: 2469.88439, Residuals: -0.97233, Convergence: 0.002568\n",
      "Epoch: 219, Loss: 2464.63872, Residuals: -0.96960, Convergence: 0.002128\n",
      "Epoch: 220, Loss: 2460.19308, Residuals: -0.96734, Convergence: 0.001807\n",
      "Epoch: 221, Loss: 2456.36992, Residuals: -0.96546, Convergence: 0.001556\n",
      "Epoch: 222, Loss: 2453.03611, Residuals: -0.96389, Convergence: 0.001359\n",
      "Epoch: 223, Loss: 2450.09096, Residuals: -0.96257, Convergence: 0.001202\n",
      "Epoch: 224, Loss: 2447.45904, Residuals: -0.96147, Convergence: 0.001075\n",
      "Epoch: 225, Loss: 2445.08244, Residuals: -0.96053, Convergence: 0.000972\n",
      "Evidence 14755.668\n",
      "\n",
      "Epoch: 225, Evidence: 14755.66797, Convergence: 0.005263\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.60e-01\n",
      "Epoch: 225, Loss: 2477.94688, Residuals: -0.96053, Convergence:   inf\n",
      "Epoch: 226, Loss: 2474.09256, Residuals: -0.95812, Convergence: 0.001558\n",
      "Epoch: 227, Loss: 2470.88758, Residuals: -0.95624, Convergence: 0.001297\n",
      "Epoch: 228, Loss: 2468.13746, Residuals: -0.95474, Convergence: 0.001114\n",
      "Epoch: 229, Loss: 2465.72838, Residuals: -0.95354, Convergence: 0.000977\n",
      "Evidence 14783.139\n",
      "\n",
      "Epoch: 229, Evidence: 14783.13867, Convergence: 0.001858\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.09e-01\n",
      "Epoch: 229, Loss: 2478.99881, Residuals: -0.95354, Convergence:   inf\n",
      "Epoch: 230, Loss: 2476.11915, Residuals: -0.95164, Convergence: 0.001163\n",
      "Epoch: 231, Loss: 2473.69902, Residuals: -0.95017, Convergence: 0.000978\n",
      "Evidence 14795.121\n",
      "\n",
      "Epoch: 231, Evidence: 14795.12109, Convergence: 0.000810\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.73e-01\n",
      "Epoch: 231, Loss: 2479.75356, Residuals: -0.95017, Convergence:   inf\n",
      "Epoch: 232, Loss: 2475.19397, Residuals: -0.94756, Convergence: 0.001842\n",
      "Epoch: 233, Loss: 2471.70065, Residuals: -0.94576, Convergence: 0.001413\n",
      "Epoch: 234, Loss: 2468.85765, Residuals: -0.94456, Convergence: 0.001152\n",
      "Epoch: 235, Loss: 2466.44094, Residuals: -0.94388, Convergence: 0.000980\n",
      "Evidence 14812.844\n",
      "\n",
      "Epoch: 235, Evidence: 14812.84375, Convergence: 0.002005\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.43e-01\n",
      "Epoch: 235, Loss: 2479.87630, Residuals: -0.94388, Convergence:   inf\n",
      "Epoch: 236, Loss: 2476.81920, Residuals: -0.94140, Convergence: 0.001234\n",
      "Epoch: 237, Loss: 2474.39319, Residuals: -0.94013, Convergence: 0.000980\n",
      "Evidence 14823.782\n",
      "\n",
      "Epoch: 237, Evidence: 14823.78223, Convergence: 0.000738\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.21e-01\n",
      "Epoch: 237, Loss: 2480.07618, Residuals: -0.94013, Convergence:   inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 238, Loss: 2475.58906, Residuals: -0.93639, Convergence: 0.001813\n",
      "Epoch: 239, Loss: 2472.39207, Residuals: -0.93740, Convergence: 0.001293\n",
      "Epoch: 240, Loss: 2469.74592, Residuals: -0.93831, Convergence: 0.001071\n",
      "Epoch: 241, Loss: 2467.39723, Residuals: -0.94100, Convergence: 0.000952\n",
      "Evidence 14839.573\n",
      "\n",
      "Epoch: 241, Evidence: 14839.57324, Convergence: 0.001801\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.08e-01\n",
      "Epoch: 241, Loss: 2479.36834, Residuals: -0.94100, Convergence:   inf\n",
      "Epoch: 242, Loss: 2477.49523, Residuals: -0.93828, Convergence: 0.000756\n",
      "Evidence 14846.423\n",
      "\n",
      "Epoch: 242, Evidence: 14846.42285, Convergence: 0.000461\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.83e-02\n",
      "Epoch: 242, Loss: 2480.31143, Residuals: -0.93828, Convergence:   inf\n",
      "Epoch: 243, Loss: 2518.25007, Residuals: -0.97998, Convergence: -0.015065\n",
      "Epoch: 243, Loss: 2478.07569, Residuals: -0.93602, Convergence: 0.000902\n",
      "Evidence 14850.543\n",
      "\n",
      "Epoch: 243, Evidence: 14850.54297, Convergence: 0.000739\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.19e-02\n",
      "Epoch: 243, Loss: 2479.72997, Residuals: -0.93602, Convergence:   inf\n",
      "Epoch: 244, Loss: 2484.98646, Residuals: -0.94026, Convergence: -0.002115\n",
      "Epoch: 244, Loss: 2479.74315, Residuals: -0.93560, Convergence: -0.000005\n",
      "Evidence 14852.138\n",
      "\n",
      "Epoch: 244, Evidence: 14852.13770, Convergence: 0.000846\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 380.38799, Residuals: -4.50575, Convergence:   inf\n",
      "Epoch: 1, Loss: 354.81409, Residuals: -4.38706, Convergence: 0.072077\n",
      "Epoch: 2, Loss: 333.83946, Residuals: -4.22413, Convergence: 0.062828\n",
      "Epoch: 3, Loss: 317.84467, Residuals: -4.06051, Convergence: 0.050323\n",
      "Epoch: 4, Loss: 305.65190, Residuals: -3.91614, Convergence: 0.039891\n",
      "Epoch: 5, Loss: 295.98697, Residuals: -3.78838, Convergence: 0.032653\n",
      "Epoch: 6, Loss: 288.14376, Residuals: -3.67696, Convergence: 0.027220\n",
      "Epoch: 7, Loss: 281.63384, Residuals: -3.58142, Convergence: 0.023115\n",
      "Epoch: 8, Loss: 276.09481, Residuals: -3.49973, Convergence: 0.020062\n",
      "Epoch: 9, Loss: 271.27104, Residuals: -3.42961, Convergence: 0.017782\n",
      "Epoch: 10, Loss: 266.98132, Residuals: -3.36900, Convergence: 0.016068\n",
      "Epoch: 11, Loss: 263.09489, Residuals: -3.31614, Convergence: 0.014772\n",
      "Epoch: 12, Loss: 259.51692, Residuals: -3.26953, Convergence: 0.013787\n",
      "Epoch: 13, Loss: 256.17922, Residuals: -3.22786, Convergence: 0.013029\n",
      "Epoch: 14, Loss: 253.03434, Residuals: -3.19000, Convergence: 0.012429\n",
      "Epoch: 15, Loss: 250.05246, Residuals: -3.15501, Convergence: 0.011925\n",
      "Epoch: 16, Loss: 247.21870, Residuals: -3.12225, Convergence: 0.011463\n",
      "Epoch: 17, Loss: 244.52312, Residuals: -3.09136, Convergence: 0.011024\n",
      "Epoch: 18, Loss: 241.94676, Residuals: -3.06196, Convergence: 0.010648\n",
      "Epoch: 19, Loss: 239.45869, Residuals: -3.03356, Convergence: 0.010390\n",
      "Epoch: 20, Loss: 237.02320, Residuals: -3.00558, Convergence: 0.010275\n",
      "Epoch: 21, Loss: 234.60807, Residuals: -2.97750, Convergence: 0.010294\n",
      "Epoch: 22, Loss: 232.18942, Residuals: -2.94891, Convergence: 0.010417\n",
      "Epoch: 23, Loss: 229.74655, Residuals: -2.91956, Convergence: 0.010633\n",
      "Epoch: 24, Loss: 227.24644, Residuals: -2.88905, Convergence: 0.011002\n",
      "Epoch: 25, Loss: 224.63681, Residuals: -2.85678, Convergence: 0.011617\n",
      "Epoch: 26, Loss: 221.88181, Residuals: -2.82228, Convergence: 0.012417\n",
      "Epoch: 27, Loss: 219.04950, Residuals: -2.78621, Convergence: 0.012930\n",
      "Epoch: 28, Loss: 216.27102, Residuals: -2.75005, Convergence: 0.012847\n",
      "Epoch: 29, Loss: 213.59724, Residuals: -2.71451, Convergence: 0.012518\n",
      "Epoch: 30, Loss: 211.01902, Residuals: -2.67963, Convergence: 0.012218\n",
      "Epoch: 31, Loss: 208.51642, Residuals: -2.64526, Convergence: 0.012002\n",
      "Epoch: 32, Loss: 206.07300, Residuals: -2.61125, Convergence: 0.011857\n",
      "Epoch: 33, Loss: 203.67773, Residuals: -2.57747, Convergence: 0.011760\n",
      "Epoch: 34, Loss: 201.32427, Residuals: -2.54383, Convergence: 0.011690\n",
      "Epoch: 35, Loss: 199.00994, Residuals: -2.51027, Convergence: 0.011629\n",
      "Epoch: 36, Loss: 196.73454, Residuals: -2.47675, Convergence: 0.011566\n",
      "Epoch: 37, Loss: 194.49947, Residuals: -2.44326, Convergence: 0.011491\n",
      "Epoch: 38, Loss: 192.30695, Residuals: -2.40981, Convergence: 0.011401\n",
      "Epoch: 39, Loss: 190.15943, Residuals: -2.37639, Convergence: 0.011293\n",
      "Epoch: 40, Loss: 188.05936, Residuals: -2.34303, Convergence: 0.011167\n",
      "Epoch: 41, Loss: 186.00895, Residuals: -2.30975, Convergence: 0.011023\n",
      "Epoch: 42, Loss: 184.01031, Residuals: -2.27658, Convergence: 0.010862\n",
      "Epoch: 43, Loss: 182.06544, Residuals: -2.24355, Convergence: 0.010682\n",
      "Epoch: 44, Loss: 180.17642, Residuals: -2.21070, Convergence: 0.010484\n",
      "Epoch: 45, Loss: 178.34533, Residuals: -2.17807, Convergence: 0.010267\n",
      "Epoch: 46, Loss: 176.57421, Residuals: -2.14571, Convergence: 0.010030\n",
      "Epoch: 47, Loss: 174.86477, Residuals: -2.11367, Convergence: 0.009776\n",
      "Epoch: 48, Loss: 173.21823, Residuals: -2.08200, Convergence: 0.009506\n",
      "Epoch: 49, Loss: 171.63512, Residuals: -2.05075, Convergence: 0.009224\n",
      "Epoch: 50, Loss: 170.11528, Residuals: -2.01997, Convergence: 0.008934\n",
      "Epoch: 51, Loss: 168.65789, Residuals: -1.98968, Convergence: 0.008641\n",
      "Epoch: 52, Loss: 167.26163, Residuals: -1.95991, Convergence: 0.008348\n",
      "Epoch: 53, Loss: 165.92483, Residuals: -1.93070, Convergence: 0.008057\n",
      "Epoch: 54, Loss: 164.64561, Residuals: -1.90205, Convergence: 0.007770\n",
      "Epoch: 55, Loss: 163.42202, Residuals: -1.87399, Convergence: 0.007487\n",
      "Epoch: 56, Loss: 162.25210, Residuals: -1.84653, Convergence: 0.007210\n",
      "Epoch: 57, Loss: 161.13395, Residuals: -1.81970, Convergence: 0.006939\n",
      "Epoch: 58, Loss: 160.06567, Residuals: -1.79350, Convergence: 0.006674\n",
      "Epoch: 59, Loss: 159.04538, Residuals: -1.76796, Convergence: 0.006415\n",
      "Epoch: 60, Loss: 158.07117, Residuals: -1.74308, Convergence: 0.006163\n",
      "Epoch: 61, Loss: 157.14106, Residuals: -1.71888, Convergence: 0.005919\n",
      "Epoch: 62, Loss: 156.25308, Residuals: -1.69535, Convergence: 0.005683\n",
      "Epoch: 63, Loss: 155.40527, Residuals: -1.67249, Convergence: 0.005455\n",
      "Epoch: 64, Loss: 154.59576, Residuals: -1.65031, Convergence: 0.005236\n",
      "Epoch: 65, Loss: 153.82282, Residuals: -1.62879, Convergence: 0.005025\n",
      "Epoch: 66, Loss: 153.08495, Residuals: -1.60793, Convergence: 0.004820\n",
      "Epoch: 67, Loss: 152.38083, Residuals: -1.58772, Convergence: 0.004621\n",
      "Epoch: 68, Loss: 151.70932, Residuals: -1.56816, Convergence: 0.004426\n",
      "Epoch: 69, Loss: 151.06945, Residuals: -1.54925, Convergence: 0.004236\n",
      "Epoch: 70, Loss: 150.46029, Residuals: -1.53099, Convergence: 0.004049\n",
      "Epoch: 71, Loss: 149.88099, Residuals: -1.51337, Convergence: 0.003865\n",
      "Epoch: 72, Loss: 149.33073, Residuals: -1.49639, Convergence: 0.003685\n",
      "Epoch: 73, Loss: 148.80865, Residuals: -1.48005, Convergence: 0.003508\n",
      "Epoch: 74, Loss: 148.31389, Residuals: -1.46435, Convergence: 0.003336\n",
      "Epoch: 75, Loss: 147.84557, Residuals: -1.44927, Convergence: 0.003168\n",
      "Epoch: 76, Loss: 147.40278, Residuals: -1.43481, Convergence: 0.003004\n",
      "Epoch: 77, Loss: 146.98458, Residuals: -1.42096, Convergence: 0.002845\n",
      "Epoch: 78, Loss: 146.59004, Residuals: -1.40771, Convergence: 0.002691\n",
      "Epoch: 79, Loss: 146.21819, Residuals: -1.39506, Convergence: 0.002543\n",
      "Epoch: 80, Loss: 145.86805, Residuals: -1.38298, Convergence: 0.002400\n",
      "Epoch: 81, Loss: 145.53860, Residuals: -1.37147, Convergence: 0.002264\n",
      "Epoch: 82, Loss: 145.22879, Residuals: -1.36051, Convergence: 0.002133\n",
      "Epoch: 83, Loss: 144.93750, Residuals: -1.35010, Convergence: 0.002010\n",
      "Epoch: 84, Loss: 144.66355, Residuals: -1.34022, Convergence: 0.001894\n",
      "Epoch: 85, Loss: 144.40565, Residuals: -1.33085, Convergence: 0.001786\n",
      "Epoch: 86, Loss: 144.16243, Residuals: -1.32197, Convergence: 0.001687\n",
      "Epoch: 87, Loss: 143.93240, Residuals: -1.31356, Convergence: 0.001598\n",
      "Epoch: 88, Loss: 143.71401, Residuals: -1.30560, Convergence: 0.001520\n",
      "Epoch: 89, Loss: 143.50564, Residuals: -1.29804, Convergence: 0.001452\n",
      "Epoch: 90, Loss: 143.30576, Residuals: -1.29087, Convergence: 0.001395\n",
      "Epoch: 91, Loss: 143.11291, Residuals: -1.28403, Convergence: 0.001347\n",
      "Epoch: 92, Loss: 142.92581, Residuals: -1.27749, Convergence: 0.001309\n",
      "Epoch: 93, Loss: 142.74341, Residuals: -1.27122, Convergence: 0.001278\n",
      "Epoch: 94, Loss: 142.56490, Residuals: -1.26518, Convergence: 0.001252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95, Loss: 142.38970, Residuals: -1.25936, Convergence: 0.001230\n",
      "Epoch: 96, Loss: 142.21745, Residuals: -1.25373, Convergence: 0.001211\n",
      "Epoch: 97, Loss: 142.04796, Residuals: -1.24827, Convergence: 0.001193\n",
      "Epoch: 98, Loss: 141.88115, Residuals: -1.24297, Convergence: 0.001176\n",
      "Epoch: 99, Loss: 141.71704, Residuals: -1.23782, Convergence: 0.001158\n",
      "Epoch: 100, Loss: 141.55572, Residuals: -1.23282, Convergence: 0.001140\n",
      "Epoch: 101, Loss: 141.39725, Residuals: -1.22795, Convergence: 0.001121\n",
      "Epoch: 102, Loss: 141.24175, Residuals: -1.22322, Convergence: 0.001101\n",
      "Epoch: 103, Loss: 141.08931, Residuals: -1.21861, Convergence: 0.001080\n",
      "Epoch: 104, Loss: 140.94000, Residuals: -1.21414, Convergence: 0.001059\n",
      "Epoch: 105, Loss: 140.79390, Residuals: -1.20978, Convergence: 0.001038\n",
      "Epoch: 106, Loss: 140.65103, Residuals: -1.20555, Convergence: 0.001016\n",
      "Epoch: 107, Loss: 140.51144, Residuals: -1.20143, Convergence: 0.000993\n",
      "Evidence -181.008\n",
      "\n",
      "Epoch: 107, Evidence: -181.00790, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.23e-01\n",
      "Epoch: 107, Loss: 1381.78261, Residuals: -1.20143, Convergence:   inf\n",
      "Epoch: 108, Loss: 1323.59910, Residuals: -1.22756, Convergence: 0.043959\n",
      "Epoch: 109, Loss: 1279.31617, Residuals: -1.24924, Convergence: 0.034615\n",
      "Epoch: 110, Loss: 1245.78279, Residuals: -1.26558, Convergence: 0.026918\n",
      "Epoch: 111, Loss: 1219.41436, Residuals: -1.27776, Convergence: 0.021624\n",
      "Epoch: 112, Loss: 1197.86803, Residuals: -1.28732, Convergence: 0.017987\n",
      "Epoch: 113, Loss: 1179.88525, Residuals: -1.29494, Convergence: 0.015241\n",
      "Epoch: 114, Loss: 1164.68560, Residuals: -1.30092, Convergence: 0.013050\n",
      "Epoch: 115, Loss: 1151.70813, Residuals: -1.30545, Convergence: 0.011268\n",
      "Epoch: 116, Loss: 1140.51938, Residuals: -1.30867, Convergence: 0.009810\n",
      "Epoch: 117, Loss: 1130.77269, Residuals: -1.31073, Convergence: 0.008620\n",
      "Epoch: 118, Loss: 1122.18420, Residuals: -1.31173, Convergence: 0.007653\n",
      "Epoch: 119, Loss: 1114.51898, Residuals: -1.31179, Convergence: 0.006878\n",
      "Epoch: 120, Loss: 1107.57730, Residuals: -1.31098, Convergence: 0.006267\n",
      "Epoch: 121, Loss: 1101.18692, Residuals: -1.30936, Convergence: 0.005803\n",
      "Epoch: 122, Loss: 1095.19607, Residuals: -1.30699, Convergence: 0.005470\n",
      "Epoch: 123, Loss: 1089.46761, Residuals: -1.30388, Convergence: 0.005258\n",
      "Epoch: 124, Loss: 1083.87722, Residuals: -1.30004, Convergence: 0.005158\n",
      "Epoch: 125, Loss: 1078.31462, Residuals: -1.29549, Convergence: 0.005159\n",
      "Epoch: 126, Loss: 1072.69336, Residuals: -1.29025, Convergence: 0.005240\n",
      "Epoch: 127, Loss: 1066.96803, Residuals: -1.28436, Convergence: 0.005366\n",
      "Epoch: 128, Loss: 1061.15622, Residuals: -1.27791, Convergence: 0.005477\n",
      "Epoch: 129, Loss: 1055.34611, Residuals: -1.27100, Convergence: 0.005505\n",
      "Epoch: 130, Loss: 1049.66796, Residuals: -1.26372, Convergence: 0.005409\n",
      "Epoch: 131, Loss: 1044.24292, Residuals: -1.25619, Convergence: 0.005195\n",
      "Epoch: 132, Loss: 1039.14306, Residuals: -1.24848, Convergence: 0.004908\n",
      "Epoch: 133, Loss: 1034.39009, Residuals: -1.24069, Convergence: 0.004595\n",
      "Epoch: 134, Loss: 1029.97062, Residuals: -1.23288, Convergence: 0.004291\n",
      "Epoch: 135, Loss: 1025.85496, Residuals: -1.22511, Convergence: 0.004012\n",
      "Epoch: 136, Loss: 1022.00901, Residuals: -1.21743, Convergence: 0.003763\n",
      "Epoch: 137, Loss: 1018.40069, Residuals: -1.20988, Convergence: 0.003543\n",
      "Epoch: 138, Loss: 1015.00249, Residuals: -1.20248, Convergence: 0.003348\n",
      "Epoch: 139, Loss: 1011.79059, Residuals: -1.19527, Convergence: 0.003174\n",
      "Epoch: 140, Loss: 1008.74642, Residuals: -1.18827, Convergence: 0.003018\n",
      "Epoch: 141, Loss: 1005.85561, Residuals: -1.18149, Convergence: 0.002874\n",
      "Epoch: 142, Loss: 1003.10507, Residuals: -1.17493, Convergence: 0.002742\n",
      "Epoch: 143, Loss: 1000.48566, Residuals: -1.16863, Convergence: 0.002618\n",
      "Epoch: 144, Loss: 997.98896, Residuals: -1.16256, Convergence: 0.002502\n",
      "Epoch: 145, Loss: 995.60828, Residuals: -1.15675, Convergence: 0.002391\n",
      "Epoch: 146, Loss: 993.33684, Residuals: -1.15119, Convergence: 0.002287\n",
      "Epoch: 147, Loss: 991.16924, Residuals: -1.14588, Convergence: 0.002187\n",
      "Epoch: 148, Loss: 989.09987, Residuals: -1.14081, Convergence: 0.002092\n",
      "Epoch: 149, Loss: 987.12339, Residuals: -1.13598, Convergence: 0.002002\n",
      "Epoch: 150, Loss: 985.23446, Residuals: -1.13138, Convergence: 0.001917\n",
      "Epoch: 151, Loss: 983.42846, Residuals: -1.12700, Convergence: 0.001836\n",
      "Epoch: 152, Loss: 981.70018, Residuals: -1.12283, Convergence: 0.001760\n",
      "Epoch: 153, Loss: 980.04561, Residuals: -1.11886, Convergence: 0.001688\n",
      "Epoch: 154, Loss: 978.46005, Residuals: -1.11508, Convergence: 0.001620\n",
      "Epoch: 155, Loss: 976.93866, Residuals: -1.11149, Convergence: 0.001557\n",
      "Epoch: 156, Loss: 975.47773, Residuals: -1.10806, Convergence: 0.001498\n",
      "Epoch: 157, Loss: 974.07220, Residuals: -1.10479, Convergence: 0.001443\n",
      "Epoch: 158, Loss: 972.71787, Residuals: -1.10167, Convergence: 0.001392\n",
      "Epoch: 159, Loss: 971.40981, Residuals: -1.09869, Convergence: 0.001347\n",
      "Epoch: 160, Loss: 970.14348, Residuals: -1.09583, Convergence: 0.001305\n",
      "Epoch: 161, Loss: 968.91370, Residuals: -1.09309, Convergence: 0.001269\n",
      "Epoch: 162, Loss: 967.71463, Residuals: -1.09044, Convergence: 0.001239\n",
      "Epoch: 163, Loss: 966.54089, Residuals: -1.08789, Convergence: 0.001214\n",
      "Epoch: 164, Loss: 965.38669, Residuals: -1.08541, Convergence: 0.001196\n",
      "Epoch: 165, Loss: 964.24565, Residuals: -1.08300, Convergence: 0.001183\n",
      "Epoch: 166, Loss: 963.11269, Residuals: -1.08064, Convergence: 0.001176\n",
      "Epoch: 167, Loss: 961.98247, Residuals: -1.07831, Convergence: 0.001175\n",
      "Epoch: 168, Loss: 960.85115, Residuals: -1.07601, Convergence: 0.001177\n",
      "Epoch: 169, Loss: 959.71707, Residuals: -1.07373, Convergence: 0.001182\n",
      "Epoch: 170, Loss: 958.58045, Residuals: -1.07147, Convergence: 0.001186\n",
      "Epoch: 171, Loss: 957.44458, Residuals: -1.06923, Convergence: 0.001186\n",
      "Epoch: 172, Loss: 956.31472, Residuals: -1.06700, Convergence: 0.001181\n",
      "Epoch: 173, Loss: 955.19758, Residuals: -1.06481, Convergence: 0.001170\n",
      "Epoch: 174, Loss: 954.10100, Residuals: -1.06267, Convergence: 0.001149\n",
      "Epoch: 175, Loss: 953.03183, Residuals: -1.06057, Convergence: 0.001122\n",
      "Epoch: 176, Loss: 951.99621, Residuals: -1.05854, Convergence: 0.001088\n",
      "Epoch: 177, Loss: 950.99902, Residuals: -1.05658, Convergence: 0.001049\n",
      "Epoch: 178, Loss: 950.04323, Residuals: -1.05469, Convergence: 0.001006\n",
      "Epoch: 179, Loss: 949.13061, Residuals: -1.05288, Convergence: 0.000962\n",
      "Evidence 11324.292\n",
      "\n",
      "Epoch: 179, Evidence: 11324.29199, Convergence: 1.015984\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.69e-01\n",
      "Epoch: 179, Loss: 2358.50513, Residuals: -1.05288, Convergence:   inf\n",
      "Epoch: 180, Loss: 2322.37514, Residuals: -1.06001, Convergence: 0.015557\n",
      "Epoch: 181, Loss: 2297.63663, Residuals: -1.05758, Convergence: 0.010767\n",
      "Epoch: 182, Loss: 2276.92901, Residuals: -1.05494, Convergence: 0.009095\n",
      "Epoch: 183, Loss: 2259.49759, Residuals: -1.05235, Convergence: 0.007715\n",
      "Epoch: 184, Loss: 2244.72401, Residuals: -1.04981, Convergence: 0.006581\n",
      "Epoch: 185, Loss: 2232.09283, Residuals: -1.04732, Convergence: 0.005659\n",
      "Epoch: 186, Loss: 2221.17307, Residuals: -1.04487, Convergence: 0.004916\n",
      "Epoch: 187, Loss: 2211.60702, Residuals: -1.04242, Convergence: 0.004325\n",
      "Epoch: 188, Loss: 2203.10512, Residuals: -1.03995, Convergence: 0.003859\n",
      "Epoch: 189, Loss: 2195.44029, Residuals: -1.03742, Convergence: 0.003491\n",
      "Epoch: 190, Loss: 2188.45061, Residuals: -1.03481, Convergence: 0.003194\n",
      "Epoch: 191, Loss: 2182.02937, Residuals: -1.03213, Convergence: 0.002943\n",
      "Epoch: 192, Loss: 2176.11132, Residuals: -1.02941, Convergence: 0.002720\n",
      "Epoch: 193, Loss: 2170.64892, Residuals: -1.02668, Convergence: 0.002516\n",
      "Epoch: 194, Loss: 2165.60487, Residuals: -1.02398, Convergence: 0.002329\n",
      "Epoch: 195, Loss: 2160.93774, Residuals: -1.02133, Convergence: 0.002160\n",
      "Epoch: 196, Loss: 2156.61082, Residuals: -1.01876, Convergence: 0.002006\n",
      "Epoch: 197, Loss: 2152.58605, Residuals: -1.01627, Convergence: 0.001870\n",
      "Epoch: 198, Loss: 2148.82947, Residuals: -1.01387, Convergence: 0.001748\n",
      "Epoch: 199, Loss: 2145.31245, Residuals: -1.01157, Convergence: 0.001639\n",
      "Epoch: 200, Loss: 2142.01072, Residuals: -1.00936, Convergence: 0.001541\n",
      "Epoch: 201, Loss: 2138.90445, Residuals: -1.00725, Convergence: 0.001452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 202, Loss: 2135.97912, Residuals: -1.00525, Convergence: 0.001370\n",
      "Epoch: 203, Loss: 2133.22236, Residuals: -1.00335, Convergence: 0.001292\n",
      "Epoch: 204, Loss: 2130.62451, Residuals: -1.00154, Convergence: 0.001219\n",
      "Epoch: 205, Loss: 2128.17734, Residuals: -0.99984, Convergence: 0.001150\n",
      "Epoch: 206, Loss: 2125.87278, Residuals: -0.99822, Convergence: 0.001084\n",
      "Epoch: 207, Loss: 2123.70307, Residuals: -0.99670, Convergence: 0.001022\n",
      "Epoch: 208, Loss: 2121.66235, Residuals: -0.99525, Convergence: 0.000962\n",
      "Evidence 14332.334\n",
      "\n",
      "Epoch: 208, Evidence: 14332.33398, Convergence: 0.209878\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.31e-01\n",
      "Epoch: 208, Loss: 2474.68274, Residuals: -0.99525, Convergence:   inf\n",
      "Epoch: 209, Loss: 2461.91304, Residuals: -0.99179, Convergence: 0.005187\n",
      "Epoch: 210, Loss: 2451.36920, Residuals: -0.98816, Convergence: 0.004301\n",
      "Epoch: 211, Loss: 2442.25426, Residuals: -0.98483, Convergence: 0.003732\n",
      "Epoch: 212, Loss: 2434.33588, Residuals: -0.98182, Convergence: 0.003253\n",
      "Epoch: 213, Loss: 2427.42702, Residuals: -0.97914, Convergence: 0.002846\n",
      "Epoch: 214, Loss: 2421.37154, Residuals: -0.97678, Convergence: 0.002501\n",
      "Epoch: 215, Loss: 2416.04072, Residuals: -0.97469, Convergence: 0.002206\n",
      "Epoch: 216, Loss: 2411.32395, Residuals: -0.97284, Convergence: 0.001956\n",
      "Epoch: 217, Loss: 2407.13028, Residuals: -0.97121, Convergence: 0.001742\n",
      "Epoch: 218, Loss: 2403.38246, Residuals: -0.96975, Convergence: 0.001559\n",
      "Epoch: 219, Loss: 2400.01490, Residuals: -0.96846, Convergence: 0.001403\n",
      "Epoch: 220, Loss: 2396.97413, Residuals: -0.96730, Convergence: 0.001269\n",
      "Epoch: 221, Loss: 2394.21356, Residuals: -0.96625, Convergence: 0.001153\n",
      "Epoch: 222, Loss: 2391.69585, Residuals: -0.96531, Convergence: 0.001053\n",
      "Epoch: 223, Loss: 2389.38805, Residuals: -0.96446, Convergence: 0.000966\n",
      "Evidence 14675.052\n",
      "\n",
      "Epoch: 223, Evidence: 14675.05176, Convergence: 0.023354\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.30e-01\n",
      "Epoch: 223, Loss: 2480.03741, Residuals: -0.96446, Convergence:   inf\n",
      "Epoch: 224, Loss: 2473.59546, Residuals: -0.96121, Convergence: 0.002604\n",
      "Epoch: 225, Loss: 2468.25509, Residuals: -0.95844, Convergence: 0.002164\n",
      "Epoch: 226, Loss: 2463.72913, Residuals: -0.95615, Convergence: 0.001837\n",
      "Epoch: 227, Loss: 2459.84494, Residuals: -0.95426, Convergence: 0.001579\n",
      "Epoch: 228, Loss: 2456.47090, Residuals: -0.95271, Convergence: 0.001374\n",
      "Epoch: 229, Loss: 2453.50573, Residuals: -0.95141, Convergence: 0.001209\n",
      "Epoch: 230, Loss: 2450.86984, Residuals: -0.95034, Convergence: 0.001075\n",
      "Epoch: 231, Loss: 2448.50288, Residuals: -0.94944, Convergence: 0.000967\n",
      "Evidence 14753.126\n",
      "\n",
      "Epoch: 231, Evidence: 14753.12598, Convergence: 0.005292\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.60e-01\n",
      "Epoch: 231, Loss: 2481.70754, Residuals: -0.94944, Convergence:   inf\n",
      "Epoch: 232, Loss: 2477.76403, Residuals: -0.94707, Convergence: 0.001592\n",
      "Epoch: 233, Loss: 2474.49372, Residuals: -0.94521, Convergence: 0.001322\n",
      "Epoch: 234, Loss: 2471.70950, Residuals: -0.94375, Convergence: 0.001126\n",
      "Epoch: 235, Loss: 2469.29370, Residuals: -0.94259, Convergence: 0.000978\n",
      "Evidence 14780.993\n",
      "\n",
      "Epoch: 235, Evidence: 14780.99316, Convergence: 0.001885\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.10e-01\n",
      "Epoch: 235, Loss: 2482.68374, Residuals: -0.94259, Convergence:   inf\n",
      "Epoch: 236, Loss: 2479.75285, Residuals: -0.94078, Convergence: 0.001182\n",
      "Epoch: 237, Loss: 2477.31175, Residuals: -0.93938, Convergence: 0.000985\n",
      "Evidence 14793.151\n",
      "\n",
      "Epoch: 237, Evidence: 14793.15137, Convergence: 0.000822\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.75e-01\n",
      "Epoch: 237, Loss: 2483.36529, Residuals: -0.93938, Convergence:   inf\n",
      "Epoch: 238, Loss: 2478.84460, Residuals: -0.93710, Convergence: 0.001824\n",
      "Epoch: 239, Loss: 2475.38735, Residuals: -0.93536, Convergence: 0.001397\n",
      "Epoch: 240, Loss: 2472.60733, Residuals: -0.93424, Convergence: 0.001124\n",
      "Epoch: 241, Loss: 2470.26466, Residuals: -0.93362, Convergence: 0.000948\n",
      "Evidence 14810.749\n",
      "\n",
      "Epoch: 241, Evidence: 14810.74902, Convergence: 0.002009\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.45e-01\n",
      "Epoch: 241, Loss: 2483.49899, Residuals: -0.93362, Convergence:   inf\n",
      "Epoch: 242, Loss: 2480.49565, Residuals: -0.93133, Convergence: 0.001211\n",
      "Epoch: 243, Loss: 2478.14200, Residuals: -0.93016, Convergence: 0.000950\n",
      "Evidence 14821.717\n",
      "\n",
      "Epoch: 243, Evidence: 14821.71680, Convergence: 0.000740\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.22e-01\n",
      "Epoch: 243, Loss: 2483.64263, Residuals: -0.93016, Convergence:   inf\n",
      "Epoch: 244, Loss: 2479.31563, Residuals: -0.92678, Convergence: 0.001745\n",
      "Epoch: 245, Loss: 2476.25817, Residuals: -0.92791, Convergence: 0.001235\n",
      "Epoch: 246, Loss: 2473.72803, Residuals: -0.92860, Convergence: 0.001023\n",
      "Epoch: 247, Loss: 2471.54837, Residuals: -0.93113, Convergence: 0.000882\n",
      "Evidence 14837.041\n",
      "\n",
      "Epoch: 247, Evidence: 14837.04102, Convergence: 0.001772\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.09e-01\n",
      "Epoch: 247, Loss: 2482.91491, Residuals: -0.93113, Convergence:   inf\n",
      "Epoch: 248, Loss: 2481.26564, Residuals: -0.92888, Convergence: 0.000665\n",
      "Evidence 14843.617\n",
      "\n",
      "Epoch: 248, Evidence: 14843.61719, Convergence: 0.000443\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.90e-02\n",
      "Epoch: 248, Loss: 2483.81490, Residuals: -0.92888, Convergence:   inf\n",
      "Epoch: 249, Loss: 2528.44908, Residuals: -0.96922, Convergence: -0.017653\n",
      "Epoch: 249, Loss: 2481.59947, Residuals: -0.92630, Convergence: 0.000893\n",
      "Evidence 14847.852\n",
      "\n",
      "Epoch: 249, Evidence: 14847.85156, Convergence: 0.000728\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.15e-02\n",
      "Epoch: 249, Loss: 2483.26958, Residuals: -0.92630, Convergence:   inf\n",
      "Epoch: 250, Loss: 2486.61898, Residuals: -0.92775, Convergence: -0.001347\n",
      "Epoch: 250, Loss: 2482.92016, Residuals: -0.92465, Convergence: 0.000141\n",
      "Evidence 14849.789\n",
      "\n",
      "Epoch: 250, Evidence: 14849.78906, Convergence: 0.000858\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 381.13199, Residuals: -4.53313, Convergence:   inf\n",
      "Epoch: 1, Loss: 355.40161, Residuals: -4.41207, Convergence: 0.072398\n",
      "Epoch: 2, Loss: 334.27993, Residuals: -4.24621, Convergence: 0.063186\n",
      "Epoch: 3, Loss: 318.16591, Residuals: -4.08017, Convergence: 0.050647\n",
      "Epoch: 4, Loss: 305.86806, Residuals: -3.93393, Convergence: 0.040206\n",
      "Epoch: 5, Loss: 296.10698, Residuals: -3.80464, Convergence: 0.032965\n",
      "Epoch: 6, Loss: 288.17890, Residuals: -3.69206, Convergence: 0.027511\n",
      "Epoch: 7, Loss: 281.59642, Residuals: -3.59569, Convergence: 0.023376\n",
      "Epoch: 8, Loss: 275.99879, Residuals: -3.51346, Convergence: 0.020281\n",
      "Epoch: 9, Loss: 271.13159, Residuals: -3.44302, Convergence: 0.017951\n",
      "Epoch: 10, Loss: 266.81405, Residuals: -3.38226, Convergence: 0.016182\n",
      "Epoch: 11, Loss: 262.91526, Residuals: -3.32940, Convergence: 0.014829\n",
      "Epoch: 12, Loss: 259.33980, Residuals: -3.28293, Convergence: 0.013787\n",
      "Epoch: 13, Loss: 256.01839, Residuals: -3.24154, Convergence: 0.012973\n",
      "Epoch: 14, Loss: 252.90125, Residuals: -3.20408, Convergence: 0.012326\n",
      "Epoch: 15, Loss: 249.95395, Residuals: -3.16956, Convergence: 0.011791\n",
      "Epoch: 16, Loss: 247.15529, Residuals: -3.13722, Convergence: 0.011323\n",
      "Epoch: 17, Loss: 244.49150, Residuals: -3.10661, Convergence: 0.010895\n",
      "Epoch: 18, Loss: 241.94504, Residuals: -3.07733, Convergence: 0.010525\n",
      "Epoch: 19, Loss: 239.48844, Residuals: -3.04896, Convergence: 0.010258\n",
      "Epoch: 20, Loss: 237.08751, Residuals: -3.02097, Convergence: 0.010127\n",
      "Epoch: 21, Loss: 234.70730, Residuals: -2.99285, Convergence: 0.010141\n",
      "Epoch: 22, Loss: 232.31638, Residuals: -2.96415, Convergence: 0.010292\n",
      "Epoch: 23, Loss: 229.88623, Residuals: -2.93449, Convergence: 0.010571\n",
      "Epoch: 24, Loss: 227.38125, Residuals: -2.90346, Convergence: 0.011017\n",
      "Epoch: 25, Loss: 224.74968, Residuals: -2.87046, Convergence: 0.011709\n",
      "Epoch: 26, Loss: 221.95825, Residuals: -2.83505, Convergence: 0.012576\n",
      "Epoch: 27, Loss: 219.08722, Residuals: -2.79801, Convergence: 0.013105\n",
      "Epoch: 28, Loss: 216.27869, Residuals: -2.76093, Convergence: 0.012986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, Loss: 213.58443, Residuals: -2.72452, Convergence: 0.012614\n",
      "Epoch: 30, Loss: 210.99533, Residuals: -2.68883, Convergence: 0.012271\n",
      "Epoch: 31, Loss: 208.49249, Residuals: -2.65372, Convergence: 0.012004\n",
      "Epoch: 32, Loss: 206.06050, Residuals: -2.61907, Convergence: 0.011802\n",
      "Epoch: 33, Loss: 203.68871, Residuals: -2.58479, Convergence: 0.011644\n",
      "Epoch: 34, Loss: 201.37035, Residuals: -2.55081, Convergence: 0.011513\n",
      "Epoch: 35, Loss: 199.10134, Residuals: -2.51708, Convergence: 0.011396\n",
      "Epoch: 36, Loss: 196.87946, Residuals: -2.48358, Convergence: 0.011285\n",
      "Epoch: 37, Loss: 194.70362, Residuals: -2.45028, Convergence: 0.011175\n",
      "Epoch: 38, Loss: 192.57340, Residuals: -2.41716, Convergence: 0.011062\n",
      "Epoch: 39, Loss: 190.48874, Residuals: -2.38420, Convergence: 0.010944\n",
      "Epoch: 40, Loss: 188.44968, Residuals: -2.35140, Convergence: 0.010820\n",
      "Epoch: 41, Loss: 186.45629, Residuals: -2.31874, Convergence: 0.010691\n",
      "Epoch: 42, Loss: 184.50857, Residuals: -2.28622, Convergence: 0.010556\n",
      "Epoch: 43, Loss: 182.60641, Residuals: -2.25383, Convergence: 0.010417\n",
      "Epoch: 44, Loss: 180.74975, Residuals: -2.22158, Convergence: 0.010272\n",
      "Epoch: 45, Loss: 178.93869, Residuals: -2.18947, Convergence: 0.010121\n",
      "Epoch: 46, Loss: 177.17362, Residuals: -2.15749, Convergence: 0.009962\n",
      "Epoch: 47, Loss: 175.45545, Residuals: -2.12568, Convergence: 0.009793\n",
      "Epoch: 48, Loss: 173.78556, Residuals: -2.09405, Convergence: 0.009609\n",
      "Epoch: 49, Loss: 172.16579, Residuals: -2.06264, Convergence: 0.009408\n",
      "Epoch: 50, Loss: 170.59810, Residuals: -2.03150, Convergence: 0.009189\n",
      "Epoch: 51, Loss: 169.08435, Residuals: -2.00068, Convergence: 0.008953\n",
      "Epoch: 52, Loss: 167.62597, Residuals: -1.97023, Convergence: 0.008700\n",
      "Epoch: 53, Loss: 166.22390, Residuals: -1.94020, Convergence: 0.008435\n",
      "Epoch: 54, Loss: 164.87850, Residuals: -1.91065, Convergence: 0.008160\n",
      "Epoch: 55, Loss: 163.58967, Residuals: -1.88161, Convergence: 0.007878\n",
      "Epoch: 56, Loss: 162.35689, Residuals: -1.85313, Convergence: 0.007593\n",
      "Epoch: 57, Loss: 161.17932, Residuals: -1.82524, Convergence: 0.007306\n",
      "Epoch: 58, Loss: 160.05593, Residuals: -1.79798, Convergence: 0.007019\n",
      "Epoch: 59, Loss: 158.98552, Residuals: -1.77138, Convergence: 0.006733\n",
      "Epoch: 60, Loss: 157.96681, Residuals: -1.74546, Convergence: 0.006449\n",
      "Epoch: 61, Loss: 156.99845, Residuals: -1.72025, Convergence: 0.006168\n",
      "Epoch: 62, Loss: 156.07903, Residuals: -1.69576, Convergence: 0.005891\n",
      "Epoch: 63, Loss: 155.20711, Residuals: -1.67202, Convergence: 0.005618\n",
      "Epoch: 64, Loss: 154.38123, Residuals: -1.64904, Convergence: 0.005350\n",
      "Epoch: 65, Loss: 153.59987, Residuals: -1.62683, Convergence: 0.005087\n",
      "Epoch: 66, Loss: 152.86146, Residuals: -1.60541, Convergence: 0.004831\n",
      "Epoch: 67, Loss: 152.16440, Residuals: -1.58478, Convergence: 0.004581\n",
      "Epoch: 68, Loss: 151.50700, Residuals: -1.56494, Convergence: 0.004339\n",
      "Epoch: 69, Loss: 150.88746, Residuals: -1.54590, Convergence: 0.004106\n",
      "Epoch: 70, Loss: 150.30388, Residuals: -1.52764, Convergence: 0.003883\n",
      "Epoch: 71, Loss: 149.75424, Residuals: -1.51015, Convergence: 0.003670\n",
      "Epoch: 72, Loss: 149.23638, Residuals: -1.49342, Convergence: 0.003470\n",
      "Epoch: 73, Loss: 148.74811, Residuals: -1.47743, Convergence: 0.003283\n",
      "Epoch: 74, Loss: 148.28719, Residuals: -1.46213, Convergence: 0.003108\n",
      "Epoch: 75, Loss: 147.85148, Residuals: -1.44749, Convergence: 0.002947\n",
      "Epoch: 76, Loss: 147.43895, Residuals: -1.43349, Convergence: 0.002798\n",
      "Epoch: 77, Loss: 147.04784, Residuals: -1.42008, Convergence: 0.002660\n",
      "Epoch: 78, Loss: 146.67657, Residuals: -1.40722, Convergence: 0.002531\n",
      "Epoch: 79, Loss: 146.32386, Residuals: -1.39490, Convergence: 0.002410\n",
      "Epoch: 80, Loss: 145.98862, Residuals: -1.38309, Convergence: 0.002296\n",
      "Epoch: 81, Loss: 145.66996, Residuals: -1.37176, Convergence: 0.002188\n",
      "Epoch: 82, Loss: 145.36712, Residuals: -1.36089, Convergence: 0.002083\n",
      "Epoch: 83, Loss: 145.07948, Residuals: -1.35047, Convergence: 0.001983\n",
      "Epoch: 84, Loss: 144.80645, Residuals: -1.34048, Convergence: 0.001885\n",
      "Epoch: 85, Loss: 144.54752, Residuals: -1.33091, Convergence: 0.001791\n",
      "Epoch: 86, Loss: 144.30221, Residuals: -1.32174, Convergence: 0.001700\n",
      "Epoch: 87, Loss: 144.07005, Residuals: -1.31297, Convergence: 0.001611\n",
      "Epoch: 88, Loss: 143.85061, Residuals: -1.30459, Convergence: 0.001526\n",
      "Epoch: 89, Loss: 143.64341, Residuals: -1.29658, Convergence: 0.001442\n",
      "Epoch: 90, Loss: 143.44800, Residuals: -1.28893, Convergence: 0.001362\n",
      "Epoch: 91, Loss: 143.26391, Residuals: -1.28165, Convergence: 0.001285\n",
      "Epoch: 92, Loss: 143.09060, Residuals: -1.27470, Convergence: 0.001211\n",
      "Epoch: 93, Loss: 142.92751, Residuals: -1.26811, Convergence: 0.001141\n",
      "Epoch: 94, Loss: 142.77396, Residuals: -1.26184, Convergence: 0.001075\n",
      "Epoch: 95, Loss: 142.62922, Residuals: -1.25589, Convergence: 0.001015\n",
      "Epoch: 96, Loss: 142.49240, Residuals: -1.25026, Convergence: 0.000960\n",
      "Evidence -184.105\n",
      "\n",
      "Epoch: 96, Evidence: -184.10474, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.24e-01\n",
      "Epoch: 96, Loss: 1358.96294, Residuals: -1.25026, Convergence:   inf\n",
      "Epoch: 97, Loss: 1295.64946, Residuals: -1.28163, Convergence: 0.048866\n",
      "Epoch: 98, Loss: 1248.10530, Residuals: -1.30578, Convergence: 0.038093\n",
      "Epoch: 99, Loss: 1212.60789, Residuals: -1.32250, Convergence: 0.029274\n",
      "Epoch: 100, Loss: 1185.09852, Residuals: -1.33379, Convergence: 0.023213\n",
      "Epoch: 101, Loss: 1162.85186, Residuals: -1.34186, Convergence: 0.019131\n",
      "Epoch: 102, Loss: 1144.38210, Residuals: -1.34782, Convergence: 0.016139\n",
      "Epoch: 103, Loss: 1128.79709, Residuals: -1.35216, Convergence: 0.013807\n",
      "Epoch: 104, Loss: 1115.48310, Residuals: -1.35514, Convergence: 0.011936\n",
      "Epoch: 105, Loss: 1103.97912, Residuals: -1.35693, Convergence: 0.010420\n",
      "Epoch: 106, Loss: 1093.92328, Residuals: -1.35765, Convergence: 0.009192\n",
      "Epoch: 107, Loss: 1085.02243, Residuals: -1.35740, Convergence: 0.008203\n",
      "Epoch: 108, Loss: 1077.03453, Residuals: -1.35627, Convergence: 0.007417\n",
      "Epoch: 109, Loss: 1069.75630, Residuals: -1.35432, Convergence: 0.006804\n",
      "Epoch: 110, Loss: 1063.01418, Residuals: -1.35158, Convergence: 0.006342\n",
      "Epoch: 111, Loss: 1056.65956, Residuals: -1.34809, Convergence: 0.006014\n",
      "Epoch: 112, Loss: 1050.56264, Residuals: -1.34385, Convergence: 0.005803\n",
      "Epoch: 113, Loss: 1044.61246, Residuals: -1.33888, Convergence: 0.005696\n",
      "Epoch: 114, Loss: 1038.71706, Residuals: -1.33318, Convergence: 0.005676\n",
      "Epoch: 115, Loss: 1032.81426, Residuals: -1.32679, Convergence: 0.005715\n",
      "Epoch: 116, Loss: 1026.88553, Residuals: -1.31977, Convergence: 0.005774\n",
      "Epoch: 117, Loss: 1020.96865, Residuals: -1.31222, Convergence: 0.005795\n",
      "Epoch: 118, Loss: 1015.15072, Residuals: -1.30427, Convergence: 0.005731\n",
      "Epoch: 119, Loss: 1009.53841, Residuals: -1.29602, Convergence: 0.005559\n",
      "Epoch: 120, Loss: 1004.21969, Residuals: -1.28759, Convergence: 0.005296\n",
      "Epoch: 121, Loss: 999.24232, Residuals: -1.27907, Convergence: 0.004981\n",
      "Epoch: 122, Loss: 994.61520, Residuals: -1.27052, Convergence: 0.004652\n",
      "Epoch: 123, Loss: 990.32190, Residuals: -1.26203, Convergence: 0.004335\n",
      "Epoch: 124, Loss: 986.33253, Residuals: -1.25363, Convergence: 0.004045\n",
      "Epoch: 125, Loss: 982.61539, Residuals: -1.24537, Convergence: 0.003783\n",
      "Epoch: 126, Loss: 979.13942, Residuals: -1.23728, Convergence: 0.003550\n",
      "Epoch: 127, Loss: 975.87774, Residuals: -1.22939, Convergence: 0.003342\n",
      "Epoch: 128, Loss: 972.80749, Residuals: -1.22173, Convergence: 0.003156\n",
      "Epoch: 129, Loss: 969.91072, Residuals: -1.21430, Convergence: 0.002987\n",
      "Epoch: 130, Loss: 967.17287, Residuals: -1.20714, Convergence: 0.002831\n",
      "Epoch: 131, Loss: 964.58206, Residuals: -1.20024, Convergence: 0.002686\n",
      "Epoch: 132, Loss: 962.12884, Residuals: -1.19362, Convergence: 0.002550\n",
      "Epoch: 133, Loss: 959.80604, Residuals: -1.18729, Convergence: 0.002420\n",
      "Epoch: 134, Loss: 957.60648, Residuals: -1.18124, Convergence: 0.002297\n",
      "Epoch: 135, Loss: 955.52402, Residuals: -1.17548, Convergence: 0.002179\n",
      "Epoch: 136, Loss: 953.55231, Residuals: -1.17000, Convergence: 0.002068\n",
      "Epoch: 137, Loss: 951.68511, Residuals: -1.16479, Convergence: 0.001962\n",
      "Epoch: 138, Loss: 949.91664, Residuals: -1.15986, Convergence: 0.001862\n",
      "Epoch: 139, Loss: 948.24002, Residuals: -1.15518, Convergence: 0.001768\n",
      "Epoch: 140, Loss: 946.64927, Residuals: -1.15074, Convergence: 0.001680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 141, Loss: 945.13792, Residuals: -1.14654, Convergence: 0.001599\n",
      "Epoch: 142, Loss: 943.69973, Residuals: -1.14257, Convergence: 0.001524\n",
      "Epoch: 143, Loss: 942.32920, Residuals: -1.13880, Convergence: 0.001454\n",
      "Epoch: 144, Loss: 941.02052, Residuals: -1.13522, Convergence: 0.001391\n",
      "Epoch: 145, Loss: 939.76858, Residuals: -1.13184, Convergence: 0.001332\n",
      "Epoch: 146, Loss: 938.56856, Residuals: -1.12862, Convergence: 0.001279\n",
      "Epoch: 147, Loss: 937.41585, Residuals: -1.12556, Convergence: 0.001230\n",
      "Epoch: 148, Loss: 936.30639, Residuals: -1.12265, Convergence: 0.001185\n",
      "Epoch: 149, Loss: 935.23650, Residuals: -1.11988, Convergence: 0.001144\n",
      "Epoch: 150, Loss: 934.20267, Residuals: -1.11723, Convergence: 0.001107\n",
      "Epoch: 151, Loss: 933.20186, Residuals: -1.11470, Convergence: 0.001072\n",
      "Epoch: 152, Loss: 932.23088, Residuals: -1.11229, Convergence: 0.001042\n",
      "Epoch: 153, Loss: 931.28744, Residuals: -1.10997, Convergence: 0.001013\n",
      "Epoch: 154, Loss: 930.36900, Residuals: -1.10775, Convergence: 0.000987\n",
      "Evidence 11018.297\n",
      "\n",
      "Epoch: 154, Evidence: 11018.29688, Convergence: 1.016709\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.78e-01\n",
      "Epoch: 154, Loss: 2331.66089, Residuals: -1.10775, Convergence:   inf\n",
      "Epoch: 155, Loss: 2291.50126, Residuals: -1.11803, Convergence: 0.017525\n",
      "Epoch: 156, Loss: 2263.15139, Residuals: -1.11750, Convergence: 0.012527\n",
      "Epoch: 157, Loss: 2239.24402, Residuals: -1.11581, Convergence: 0.010677\n",
      "Epoch: 158, Loss: 2218.82606, Residuals: -1.11364, Convergence: 0.009202\n",
      "Epoch: 159, Loss: 2201.23653, Residuals: -1.11113, Convergence: 0.007991\n",
      "Epoch: 160, Loss: 2185.95889, Residuals: -1.10835, Convergence: 0.006989\n",
      "Epoch: 161, Loss: 2172.57080, Residuals: -1.10533, Convergence: 0.006162\n",
      "Epoch: 162, Loss: 2160.72140, Residuals: -1.10209, Convergence: 0.005484\n",
      "Epoch: 163, Loss: 2150.11592, Residuals: -1.09863, Convergence: 0.004933\n",
      "Epoch: 164, Loss: 2140.51013, Residuals: -1.09496, Convergence: 0.004488\n",
      "Epoch: 165, Loss: 2131.70891, Residuals: -1.09109, Convergence: 0.004129\n",
      "Epoch: 166, Loss: 2123.56810, Residuals: -1.08703, Convergence: 0.003834\n",
      "Epoch: 167, Loss: 2115.99482, Residuals: -1.08281, Convergence: 0.003579\n",
      "Epoch: 168, Loss: 2108.94066, Residuals: -1.07848, Convergence: 0.003345\n",
      "Epoch: 169, Loss: 2102.38539, Residuals: -1.07411, Convergence: 0.003118\n",
      "Epoch: 170, Loss: 2096.31761, Residuals: -1.06975, Convergence: 0.002894\n",
      "Epoch: 171, Loss: 2090.72331, Residuals: -1.06548, Convergence: 0.002676\n",
      "Epoch: 172, Loss: 2085.58154, Residuals: -1.06131, Convergence: 0.002465\n",
      "Epoch: 173, Loss: 2080.86242, Residuals: -1.05730, Convergence: 0.002268\n",
      "Epoch: 174, Loss: 2076.53270, Residuals: -1.05345, Convergence: 0.002085\n",
      "Epoch: 175, Loss: 2072.55677, Residuals: -1.04979, Convergence: 0.001918\n",
      "Epoch: 176, Loss: 2068.89942, Residuals: -1.04630, Convergence: 0.001768\n",
      "Epoch: 177, Loss: 2065.52726, Residuals: -1.04300, Convergence: 0.001633\n",
      "Epoch: 178, Loss: 2062.40948, Residuals: -1.03987, Convergence: 0.001512\n",
      "Epoch: 179, Loss: 2059.51861, Residuals: -1.03693, Convergence: 0.001404\n",
      "Epoch: 180, Loss: 2056.83036, Residuals: -1.03415, Convergence: 0.001307\n",
      "Epoch: 181, Loss: 2054.32400, Residuals: -1.03155, Convergence: 0.001220\n",
      "Epoch: 182, Loss: 2051.98086, Residuals: -1.02910, Convergence: 0.001142\n",
      "Epoch: 183, Loss: 2049.78670, Residuals: -1.02680, Convergence: 0.001070\n",
      "Epoch: 184, Loss: 2047.72770, Residuals: -1.02464, Convergence: 0.001006\n",
      "Epoch: 185, Loss: 2045.79278, Residuals: -1.02263, Convergence: 0.000946\n",
      "Evidence 14230.573\n",
      "\n",
      "Epoch: 185, Evidence: 14230.57324, Convergence: 0.225731\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.41e-01\n",
      "Epoch: 185, Loss: 2455.33377, Residuals: -1.02263, Convergence:   inf\n",
      "Epoch: 186, Loss: 2441.09692, Residuals: -1.01930, Convergence: 0.005832\n",
      "Epoch: 187, Loss: 2429.60053, Residuals: -1.01557, Convergence: 0.004732\n",
      "Epoch: 188, Loss: 2419.79103, Residuals: -1.01200, Convergence: 0.004054\n",
      "Epoch: 189, Loss: 2411.34820, Residuals: -1.00869, Convergence: 0.003501\n",
      "Epoch: 190, Loss: 2404.03364, Residuals: -1.00566, Convergence: 0.003043\n",
      "Epoch: 191, Loss: 2397.65596, Residuals: -1.00290, Convergence: 0.002660\n",
      "Epoch: 192, Loss: 2392.06031, Residuals: -1.00038, Convergence: 0.002339\n",
      "Epoch: 193, Loss: 2387.11823, Residuals: -0.99808, Convergence: 0.002070\n",
      "Epoch: 194, Loss: 2382.72426, Residuals: -0.99599, Convergence: 0.001844\n",
      "Epoch: 195, Loss: 2378.79050, Residuals: -0.99408, Convergence: 0.001654\n",
      "Epoch: 196, Loss: 2375.24553, Residuals: -0.99233, Convergence: 0.001492\n",
      "Epoch: 197, Loss: 2372.03027, Residuals: -0.99073, Convergence: 0.001355\n",
      "Epoch: 198, Loss: 2369.09668, Residuals: -0.98926, Convergence: 0.001238\n",
      "Epoch: 199, Loss: 2366.40596, Residuals: -0.98791, Convergence: 0.001137\n",
      "Epoch: 200, Loss: 2363.92512, Residuals: -0.98667, Convergence: 0.001049\n",
      "Epoch: 201, Loss: 2361.62770, Residuals: -0.98552, Convergence: 0.000973\n",
      "Evidence 14671.442\n",
      "\n",
      "Epoch: 201, Evidence: 14671.44238, Convergence: 0.030049\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.37e-01\n",
      "Epoch: 201, Loss: 2461.65371, Residuals: -0.98552, Convergence:   inf\n",
      "Epoch: 202, Loss: 2455.22007, Residuals: -0.98242, Convergence: 0.002620\n",
      "Epoch: 203, Loss: 2449.90984, Residuals: -0.97966, Convergence: 0.002168\n",
      "Epoch: 204, Loss: 2445.39981, Residuals: -0.97730, Convergence: 0.001844\n",
      "Epoch: 205, Loss: 2441.51224, Residuals: -0.97529, Convergence: 0.001592\n",
      "Epoch: 206, Loss: 2438.11500, Residuals: -0.97356, Convergence: 0.001393\n",
      "Epoch: 207, Loss: 2435.10632, Residuals: -0.97207, Convergence: 0.001236\n",
      "Epoch: 208, Loss: 2432.41018, Residuals: -0.97078, Convergence: 0.001108\n",
      "Epoch: 209, Loss: 2429.96802, Residuals: -0.96966, Convergence: 0.001005\n",
      "Epoch: 210, Loss: 2427.73508, Residuals: -0.96867, Convergence: 0.000920\n",
      "Evidence 14755.155\n",
      "\n",
      "Epoch: 210, Evidence: 14755.15527, Convergence: 0.005673\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.64e-01\n",
      "Epoch: 210, Loss: 2463.20714, Residuals: -0.96867, Convergence:   inf\n",
      "Epoch: 211, Loss: 2459.43125, Residuals: -0.96627, Convergence: 0.001535\n",
      "Epoch: 212, Loss: 2456.28431, Residuals: -0.96434, Convergence: 0.001281\n",
      "Epoch: 213, Loss: 2453.57398, Residuals: -0.96278, Convergence: 0.001105\n",
      "Epoch: 214, Loss: 2451.18883, Residuals: -0.96151, Convergence: 0.000973\n",
      "Evidence 14783.229\n",
      "\n",
      "Epoch: 214, Evidence: 14783.22949, Convergence: 0.001899\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.12e-01\n",
      "Epoch: 214, Loss: 2464.22542, Residuals: -0.96151, Convergence:   inf\n",
      "Epoch: 215, Loss: 2461.37715, Residuals: -0.95955, Convergence: 0.001157\n",
      "Epoch: 216, Loss: 2458.97595, Residuals: -0.95803, Convergence: 0.000977\n",
      "Evidence 14795.128\n",
      "\n",
      "Epoch: 216, Evidence: 14795.12793, Convergence: 0.000804\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.76e-01\n",
      "Epoch: 216, Loss: 2464.97292, Residuals: -0.95803, Convergence:   inf\n",
      "Epoch: 217, Loss: 2460.40782, Residuals: -0.95536, Convergence: 0.001855\n",
      "Epoch: 218, Loss: 2456.92066, Residuals: -0.95354, Convergence: 0.001419\n",
      "Epoch: 219, Loss: 2454.05474, Residuals: -0.95219, Convergence: 0.001168\n",
      "Epoch: 220, Loss: 2451.60113, Residuals: -0.95123, Convergence: 0.001001\n",
      "Epoch: 221, Loss: 2449.43771, Residuals: -0.95052, Convergence: 0.000883\n",
      "Evidence 14815.275\n",
      "\n",
      "Epoch: 221, Evidence: 14815.27539, Convergence: 0.002163\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.45e-01\n",
      "Epoch: 221, Loss: 2464.98539, Residuals: -0.95052, Convergence:   inf\n",
      "Epoch: 222, Loss: 2462.12685, Residuals: -0.94800, Convergence: 0.001161\n",
      "Epoch: 223, Loss: 2459.82296, Residuals: -0.94656, Convergence: 0.000937\n",
      "Evidence 14826.431\n",
      "\n",
      "Epoch: 223, Evidence: 14826.43066, Convergence: 0.000752\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.23e-01\n",
      "Epoch: 223, Loss: 2465.24394, Residuals: -0.94656, Convergence:   inf\n",
      "Epoch: 224, Loss: 2460.97170, Residuals: -0.94280, Convergence: 0.001736\n",
      "Epoch: 225, Loss: 2457.84211, Residuals: -0.94297, Convergence: 0.001273\n",
      "Epoch: 226, Loss: 2455.25685, Residuals: -0.94273, Convergence: 0.001053\n",
      "Epoch: 227, Loss: 2452.96957, Residuals: -0.94494, Convergence: 0.000932\n",
      "Evidence 14841.731\n",
      "\n",
      "Epoch: 227, Evidence: 14841.73145, Convergence: 0.001783\n",
      "Updating hyper-parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 182, Updated regularization: 1.10e-01\n",
      "Epoch: 227, Loss: 2464.65000, Residuals: -0.94494, Convergence:   inf\n",
      "Epoch: 228, Loss: 2463.06411, Residuals: -0.94363, Convergence: 0.000644\n",
      "Evidence 14847.859\n",
      "\n",
      "Epoch: 228, Evidence: 14847.85938, Convergence: 0.000413\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.92e-02\n",
      "Epoch: 228, Loss: 2465.64960, Residuals: -0.94363, Convergence:   inf\n",
      "Epoch: 229, Loss: 2513.33792, Residuals: -0.98749, Convergence: -0.018974\n",
      "Epoch: 229, Loss: 2463.37268, Residuals: -0.94118, Convergence: 0.000924\n",
      "Evidence 14852.121\n",
      "\n",
      "Epoch: 229, Evidence: 14852.12109, Convergence: 0.000700\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.21e-02\n",
      "Epoch: 229, Loss: 2465.11192, Residuals: -0.94118, Convergence:   inf\n",
      "Epoch: 230, Loss: 2469.63060, Residuals: -0.94526, Convergence: -0.001830\n",
      "Epoch: 230, Loss: 2465.16922, Residuals: -0.94049, Convergence: -0.000023\n",
      "Evidence 14853.599\n",
      "\n",
      "Epoch: 230, Evidence: 14853.59863, Convergence: 0.000799\n",
      "Total samples: 181, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 381.07333, Residuals: -4.54001, Convergence:   inf\n",
      "Epoch: 1, Loss: 355.43875, Residuals: -4.42008, Convergence: 0.072121\n",
      "Epoch: 2, Loss: 334.40963, Residuals: -4.25532, Convergence: 0.062884\n",
      "Epoch: 3, Loss: 318.35435, Residuals: -4.09006, Convergence: 0.050432\n",
      "Epoch: 4, Loss: 306.11216, Residuals: -3.94439, Convergence: 0.039992\n",
      "Epoch: 5, Loss: 296.40496, Residuals: -3.81546, Convergence: 0.032750\n",
      "Epoch: 6, Loss: 288.52996, Residuals: -3.70309, Convergence: 0.027294\n",
      "Epoch: 7, Loss: 281.99858, Residuals: -3.60686, Convergence: 0.023161\n",
      "Epoch: 8, Loss: 276.44895, Residuals: -3.52469, Convergence: 0.020075\n",
      "Epoch: 9, Loss: 271.62595, Residuals: -3.45424, Convergence: 0.017756\n",
      "Epoch: 10, Loss: 267.34833, Residuals: -3.39340, Convergence: 0.016000\n",
      "Epoch: 11, Loss: 263.48495, Residuals: -3.34039, Convergence: 0.014663\n",
      "Epoch: 12, Loss: 259.94031, Residuals: -3.29371, Convergence: 0.013636\n",
      "Epoch: 13, Loss: 256.64533, Residuals: -3.25207, Convergence: 0.012839\n",
      "Epoch: 14, Loss: 253.55096, Residuals: -3.21433, Convergence: 0.012204\n",
      "Epoch: 15, Loss: 250.62465, Residuals: -3.17955, Convergence: 0.011676\n",
      "Epoch: 16, Loss: 247.84811, Residuals: -3.14705, Convergence: 0.011203\n",
      "Epoch: 17, Loss: 245.20966, Residuals: -3.11642, Convergence: 0.010760\n",
      "Epoch: 18, Loss: 242.69132, Residuals: -3.08728, Convergence: 0.010377\n",
      "Epoch: 19, Loss: 240.26372, Residuals: -3.05916, Convergence: 0.010104\n",
      "Epoch: 20, Loss: 237.89116, Residuals: -3.03148, Convergence: 0.009973\n",
      "Epoch: 21, Loss: 235.53867, Residuals: -3.00369, Convergence: 0.009988\n",
      "Epoch: 22, Loss: 233.17722, Residuals: -2.97532, Convergence: 0.010127\n",
      "Epoch: 23, Loss: 230.78246, Residuals: -2.94603, Convergence: 0.010377\n",
      "Epoch: 24, Loss: 228.32257, Residuals: -2.91548, Convergence: 0.010774\n",
      "Epoch: 25, Loss: 225.74706, Residuals: -2.88307, Convergence: 0.011409\n",
      "Epoch: 26, Loss: 223.01400, Residuals: -2.84829, Convergence: 0.012255\n",
      "Epoch: 27, Loss: 220.17944, Residuals: -2.81166, Convergence: 0.012874\n",
      "Epoch: 28, Loss: 217.37993, Residuals: -2.77468, Convergence: 0.012878\n",
      "Epoch: 29, Loss: 214.67882, Residuals: -2.73822, Convergence: 0.012582\n",
      "Epoch: 30, Loss: 212.07216, Residuals: -2.70236, Convergence: 0.012291\n",
      "Epoch: 31, Loss: 209.54221, Residuals: -2.66698, Convergence: 0.012074\n",
      "Epoch: 32, Loss: 207.07405, Residuals: -2.63196, Convergence: 0.011919\n",
      "Epoch: 33, Loss: 204.65777, Residuals: -2.59720, Convergence: 0.011806\n",
      "Epoch: 34, Loss: 202.28770, Residuals: -2.56262, Convergence: 0.011716\n",
      "Epoch: 35, Loss: 199.96122, Residuals: -2.52820, Convergence: 0.011635\n",
      "Epoch: 36, Loss: 197.67769, Residuals: -2.49389, Convergence: 0.011552\n",
      "Epoch: 37, Loss: 195.43772, Residuals: -2.45967, Convergence: 0.011461\n",
      "Epoch: 38, Loss: 193.24249, Residuals: -2.42555, Convergence: 0.011360\n",
      "Epoch: 39, Loss: 191.09343, Residuals: -2.39151, Convergence: 0.011246\n",
      "Epoch: 40, Loss: 188.99199, Residuals: -2.35756, Convergence: 0.011119\n",
      "Epoch: 41, Loss: 186.93961, Residuals: -2.32372, Convergence: 0.010979\n",
      "Epoch: 42, Loss: 184.93772, Residuals: -2.29000, Convergence: 0.010825\n",
      "Epoch: 43, Loss: 182.98791, Residuals: -2.25642, Convergence: 0.010655\n",
      "Epoch: 44, Loss: 181.09201, Residuals: -2.22302, Convergence: 0.010469\n",
      "Epoch: 45, Loss: 179.25211, Residuals: -2.18985, Convergence: 0.010264\n",
      "Epoch: 46, Loss: 177.47054, Residuals: -2.15695, Convergence: 0.010039\n",
      "Epoch: 47, Loss: 175.74963, Residuals: -2.12439, Convergence: 0.009792\n",
      "Epoch: 48, Loss: 174.09146, Residuals: -2.09223, Convergence: 0.009525\n",
      "Epoch: 49, Loss: 172.49766, Residuals: -2.06055, Convergence: 0.009240\n",
      "Epoch: 50, Loss: 170.96915, Residuals: -2.02940, Convergence: 0.008940\n",
      "Epoch: 51, Loss: 169.50616, Residuals: -1.99885, Convergence: 0.008631\n",
      "Epoch: 52, Loss: 168.10817, Residuals: -1.96893, Convergence: 0.008316\n",
      "Epoch: 53, Loss: 166.77396, Residuals: -1.93970, Convergence: 0.008000\n",
      "Epoch: 54, Loss: 165.50169, Residuals: -1.91118, Convergence: 0.007687\n",
      "Epoch: 55, Loss: 164.28898, Residuals: -1.88340, Convergence: 0.007382\n",
      "Epoch: 56, Loss: 163.13307, Residuals: -1.85637, Convergence: 0.007086\n",
      "Epoch: 57, Loss: 162.03082, Residuals: -1.83009, Convergence: 0.006803\n",
      "Epoch: 58, Loss: 160.97900, Residuals: -1.80456, Convergence: 0.006534\n",
      "Epoch: 59, Loss: 159.97426, Residuals: -1.77974, Convergence: 0.006281\n",
      "Epoch: 60, Loss: 159.01342, Residuals: -1.75564, Convergence: 0.006043\n",
      "Epoch: 61, Loss: 158.09350, Residuals: -1.73221, Convergence: 0.005819\n",
      "Epoch: 62, Loss: 157.21188, Residuals: -1.70944, Convergence: 0.005608\n",
      "Epoch: 63, Loss: 156.36635, Residuals: -1.68731, Convergence: 0.005407\n",
      "Epoch: 64, Loss: 155.55511, Residuals: -1.66579, Convergence: 0.005215\n",
      "Epoch: 65, Loss: 154.77673, Residuals: -1.64487, Convergence: 0.005029\n",
      "Epoch: 66, Loss: 154.03010, Residuals: -1.62455, Convergence: 0.004847\n",
      "Epoch: 67, Loss: 153.31431, Residuals: -1.60482, Convergence: 0.004669\n",
      "Epoch: 68, Loss: 152.62863, Residuals: -1.58568, Convergence: 0.004492\n",
      "Epoch: 69, Loss: 151.97237, Residuals: -1.56712, Convergence: 0.004318\n",
      "Epoch: 70, Loss: 151.34490, Residuals: -1.54915, Convergence: 0.004146\n",
      "Epoch: 71, Loss: 150.74555, Residuals: -1.53175, Convergence: 0.003976\n",
      "Epoch: 72, Loss: 150.17365, Residuals: -1.51494, Convergence: 0.003808\n",
      "Epoch: 73, Loss: 149.62847, Residuals: -1.49870, Convergence: 0.003644\n",
      "Epoch: 74, Loss: 149.10920, Residuals: -1.48304, Convergence: 0.003482\n",
      "Epoch: 75, Loss: 148.61500, Residuals: -1.46794, Convergence: 0.003325\n",
      "Epoch: 76, Loss: 148.14501, Residuals: -1.45339, Convergence: 0.003173\n",
      "Epoch: 77, Loss: 147.69828, Residuals: -1.43939, Convergence: 0.003025\n",
      "Epoch: 78, Loss: 147.27388, Residuals: -1.42592, Convergence: 0.002882\n",
      "Epoch: 79, Loss: 146.87085, Residuals: -1.41297, Convergence: 0.002744\n",
      "Epoch: 80, Loss: 146.48821, Residuals: -1.40053, Convergence: 0.002612\n",
      "Epoch: 81, Loss: 146.12501, Residuals: -1.38859, Convergence: 0.002486\n",
      "Epoch: 82, Loss: 145.78032, Residuals: -1.37713, Convergence: 0.002364\n",
      "Epoch: 83, Loss: 145.45324, Residuals: -1.36613, Convergence: 0.002249\n",
      "Epoch: 84, Loss: 145.14288, Residuals: -1.35558, Convergence: 0.002138\n",
      "Epoch: 85, Loss: 144.84842, Residuals: -1.34547, Convergence: 0.002033\n",
      "Epoch: 86, Loss: 144.56909, Residuals: -1.33577, Convergence: 0.001932\n",
      "Epoch: 87, Loss: 144.30414, Residuals: -1.32648, Convergence: 0.001836\n",
      "Epoch: 88, Loss: 144.05290, Residuals: -1.31758, Convergence: 0.001744\n",
      "Epoch: 89, Loss: 143.81474, Residuals: -1.30906, Convergence: 0.001656\n",
      "Epoch: 90, Loss: 143.58906, Residuals: -1.30089, Convergence: 0.001572\n",
      "Epoch: 91, Loss: 143.37534, Residuals: -1.29308, Convergence: 0.001491\n",
      "Epoch: 92, Loss: 143.17310, Residuals: -1.28560, Convergence: 0.001413\n",
      "Epoch: 93, Loss: 142.98188, Residuals: -1.27844, Convergence: 0.001337\n",
      "Epoch: 94, Loss: 142.80128, Residuals: -1.27160, Convergence: 0.001265\n",
      "Epoch: 95, Loss: 142.63092, Residuals: -1.26506, Convergence: 0.001194\n",
      "Epoch: 96, Loss: 142.47044, Residuals: -1.25883, Convergence: 0.001126\n",
      "Epoch: 97, Loss: 142.31948, Residuals: -1.25288, Convergence: 0.001061\n",
      "Epoch: 98, Loss: 142.17770, Residuals: -1.24721, Convergence: 0.000997\n",
      "Evidence -183.811\n",
      "\n",
      "Epoch: 98, Evidence: -183.81116, Convergence:   inf\n",
      "Updating hyper-parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 181, Updated regularization: 7.25e-01\n",
      "Epoch: 98, Loss: 1359.27050, Residuals: -1.24721, Convergence:   inf\n",
      "Epoch: 99, Loss: 1297.96629, Residuals: -1.27518, Convergence: 0.047231\n",
      "Epoch: 100, Loss: 1251.05703, Residuals: -1.29733, Convergence: 0.037496\n",
      "Epoch: 101, Loss: 1215.45120, Residuals: -1.31341, Convergence: 0.029294\n",
      "Epoch: 102, Loss: 1187.74013, Residuals: -1.32471, Convergence: 0.023331\n",
      "Epoch: 103, Loss: 1165.36691, Residuals: -1.33292, Convergence: 0.019198\n",
      "Epoch: 104, Loss: 1146.81343, Residuals: -1.33905, Convergence: 0.016178\n",
      "Epoch: 105, Loss: 1131.16128, Residuals: -1.34359, Convergence: 0.013837\n",
      "Epoch: 106, Loss: 1117.78868, Residuals: -1.34677, Convergence: 0.011963\n",
      "Epoch: 107, Loss: 1106.23689, Residuals: -1.34876, Convergence: 0.010442\n",
      "Epoch: 108, Loss: 1096.14877, Residuals: -1.34968, Convergence: 0.009203\n",
      "Epoch: 109, Loss: 1087.23771, Residuals: -1.34964, Convergence: 0.008196\n",
      "Epoch: 110, Loss: 1079.26753, Residuals: -1.34873, Convergence: 0.007385\n",
      "Epoch: 111, Loss: 1072.04060, Residuals: -1.34700, Convergence: 0.006741\n",
      "Epoch: 112, Loss: 1065.38733, Residuals: -1.34452, Convergence: 0.006245\n",
      "Epoch: 113, Loss: 1059.15995, Residuals: -1.34131, Convergence: 0.005880\n",
      "Epoch: 114, Loss: 1053.22785, Residuals: -1.33739, Convergence: 0.005632\n",
      "Epoch: 115, Loss: 1047.47672, Residuals: -1.33277, Convergence: 0.005490\n",
      "Epoch: 116, Loss: 1041.80944, Residuals: -1.32746, Convergence: 0.005440\n",
      "Epoch: 117, Loss: 1036.15596, Residuals: -1.32149, Convergence: 0.005456\n",
      "Epoch: 118, Loss: 1030.48385, Residuals: -1.31492, Convergence: 0.005504\n",
      "Epoch: 119, Loss: 1024.81202, Residuals: -1.30783, Convergence: 0.005535\n",
      "Epoch: 120, Loss: 1019.20838, Residuals: -1.30032, Convergence: 0.005498\n",
      "Epoch: 121, Loss: 1013.76755, Residuals: -1.29250, Convergence: 0.005367\n",
      "Epoch: 122, Loss: 1008.57624, Residuals: -1.28447, Convergence: 0.005147\n",
      "Epoch: 123, Loss: 1003.68863, Residuals: -1.27630, Convergence: 0.004870\n",
      "Epoch: 124, Loss: 999.12282, Residuals: -1.26809, Convergence: 0.004570\n",
      "Epoch: 125, Loss: 994.87067, Residuals: -1.25988, Convergence: 0.004274\n",
      "Epoch: 126, Loss: 990.91020, Residuals: -1.25174, Convergence: 0.003997\n",
      "Epoch: 127, Loss: 987.21412, Residuals: -1.24371, Convergence: 0.003744\n",
      "Epoch: 128, Loss: 983.75564, Residuals: -1.23582, Convergence: 0.003516\n",
      "Epoch: 129, Loss: 980.51015, Residuals: -1.22811, Convergence: 0.003310\n",
      "Epoch: 130, Loss: 977.45614, Residuals: -1.22060, Convergence: 0.003124\n",
      "Epoch: 131, Loss: 974.57572, Residuals: -1.21330, Convergence: 0.002956\n",
      "Epoch: 132, Loss: 971.85366, Residuals: -1.20623, Convergence: 0.002801\n",
      "Epoch: 133, Loss: 969.27663, Residuals: -1.19941, Convergence: 0.002659\n",
      "Epoch: 134, Loss: 966.83419, Residuals: -1.19283, Convergence: 0.002526\n",
      "Epoch: 135, Loss: 964.51637, Residuals: -1.18651, Convergence: 0.002403\n",
      "Epoch: 136, Loss: 962.31487, Residuals: -1.18045, Convergence: 0.002288\n",
      "Epoch: 137, Loss: 960.22188, Residuals: -1.17465, Convergence: 0.002180\n",
      "Epoch: 138, Loss: 958.23041, Residuals: -1.16911, Convergence: 0.002078\n",
      "Epoch: 139, Loss: 956.33399, Residuals: -1.16381, Convergence: 0.001983\n",
      "Epoch: 140, Loss: 954.52592, Residuals: -1.15877, Convergence: 0.001894\n",
      "Epoch: 141, Loss: 952.80072, Residuals: -1.15396, Convergence: 0.001811\n",
      "Epoch: 142, Loss: 951.15262, Residuals: -1.14938, Convergence: 0.001733\n",
      "Epoch: 143, Loss: 949.57650, Residuals: -1.14503, Convergence: 0.001660\n",
      "Epoch: 144, Loss: 948.06736, Residuals: -1.14088, Convergence: 0.001592\n",
      "Epoch: 145, Loss: 946.62016, Residuals: -1.13694, Convergence: 0.001529\n",
      "Epoch: 146, Loss: 945.23077, Residuals: -1.13319, Convergence: 0.001470\n",
      "Epoch: 147, Loss: 943.89469, Residuals: -1.12961, Convergence: 0.001415\n",
      "Epoch: 148, Loss: 942.60817, Residuals: -1.12621, Convergence: 0.001365\n",
      "Epoch: 149, Loss: 941.36708, Residuals: -1.12296, Convergence: 0.001318\n",
      "Epoch: 150, Loss: 940.16796, Residuals: -1.11987, Convergence: 0.001275\n",
      "Epoch: 151, Loss: 939.00713, Residuals: -1.11691, Convergence: 0.001236\n",
      "Epoch: 152, Loss: 937.88172, Residuals: -1.11408, Convergence: 0.001200\n",
      "Epoch: 153, Loss: 936.78820, Residuals: -1.11136, Convergence: 0.001167\n",
      "Epoch: 154, Loss: 935.72369, Residuals: -1.10876, Convergence: 0.001138\n",
      "Epoch: 155, Loss: 934.68485, Residuals: -1.10626, Convergence: 0.001111\n",
      "Epoch: 156, Loss: 933.66939, Residuals: -1.10384, Convergence: 0.001088\n",
      "Epoch: 157, Loss: 932.67420, Residuals: -1.10152, Convergence: 0.001067\n",
      "Epoch: 158, Loss: 931.69646, Residuals: -1.09926, Convergence: 0.001049\n",
      "Epoch: 159, Loss: 930.73368, Residuals: -1.09707, Convergence: 0.001034\n",
      "Epoch: 160, Loss: 929.78274, Residuals: -1.09494, Convergence: 0.001023\n",
      "Epoch: 161, Loss: 928.84105, Residuals: -1.09285, Convergence: 0.001014\n",
      "Epoch: 162, Loss: 927.90630, Residuals: -1.09080, Convergence: 0.001007\n",
      "Epoch: 163, Loss: 926.97604, Residuals: -1.08879, Convergence: 0.001004\n",
      "Epoch: 164, Loss: 926.04745, Residuals: -1.08680, Convergence: 0.001003\n",
      "Epoch: 165, Loss: 925.11924, Residuals: -1.08483, Convergence: 0.001003\n",
      "Epoch: 166, Loss: 924.18989, Residuals: -1.08287, Convergence: 0.001006\n",
      "Epoch: 167, Loss: 923.25956, Residuals: -1.08091, Convergence: 0.001008\n",
      "Epoch: 168, Loss: 922.32852, Residuals: -1.07897, Convergence: 0.001009\n",
      "Epoch: 169, Loss: 921.39902, Residuals: -1.07702, Convergence: 0.001009\n",
      "Epoch: 170, Loss: 920.47429, Residuals: -1.07509, Convergence: 0.001005\n",
      "Epoch: 171, Loss: 919.55848, Residuals: -1.07318, Convergence: 0.000996\n",
      "Evidence 11022.713\n",
      "\n",
      "Epoch: 171, Evidence: 11022.71289, Convergence: 1.016676\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 5.76e-01\n",
      "Epoch: 171, Loss: 2334.29270, Residuals: -1.07318, Convergence:   inf\n",
      "Epoch: 172, Loss: 2293.60690, Residuals: -1.08064, Convergence: 0.017739\n",
      "Epoch: 173, Loss: 2266.36862, Residuals: -1.07918, Convergence: 0.012018\n",
      "Epoch: 174, Loss: 2243.73651, Residuals: -1.07689, Convergence: 0.010087\n",
      "Epoch: 175, Loss: 2224.67130, Residuals: -1.07441, Convergence: 0.008570\n",
      "Epoch: 176, Loss: 2208.48834, Residuals: -1.07186, Convergence: 0.007328\n",
      "Epoch: 177, Loss: 2194.65465, Residuals: -1.06927, Convergence: 0.006303\n",
      "Epoch: 178, Loss: 2182.73147, Residuals: -1.06666, Convergence: 0.005463\n",
      "Epoch: 179, Loss: 2172.35116, Residuals: -1.06404, Convergence: 0.004778\n",
      "Epoch: 180, Loss: 2163.20545, Residuals: -1.06140, Convergence: 0.004228\n",
      "Epoch: 181, Loss: 2155.03864, Residuals: -1.05872, Convergence: 0.003790\n",
      "Epoch: 182, Loss: 2147.64788, Residuals: -1.05598, Convergence: 0.003441\n",
      "Epoch: 183, Loss: 2140.88509, Residuals: -1.05318, Convergence: 0.003159\n",
      "Epoch: 184, Loss: 2134.65548, Residuals: -1.05033, Convergence: 0.002918\n",
      "Epoch: 185, Loss: 2128.90663, Residuals: -1.04744, Convergence: 0.002700\n",
      "Epoch: 186, Loss: 2123.61019, Residuals: -1.04456, Convergence: 0.002494\n",
      "Epoch: 187, Loss: 2118.74262, Residuals: -1.04172, Convergence: 0.002297\n",
      "Epoch: 188, Loss: 2114.27957, Residuals: -1.03898, Convergence: 0.002111\n",
      "Epoch: 189, Loss: 2110.19053, Residuals: -1.03633, Convergence: 0.001938\n",
      "Epoch: 190, Loss: 2106.44223, Residuals: -1.03380, Convergence: 0.001779\n",
      "Epoch: 191, Loss: 2103.00012, Residuals: -1.03139, Convergence: 0.001637\n",
      "Epoch: 192, Loss: 2099.82981, Residuals: -1.02909, Convergence: 0.001510\n",
      "Epoch: 193, Loss: 2096.89835, Residuals: -1.02691, Convergence: 0.001398\n",
      "Epoch: 194, Loss: 2094.17576, Residuals: -1.02482, Convergence: 0.001300\n",
      "Epoch: 195, Loss: 2091.63527, Residuals: -1.02284, Convergence: 0.001215\n",
      "Epoch: 196, Loss: 2089.25404, Residuals: -1.02094, Convergence: 0.001140\n",
      "Epoch: 197, Loss: 2087.01277, Residuals: -1.01911, Convergence: 0.001074\n",
      "Epoch: 198, Loss: 2084.89642, Residuals: -1.01736, Convergence: 0.001015\n",
      "Epoch: 199, Loss: 2082.89159, Residuals: -1.01568, Convergence: 0.000963\n",
      "Evidence 14181.192\n",
      "\n",
      "Epoch: 199, Evidence: 14181.19238, Convergence: 0.222723\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 4.39e-01\n",
      "Epoch: 199, Loss: 2461.47021, Residuals: -1.01568, Convergence:   inf\n",
      "Epoch: 200, Loss: 2448.09580, Residuals: -1.01370, Convergence: 0.005463\n",
      "Epoch: 201, Loss: 2436.96180, Residuals: -1.01097, Convergence: 0.004569\n",
      "Epoch: 202, Loss: 2427.16522, Residuals: -1.00817, Convergence: 0.004036\n",
      "Epoch: 203, Loss: 2418.50818, Residuals: -1.00540, Convergence: 0.003579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 204, Loss: 2410.84186, Residuals: -1.00276, Convergence: 0.003180\n",
      "Epoch: 205, Loss: 2404.04247, Residuals: -1.00026, Convergence: 0.002828\n",
      "Epoch: 206, Loss: 2398.00083, Residuals: -0.99791, Convergence: 0.002519\n",
      "Epoch: 207, Loss: 2392.62237, Residuals: -0.99573, Convergence: 0.002248\n",
      "Epoch: 208, Loss: 2387.81983, Residuals: -0.99369, Convergence: 0.002011\n",
      "Epoch: 209, Loss: 2383.51906, Residuals: -0.99179, Convergence: 0.001804\n",
      "Epoch: 210, Loss: 2379.65339, Residuals: -0.99001, Convergence: 0.001624\n",
      "Epoch: 211, Loss: 2376.16396, Residuals: -0.98835, Convergence: 0.001469\n",
      "Epoch: 212, Loss: 2373.00107, Residuals: -0.98679, Convergence: 0.001333\n",
      "Epoch: 213, Loss: 2370.12168, Residuals: -0.98533, Convergence: 0.001215\n",
      "Epoch: 214, Loss: 2367.48846, Residuals: -0.98396, Convergence: 0.001112\n",
      "Epoch: 215, Loss: 2365.06954, Residuals: -0.98267, Convergence: 0.001023\n",
      "Epoch: 216, Loss: 2362.83891, Residuals: -0.98146, Convergence: 0.000944\n",
      "Evidence 14575.356\n",
      "\n",
      "Epoch: 216, Evidence: 14575.35645, Convergence: 0.027043\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 3.37e-01\n",
      "Epoch: 216, Loss: 2466.31587, Residuals: -0.98146, Convergence:   inf\n",
      "Epoch: 217, Loss: 2459.37431, Residuals: -0.97826, Convergence: 0.002822\n",
      "Epoch: 218, Loss: 2453.59571, Residuals: -0.97543, Convergence: 0.002355\n",
      "Epoch: 219, Loss: 2448.68217, Residuals: -0.97299, Convergence: 0.002007\n",
      "Epoch: 220, Loss: 2444.45622, Residuals: -0.97089, Convergence: 0.001729\n",
      "Epoch: 221, Loss: 2440.78089, Residuals: -0.96908, Convergence: 0.001506\n",
      "Epoch: 222, Loss: 2437.55100, Residuals: -0.96752, Convergence: 0.001325\n",
      "Epoch: 223, Loss: 2434.68048, Residuals: -0.96617, Convergence: 0.001179\n",
      "Epoch: 224, Loss: 2432.10502, Residuals: -0.96499, Convergence: 0.001059\n",
      "Epoch: 225, Loss: 2429.77357, Residuals: -0.96397, Convergence: 0.000960\n",
      "Evidence 14667.121\n",
      "\n",
      "Epoch: 225, Evidence: 14667.12109, Convergence: 0.006256\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 2.64e-01\n",
      "Epoch: 225, Loss: 2467.73945, Residuals: -0.96397, Convergence:   inf\n",
      "Epoch: 226, Loss: 2463.58742, Residuals: -0.96146, Convergence: 0.001685\n",
      "Epoch: 227, Loss: 2460.13386, Residuals: -0.95948, Convergence: 0.001404\n",
      "Epoch: 228, Loss: 2457.19325, Residuals: -0.95789, Convergence: 0.001197\n",
      "Epoch: 229, Loss: 2454.64409, Residuals: -0.95660, Convergence: 0.001039\n",
      "Epoch: 230, Loss: 2452.39816, Residuals: -0.95555, Convergence: 0.000916\n",
      "Evidence 14700.125\n",
      "\n",
      "Epoch: 230, Evidence: 14700.12500, Convergence: 0.002245\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 2.12e-01\n",
      "Epoch: 230, Loss: 2468.54743, Residuals: -0.95555, Convergence:   inf\n",
      "Epoch: 231, Loss: 2465.61540, Residuals: -0.95376, Convergence: 0.001189\n",
      "Epoch: 232, Loss: 2463.17377, Residuals: -0.95238, Convergence: 0.000991\n",
      "Evidence 14713.275\n",
      "\n",
      "Epoch: 232, Evidence: 14713.27539, Convergence: 0.000894\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.76e-01\n",
      "Epoch: 232, Loss: 2469.17495, Residuals: -0.95238, Convergence:   inf\n",
      "Epoch: 233, Loss: 2464.73327, Residuals: -0.95027, Convergence: 0.001802\n",
      "Epoch: 234, Loss: 2461.27557, Residuals: -0.94860, Convergence: 0.001405\n",
      "Epoch: 235, Loss: 2458.48793, Residuals: -0.94743, Convergence: 0.001134\n",
      "Epoch: 236, Loss: 2456.13176, Residuals: -0.94671, Convergence: 0.000959\n",
      "Evidence 14730.968\n",
      "\n",
      "Epoch: 236, Evidence: 14730.96777, Convergence: 0.002094\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.45e-01\n",
      "Epoch: 236, Loss: 2469.29766, Residuals: -0.94671, Convergence:   inf\n",
      "Epoch: 237, Loss: 2466.34339, Residuals: -0.94446, Convergence: 0.001198\n",
      "Epoch: 238, Loss: 2464.00421, Residuals: -0.94323, Convergence: 0.000949\n",
      "Evidence 14741.879\n",
      "\n",
      "Epoch: 238, Evidence: 14741.87891, Convergence: 0.000740\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.23e-01\n",
      "Epoch: 238, Loss: 2469.45517, Residuals: -0.94323, Convergence:   inf\n",
      "Epoch: 239, Loss: 2465.15110, Residuals: -0.93987, Convergence: 0.001746\n",
      "Epoch: 240, Loss: 2462.06414, Residuals: -0.94063, Convergence: 0.001254\n",
      "Epoch: 241, Loss: 2459.48601, Residuals: -0.94100, Convergence: 0.001048\n",
      "Epoch: 242, Loss: 2457.21091, Residuals: -0.94338, Convergence: 0.000926\n",
      "Evidence 14757.355\n",
      "\n",
      "Epoch: 242, Evidence: 14757.35547, Convergence: 0.001788\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.09e-01\n",
      "Epoch: 242, Loss: 2468.75128, Residuals: -0.94338, Convergence:   inf\n",
      "Epoch: 243, Loss: 2467.02687, Residuals: -0.94062, Convergence: 0.000699\n",
      "Evidence 14763.859\n",
      "\n",
      "Epoch: 243, Evidence: 14763.85938, Convergence: 0.000441\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 8.96e-02\n",
      "Epoch: 243, Loss: 2469.72221, Residuals: -0.94062, Convergence:   inf\n",
      "Epoch: 244, Loss: 2510.31506, Residuals: -0.97830, Convergence: -0.016170\n",
      "Epoch: 244, Loss: 2467.48378, Residuals: -0.93859, Convergence: 0.000907\n",
      "Evidence 14768.100\n",
      "\n",
      "Epoch: 244, Evidence: 14768.09961, Convergence: 0.000728\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 8.27e-02\n",
      "Epoch: 244, Loss: 2469.14281, Residuals: -0.93859, Convergence:   inf\n",
      "Epoch: 245, Loss: 2474.35649, Residuals: -0.93957, Convergence: -0.002107\n",
      "Epoch: 245, Loss: 2469.21112, Residuals: -0.93687, Convergence: -0.000028\n",
      "Evidence 14769.684\n",
      "\n",
      "Epoch: 245, Evidence: 14769.68359, Convergence: 0.000835\n",
      "Total samples: 183, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 380.75692, Residuals: -4.52159, Convergence:   inf\n",
      "Epoch: 1, Loss: 355.12532, Residuals: -4.40187, Convergence: 0.072176\n",
      "Epoch: 2, Loss: 334.10764, Residuals: -4.23783, Convergence: 0.062907\n",
      "Epoch: 3, Loss: 318.05227, Residuals: -4.07311, Convergence: 0.050480\n",
      "Epoch: 4, Loss: 305.81171, Residuals: -3.92799, Convergence: 0.040026\n",
      "Epoch: 5, Loss: 296.10598, Residuals: -3.79964, Convergence: 0.032778\n",
      "Epoch: 6, Loss: 288.22693, Residuals: -3.68778, Convergence: 0.027336\n",
      "Epoch: 7, Loss: 281.68339, Residuals: -3.59191, Convergence: 0.023230\n",
      "Epoch: 8, Loss: 276.11271, Residuals: -3.50999, Convergence: 0.020175\n",
      "Epoch: 9, Loss: 271.25933, Residuals: -3.43972, Convergence: 0.017892\n",
      "Epoch: 10, Loss: 266.94184, Residuals: -3.37902, Convergence: 0.016174\n",
      "Epoch: 11, Loss: 263.02919, Residuals: -3.32614, Convergence: 0.014875\n",
      "Epoch: 12, Loss: 259.42603, Residuals: -3.27956, Convergence: 0.013889\n",
      "Epoch: 13, Loss: 256.06340, Residuals: -3.23799, Convergence: 0.013132\n",
      "Epoch: 14, Loss: 252.89238, Residuals: -3.20027, Convergence: 0.012539\n",
      "Epoch: 15, Loss: 249.88023, Residuals: -3.16542, Convergence: 0.012054\n",
      "Epoch: 16, Loss: 247.00764, Residuals: -3.13272, Convergence: 0.011630\n",
      "Epoch: 17, Loss: 244.26124, Residuals: -3.10167, Convergence: 0.011244\n",
      "Epoch: 18, Loss: 241.62198, Residuals: -3.07183, Convergence: 0.010923\n",
      "Epoch: 19, Loss: 239.06103, Residuals: -3.04267, Convergence: 0.010713\n",
      "Epoch: 20, Loss: 236.54572, Residuals: -3.01365, Convergence: 0.010634\n",
      "Epoch: 21, Loss: 234.04854, Residuals: -2.98431, Convergence: 0.010669\n",
      "Epoch: 22, Loss: 231.55099, Residuals: -2.95441, Convergence: 0.010786\n",
      "Epoch: 23, Loss: 229.03316, Residuals: -2.92376, Convergence: 0.010993\n",
      "Epoch: 24, Loss: 226.45647, Residuals: -2.89198, Convergence: 0.011378\n",
      "Epoch: 25, Loss: 223.76464, Residuals: -2.85842, Convergence: 0.012030\n",
      "Epoch: 26, Loss: 220.93551, Residuals: -2.82276, Convergence: 0.012805\n",
      "Epoch: 27, Loss: 218.06035, Residuals: -2.78594, Convergence: 0.013185\n",
      "Epoch: 28, Loss: 215.25673, Residuals: -2.74932, Convergence: 0.013025\n",
      "Epoch: 29, Loss: 212.55479, Residuals: -2.71339, Convergence: 0.012712\n",
      "Epoch: 30, Loss: 209.93993, Residuals: -2.67809, Convergence: 0.012455\n",
      "Epoch: 31, Loss: 207.39252, Residuals: -2.64322, Convergence: 0.012283\n",
      "Epoch: 32, Loss: 204.89752, Residuals: -2.60864, Convergence: 0.012177\n",
      "Epoch: 33, Loss: 202.44531, Residuals: -2.57421, Convergence: 0.012113\n",
      "Epoch: 34, Loss: 200.03081, Residuals: -2.53983, Convergence: 0.012071\n",
      "Epoch: 35, Loss: 197.65244, Residuals: -2.50546, Convergence: 0.012033\n",
      "Epoch: 36, Loss: 195.31098, Residuals: -2.47106, Convergence: 0.011988\n",
      "Epoch: 37, Loss: 193.00874, Residuals: -2.43664, Convergence: 0.011928\n",
      "Epoch: 38, Loss: 190.74883, Residuals: -2.40220, Convergence: 0.011848\n",
      "Epoch: 39, Loss: 188.53469, Residuals: -2.36779, Convergence: 0.011744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40, Loss: 186.36973, Residuals: -2.33343, Convergence: 0.011616\n",
      "Epoch: 41, Loss: 184.25716, Residuals: -2.29918, Convergence: 0.011465\n",
      "Epoch: 42, Loss: 182.19989, Residuals: -2.26508, Convergence: 0.011291\n",
      "Epoch: 43, Loss: 180.20048, Residuals: -2.23117, Convergence: 0.011095\n",
      "Epoch: 44, Loss: 178.26117, Residuals: -2.19751, Convergence: 0.010879\n",
      "Epoch: 45, Loss: 176.38382, Residuals: -2.16416, Convergence: 0.010644\n",
      "Epoch: 46, Loss: 174.57000, Residuals: -2.13115, Convergence: 0.010390\n",
      "Epoch: 47, Loss: 172.82086, Residuals: -2.09854, Convergence: 0.010121\n",
      "Epoch: 48, Loss: 171.13721, Residuals: -2.06638, Convergence: 0.009838\n",
      "Epoch: 49, Loss: 169.51946, Residuals: -2.03470, Convergence: 0.009543\n",
      "Epoch: 50, Loss: 167.96758, Residuals: -2.00356, Convergence: 0.009239\n",
      "Epoch: 51, Loss: 166.48116, Residuals: -1.97299, Convergence: 0.008928\n",
      "Epoch: 52, Loss: 165.05936, Residuals: -1.94301, Convergence: 0.008614\n",
      "Epoch: 53, Loss: 163.70100, Residuals: -1.91364, Convergence: 0.008298\n",
      "Epoch: 54, Loss: 162.40462, Residuals: -1.88491, Convergence: 0.007982\n",
      "Epoch: 55, Loss: 161.16856, Residuals: -1.85683, Convergence: 0.007669\n",
      "Epoch: 56, Loss: 159.99105, Residuals: -1.82939, Convergence: 0.007360\n",
      "Epoch: 57, Loss: 158.87031, Residuals: -1.80261, Convergence: 0.007054\n",
      "Epoch: 58, Loss: 157.80457, Residuals: -1.77649, Convergence: 0.006754\n",
      "Epoch: 59, Loss: 156.79212, Residuals: -1.75103, Convergence: 0.006457\n",
      "Epoch: 60, Loss: 155.83126, Residuals: -1.72624, Convergence: 0.006166\n",
      "Epoch: 61, Loss: 154.92033, Residuals: -1.70213, Convergence: 0.005880\n",
      "Epoch: 62, Loss: 154.05765, Residuals: -1.67869, Convergence: 0.005600\n",
      "Epoch: 63, Loss: 153.24156, Residuals: -1.65595, Convergence: 0.005326\n",
      "Epoch: 64, Loss: 152.47034, Residuals: -1.63391, Convergence: 0.005058\n",
      "Epoch: 65, Loss: 151.74229, Residuals: -1.61257, Convergence: 0.004798\n",
      "Epoch: 66, Loss: 151.05568, Residuals: -1.59195, Convergence: 0.004545\n",
      "Epoch: 67, Loss: 150.40877, Residuals: -1.57204, Convergence: 0.004301\n",
      "Epoch: 68, Loss: 149.79982, Residuals: -1.55285, Convergence: 0.004065\n",
      "Epoch: 69, Loss: 149.22706, Residuals: -1.53437, Convergence: 0.003838\n",
      "Epoch: 70, Loss: 148.68871, Residuals: -1.51662, Convergence: 0.003621\n",
      "Epoch: 71, Loss: 148.18293, Residuals: -1.49958, Convergence: 0.003413\n",
      "Epoch: 72, Loss: 147.70784, Residuals: -1.48326, Convergence: 0.003216\n",
      "Epoch: 73, Loss: 147.26147, Residuals: -1.46764, Convergence: 0.003031\n",
      "Epoch: 74, Loss: 146.84181, Residuals: -1.45270, Convergence: 0.002858\n",
      "Epoch: 75, Loss: 146.44676, Residuals: -1.43844, Convergence: 0.002698\n",
      "Epoch: 76, Loss: 146.07423, Residuals: -1.42482, Convergence: 0.002550\n",
      "Epoch: 77, Loss: 145.72212, Residuals: -1.41182, Convergence: 0.002416\n",
      "Epoch: 78, Loss: 145.38845, Residuals: -1.39942, Convergence: 0.002295\n",
      "Epoch: 79, Loss: 145.07133, Residuals: -1.38757, Convergence: 0.002186\n",
      "Epoch: 80, Loss: 144.76903, Residuals: -1.37624, Convergence: 0.002088\n",
      "Epoch: 81, Loss: 144.47996, Residuals: -1.36542, Convergence: 0.002001\n",
      "Epoch: 82, Loss: 144.20268, Residuals: -1.35507, Convergence: 0.001923\n",
      "Epoch: 83, Loss: 143.93584, Residuals: -1.34515, Convergence: 0.001854\n",
      "Epoch: 84, Loss: 143.67815, Residuals: -1.33566, Convergence: 0.001794\n",
      "Epoch: 85, Loss: 143.42837, Residuals: -1.32655, Convergence: 0.001741\n",
      "Epoch: 86, Loss: 143.18536, Residuals: -1.31780, Convergence: 0.001697\n",
      "Epoch: 87, Loss: 142.94808, Residuals: -1.30938, Convergence: 0.001660\n",
      "Epoch: 88, Loss: 142.71568, Residuals: -1.30126, Convergence: 0.001628\n",
      "Epoch: 89, Loss: 142.48749, Residuals: -1.29342, Convergence: 0.001601\n",
      "Epoch: 90, Loss: 142.26308, Residuals: -1.28582, Convergence: 0.001577\n",
      "Epoch: 91, Loss: 142.04221, Residuals: -1.27846, Convergence: 0.001555\n",
      "Epoch: 92, Loss: 141.82484, Residuals: -1.27131, Convergence: 0.001533\n",
      "Epoch: 93, Loss: 141.61104, Residuals: -1.26437, Convergence: 0.001510\n",
      "Epoch: 94, Loss: 141.40101, Residuals: -1.25763, Convergence: 0.001485\n",
      "Epoch: 95, Loss: 141.19496, Residuals: -1.25109, Convergence: 0.001459\n",
      "Epoch: 96, Loss: 140.99310, Residuals: -1.24474, Convergence: 0.001432\n",
      "Epoch: 97, Loss: 140.79566, Residuals: -1.23858, Convergence: 0.001402\n",
      "Epoch: 98, Loss: 140.60284, Residuals: -1.23260, Convergence: 0.001371\n",
      "Epoch: 99, Loss: 140.41478, Residuals: -1.22681, Convergence: 0.001339\n",
      "Epoch: 100, Loss: 140.23159, Residuals: -1.22120, Convergence: 0.001306\n",
      "Epoch: 101, Loss: 140.05335, Residuals: -1.21576, Convergence: 0.001273\n",
      "Epoch: 102, Loss: 139.88009, Residuals: -1.21051, Convergence: 0.001239\n",
      "Epoch: 103, Loss: 139.71181, Residuals: -1.20542, Convergence: 0.001204\n",
      "Epoch: 104, Loss: 139.54847, Residuals: -1.20050, Convergence: 0.001170\n",
      "Epoch: 105, Loss: 139.39002, Residuals: -1.19574, Convergence: 0.001137\n",
      "Epoch: 106, Loss: 139.23637, Residuals: -1.19113, Convergence: 0.001104\n",
      "Epoch: 107, Loss: 139.08743, Residuals: -1.18668, Convergence: 0.001071\n",
      "Epoch: 108, Loss: 138.94308, Residuals: -1.18238, Convergence: 0.001039\n",
      "Epoch: 109, Loss: 138.80322, Residuals: -1.17822, Convergence: 0.001008\n",
      "Epoch: 110, Loss: 138.66770, Residuals: -1.17419, Convergence: 0.000977\n",
      "Evidence -178.953\n",
      "\n",
      "Epoch: 110, Evidence: -178.95270, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.23e-01\n",
      "Epoch: 110, Loss: 1369.92570, Residuals: -1.17419, Convergence:   inf\n",
      "Epoch: 111, Loss: 1314.85699, Residuals: -1.19956, Convergence: 0.041882\n",
      "Epoch: 112, Loss: 1272.13640, Residuals: -1.22130, Convergence: 0.033582\n",
      "Epoch: 113, Loss: 1239.18401, Residuals: -1.23894, Convergence: 0.026592\n",
      "Epoch: 114, Loss: 1213.15709, Residuals: -1.25297, Convergence: 0.021454\n",
      "Epoch: 115, Loss: 1191.93246, Residuals: -1.26435, Convergence: 0.017807\n",
      "Epoch: 116, Loss: 1174.24264, Residuals: -1.27362, Convergence: 0.015065\n",
      "Epoch: 117, Loss: 1159.29568, Residuals: -1.28109, Convergence: 0.012893\n",
      "Epoch: 118, Loss: 1146.53774, Residuals: -1.28695, Convergence: 0.011127\n",
      "Epoch: 119, Loss: 1135.55117, Residuals: -1.29138, Convergence: 0.009675\n",
      "Epoch: 120, Loss: 1126.00791, Residuals: -1.29455, Convergence: 0.008475\n",
      "Epoch: 121, Loss: 1117.64437, Residuals: -1.29658, Convergence: 0.007483\n",
      "Epoch: 122, Loss: 1110.24363, Residuals: -1.29763, Convergence: 0.006666\n",
      "Epoch: 123, Loss: 1103.62612, Residuals: -1.29780, Convergence: 0.005996\n",
      "Epoch: 124, Loss: 1097.63864, Residuals: -1.29719, Convergence: 0.005455\n",
      "Epoch: 125, Loss: 1092.15081, Residuals: -1.29589, Convergence: 0.005025\n",
      "Epoch: 126, Loss: 1087.04998, Residuals: -1.29396, Convergence: 0.004692\n",
      "Epoch: 127, Loss: 1082.23870, Residuals: -1.29145, Convergence: 0.004446\n",
      "Epoch: 128, Loss: 1077.63347, Residuals: -1.28839, Convergence: 0.004273\n",
      "Epoch: 129, Loss: 1073.16407, Residuals: -1.28483, Convergence: 0.004165\n",
      "Epoch: 130, Loss: 1068.77168, Residuals: -1.28077, Convergence: 0.004110\n",
      "Epoch: 131, Loss: 1064.40692, Residuals: -1.27622, Convergence: 0.004101\n",
      "Epoch: 132, Loss: 1060.02340, Residuals: -1.27120, Convergence: 0.004135\n",
      "Epoch: 133, Loss: 1055.57347, Residuals: -1.26569, Convergence: 0.004216\n",
      "Epoch: 134, Loss: 1051.00723, Residuals: -1.25970, Convergence: 0.004345\n",
      "Epoch: 135, Loss: 1046.28110, Residuals: -1.25323, Convergence: 0.004517\n",
      "Epoch: 136, Loss: 1041.37919, Residuals: -1.24635, Convergence: 0.004707\n",
      "Epoch: 137, Loss: 1036.33925, Residuals: -1.23912, Convergence: 0.004863\n",
      "Epoch: 138, Loss: 1031.25870, Residuals: -1.23163, Convergence: 0.004927\n",
      "Epoch: 139, Loss: 1026.26375, Residuals: -1.22398, Convergence: 0.004867\n",
      "Epoch: 140, Loss: 1021.46427, Residuals: -1.21623, Convergence: 0.004699\n",
      "Epoch: 141, Loss: 1016.92505, Residuals: -1.20846, Convergence: 0.004464\n",
      "Epoch: 142, Loss: 1012.66790, Residuals: -1.20072, Convergence: 0.004204\n",
      "Epoch: 143, Loss: 1008.68761, Residuals: -1.19307, Convergence: 0.003946\n",
      "Epoch: 144, Loss: 1004.96718, Residuals: -1.18557, Convergence: 0.003702\n",
      "Epoch: 145, Loss: 1001.48590, Residuals: -1.17825, Convergence: 0.003476\n",
      "Epoch: 146, Loss: 998.22424, Residuals: -1.17114, Convergence: 0.003267\n",
      "Epoch: 147, Loss: 995.16412, Residuals: -1.16429, Convergence: 0.003075\n",
      "Epoch: 148, Loss: 992.29030, Residuals: -1.15770, Convergence: 0.002896\n",
      "Epoch: 149, Loss: 989.58833, Residuals: -1.15138, Convergence: 0.002730\n",
      "Epoch: 150, Loss: 987.04555, Residuals: -1.14536, Convergence: 0.002576\n",
      "Epoch: 151, Loss: 984.64954, Residuals: -1.13962, Convergence: 0.002433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 152, Loss: 982.38917, Residuals: -1.13417, Convergence: 0.002301\n",
      "Epoch: 153, Loss: 980.25330, Residuals: -1.12900, Convergence: 0.002179\n",
      "Epoch: 154, Loss: 978.23217, Residuals: -1.12411, Convergence: 0.002066\n",
      "Epoch: 155, Loss: 976.31552, Residuals: -1.11947, Convergence: 0.001963\n",
      "Epoch: 156, Loss: 974.49455, Residuals: -1.11507, Convergence: 0.001869\n",
      "Epoch: 157, Loss: 972.76052, Residuals: -1.11091, Convergence: 0.001783\n",
      "Epoch: 158, Loss: 971.10527, Residuals: -1.10697, Convergence: 0.001705\n",
      "Epoch: 159, Loss: 969.52125, Residuals: -1.10323, Convergence: 0.001634\n",
      "Epoch: 160, Loss: 968.00184, Residuals: -1.09967, Convergence: 0.001570\n",
      "Epoch: 161, Loss: 966.53978, Residuals: -1.09629, Convergence: 0.001513\n",
      "Epoch: 162, Loss: 965.12916, Residuals: -1.09306, Convergence: 0.001462\n",
      "Epoch: 163, Loss: 963.76348, Residuals: -1.08997, Convergence: 0.001417\n",
      "Epoch: 164, Loss: 962.43674, Residuals: -1.08701, Convergence: 0.001379\n",
      "Epoch: 165, Loss: 961.14304, Residuals: -1.08417, Convergence: 0.001346\n",
      "Epoch: 166, Loss: 959.87640, Residuals: -1.08142, Convergence: 0.001320\n",
      "Epoch: 167, Loss: 958.63089, Residuals: -1.07875, Convergence: 0.001299\n",
      "Epoch: 168, Loss: 957.40103, Residuals: -1.07616, Convergence: 0.001285\n",
      "Epoch: 169, Loss: 956.18158, Residuals: -1.07361, Convergence: 0.001275\n",
      "Epoch: 170, Loss: 954.96797, Residuals: -1.07112, Convergence: 0.001271\n",
      "Epoch: 171, Loss: 953.75691, Residuals: -1.06865, Convergence: 0.001270\n",
      "Epoch: 172, Loss: 952.54643, Residuals: -1.06620, Convergence: 0.001271\n",
      "Epoch: 173, Loss: 951.33701, Residuals: -1.06378, Convergence: 0.001271\n",
      "Epoch: 174, Loss: 950.13061, Residuals: -1.06137, Convergence: 0.001270\n",
      "Epoch: 175, Loss: 948.93267, Residuals: -1.05898, Convergence: 0.001262\n",
      "Epoch: 176, Loss: 947.74957, Residuals: -1.05662, Convergence: 0.001248\n",
      "Epoch: 177, Loss: 946.58983, Residuals: -1.05430, Convergence: 0.001225\n",
      "Epoch: 178, Loss: 945.46136, Residuals: -1.05204, Convergence: 0.001194\n",
      "Epoch: 179, Loss: 944.37190, Residuals: -1.04985, Convergence: 0.001154\n",
      "Epoch: 180, Loss: 943.32726, Residuals: -1.04774, Convergence: 0.001107\n",
      "Epoch: 181, Loss: 942.33074, Residuals: -1.04572, Convergence: 0.001058\n",
      "Epoch: 182, Loss: 941.38525, Residuals: -1.04379, Convergence: 0.001004\n",
      "Epoch: 183, Loss: 940.49066, Residuals: -1.04196, Convergence: 0.000951\n",
      "Evidence 11516.102\n",
      "\n",
      "Epoch: 183, Evidence: 11516.10156, Convergence: 1.015539\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 5.69e-01\n",
      "Epoch: 183, Loss: 2356.86651, Residuals: -1.04196, Convergence:   inf\n",
      "Epoch: 184, Loss: 2319.03439, Residuals: -1.04866, Convergence: 0.016314\n",
      "Epoch: 185, Loss: 2294.35300, Residuals: -1.04715, Convergence: 0.010757\n",
      "Epoch: 186, Loss: 2273.73130, Residuals: -1.04543, Convergence: 0.009070\n",
      "Epoch: 187, Loss: 2256.39096, Residuals: -1.04368, Convergence: 0.007685\n",
      "Epoch: 188, Loss: 2241.70245, Residuals: -1.04191, Convergence: 0.006552\n",
      "Epoch: 189, Loss: 2229.14073, Residuals: -1.04012, Convergence: 0.005635\n",
      "Epoch: 190, Loss: 2218.26833, Residuals: -1.03830, Convergence: 0.004901\n",
      "Epoch: 191, Loss: 2208.71935, Residuals: -1.03643, Convergence: 0.004323\n",
      "Epoch: 192, Loss: 2200.19938, Residuals: -1.03448, Convergence: 0.003872\n",
      "Epoch: 193, Loss: 2192.48241, Residuals: -1.03242, Convergence: 0.003520\n",
      "Epoch: 194, Loss: 2185.41238, Residuals: -1.03024, Convergence: 0.003235\n",
      "Epoch: 195, Loss: 2178.89759, Residuals: -1.02794, Convergence: 0.002990\n",
      "Epoch: 196, Loss: 2172.88750, Residuals: -1.02557, Convergence: 0.002766\n",
      "Epoch: 197, Loss: 2167.34850, Residuals: -1.02317, Convergence: 0.002556\n",
      "Epoch: 198, Loss: 2162.25194, Residuals: -1.02078, Convergence: 0.002357\n",
      "Epoch: 199, Loss: 2157.56287, Residuals: -1.01843, Convergence: 0.002173\n",
      "Epoch: 200, Loss: 2153.24701, Residuals: -1.01615, Convergence: 0.002004\n",
      "Epoch: 201, Loss: 2149.26626, Residuals: -1.01394, Convergence: 0.001852\n",
      "Epoch: 202, Loss: 2145.58445, Residuals: -1.01181, Convergence: 0.001716\n",
      "Epoch: 203, Loss: 2142.17027, Residuals: -1.00977, Convergence: 0.001594\n",
      "Epoch: 204, Loss: 2138.99309, Residuals: -1.00781, Convergence: 0.001485\n",
      "Epoch: 205, Loss: 2136.02685, Residuals: -1.00594, Convergence: 0.001389\n",
      "Epoch: 206, Loss: 2133.24820, Residuals: -1.00415, Convergence: 0.001303\n",
      "Epoch: 207, Loss: 2130.63829, Residuals: -1.00243, Convergence: 0.001225\n",
      "Epoch: 208, Loss: 2128.18015, Residuals: -1.00080, Convergence: 0.001155\n",
      "Epoch: 209, Loss: 2125.85989, Residuals: -0.99923, Convergence: 0.001091\n",
      "Epoch: 210, Loss: 2123.66660, Residuals: -0.99774, Convergence: 0.001033\n",
      "Epoch: 211, Loss: 2121.58939, Residuals: -0.99631, Convergence: 0.000979\n",
      "Evidence 14587.195\n",
      "\n",
      "Epoch: 211, Evidence: 14587.19531, Convergence: 0.210534\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 4.31e-01\n",
      "Epoch: 211, Loss: 2475.33116, Residuals: -0.99631, Convergence:   inf\n",
      "Epoch: 212, Loss: 2461.87504, Residuals: -0.99301, Convergence: 0.005466\n",
      "Epoch: 213, Loss: 2450.95713, Residuals: -0.98960, Convergence: 0.004455\n",
      "Epoch: 214, Loss: 2441.58032, Residuals: -0.98645, Convergence: 0.003840\n",
      "Epoch: 215, Loss: 2433.47072, Residuals: -0.98360, Convergence: 0.003333\n",
      "Epoch: 216, Loss: 2426.41392, Residuals: -0.98106, Convergence: 0.002908\n",
      "Epoch: 217, Loss: 2420.23792, Residuals: -0.97879, Convergence: 0.002552\n",
      "Epoch: 218, Loss: 2414.80210, Residuals: -0.97677, Convergence: 0.002251\n",
      "Epoch: 219, Loss: 2409.99032, Residuals: -0.97497, Convergence: 0.001997\n",
      "Epoch: 220, Loss: 2405.70630, Residuals: -0.97336, Convergence: 0.001781\n",
      "Epoch: 221, Loss: 2401.86932, Residuals: -0.97191, Convergence: 0.001597\n",
      "Epoch: 222, Loss: 2398.41409, Residuals: -0.97061, Convergence: 0.001441\n",
      "Epoch: 223, Loss: 2395.28557, Residuals: -0.96944, Convergence: 0.001306\n",
      "Epoch: 224, Loss: 2392.43616, Residuals: -0.96837, Convergence: 0.001191\n",
      "Epoch: 225, Loss: 2389.82964, Residuals: -0.96740, Convergence: 0.001091\n",
      "Epoch: 226, Loss: 2387.43360, Residuals: -0.96651, Convergence: 0.001004\n",
      "Epoch: 227, Loss: 2385.22233, Residuals: -0.96570, Convergence: 0.000927\n",
      "Evidence 14931.027\n",
      "\n",
      "Epoch: 227, Evidence: 14931.02734, Convergence: 0.023028\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 3.29e-01\n",
      "Epoch: 227, Loss: 2479.65074, Residuals: -0.96570, Convergence:   inf\n",
      "Epoch: 228, Loss: 2473.15101, Residuals: -0.96257, Convergence: 0.002628\n",
      "Epoch: 229, Loss: 2467.78526, Residuals: -0.96000, Convergence: 0.002174\n",
      "Epoch: 230, Loss: 2463.22844, Residuals: -0.95790, Convergence: 0.001850\n",
      "Epoch: 231, Loss: 2459.30097, Residuals: -0.95617, Convergence: 0.001597\n",
      "Epoch: 232, Loss: 2455.87488, Residuals: -0.95474, Convergence: 0.001395\n",
      "Epoch: 233, Loss: 2452.85016, Residuals: -0.95355, Convergence: 0.001233\n",
      "Epoch: 234, Loss: 2450.15250, Residuals: -0.95255, Convergence: 0.001101\n",
      "Epoch: 235, Loss: 2447.72221, Residuals: -0.95171, Convergence: 0.000993\n",
      "Evidence 15012.209\n",
      "\n",
      "Epoch: 235, Evidence: 15012.20898, Convergence: 0.005408\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.58e-01\n",
      "Epoch: 235, Loss: 2481.07695, Residuals: -0.95171, Convergence:   inf\n",
      "Epoch: 236, Loss: 2477.11031, Residuals: -0.94946, Convergence: 0.001601\n",
      "Epoch: 237, Loss: 2473.80843, Residuals: -0.94776, Convergence: 0.001335\n",
      "Epoch: 238, Loss: 2470.97486, Residuals: -0.94643, Convergence: 0.001147\n",
      "Epoch: 239, Loss: 2468.49872, Residuals: -0.94539, Convergence: 0.001003\n",
      "Epoch: 240, Loss: 2466.30301, Residuals: -0.94456, Convergence: 0.000890\n",
      "Evidence 15042.635\n",
      "\n",
      "Epoch: 240, Evidence: 15042.63477, Convergence: 0.002023\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.07e-01\n",
      "Epoch: 240, Loss: 2481.86565, Residuals: -0.94456, Convergence:   inf\n",
      "Epoch: 241, Loss: 2479.05151, Residuals: -0.94292, Convergence: 0.001135\n",
      "Epoch: 242, Loss: 2476.68749, Residuals: -0.94172, Convergence: 0.000955\n",
      "Evidence 15055.262\n",
      "\n",
      "Epoch: 242, Evidence: 15055.26172, Convergence: 0.000839\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.72e-01\n",
      "Epoch: 242, Loss: 2482.52194, Residuals: -0.94172, Convergence:   inf\n",
      "Epoch: 243, Loss: 2478.09342, Residuals: -0.93990, Convergence: 0.001787\n",
      "Epoch: 244, Loss: 2474.73451, Residuals: -0.93861, Convergence: 0.001357\n",
      "Epoch: 245, Loss: 2472.00920, Residuals: -0.93774, Convergence: 0.001102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 246, Loss: 2469.69027, Residuals: -0.93728, Convergence: 0.000939\n",
      "Evidence 15072.608\n",
      "\n",
      "Epoch: 246, Evidence: 15072.60840, Convergence: 0.001989\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.42e-01\n",
      "Epoch: 246, Loss: 2482.58343, Residuals: -0.93728, Convergence:   inf\n",
      "Epoch: 247, Loss: 2479.71895, Residuals: -0.93532, Convergence: 0.001155\n",
      "Epoch: 248, Loss: 2477.44438, Residuals: -0.93430, Convergence: 0.000918\n",
      "Evidence 15083.259\n",
      "\n",
      "Epoch: 248, Evidence: 15083.25879, Convergence: 0.000706\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.20e-01\n",
      "Epoch: 248, Loss: 2482.71721, Residuals: -0.93430, Convergence:   inf\n",
      "Epoch: 249, Loss: 2478.57924, Residuals: -0.93135, Convergence: 0.001669\n",
      "Epoch: 250, Loss: 2475.56815, Residuals: -0.93240, Convergence: 0.001216\n",
      "Epoch: 251, Loss: 2473.03974, Residuals: -0.93291, Convergence: 0.001022\n",
      "Epoch: 252, Loss: 2470.80545, Residuals: -0.93526, Convergence: 0.000904\n",
      "Evidence 15098.269\n",
      "\n",
      "Epoch: 252, Evidence: 15098.26855, Convergence: 0.001700\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.07e-01\n",
      "Epoch: 252, Loss: 2481.90716, Residuals: -0.93526, Convergence:   inf\n",
      "Epoch: 253, Loss: 2480.62023, Residuals: -0.93269, Convergence: 0.000519\n",
      "Evidence 15104.335\n",
      "\n",
      "Epoch: 253, Evidence: 15104.33496, Convergence: 0.000402\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 8.73e-02\n",
      "Epoch: 253, Loss: 2482.81797, Residuals: -0.93269, Convergence:   inf\n",
      "Epoch: 254, Loss: 2528.86016, Residuals: -0.96814, Convergence: -0.018207\n",
      "Epoch: 254, Loss: 2480.70897, Residuals: -0.92988, Convergence: 0.000850\n",
      "Evidence 15108.475\n",
      "\n",
      "Epoch: 254, Evidence: 15108.47461, Convergence: 0.000676\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 8.04e-02\n",
      "Epoch: 254, Loss: 2482.26319, Residuals: -0.92988, Convergence:   inf\n",
      "Epoch: 255, Loss: 2489.24794, Residuals: -0.92852, Convergence: -0.002806\n",
      "Epoch: 255, Loss: 2482.80614, Residuals: -0.92705, Convergence: -0.000219\n",
      "Evidence 15109.422\n",
      "\n",
      "Epoch: 255, Evidence: 15109.42188, Convergence: 0.000738\n",
      "Total samples: 181, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 382.90488, Residuals: -4.57014, Convergence:   inf\n",
      "Epoch: 1, Loss: 357.06384, Residuals: -4.44862, Convergence: 0.072371\n",
      "Epoch: 2, Loss: 335.82970, Residuals: -4.28162, Convergence: 0.063229\n",
      "Epoch: 3, Loss: 319.63949, Residuals: -4.11494, Convergence: 0.050651\n",
      "Epoch: 4, Loss: 307.28307, Residuals: -3.96811, Convergence: 0.040212\n",
      "Epoch: 5, Loss: 297.47311, Residuals: -3.83825, Convergence: 0.032978\n",
      "Epoch: 6, Loss: 289.50077, Residuals: -3.72512, Convergence: 0.027538\n",
      "Epoch: 7, Loss: 282.87487, Residuals: -3.62824, Convergence: 0.023423\n",
      "Epoch: 8, Loss: 277.23190, Residuals: -3.54551, Convergence: 0.020355\n",
      "Epoch: 9, Loss: 272.31524, Residuals: -3.47454, Convergence: 0.018055\n",
      "Epoch: 10, Loss: 267.94223, Residuals: -3.41320, Convergence: 0.016321\n",
      "Epoch: 11, Loss: 263.98028, Residuals: -3.35967, Convergence: 0.015009\n",
      "Epoch: 12, Loss: 260.33240, Residuals: -3.31237, Convergence: 0.014012\n",
      "Epoch: 13, Loss: 256.92803, Residuals: -3.26996, Convergence: 0.013250\n",
      "Epoch: 14, Loss: 253.71734, Residuals: -3.23126, Convergence: 0.012655\n",
      "Epoch: 15, Loss: 250.66916, Residuals: -3.19533, Convergence: 0.012160\n",
      "Epoch: 16, Loss: 247.76876, Residuals: -3.16159, Convergence: 0.011706\n",
      "Epoch: 17, Loss: 245.00647, Residuals: -3.12968, Convergence: 0.011274\n",
      "Epoch: 18, Loss: 242.36271, Residuals: -3.09926, Convergence: 0.010908\n",
      "Epoch: 19, Loss: 239.80572, Residuals: -3.06979, Convergence: 0.010663\n",
      "Epoch: 20, Loss: 237.29918, Residuals: -3.04070, Convergence: 0.010563\n",
      "Epoch: 21, Loss: 234.80980, Residuals: -3.01143, Convergence: 0.010602\n",
      "Epoch: 22, Loss: 232.31123, Residuals: -2.98158, Convergence: 0.010755\n",
      "Epoch: 23, Loss: 229.77950, Residuals: -2.95084, Convergence: 0.011018\n",
      "Epoch: 24, Loss: 227.17928, Residuals: -2.91879, Convergence: 0.011446\n",
      "Epoch: 25, Loss: 224.45670, Residuals: -2.88483, Convergence: 0.012130\n",
      "Epoch: 26, Loss: 221.57418, Residuals: -2.84846, Convergence: 0.013009\n",
      "Epoch: 27, Loss: 218.60319, Residuals: -2.81036, Convergence: 0.013591\n",
      "Epoch: 28, Loss: 215.68483, Residuals: -2.77209, Convergence: 0.013531\n",
      "Epoch: 29, Loss: 212.87628, Residuals: -2.73444, Convergence: 0.013193\n",
      "Epoch: 30, Loss: 210.17193, Residuals: -2.69747, Convergence: 0.012867\n",
      "Epoch: 31, Loss: 207.55504, Residuals: -2.66109, Convergence: 0.012608\n",
      "Epoch: 32, Loss: 205.01187, Residuals: -2.62518, Convergence: 0.012405\n",
      "Epoch: 33, Loss: 202.53312, Residuals: -2.58966, Convergence: 0.012239\n",
      "Epoch: 34, Loss: 200.11289, Residuals: -2.55447, Convergence: 0.012094\n",
      "Epoch: 35, Loss: 197.74772, Residuals: -2.51955, Convergence: 0.011961\n",
      "Epoch: 36, Loss: 195.43567, Residuals: -2.48488, Convergence: 0.011830\n",
      "Epoch: 37, Loss: 193.17580, Residuals: -2.45044, Convergence: 0.011699\n",
      "Epoch: 38, Loss: 190.96781, Residuals: -2.41620, Convergence: 0.011562\n",
      "Epoch: 39, Loss: 188.81181, Residuals: -2.38216, Convergence: 0.011419\n",
      "Epoch: 40, Loss: 186.70819, Residuals: -2.34833, Convergence: 0.011267\n",
      "Epoch: 41, Loss: 184.65758, Residuals: -2.31471, Convergence: 0.011105\n",
      "Epoch: 42, Loss: 182.66092, Residuals: -2.28133, Convergence: 0.010931\n",
      "Epoch: 43, Loss: 180.71946, Residuals: -2.24821, Convergence: 0.010743\n",
      "Epoch: 44, Loss: 178.83479, Residuals: -2.21538, Convergence: 0.010539\n",
      "Epoch: 45, Loss: 177.00876, Residuals: -2.18288, Convergence: 0.010316\n",
      "Epoch: 46, Loss: 175.24321, Residuals: -2.15074, Convergence: 0.010075\n",
      "Epoch: 47, Loss: 173.53965, Residuals: -2.11900, Convergence: 0.009817\n",
      "Epoch: 48, Loss: 171.89913, Residuals: -2.08769, Convergence: 0.009544\n",
      "Epoch: 49, Loss: 170.32203, Residuals: -2.05683, Convergence: 0.009260\n",
      "Epoch: 50, Loss: 168.80813, Residuals: -2.02647, Convergence: 0.008968\n",
      "Epoch: 51, Loss: 167.35662, Residuals: -1.99662, Convergence: 0.008673\n",
      "Epoch: 52, Loss: 165.96618, Residuals: -1.96732, Convergence: 0.008378\n",
      "Epoch: 53, Loss: 164.63507, Residuals: -1.93858, Convergence: 0.008085\n",
      "Epoch: 54, Loss: 163.36120, Residuals: -1.91042, Convergence: 0.007798\n",
      "Epoch: 55, Loss: 162.14221, Residuals: -1.88287, Convergence: 0.007518\n",
      "Epoch: 56, Loss: 160.97557, Residuals: -1.85592, Convergence: 0.007247\n",
      "Epoch: 57, Loss: 159.85868, Residuals: -1.82959, Convergence: 0.006987\n",
      "Epoch: 58, Loss: 158.78901, Residuals: -1.80387, Convergence: 0.006736\n",
      "Epoch: 59, Loss: 157.76416, Residuals: -1.77877, Convergence: 0.006496\n",
      "Epoch: 60, Loss: 156.78196, Residuals: -1.75427, Convergence: 0.006265\n",
      "Epoch: 61, Loss: 155.84049, Residuals: -1.73040, Convergence: 0.006041\n",
      "Epoch: 62, Loss: 154.93814, Residuals: -1.70713, Convergence: 0.005824\n",
      "Epoch: 63, Loss: 154.07350, Residuals: -1.68448, Convergence: 0.005612\n",
      "Epoch: 64, Loss: 153.24542, Residuals: -1.66245, Convergence: 0.005404\n",
      "Epoch: 65, Loss: 152.45288, Residuals: -1.64104, Convergence: 0.005199\n",
      "Epoch: 66, Loss: 151.69497, Residuals: -1.62027, Convergence: 0.004996\n",
      "Epoch: 67, Loss: 150.97084, Residuals: -1.60013, Convergence: 0.004796\n",
      "Epoch: 68, Loss: 150.27968, Residuals: -1.58063, Convergence: 0.004599\n",
      "Epoch: 69, Loss: 149.62066, Residuals: -1.56177, Convergence: 0.004405\n",
      "Epoch: 70, Loss: 148.99293, Residuals: -1.54356, Convergence: 0.004213\n",
      "Epoch: 71, Loss: 148.39561, Residuals: -1.52598, Convergence: 0.004025\n",
      "Epoch: 72, Loss: 147.82776, Residuals: -1.50904, Convergence: 0.003841\n",
      "Epoch: 73, Loss: 147.28841, Residuals: -1.49274, Convergence: 0.003662\n",
      "Epoch: 74, Loss: 146.77655, Residuals: -1.47705, Convergence: 0.003487\n",
      "Epoch: 75, Loss: 146.29114, Residuals: -1.46197, Convergence: 0.003318\n",
      "Epoch: 76, Loss: 145.83110, Residuals: -1.44749, Convergence: 0.003155\n",
      "Epoch: 77, Loss: 145.39533, Residuals: -1.43360, Convergence: 0.002997\n",
      "Epoch: 78, Loss: 144.98276, Residuals: -1.42027, Convergence: 0.002846\n",
      "Epoch: 79, Loss: 144.59228, Residuals: -1.40749, Convergence: 0.002701\n",
      "Epoch: 80, Loss: 144.22282, Residuals: -1.39525, Convergence: 0.002562\n",
      "Epoch: 81, Loss: 143.87331, Residuals: -1.38352, Convergence: 0.002429\n",
      "Epoch: 82, Loss: 143.54273, Residuals: -1.37229, Convergence: 0.002303\n",
      "Epoch: 83, Loss: 143.23008, Residuals: -1.36153, Convergence: 0.002183\n",
      "Epoch: 84, Loss: 142.93440, Residuals: -1.35123, Convergence: 0.002069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 85, Loss: 142.65476, Residuals: -1.34137, Convergence: 0.001960\n",
      "Epoch: 86, Loss: 142.39030, Residuals: -1.33192, Convergence: 0.001857\n",
      "Epoch: 87, Loss: 142.14019, Residuals: -1.32288, Convergence: 0.001760\n",
      "Epoch: 88, Loss: 141.90369, Residuals: -1.31422, Convergence: 0.001667\n",
      "Epoch: 89, Loss: 141.68003, Residuals: -1.30593, Convergence: 0.001579\n",
      "Epoch: 90, Loss: 141.46856, Residuals: -1.29798, Convergence: 0.001495\n",
      "Epoch: 91, Loss: 141.26867, Residuals: -1.29037, Convergence: 0.001415\n",
      "Epoch: 92, Loss: 141.07977, Residuals: -1.28308, Convergence: 0.001339\n",
      "Epoch: 93, Loss: 140.90134, Residuals: -1.27609, Convergence: 0.001266\n",
      "Epoch: 94, Loss: 140.73291, Residuals: -1.26939, Convergence: 0.001197\n",
      "Epoch: 95, Loss: 140.57404, Residuals: -1.26297, Convergence: 0.001130\n",
      "Epoch: 96, Loss: 140.42433, Residuals: -1.25682, Convergence: 0.001066\n",
      "Epoch: 97, Loss: 140.28343, Residuals: -1.25093, Convergence: 0.001004\n",
      "Epoch: 98, Loss: 140.15099, Residuals: -1.24529, Convergence: 0.000945\n",
      "Evidence -181.351\n",
      "\n",
      "Epoch: 98, Evidence: -181.35080, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 7.24e-01\n",
      "Epoch: 98, Loss: 1352.58102, Residuals: -1.24529, Convergence:   inf\n",
      "Epoch: 99, Loss: 1292.66201, Residuals: -1.27521, Convergence: 0.046353\n",
      "Epoch: 100, Loss: 1247.21889, Residuals: -1.29789, Convergence: 0.036436\n",
      "Epoch: 101, Loss: 1212.81496, Residuals: -1.31339, Convergence: 0.028367\n",
      "Epoch: 102, Loss: 1185.88318, Residuals: -1.32364, Convergence: 0.022710\n",
      "Epoch: 103, Loss: 1163.95985, Residuals: -1.33075, Convergence: 0.018835\n",
      "Epoch: 104, Loss: 1145.66061, Residuals: -1.33581, Convergence: 0.015973\n",
      "Epoch: 105, Loss: 1130.15804, Residuals: -1.33932, Convergence: 0.013717\n",
      "Epoch: 106, Loss: 1116.88903, Residuals: -1.34154, Convergence: 0.011880\n",
      "Epoch: 107, Loss: 1105.43344, Residuals: -1.34265, Convergence: 0.010363\n",
      "Epoch: 108, Loss: 1095.46211, Residuals: -1.34279, Convergence: 0.009102\n",
      "Epoch: 109, Loss: 1086.70858, Residuals: -1.34208, Convergence: 0.008055\n",
      "Epoch: 110, Loss: 1078.95375, Residuals: -1.34063, Convergence: 0.007187\n",
      "Epoch: 111, Loss: 1072.01471, Residuals: -1.33851, Convergence: 0.006473\n",
      "Epoch: 112, Loss: 1065.73415, Residuals: -1.33581, Convergence: 0.005893\n",
      "Epoch: 113, Loss: 1059.97536, Residuals: -1.33258, Convergence: 0.005433\n",
      "Epoch: 114, Loss: 1054.61412, Residuals: -1.32884, Convergence: 0.005084\n",
      "Epoch: 115, Loss: 1049.53343, Residuals: -1.32462, Convergence: 0.004841\n",
      "Epoch: 116, Loss: 1044.62184, Residuals: -1.31992, Convergence: 0.004702\n",
      "Epoch: 117, Loss: 1039.77190, Residuals: -1.31474, Convergence: 0.004664\n",
      "Epoch: 118, Loss: 1034.88935, Residuals: -1.30907, Convergence: 0.004718\n",
      "Epoch: 119, Loss: 1029.91081, Residuals: -1.30293, Convergence: 0.004834\n",
      "Epoch: 120, Loss: 1024.82645, Residuals: -1.29636, Convergence: 0.004961\n",
      "Epoch: 121, Loss: 1019.69071, Residuals: -1.28942, Convergence: 0.005037\n",
      "Epoch: 122, Loss: 1014.60395, Residuals: -1.28219, Convergence: 0.005014\n",
      "Epoch: 123, Loss: 1009.67008, Residuals: -1.27472, Convergence: 0.004887\n",
      "Epoch: 124, Loss: 1004.96235, Residuals: -1.26709, Convergence: 0.004684\n",
      "Epoch: 125, Loss: 1000.51675, Residuals: -1.25936, Convergence: 0.004443\n",
      "Epoch: 126, Loss: 996.33815, Residuals: -1.25160, Convergence: 0.004194\n",
      "Epoch: 127, Loss: 992.41687, Residuals: -1.24387, Convergence: 0.003951\n",
      "Epoch: 128, Loss: 988.73446, Residuals: -1.23620, Convergence: 0.003724\n",
      "Epoch: 129, Loss: 985.27062, Residuals: -1.22863, Convergence: 0.003516\n",
      "Epoch: 130, Loss: 982.00607, Residuals: -1.22120, Convergence: 0.003324\n",
      "Epoch: 131, Loss: 978.92254, Residuals: -1.21393, Convergence: 0.003150\n",
      "Epoch: 132, Loss: 976.00414, Residuals: -1.20683, Convergence: 0.002990\n",
      "Epoch: 133, Loss: 973.23729, Residuals: -1.19994, Convergence: 0.002843\n",
      "Epoch: 134, Loss: 970.61028, Residuals: -1.19325, Convergence: 0.002707\n",
      "Epoch: 135, Loss: 968.11277, Residuals: -1.18678, Convergence: 0.002580\n",
      "Epoch: 136, Loss: 965.73664, Residuals: -1.18054, Convergence: 0.002460\n",
      "Epoch: 137, Loss: 963.47380, Residuals: -1.17454, Convergence: 0.002349\n",
      "Epoch: 138, Loss: 961.31774, Residuals: -1.16877, Convergence: 0.002243\n",
      "Epoch: 139, Loss: 959.26176, Residuals: -1.16324, Convergence: 0.002143\n",
      "Epoch: 140, Loss: 957.30062, Residuals: -1.15795, Convergence: 0.002049\n",
      "Epoch: 141, Loss: 955.42809, Residuals: -1.15289, Convergence: 0.001960\n",
      "Epoch: 142, Loss: 953.63906, Residuals: -1.14805, Convergence: 0.001876\n",
      "Epoch: 143, Loss: 951.92824, Residuals: -1.14344, Convergence: 0.001797\n",
      "Epoch: 144, Loss: 950.29019, Residuals: -1.13903, Convergence: 0.001724\n",
      "Epoch: 145, Loss: 948.72043, Residuals: -1.13483, Convergence: 0.001655\n",
      "Epoch: 146, Loss: 947.21419, Residuals: -1.13082, Convergence: 0.001590\n",
      "Epoch: 147, Loss: 945.76705, Residuals: -1.12699, Convergence: 0.001530\n",
      "Epoch: 148, Loss: 944.37486, Residuals: -1.12333, Convergence: 0.001474\n",
      "Epoch: 149, Loss: 943.03364, Residuals: -1.11985, Convergence: 0.001422\n",
      "Epoch: 150, Loss: 941.73953, Residuals: -1.11651, Convergence: 0.001374\n",
      "Epoch: 151, Loss: 940.48908, Residuals: -1.11332, Convergence: 0.001330\n",
      "Epoch: 152, Loss: 939.27946, Residuals: -1.11026, Convergence: 0.001288\n",
      "Epoch: 153, Loss: 938.10701, Residuals: -1.10733, Convergence: 0.001250\n",
      "Epoch: 154, Loss: 936.96937, Residuals: -1.10452, Convergence: 0.001214\n",
      "Epoch: 155, Loss: 935.86306, Residuals: -1.10182, Convergence: 0.001182\n",
      "Epoch: 156, Loss: 934.78591, Residuals: -1.09922, Convergence: 0.001152\n",
      "Epoch: 157, Loss: 933.73440, Residuals: -1.09671, Convergence: 0.001126\n",
      "Epoch: 158, Loss: 932.70578, Residuals: -1.09428, Convergence: 0.001103\n",
      "Epoch: 159, Loss: 931.69730, Residuals: -1.09193, Convergence: 0.001082\n",
      "Epoch: 160, Loss: 930.70588, Residuals: -1.08965, Convergence: 0.001065\n",
      "Epoch: 161, Loss: 929.72748, Residuals: -1.08742, Convergence: 0.001052\n",
      "Epoch: 162, Loss: 928.75901, Residuals: -1.08525, Convergence: 0.001043\n",
      "Epoch: 163, Loss: 927.79649, Residuals: -1.08311, Convergence: 0.001037\n",
      "Epoch: 164, Loss: 926.83694, Residuals: -1.08099, Convergence: 0.001035\n",
      "Epoch: 165, Loss: 925.87728, Residuals: -1.07890, Convergence: 0.001036\n",
      "Epoch: 166, Loss: 924.91546, Residuals: -1.07682, Convergence: 0.001040\n",
      "Epoch: 167, Loss: 923.95087, Residuals: -1.07475, Convergence: 0.001044\n",
      "Epoch: 168, Loss: 922.98442, Residuals: -1.07269, Convergence: 0.001047\n",
      "Epoch: 169, Loss: 922.01902, Residuals: -1.07063, Convergence: 0.001047\n",
      "Epoch: 170, Loss: 921.05890, Residuals: -1.06860, Convergence: 0.001042\n",
      "Epoch: 171, Loss: 920.10924, Residuals: -1.06658, Convergence: 0.001032\n",
      "Epoch: 172, Loss: 919.17573, Residuals: -1.06460, Convergence: 0.001016\n",
      "Epoch: 173, Loss: 918.26343, Residuals: -1.06266, Convergence: 0.000993\n",
      "Evidence 11039.424\n",
      "\n",
      "Epoch: 173, Evidence: 11039.42383, Convergence: 1.016428\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 5.74e-01\n",
      "Epoch: 173, Loss: 2318.92751, Residuals: -1.06266, Convergence:   inf\n",
      "Epoch: 174, Loss: 2279.13422, Residuals: -1.07237, Convergence: 0.017460\n",
      "Epoch: 175, Loss: 2251.74808, Residuals: -1.07131, Convergence: 0.012162\n",
      "Epoch: 176, Loss: 2229.05171, Residuals: -1.06939, Convergence: 0.010182\n",
      "Epoch: 177, Loss: 2209.98617, Residuals: -1.06724, Convergence: 0.008627\n",
      "Epoch: 178, Loss: 2193.83897, Residuals: -1.06498, Convergence: 0.007360\n",
      "Epoch: 179, Loss: 2180.05692, Residuals: -1.06264, Convergence: 0.006322\n",
      "Epoch: 180, Loss: 2168.18719, Residuals: -1.06026, Convergence: 0.005474\n",
      "Epoch: 181, Loss: 2157.85289, Residuals: -1.05783, Convergence: 0.004789\n",
      "Epoch: 182, Loss: 2148.73762, Residuals: -1.05535, Convergence: 0.004242\n",
      "Epoch: 183, Loss: 2140.57468, Residuals: -1.05278, Convergence: 0.003813\n",
      "Epoch: 184, Loss: 2133.15150, Residuals: -1.05010, Convergence: 0.003480\n",
      "Epoch: 185, Loss: 2126.30867, Residuals: -1.04729, Convergence: 0.003218\n",
      "Epoch: 186, Loss: 2119.94494, Residuals: -1.04434, Convergence: 0.003002\n",
      "Epoch: 187, Loss: 2114.00776, Residuals: -1.04128, Convergence: 0.002808\n",
      "Epoch: 188, Loss: 2108.47133, Residuals: -1.03816, Convergence: 0.002626\n",
      "Epoch: 189, Loss: 2103.32005, Residuals: -1.03502, Convergence: 0.002449\n",
      "Epoch: 190, Loss: 2098.53479, Residuals: -1.03191, Convergence: 0.002280\n",
      "Epoch: 191, Loss: 2094.09273, Residuals: -1.02887, Convergence: 0.002121\n",
      "Epoch: 192, Loss: 2089.96823, Residuals: -1.02591, Convergence: 0.001973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 193, Loss: 2086.13499, Residuals: -1.02306, Convergence: 0.001837\n",
      "Epoch: 194, Loss: 2082.56771, Residuals: -1.02033, Convergence: 0.001713\n",
      "Epoch: 195, Loss: 2079.24407, Residuals: -1.01773, Convergence: 0.001598\n",
      "Epoch: 196, Loss: 2076.14391, Residuals: -1.01526, Convergence: 0.001493\n",
      "Epoch: 197, Loss: 2073.24859, Residuals: -1.01291, Convergence: 0.001397\n",
      "Epoch: 198, Loss: 2070.54261, Residuals: -1.01070, Convergence: 0.001307\n",
      "Epoch: 199, Loss: 2068.01139, Residuals: -1.00861, Convergence: 0.001224\n",
      "Epoch: 200, Loss: 2065.64172, Residuals: -1.00663, Convergence: 0.001147\n",
      "Epoch: 201, Loss: 2063.42203, Residuals: -1.00478, Convergence: 0.001076\n",
      "Epoch: 202, Loss: 2061.34078, Residuals: -1.00303, Convergence: 0.001010\n",
      "Epoch: 203, Loss: 2059.38858, Residuals: -1.00137, Convergence: 0.000948\n",
      "Evidence 14269.122\n",
      "\n",
      "Epoch: 203, Evidence: 14269.12207, Convergence: 0.226342\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 4.36e-01\n",
      "Epoch: 203, Loss: 2449.53361, Residuals: -1.00137, Convergence:   inf\n",
      "Epoch: 204, Loss: 2435.88938, Residuals: -0.99916, Convergence: 0.005601\n",
      "Epoch: 205, Loss: 2424.74891, Residuals: -0.99642, Convergence: 0.004594\n",
      "Epoch: 206, Loss: 2415.14897, Residuals: -0.99372, Convergence: 0.003975\n",
      "Epoch: 207, Loss: 2406.81588, Residuals: -0.99114, Convergence: 0.003462\n",
      "Epoch: 208, Loss: 2399.54204, Residuals: -0.98874, Convergence: 0.003031\n",
      "Epoch: 209, Loss: 2393.16217, Residuals: -0.98651, Convergence: 0.002666\n",
      "Epoch: 210, Loss: 2387.54065, Residuals: -0.98446, Convergence: 0.002355\n",
      "Epoch: 211, Loss: 2382.56360, Residuals: -0.98258, Convergence: 0.002089\n",
      "Epoch: 212, Loss: 2378.13638, Residuals: -0.98086, Convergence: 0.001862\n",
      "Epoch: 213, Loss: 2374.17840, Residuals: -0.97929, Convergence: 0.001667\n",
      "Epoch: 214, Loss: 2370.62198, Residuals: -0.97785, Convergence: 0.001500\n",
      "Epoch: 215, Loss: 2367.40980, Residuals: -0.97654, Convergence: 0.001357\n",
      "Epoch: 216, Loss: 2364.49410, Residuals: -0.97534, Convergence: 0.001233\n",
      "Epoch: 217, Loss: 2361.83365, Residuals: -0.97425, Convergence: 0.001126\n",
      "Epoch: 218, Loss: 2359.39522, Residuals: -0.97326, Convergence: 0.001033\n",
      "Epoch: 219, Loss: 2357.15040, Residuals: -0.97237, Convergence: 0.000952\n",
      "Evidence 14666.295\n",
      "\n",
      "Epoch: 219, Evidence: 14666.29492, Convergence: 0.027081\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 3.34e-01\n",
      "Epoch: 219, Loss: 2454.49850, Residuals: -0.97237, Convergence:   inf\n",
      "Epoch: 220, Loss: 2448.00866, Residuals: -0.96992, Convergence: 0.002651\n",
      "Epoch: 221, Loss: 2442.59192, Residuals: -0.96780, Convergence: 0.002218\n",
      "Epoch: 222, Loss: 2437.97899, Residuals: -0.96600, Convergence: 0.001892\n",
      "Epoch: 223, Loss: 2434.00593, Residuals: -0.96449, Convergence: 0.001632\n",
      "Epoch: 224, Loss: 2430.54533, Residuals: -0.96323, Convergence: 0.001424\n",
      "Epoch: 225, Loss: 2427.49671, Residuals: -0.96219, Convergence: 0.001256\n",
      "Epoch: 226, Loss: 2424.78147, Residuals: -0.96134, Convergence: 0.001120\n",
      "Epoch: 227, Loss: 2422.33858, Residuals: -0.96064, Convergence: 0.001008\n",
      "Epoch: 228, Loss: 2420.11907, Residuals: -0.96008, Convergence: 0.000917\n",
      "Evidence 14752.225\n",
      "\n",
      "Epoch: 228, Evidence: 14752.22461, Convergence: 0.005825\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 2.62e-01\n",
      "Epoch: 228, Loss: 2455.91713, Residuals: -0.96008, Convergence:   inf\n",
      "Epoch: 229, Loss: 2452.07454, Residuals: -0.95838, Convergence: 0.001567\n",
      "Epoch: 230, Loss: 2448.84934, Residuals: -0.95708, Convergence: 0.001317\n",
      "Epoch: 231, Loss: 2446.07828, Residuals: -0.95610, Convergence: 0.001133\n",
      "Epoch: 232, Loss: 2443.65549, Residuals: -0.95537, Convergence: 0.000991\n",
      "Evidence 14781.556\n",
      "\n",
      "Epoch: 232, Evidence: 14781.55566, Convergence: 0.001984\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 2.12e-01\n",
      "Epoch: 232, Loss: 2456.96015, Residuals: -0.95537, Convergence:   inf\n",
      "Epoch: 233, Loss: 2454.05088, Residuals: -0.95410, Convergence: 0.001185\n",
      "Epoch: 234, Loss: 2451.59544, Residuals: -0.95320, Convergence: 0.001002\n",
      "Epoch: 235, Loss: 2449.46399, Residuals: -0.95257, Convergence: 0.000870\n",
      "Evidence 14796.218\n",
      "\n",
      "Epoch: 235, Evidence: 14796.21777, Convergence: 0.000991\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.76e-01\n",
      "Epoch: 235, Loss: 2457.66563, Residuals: -0.95257, Convergence:   inf\n",
      "Epoch: 236, Loss: 2453.29120, Residuals: -0.95127, Convergence: 0.001783\n",
      "Epoch: 237, Loss: 2449.93136, Residuals: -0.95069, Convergence: 0.001371\n",
      "Epoch: 238, Loss: 2447.18295, Residuals: -0.95052, Convergence: 0.001123\n",
      "Epoch: 239, Loss: 2444.83337, Residuals: -0.95064, Convergence: 0.000961\n",
      "Evidence 14814.175\n",
      "\n",
      "Epoch: 239, Evidence: 14814.17480, Convergence: 0.002202\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.46e-01\n",
      "Epoch: 239, Loss: 2457.74139, Residuals: -0.95064, Convergence:   inf\n",
      "Epoch: 240, Loss: 2454.79317, Residuals: -0.94961, Convergence: 0.001201\n",
      "Epoch: 241, Loss: 2452.43085, Residuals: -0.94947, Convergence: 0.000963\n",
      "Evidence 14825.036\n",
      "\n",
      "Epoch: 241, Evidence: 14825.03613, Convergence: 0.000733\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.24e-01\n",
      "Epoch: 241, Loss: 2457.96269, Residuals: -0.94947, Convergence:   inf\n",
      "Epoch: 242, Loss: 2453.58194, Residuals: -0.94881, Convergence: 0.001785\n",
      "Epoch: 243, Loss: 2450.36138, Residuals: -0.95103, Convergence: 0.001314\n",
      "Epoch: 244, Loss: 2447.69306, Residuals: -0.95192, Convergence: 0.001090\n",
      "Epoch: 245, Loss: 2445.32777, Residuals: -0.95459, Convergence: 0.000967\n",
      "Evidence 14840.844\n",
      "\n",
      "Epoch: 245, Evidence: 14840.84375, Convergence: 0.001797\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.10e-01\n",
      "Epoch: 245, Loss: 2457.22788, Residuals: -0.95459, Convergence:   inf\n",
      "Epoch: 246, Loss: 2455.20709, Residuals: -0.95517, Convergence: 0.000823\n",
      "Evidence 14847.621\n",
      "\n",
      "Epoch: 246, Evidence: 14847.62109, Convergence: 0.000456\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 9.05e-02\n",
      "Epoch: 246, Loss: 2458.15338, Residuals: -0.95517, Convergence:   inf\n",
      "Epoch: 247, Loss: 2501.67982, Residuals: -0.99872, Convergence: -0.017399\n",
      "Epoch: 247, Loss: 2455.67162, Residuals: -0.95634, Convergence: 0.001011\n",
      "Epoch: 248, Loss: 2455.19033, Residuals: -0.95794, Convergence: 0.000196\n",
      "Evidence 14852.843\n",
      "\n",
      "Epoch: 248, Evidence: 14852.84277, Convergence: 0.000808\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 7.54e-02\n",
      "Epoch: 248, Loss: 2458.07064, Residuals: -0.95794, Convergence:   inf\n",
      "Epoch: 249, Loss: 2513.24221, Residuals: -1.00854, Convergence: -0.021952\n",
      "Epoch: 249, Loss: 2455.55339, Residuals: -0.95843, Convergence: 0.001025\n",
      "Epoch: 250, Loss: 2455.02826, Residuals: -0.95976, Convergence: 0.000214\n",
      "Evidence 14858.017\n",
      "\n",
      "Epoch: 250, Evidence: 14858.01660, Convergence: 0.001156\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 6.44e-02\n",
      "Epoch: 250, Loss: 2457.85844, Residuals: -0.95976, Convergence:   inf\n",
      "Epoch: 251, Loss: 2455.84446, Residuals: -0.96176, Convergence: 0.000820\n",
      "Evidence 14861.835\n",
      "\n",
      "Epoch: 251, Evidence: 14861.83496, Convergence: 0.000257\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 6.07e-02\n",
      "Epoch: 251, Loss: 2457.57343, Residuals: -0.96176, Convergence:   inf\n",
      "Epoch: 252, Loss: 2463.98550, Residuals: -0.98056, Convergence: -0.002602\n",
      "Epoch: 252, Loss: 2457.64284, Residuals: -0.96418, Convergence: -0.000028\n",
      "Evidence 14862.973\n",
      "\n",
      "Epoch: 252, Evidence: 14862.97266, Convergence: 0.000333\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 5.24e-02\n",
      "Epoch: 252, Loss: 2458.02251, Residuals: -0.96418, Convergence:   inf\n",
      "Epoch: 253, Loss: 2519.59371, Residuals: -1.03156, Convergence: -0.024437\n",
      "Epoch: 253, Loss: 2456.03295, Residuals: -0.96493, Convergence: 0.000810\n",
      "Evidence 14866.123\n",
      "\n",
      "Epoch: 253, Evidence: 14866.12305, Convergence: 0.000545\n",
      "Total samples: 184, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 386.01321, Residuals: -4.53787, Convergence:   inf\n",
      "Epoch: 1, Loss: 360.12526, Residuals: -4.41662, Convergence: 0.071886\n",
      "Epoch: 2, Loss: 338.88336, Residuals: -4.25082, Convergence: 0.062682\n",
      "Epoch: 3, Loss: 322.65081, Residuals: -4.08449, Convergence: 0.050310\n",
      "Epoch: 4, Loss: 310.26944, Residuals: -3.93797, Convergence: 0.039905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Loss: 300.44529, Residuals: -3.80842, Convergence: 0.032699\n",
      "Epoch: 6, Loss: 292.46741, Residuals: -3.69569, Convergence: 0.027278\n",
      "Epoch: 7, Loss: 285.84074, Residuals: -3.59924, Convergence: 0.023183\n",
      "Epoch: 8, Loss: 280.19892, Residuals: -3.51690, Convergence: 0.020135\n",
      "Epoch: 9, Loss: 275.28374, Residuals: -3.44626, Convergence: 0.017855\n",
      "Epoch: 10, Loss: 270.91142, Residuals: -3.38520, Convergence: 0.016139\n",
      "Epoch: 11, Loss: 266.94868, Residuals: -3.33190, Convergence: 0.014845\n",
      "Epoch: 12, Loss: 263.29843, Residuals: -3.28483, Convergence: 0.013864\n",
      "Epoch: 13, Loss: 259.89054, Residuals: -3.24267, Convergence: 0.013113\n",
      "Epoch: 14, Loss: 256.67553, Residuals: -3.20425, Convergence: 0.012526\n",
      "Epoch: 15, Loss: 253.62136, Residuals: -3.16860, Convergence: 0.012042\n",
      "Epoch: 16, Loss: 250.71157, Residuals: -3.13507, Convergence: 0.011606\n",
      "Epoch: 17, Loss: 247.93598, Residuals: -3.10327, Convergence: 0.011195\n",
      "Epoch: 18, Loss: 245.27577, Residuals: -3.07284, Convergence: 0.010846\n",
      "Epoch: 19, Loss: 242.69946, Residuals: -3.04330, Convergence: 0.010615\n",
      "Epoch: 20, Loss: 240.17063, Residuals: -3.01408, Convergence: 0.010529\n",
      "Epoch: 21, Loss: 237.65779, Residuals: -2.98469, Convergence: 0.010573\n",
      "Epoch: 22, Loss: 235.13908, Residuals: -2.95483, Convergence: 0.010712\n",
      "Epoch: 23, Loss: 232.59306, Residuals: -2.92424, Convergence: 0.010946\n",
      "Epoch: 24, Loss: 229.98091, Residuals: -2.89253, Convergence: 0.011358\n",
      "Epoch: 25, Loss: 227.24484, Residuals: -2.85902, Convergence: 0.012040\n",
      "Epoch: 26, Loss: 224.35655, Residuals: -2.82331, Convergence: 0.012874\n",
      "Epoch: 27, Loss: 221.40400, Residuals: -2.78623, Convergence: 0.013336\n",
      "Epoch: 28, Loss: 218.51392, Residuals: -2.74918, Convergence: 0.013226\n",
      "Epoch: 29, Loss: 215.72413, Residuals: -2.71271, Convergence: 0.012932\n",
      "Epoch: 30, Loss: 213.02396, Residuals: -2.67681, Convergence: 0.012675\n",
      "Epoch: 31, Loss: 210.39712, Residuals: -2.64135, Convergence: 0.012485\n",
      "Epoch: 32, Loss: 207.83173, Residuals: -2.60620, Convergence: 0.012344\n",
      "Epoch: 33, Loss: 205.32076, Residuals: -2.57130, Convergence: 0.012229\n",
      "Epoch: 34, Loss: 202.86095, Residuals: -2.53656, Convergence: 0.012126\n",
      "Epoch: 35, Loss: 200.45149, Residuals: -2.50197, Convergence: 0.012020\n",
      "Epoch: 36, Loss: 198.09316, Residuals: -2.46750, Convergence: 0.011905\n",
      "Epoch: 37, Loss: 195.78752, Residuals: -2.43314, Convergence: 0.011776\n",
      "Epoch: 38, Loss: 193.53652, Residuals: -2.39892, Convergence: 0.011631\n",
      "Epoch: 39, Loss: 191.34208, Residuals: -2.36483, Convergence: 0.011469\n",
      "Epoch: 40, Loss: 189.20606, Residuals: -2.33091, Convergence: 0.011289\n",
      "Epoch: 41, Loss: 187.13010, Residuals: -2.29718, Convergence: 0.011094\n",
      "Epoch: 42, Loss: 185.11572, Residuals: -2.26368, Convergence: 0.010882\n",
      "Epoch: 43, Loss: 183.16435, Residuals: -2.23044, Convergence: 0.010654\n",
      "Epoch: 44, Loss: 181.27738, Residuals: -2.19752, Convergence: 0.010409\n",
      "Epoch: 45, Loss: 179.45612, Residuals: -2.16497, Convergence: 0.010149\n",
      "Epoch: 46, Loss: 177.70177, Residuals: -2.13284, Convergence: 0.009872\n",
      "Epoch: 47, Loss: 176.01510, Residuals: -2.10118, Convergence: 0.009583\n",
      "Epoch: 48, Loss: 174.39641, Residuals: -2.07005, Convergence: 0.009282\n",
      "Epoch: 49, Loss: 172.84530, Residuals: -2.03948, Convergence: 0.008974\n",
      "Epoch: 50, Loss: 171.36073, Residuals: -2.00949, Convergence: 0.008663\n",
      "Epoch: 51, Loss: 169.94110, Residuals: -1.98012, Convergence: 0.008354\n",
      "Epoch: 52, Loss: 168.58434, Residuals: -1.95138, Convergence: 0.008048\n",
      "Epoch: 53, Loss: 167.28807, Residuals: -1.92327, Convergence: 0.007749\n",
      "Epoch: 54, Loss: 166.04964, Residuals: -1.89579, Convergence: 0.007458\n",
      "Epoch: 55, Loss: 164.86625, Residuals: -1.86897, Convergence: 0.007178\n",
      "Epoch: 56, Loss: 163.73502, Residuals: -1.84278, Convergence: 0.006909\n",
      "Epoch: 57, Loss: 162.65303, Residuals: -1.81722, Convergence: 0.006652\n",
      "Epoch: 58, Loss: 161.61749, Residuals: -1.79230, Convergence: 0.006407\n",
      "Epoch: 59, Loss: 160.62573, Residuals: -1.76800, Convergence: 0.006174\n",
      "Epoch: 60, Loss: 159.67533, Residuals: -1.74431, Convergence: 0.005952\n",
      "Epoch: 61, Loss: 158.76414, Residuals: -1.72123, Convergence: 0.005739\n",
      "Epoch: 62, Loss: 157.89026, Residuals: -1.69875, Convergence: 0.005535\n",
      "Epoch: 63, Loss: 157.05205, Residuals: -1.67686, Convergence: 0.005337\n",
      "Epoch: 64, Loss: 156.24806, Residuals: -1.65557, Convergence: 0.005146\n",
      "Epoch: 65, Loss: 155.47705, Residuals: -1.63487, Convergence: 0.004959\n",
      "Epoch: 66, Loss: 154.73790, Residuals: -1.61476, Convergence: 0.004777\n",
      "Epoch: 67, Loss: 154.02959, Residuals: -1.59523, Convergence: 0.004599\n",
      "Epoch: 68, Loss: 153.35117, Residuals: -1.57630, Convergence: 0.004424\n",
      "Epoch: 69, Loss: 152.70177, Residuals: -1.55794, Convergence: 0.004253\n",
      "Epoch: 70, Loss: 152.08052, Residuals: -1.54017, Convergence: 0.004085\n",
      "Epoch: 71, Loss: 151.48658, Residuals: -1.52297, Convergence: 0.003921\n",
      "Epoch: 72, Loss: 150.91912, Residuals: -1.50635, Convergence: 0.003760\n",
      "Epoch: 73, Loss: 150.37728, Residuals: -1.49029, Convergence: 0.003603\n",
      "Epoch: 74, Loss: 149.86023, Residuals: -1.47479, Convergence: 0.003450\n",
      "Epoch: 75, Loss: 149.36710, Residuals: -1.45984, Convergence: 0.003301\n",
      "Epoch: 76, Loss: 148.89702, Residuals: -1.44543, Convergence: 0.003157\n",
      "Epoch: 77, Loss: 148.44910, Residuals: -1.43155, Convergence: 0.003017\n",
      "Epoch: 78, Loss: 148.02242, Residuals: -1.41819, Convergence: 0.002882\n",
      "Epoch: 79, Loss: 147.61610, Residuals: -1.40534, Convergence: 0.002753\n",
      "Epoch: 80, Loss: 147.22923, Residuals: -1.39298, Convergence: 0.002628\n",
      "Epoch: 81, Loss: 146.86088, Residuals: -1.38110, Convergence: 0.002508\n",
      "Epoch: 82, Loss: 146.51019, Residuals: -1.36969, Convergence: 0.002394\n",
      "Epoch: 83, Loss: 146.17627, Residuals: -1.35873, Convergence: 0.002284\n",
      "Epoch: 84, Loss: 145.85829, Residuals: -1.34821, Convergence: 0.002180\n",
      "Epoch: 85, Loss: 145.55543, Residuals: -1.33810, Convergence: 0.002081\n",
      "Epoch: 86, Loss: 145.26692, Residuals: -1.32841, Convergence: 0.001986\n",
      "Epoch: 87, Loss: 144.99201, Residuals: -1.31910, Convergence: 0.001896\n",
      "Epoch: 88, Loss: 144.73003, Residuals: -1.31017, Convergence: 0.001810\n",
      "Epoch: 89, Loss: 144.48031, Residuals: -1.30161, Convergence: 0.001728\n",
      "Epoch: 90, Loss: 144.24227, Residuals: -1.29339, Convergence: 0.001650\n",
      "Epoch: 91, Loss: 144.01535, Residuals: -1.28550, Convergence: 0.001576\n",
      "Epoch: 92, Loss: 143.79905, Residuals: -1.27794, Convergence: 0.001504\n",
      "Epoch: 93, Loss: 143.59288, Residuals: -1.27068, Convergence: 0.001436\n",
      "Epoch: 94, Loss: 143.39647, Residuals: -1.26372, Convergence: 0.001370\n",
      "Epoch: 95, Loss: 143.20939, Residuals: -1.25704, Convergence: 0.001306\n",
      "Epoch: 96, Loss: 143.03133, Residuals: -1.25063, Convergence: 0.001245\n",
      "Epoch: 97, Loss: 142.86198, Residuals: -1.24448, Convergence: 0.001185\n",
      "Epoch: 98, Loss: 142.70107, Residuals: -1.23858, Convergence: 0.001128\n",
      "Epoch: 99, Loss: 142.54836, Residuals: -1.23293, Convergence: 0.001071\n",
      "Epoch: 100, Loss: 142.40363, Residuals: -1.22751, Convergence: 0.001016\n",
      "Epoch: 101, Loss: 142.26669, Residuals: -1.22232, Convergence: 0.000963\n",
      "Evidence -184.210\n",
      "\n",
      "Epoch: 101, Evidence: -184.21043, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 7.25e-01\n",
      "Epoch: 101, Loss: 1389.31076, Residuals: -1.22232, Convergence:   inf\n",
      "Epoch: 102, Loss: 1326.60232, Residuals: -1.25321, Convergence: 0.047270\n",
      "Epoch: 103, Loss: 1278.86908, Residuals: -1.27760, Convergence: 0.037325\n",
      "Epoch: 104, Loss: 1242.82625, Residuals: -1.29532, Convergence: 0.029001\n",
      "Epoch: 105, Loss: 1214.84748, Residuals: -1.30796, Convergence: 0.023031\n",
      "Epoch: 106, Loss: 1192.28583, Residuals: -1.31744, Convergence: 0.018923\n",
      "Epoch: 107, Loss: 1173.60002, Residuals: -1.32480, Convergence: 0.015922\n",
      "Epoch: 108, Loss: 1157.85866, Residuals: -1.33052, Convergence: 0.013595\n",
      "Epoch: 109, Loss: 1144.42671, Residuals: -1.33480, Convergence: 0.011737\n",
      "Epoch: 110, Loss: 1132.83219, Residuals: -1.33782, Convergence: 0.010235\n",
      "Epoch: 111, Loss: 1122.70710, Residuals: -1.33966, Convergence: 0.009018\n",
      "Epoch: 112, Loss: 1113.75290, Residuals: -1.34045, Convergence: 0.008040\n",
      "Epoch: 113, Loss: 1105.72265, Residuals: -1.34025, Convergence: 0.007262\n",
      "Epoch: 114, Loss: 1098.40694, Residuals: -1.33912, Convergence: 0.006660\n",
      "Epoch: 115, Loss: 1091.62561, Residuals: -1.33711, Convergence: 0.006212\n",
      "Epoch: 116, Loss: 1085.21978, Residuals: -1.33425, Convergence: 0.005903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 117, Loss: 1079.05136, Residuals: -1.33056, Convergence: 0.005717\n",
      "Epoch: 118, Loss: 1073.00210, Residuals: -1.32607, Convergence: 0.005638\n",
      "Epoch: 119, Loss: 1066.98084, Residuals: -1.32080, Convergence: 0.005643\n",
      "Epoch: 120, Loss: 1060.93738, Residuals: -1.31482, Convergence: 0.005696\n",
      "Epoch: 121, Loss: 1054.87777, Residuals: -1.30823, Convergence: 0.005744\n",
      "Epoch: 122, Loss: 1048.86887, Residuals: -1.30114, Convergence: 0.005729\n",
      "Epoch: 123, Loss: 1043.01661, Residuals: -1.29365, Convergence: 0.005611\n",
      "Epoch: 124, Loss: 1037.42352, Residuals: -1.28588, Convergence: 0.005391\n",
      "Epoch: 125, Loss: 1032.15754, Residuals: -1.27791, Convergence: 0.005102\n",
      "Epoch: 126, Loss: 1027.24176, Residuals: -1.26983, Convergence: 0.004785\n",
      "Epoch: 127, Loss: 1022.66713, Residuals: -1.26170, Convergence: 0.004473\n",
      "Epoch: 128, Loss: 1018.40723, Residuals: -1.25359, Convergence: 0.004183\n",
      "Epoch: 129, Loss: 1014.42939, Residuals: -1.24554, Convergence: 0.003921\n",
      "Epoch: 130, Loss: 1010.70150, Residuals: -1.23760, Convergence: 0.003688\n",
      "Epoch: 131, Loss: 1007.19426, Residuals: -1.22979, Convergence: 0.003482\n",
      "Epoch: 132, Loss: 1003.88267, Residuals: -1.22215, Convergence: 0.003299\n",
      "Epoch: 133, Loss: 1000.74520, Residuals: -1.21469, Convergence: 0.003135\n",
      "Epoch: 134, Loss: 997.76513, Residuals: -1.20743, Convergence: 0.002987\n",
      "Epoch: 135, Loss: 994.92786, Residuals: -1.20038, Convergence: 0.002852\n",
      "Epoch: 136, Loss: 992.22214, Residuals: -1.19356, Convergence: 0.002727\n",
      "Epoch: 137, Loss: 989.63821, Residuals: -1.18697, Convergence: 0.002611\n",
      "Epoch: 138, Loss: 987.16838, Residuals: -1.18062, Convergence: 0.002502\n",
      "Epoch: 139, Loss: 984.80621, Residuals: -1.17451, Convergence: 0.002399\n",
      "Epoch: 140, Loss: 982.54545, Residuals: -1.16863, Convergence: 0.002301\n",
      "Epoch: 141, Loss: 980.38148, Residuals: -1.16300, Convergence: 0.002207\n",
      "Epoch: 142, Loss: 978.30905, Residuals: -1.15759, Convergence: 0.002118\n",
      "Epoch: 143, Loss: 976.32456, Residuals: -1.15242, Convergence: 0.002033\n",
      "Epoch: 144, Loss: 974.42313, Residuals: -1.14746, Convergence: 0.001951\n",
      "Epoch: 145, Loss: 972.60128, Residuals: -1.14273, Convergence: 0.001873\n",
      "Epoch: 146, Loss: 970.85479, Residuals: -1.13819, Convergence: 0.001799\n",
      "Epoch: 147, Loss: 969.17980, Residuals: -1.13386, Convergence: 0.001728\n",
      "Epoch: 148, Loss: 967.57189, Residuals: -1.12971, Convergence: 0.001662\n",
      "Epoch: 149, Loss: 966.02709, Residuals: -1.12575, Convergence: 0.001599\n",
      "Epoch: 150, Loss: 964.54059, Residuals: -1.12195, Convergence: 0.001541\n",
      "Epoch: 151, Loss: 963.10773, Residuals: -1.11831, Convergence: 0.001488\n",
      "Epoch: 152, Loss: 961.72382, Residuals: -1.11481, Convergence: 0.001439\n",
      "Epoch: 153, Loss: 960.38357, Residuals: -1.11145, Convergence: 0.001396\n",
      "Epoch: 154, Loss: 959.08122, Residuals: -1.10822, Convergence: 0.001358\n",
      "Epoch: 155, Loss: 957.81160, Residuals: -1.10509, Convergence: 0.001326\n",
      "Epoch: 156, Loss: 956.56873, Residuals: -1.10206, Convergence: 0.001299\n",
      "Epoch: 157, Loss: 955.34723, Residuals: -1.09912, Convergence: 0.001279\n",
      "Epoch: 158, Loss: 954.14166, Residuals: -1.09624, Convergence: 0.001264\n",
      "Epoch: 159, Loss: 952.94775, Residuals: -1.09342, Convergence: 0.001253\n",
      "Epoch: 160, Loss: 951.76263, Residuals: -1.09065, Convergence: 0.001245\n",
      "Epoch: 161, Loss: 950.58446, Residuals: -1.08792, Convergence: 0.001239\n",
      "Epoch: 162, Loss: 949.41491, Residuals: -1.08523, Convergence: 0.001232\n",
      "Epoch: 163, Loss: 948.25628, Residuals: -1.08258, Convergence: 0.001222\n",
      "Epoch: 164, Loss: 947.11371, Residuals: -1.07998, Convergence: 0.001206\n",
      "Epoch: 165, Loss: 945.99240, Residuals: -1.07743, Convergence: 0.001185\n",
      "Epoch: 166, Loss: 944.89842, Residuals: -1.07495, Convergence: 0.001158\n",
      "Epoch: 167, Loss: 943.83626, Residuals: -1.07254, Convergence: 0.001125\n",
      "Epoch: 168, Loss: 942.81031, Residuals: -1.07021, Convergence: 0.001088\n",
      "Epoch: 169, Loss: 941.82335, Residuals: -1.06796, Convergence: 0.001048\n",
      "Epoch: 170, Loss: 940.87682, Residuals: -1.06580, Convergence: 0.001006\n",
      "Epoch: 171, Loss: 939.97160, Residuals: -1.06373, Convergence: 0.000963\n",
      "Evidence 11313.072\n",
      "\n",
      "Epoch: 171, Evidence: 11313.07227, Convergence: 1.016283\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 5.74e-01\n",
      "Epoch: 171, Loss: 2375.83782, Residuals: -1.06373, Convergence:   inf\n",
      "Epoch: 172, Loss: 2338.97738, Residuals: -1.07214, Convergence: 0.015759\n",
      "Epoch: 173, Loss: 2312.30940, Residuals: -1.07013, Convergence: 0.011533\n",
      "Epoch: 174, Loss: 2290.15005, Residuals: -1.06738, Convergence: 0.009676\n",
      "Epoch: 175, Loss: 2271.55167, Residuals: -1.06447, Convergence: 0.008188\n",
      "Epoch: 176, Loss: 2255.80155, Residuals: -1.06151, Convergence: 0.006982\n",
      "Epoch: 177, Loss: 2242.34265, Residuals: -1.05853, Convergence: 0.006002\n",
      "Epoch: 178, Loss: 2230.72152, Residuals: -1.05552, Convergence: 0.005210\n",
      "Epoch: 179, Loss: 2220.56303, Residuals: -1.05249, Convergence: 0.004575\n",
      "Epoch: 180, Loss: 2211.55670, Residuals: -1.04941, Convergence: 0.004072\n",
      "Epoch: 181, Loss: 2203.45493, Residuals: -1.04626, Convergence: 0.003677\n",
      "Epoch: 182, Loss: 2196.07160, Residuals: -1.04301, Convergence: 0.003362\n",
      "Epoch: 183, Loss: 2189.28212, Residuals: -1.03967, Convergence: 0.003101\n",
      "Epoch: 184, Loss: 2183.01053, Residuals: -1.03626, Convergence: 0.002873\n",
      "Epoch: 185, Loss: 2177.21398, Residuals: -1.03282, Convergence: 0.002662\n",
      "Epoch: 186, Loss: 2171.85997, Residuals: -1.02939, Convergence: 0.002465\n",
      "Epoch: 187, Loss: 2166.91809, Residuals: -1.02601, Convergence: 0.002281\n",
      "Epoch: 188, Loss: 2162.35468, Residuals: -1.02272, Convergence: 0.002110\n",
      "Epoch: 189, Loss: 2158.13646, Residuals: -1.01953, Convergence: 0.001955\n",
      "Epoch: 190, Loss: 2154.22914, Residuals: -1.01645, Convergence: 0.001814\n",
      "Epoch: 191, Loss: 2150.60066, Residuals: -1.01350, Convergence: 0.001687\n",
      "Epoch: 192, Loss: 2147.22196, Residuals: -1.01068, Convergence: 0.001574\n",
      "Epoch: 193, Loss: 2144.06618, Residuals: -1.00798, Convergence: 0.001472\n",
      "Epoch: 194, Loss: 2141.11041, Residuals: -1.00542, Convergence: 0.001380\n",
      "Epoch: 195, Loss: 2138.33330, Residuals: -1.00297, Convergence: 0.001299\n",
      "Epoch: 196, Loss: 2135.71755, Residuals: -1.00065, Convergence: 0.001225\n",
      "Epoch: 197, Loss: 2133.24806, Residuals: -0.99844, Convergence: 0.001158\n",
      "Epoch: 198, Loss: 2130.91140, Residuals: -0.99635, Convergence: 0.001097\n",
      "Epoch: 199, Loss: 2128.69694, Residuals: -0.99437, Convergence: 0.001040\n",
      "Epoch: 200, Loss: 2126.59636, Residuals: -0.99250, Convergence: 0.000988\n",
      "Evidence 14468.804\n",
      "\n",
      "Epoch: 200, Evidence: 14468.80371, Convergence: 0.218106\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 4.35e-01\n",
      "Epoch: 200, Loss: 2501.50304, Residuals: -0.99250, Convergence:   inf\n",
      "Epoch: 201, Loss: 2488.04351, Residuals: -0.98900, Convergence: 0.005410\n",
      "Epoch: 202, Loss: 2476.98829, Residuals: -0.98501, Convergence: 0.004463\n",
      "Epoch: 203, Loss: 2467.44642, Residuals: -0.98124, Convergence: 0.003867\n",
      "Epoch: 204, Loss: 2459.16044, Residuals: -0.97775, Convergence: 0.003369\n",
      "Epoch: 205, Loss: 2451.92812, Residuals: -0.97457, Convergence: 0.002950\n",
      "Epoch: 206, Loss: 2445.58750, Residuals: -0.97168, Convergence: 0.002593\n",
      "Epoch: 207, Loss: 2440.00237, Residuals: -0.96907, Convergence: 0.002289\n",
      "Epoch: 208, Loss: 2435.05776, Residuals: -0.96672, Convergence: 0.002031\n",
      "Epoch: 209, Loss: 2430.65725, Residuals: -0.96460, Convergence: 0.001810\n",
      "Epoch: 210, Loss: 2426.72074, Residuals: -0.96268, Convergence: 0.001622\n",
      "Epoch: 211, Loss: 2423.17939, Residuals: -0.96095, Convergence: 0.001461\n",
      "Epoch: 212, Loss: 2419.97694, Residuals: -0.95939, Convergence: 0.001323\n",
      "Epoch: 213, Loss: 2417.06621, Residuals: -0.95797, Convergence: 0.001204\n",
      "Epoch: 214, Loss: 2414.40883, Residuals: -0.95669, Convergence: 0.001101\n",
      "Epoch: 215, Loss: 2411.97002, Residuals: -0.95552, Convergence: 0.001011\n",
      "Epoch: 216, Loss: 2409.72351, Residuals: -0.95447, Convergence: 0.000932\n",
      "Evidence 14839.079\n",
      "\n",
      "Epoch: 216, Evidence: 14839.07910, Convergence: 0.024953\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 3.31e-01\n",
      "Epoch: 216, Loss: 2506.40681, Residuals: -0.95447, Convergence:   inf\n",
      "Epoch: 217, Loss: 2499.86303, Residuals: -0.95113, Convergence: 0.002618\n",
      "Epoch: 218, Loss: 2494.45776, Residuals: -0.94831, Convergence: 0.002167\n",
      "Epoch: 219, Loss: 2489.88478, Residuals: -0.94597, Convergence: 0.001837\n",
      "Epoch: 220, Loss: 2485.96151, Residuals: -0.94404, Convergence: 0.001578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 221, Loss: 2482.55220, Residuals: -0.94245, Convergence: 0.001373\n",
      "Epoch: 222, Loss: 2479.55175, Residuals: -0.94113, Convergence: 0.001210\n",
      "Epoch: 223, Loss: 2476.88015, Residuals: -0.94005, Convergence: 0.001079\n",
      "Epoch: 224, Loss: 2474.47718, Residuals: -0.93915, Convergence: 0.000971\n",
      "Evidence 14921.724\n",
      "\n",
      "Epoch: 224, Evidence: 14921.72363, Convergence: 0.005539\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 2.58e-01\n",
      "Epoch: 224, Loss: 2508.18033, Residuals: -0.93915, Convergence:   inf\n",
      "Epoch: 225, Loss: 2504.21126, Residuals: -0.93676, Convergence: 0.001585\n",
      "Epoch: 226, Loss: 2500.92529, Residuals: -0.93493, Convergence: 0.001314\n",
      "Epoch: 227, Loss: 2498.12399, Residuals: -0.93352, Convergence: 0.001121\n",
      "Epoch: 228, Loss: 2495.68647, Residuals: -0.93243, Convergence: 0.000977\n",
      "Evidence 14950.083\n",
      "\n",
      "Epoch: 228, Evidence: 14950.08301, Convergence: 0.001897\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 2.08e-01\n",
      "Epoch: 228, Loss: 2509.25692, Residuals: -0.93243, Convergence:   inf\n",
      "Epoch: 229, Loss: 2506.30719, Residuals: -0.93060, Convergence: 0.001177\n",
      "Epoch: 230, Loss: 2503.84976, Residuals: -0.92924, Convergence: 0.000981\n",
      "Evidence 14962.426\n",
      "\n",
      "Epoch: 230, Evidence: 14962.42578, Convergence: 0.000825\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.72e-01\n",
      "Epoch: 230, Loss: 2510.03635, Residuals: -0.92924, Convergence:   inf\n",
      "Epoch: 231, Loss: 2505.43293, Residuals: -0.92689, Convergence: 0.001837\n",
      "Epoch: 232, Loss: 2501.95958, Residuals: -0.92533, Convergence: 0.001388\n",
      "Epoch: 233, Loss: 2499.16628, Residuals: -0.92432, Convergence: 0.001118\n",
      "Epoch: 234, Loss: 2496.80563, Residuals: -0.92379, Convergence: 0.000945\n",
      "Evidence 14980.204\n",
      "\n",
      "Epoch: 234, Evidence: 14980.20410, Convergence: 0.002011\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.42e-01\n",
      "Epoch: 234, Loss: 2510.17958, Residuals: -0.92379, Convergence:   inf\n",
      "Epoch: 235, Loss: 2507.16777, Residuals: -0.92142, Convergence: 0.001201\n",
      "Epoch: 236, Loss: 2504.80846, Residuals: -0.92023, Convergence: 0.000942\n",
      "Evidence 14991.268\n",
      "\n",
      "Epoch: 236, Evidence: 14991.26758, Convergence: 0.000738\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.20e-01\n",
      "Epoch: 236, Loss: 2510.38319, Residuals: -0.92023, Convergence:   inf\n",
      "Epoch: 237, Loss: 2506.04184, Residuals: -0.91670, Convergence: 0.001732\n",
      "Epoch: 238, Loss: 2502.96823, Residuals: -0.91766, Convergence: 0.001228\n",
      "Epoch: 239, Loss: 2500.37096, Residuals: -0.91841, Convergence: 0.001039\n",
      "Epoch: 240, Loss: 2498.05014, Residuals: -0.92083, Convergence: 0.000929\n",
      "Evidence 15006.823\n",
      "\n",
      "Epoch: 240, Evidence: 15006.82324, Convergence: 0.001774\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.06e-01\n",
      "Epoch: 240, Loss: 2509.66588, Residuals: -0.92083, Convergence:   inf\n",
      "Epoch: 241, Loss: 2508.03887, Residuals: -0.91762, Convergence: 0.000649\n",
      "Evidence 15013.484\n",
      "\n",
      "Epoch: 241, Evidence: 15013.48438, Convergence: 0.000444\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 8.73e-02\n",
      "Epoch: 241, Loss: 2510.63450, Residuals: -0.91762, Convergence:   inf\n",
      "Epoch: 242, Loss: 2552.69118, Residuals: -0.95606, Convergence: -0.016475\n",
      "Epoch: 242, Loss: 2508.47807, Residuals: -0.91603, Convergence: 0.000860\n",
      "Evidence 15017.627\n",
      "\n",
      "Epoch: 242, Evidence: 15017.62695, Convergence: 0.000719\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 8.05e-02\n",
      "Epoch: 242, Loss: 2510.07639, Residuals: -0.91603, Convergence:   inf\n",
      "Epoch: 243, Loss: 2515.56942, Residuals: -0.91805, Convergence: -0.002184\n",
      "Epoch: 243, Loss: 2510.16233, Residuals: -0.91482, Convergence: -0.000034\n",
      "Evidence 15019.244\n",
      "\n",
      "Epoch: 243, Evidence: 15019.24414, Convergence: 0.000827\n",
      "Total samples: 185, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 383.99655, Residuals: -4.48877, Convergence:   inf\n",
      "Epoch: 1, Loss: 358.16887, Residuals: -4.36975, Convergence: 0.072110\n",
      "Epoch: 2, Loss: 337.00486, Residuals: -4.20661, Convergence: 0.062800\n",
      "Epoch: 3, Loss: 320.81257, Residuals: -4.04243, Convergence: 0.050473\n",
      "Epoch: 4, Loss: 308.45624, Residuals: -3.89748, Convergence: 0.040059\n",
      "Epoch: 5, Loss: 298.64806, Residuals: -3.76908, Convergence: 0.032842\n",
      "Epoch: 6, Loss: 290.67897, Residuals: -3.65716, Convergence: 0.027415\n",
      "Epoch: 7, Loss: 284.05407, Residuals: -3.56127, Convergence: 0.023323\n",
      "Epoch: 8, Loss: 278.40738, Residuals: -3.47931, Convergence: 0.020282\n",
      "Epoch: 9, Loss: 273.48146, Residuals: -3.40897, Convergence: 0.018012\n",
      "Epoch: 10, Loss: 269.09366, Residuals: -3.34816, Convergence: 0.016306\n",
      "Epoch: 11, Loss: 265.11230, Residuals: -3.29509, Convergence: 0.015018\n",
      "Epoch: 12, Loss: 261.44224, Residuals: -3.24825, Convergence: 0.014038\n",
      "Epoch: 13, Loss: 258.01563, Residuals: -3.20633, Convergence: 0.013281\n",
      "Epoch: 14, Loss: 254.78516, Residuals: -3.16818, Convergence: 0.012679\n",
      "Epoch: 15, Loss: 251.71963, Residuals: -3.13284, Convergence: 0.012178\n",
      "Epoch: 16, Loss: 248.80059, Residuals: -3.09963, Convergence: 0.011732\n",
      "Epoch: 17, Loss: 246.01387, Residuals: -3.06811, Convergence: 0.011327\n",
      "Epoch: 18, Loss: 243.33781, Residuals: -3.03790, Convergence: 0.010997\n",
      "Epoch: 19, Loss: 240.74032, Residuals: -3.00851, Convergence: 0.010790\n",
      "Epoch: 20, Loss: 238.18636, Residuals: -2.97940, Convergence: 0.010723\n",
      "Epoch: 21, Loss: 235.64746, Residuals: -2.95016, Convergence: 0.010774\n",
      "Epoch: 22, Loss: 233.10376, Residuals: -2.92051, Convergence: 0.010912\n",
      "Epoch: 23, Loss: 230.53067, Residuals: -2.89021, Convergence: 0.011162\n",
      "Epoch: 24, Loss: 227.88247, Residuals: -2.85875, Convergence: 0.011621\n",
      "Epoch: 25, Loss: 225.09871, Residuals: -2.82542, Convergence: 0.012367\n",
      "Epoch: 26, Loss: 222.16739, Residuals: -2.78995, Convergence: 0.013194\n",
      "Epoch: 27, Loss: 219.19816, Residuals: -2.75342, Convergence: 0.013546\n",
      "Epoch: 28, Loss: 216.30281, Residuals: -2.71709, Convergence: 0.013386\n",
      "Epoch: 29, Loss: 213.50133, Residuals: -2.68128, Convergence: 0.013122\n",
      "Epoch: 30, Loss: 210.77802, Residuals: -2.64587, Convergence: 0.012920\n",
      "Epoch: 31, Loss: 208.11631, Residuals: -2.61069, Convergence: 0.012790\n",
      "Epoch: 32, Loss: 205.50561, Residuals: -2.57561, Convergence: 0.012704\n",
      "Epoch: 33, Loss: 202.94094, Residuals: -2.54057, Convergence: 0.012637\n",
      "Epoch: 34, Loss: 200.42142, Residuals: -2.50553, Convergence: 0.012571\n",
      "Epoch: 35, Loss: 197.94878, Residuals: -2.47049, Convergence: 0.012491\n",
      "Epoch: 36, Loss: 195.52617, Residuals: -2.43548, Convergence: 0.012390\n",
      "Epoch: 37, Loss: 193.15732, Residuals: -2.40053, Convergence: 0.012264\n",
      "Epoch: 38, Loss: 190.84593, Residuals: -2.36568, Convergence: 0.012111\n",
      "Epoch: 39, Loss: 188.59536, Residuals: -2.33098, Convergence: 0.011933\n",
      "Epoch: 40, Loss: 186.40848, Residuals: -2.29649, Convergence: 0.011732\n",
      "Epoch: 41, Loss: 184.28758, Residuals: -2.26224, Convergence: 0.011509\n",
      "Epoch: 42, Loss: 182.23440, Residuals: -2.22828, Convergence: 0.011267\n",
      "Epoch: 43, Loss: 180.25015, Residuals: -2.19465, Convergence: 0.011008\n",
      "Epoch: 44, Loss: 178.33561, Residuals: -2.16140, Convergence: 0.010736\n",
      "Epoch: 45, Loss: 176.49106, Residuals: -2.12857, Convergence: 0.010451\n",
      "Epoch: 46, Loss: 174.71644, Residuals: -2.09619, Convergence: 0.010157\n",
      "Epoch: 47, Loss: 173.01127, Residuals: -2.06429, Convergence: 0.009856\n",
      "Epoch: 48, Loss: 171.37472, Residuals: -2.03291, Convergence: 0.009550\n",
      "Epoch: 49, Loss: 169.80563, Residuals: -2.00208, Convergence: 0.009241\n",
      "Epoch: 50, Loss: 168.30255, Residuals: -1.97182, Convergence: 0.008931\n",
      "Epoch: 51, Loss: 166.86382, Residuals: -1.94215, Convergence: 0.008622\n",
      "Epoch: 52, Loss: 165.48761, Residuals: -1.91308, Convergence: 0.008316\n",
      "Epoch: 53, Loss: 164.17207, Residuals: -1.88462, Convergence: 0.008013\n",
      "Epoch: 54, Loss: 162.91538, Residuals: -1.85678, Convergence: 0.007714\n",
      "Epoch: 55, Loss: 161.71582, Residuals: -1.82957, Convergence: 0.007418\n",
      "Epoch: 56, Loss: 160.57181, Residuals: -1.80299, Convergence: 0.007125\n",
      "Epoch: 57, Loss: 159.48180, Residuals: -1.77705, Convergence: 0.006835\n",
      "Epoch: 58, Loss: 158.44430, Residuals: -1.75175, Convergence: 0.006548\n",
      "Epoch: 59, Loss: 157.45773, Residuals: -1.72710, Convergence: 0.006266\n",
      "Epoch: 60, Loss: 156.52047, Residuals: -1.70311, Convergence: 0.005988\n",
      "Epoch: 61, Loss: 155.63079, Residuals: -1.67978, Convergence: 0.005717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62, Loss: 154.78691, Residuals: -1.65712, Convergence: 0.005452\n",
      "Epoch: 63, Loss: 153.98696, Residuals: -1.63513, Convergence: 0.005195\n",
      "Epoch: 64, Loss: 153.22902, Residuals: -1.61382, Convergence: 0.004946\n",
      "Epoch: 65, Loss: 152.51107, Residuals: -1.59319, Convergence: 0.004708\n",
      "Epoch: 66, Loss: 151.83103, Residuals: -1.57324, Convergence: 0.004479\n",
      "Epoch: 67, Loss: 151.18667, Residuals: -1.55396, Convergence: 0.004262\n",
      "Epoch: 68, Loss: 150.57571, Residuals: -1.53536, Convergence: 0.004058\n",
      "Epoch: 69, Loss: 149.99577, Residuals: -1.51741, Convergence: 0.003866\n",
      "Epoch: 70, Loss: 149.44447, Residuals: -1.50009, Convergence: 0.003689\n",
      "Epoch: 71, Loss: 148.91948, Residuals: -1.48338, Convergence: 0.003525\n",
      "Epoch: 72, Loss: 148.41865, Residuals: -1.46726, Convergence: 0.003374\n",
      "Epoch: 73, Loss: 147.94005, Residuals: -1.45169, Convergence: 0.003235\n",
      "Epoch: 74, Loss: 147.48202, Residuals: -1.43667, Convergence: 0.003106\n",
      "Epoch: 75, Loss: 147.04320, Residuals: -1.42216, Convergence: 0.002984\n",
      "Epoch: 76, Loss: 146.62249, Residuals: -1.40815, Convergence: 0.002869\n",
      "Epoch: 77, Loss: 146.21901, Residuals: -1.39462, Convergence: 0.002759\n",
      "Epoch: 78, Loss: 145.83210, Residuals: -1.38157, Convergence: 0.002653\n",
      "Epoch: 79, Loss: 145.46120, Residuals: -1.36898, Convergence: 0.002550\n",
      "Epoch: 80, Loss: 145.10585, Residuals: -1.35684, Convergence: 0.002449\n",
      "Epoch: 81, Loss: 144.76568, Residuals: -1.34515, Convergence: 0.002350\n",
      "Epoch: 82, Loss: 144.44032, Residuals: -1.33389, Convergence: 0.002253\n",
      "Epoch: 83, Loss: 144.12945, Residuals: -1.32307, Convergence: 0.002157\n",
      "Epoch: 84, Loss: 143.83278, Residuals: -1.31266, Convergence: 0.002063\n",
      "Epoch: 85, Loss: 143.54998, Residuals: -1.30266, Convergence: 0.001970\n",
      "Epoch: 86, Loss: 143.28077, Residuals: -1.29307, Convergence: 0.001879\n",
      "Epoch: 87, Loss: 143.02485, Residuals: -1.28387, Convergence: 0.001789\n",
      "Epoch: 88, Loss: 142.78192, Residuals: -1.27506, Convergence: 0.001701\n",
      "Epoch: 89, Loss: 142.55166, Residuals: -1.26663, Convergence: 0.001615\n",
      "Epoch: 90, Loss: 142.33374, Residuals: -1.25857, Convergence: 0.001531\n",
      "Epoch: 91, Loss: 142.12782, Residuals: -1.25088, Convergence: 0.001449\n",
      "Epoch: 92, Loss: 141.93351, Residuals: -1.24356, Convergence: 0.001369\n",
      "Epoch: 93, Loss: 141.75037, Residuals: -1.23658, Convergence: 0.001292\n",
      "Epoch: 94, Loss: 141.57789, Residuals: -1.22995, Convergence: 0.001218\n",
      "Epoch: 95, Loss: 141.41550, Residuals: -1.22366, Convergence: 0.001148\n",
      "Epoch: 96, Loss: 141.26247, Residuals: -1.21771, Convergence: 0.001083\n",
      "Epoch: 97, Loss: 141.11799, Residuals: -1.21208, Convergence: 0.001024\n",
      "Epoch: 98, Loss: 140.98112, Residuals: -1.20675, Convergence: 0.000971\n",
      "Evidence -182.644\n",
      "\n",
      "Epoch: 98, Evidence: -182.64418, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 7.24e-01\n",
      "Epoch: 98, Loss: 1383.68365, Residuals: -1.20675, Convergence:   inf\n",
      "Epoch: 99, Loss: 1321.03239, Residuals: -1.23664, Convergence: 0.047426\n",
      "Epoch: 100, Loss: 1273.33272, Residuals: -1.26030, Convergence: 0.037460\n",
      "Epoch: 101, Loss: 1237.27004, Residuals: -1.27731, Convergence: 0.029147\n",
      "Epoch: 102, Loss: 1209.16057, Residuals: -1.28916, Convergence: 0.023247\n",
      "Epoch: 103, Loss: 1186.40582, Residuals: -1.29777, Convergence: 0.019180\n",
      "Epoch: 104, Loss: 1167.52767, Residuals: -1.30423, Convergence: 0.016169\n",
      "Epoch: 105, Loss: 1151.62237, Residuals: -1.30904, Convergence: 0.013811\n",
      "Epoch: 106, Loss: 1138.06329, Residuals: -1.31247, Convergence: 0.011914\n",
      "Epoch: 107, Loss: 1126.37849, Residuals: -1.31468, Convergence: 0.010374\n",
      "Epoch: 108, Loss: 1116.19555, Residuals: -1.31581, Convergence: 0.009123\n",
      "Epoch: 109, Loss: 1107.21098, Residuals: -1.31594, Convergence: 0.008115\n",
      "Epoch: 110, Loss: 1099.17145, Residuals: -1.31516, Convergence: 0.007314\n",
      "Epoch: 111, Loss: 1091.86068, Residuals: -1.31351, Convergence: 0.006696\n",
      "Epoch: 112, Loss: 1085.08994, Residuals: -1.31103, Convergence: 0.006240\n",
      "Epoch: 113, Loss: 1078.69123, Residuals: -1.30774, Convergence: 0.005932\n",
      "Epoch: 114, Loss: 1072.51664, Residuals: -1.30365, Convergence: 0.005757\n",
      "Epoch: 115, Loss: 1066.43969, Residuals: -1.29878, Convergence: 0.005698\n",
      "Epoch: 116, Loss: 1060.36856, Residuals: -1.29314, Convergence: 0.005725\n",
      "Epoch: 117, Loss: 1054.26355, Residuals: -1.28681, Convergence: 0.005791\n",
      "Epoch: 118, Loss: 1048.15352, Residuals: -1.27990, Convergence: 0.005829\n",
      "Epoch: 119, Loss: 1042.12709, Residuals: -1.27251, Convergence: 0.005783\n",
      "Epoch: 120, Loss: 1036.29983, Residuals: -1.26477, Convergence: 0.005623\n",
      "Epoch: 121, Loss: 1030.76892, Residuals: -1.25678, Convergence: 0.005366\n",
      "Epoch: 122, Loss: 1025.58685, Residuals: -1.24864, Convergence: 0.005053\n",
      "Epoch: 123, Loss: 1020.76495, Residuals: -1.24042, Convergence: 0.004724\n",
      "Epoch: 124, Loss: 1016.28760, Residuals: -1.23220, Convergence: 0.004406\n",
      "Epoch: 125, Loss: 1012.12471, Residuals: -1.22404, Convergence: 0.004113\n",
      "Epoch: 126, Loss: 1008.24366, Residuals: -1.21598, Convergence: 0.003849\n",
      "Epoch: 127, Loss: 1004.61326, Residuals: -1.20805, Convergence: 0.003614\n",
      "Epoch: 128, Loss: 1001.20522, Residuals: -1.20030, Convergence: 0.003404\n",
      "Epoch: 129, Loss: 997.99645, Residuals: -1.19274, Convergence: 0.003215\n",
      "Epoch: 130, Loss: 994.96630, Residuals: -1.18540, Convergence: 0.003045\n",
      "Epoch: 131, Loss: 992.09889, Residuals: -1.17829, Convergence: 0.002890\n",
      "Epoch: 132, Loss: 989.38048, Residuals: -1.17142, Convergence: 0.002748\n",
      "Epoch: 133, Loss: 986.79990, Residuals: -1.16481, Convergence: 0.002615\n",
      "Epoch: 134, Loss: 984.34783, Residuals: -1.15846, Convergence: 0.002491\n",
      "Epoch: 135, Loss: 982.01580, Residuals: -1.15237, Convergence: 0.002375\n",
      "Epoch: 136, Loss: 979.79665, Residuals: -1.14654, Convergence: 0.002265\n",
      "Epoch: 137, Loss: 977.68393, Residuals: -1.14097, Convergence: 0.002161\n",
      "Epoch: 138, Loss: 975.67110, Residuals: -1.13566, Convergence: 0.002063\n",
      "Epoch: 139, Loss: 973.75290, Residuals: -1.13060, Convergence: 0.001970\n",
      "Epoch: 140, Loss: 971.92334, Residuals: -1.12578, Convergence: 0.001882\n",
      "Epoch: 141, Loss: 970.17713, Residuals: -1.12118, Convergence: 0.001800\n",
      "Epoch: 142, Loss: 968.50914, Residuals: -1.11682, Convergence: 0.001722\n",
      "Epoch: 143, Loss: 966.91442, Residuals: -1.11266, Convergence: 0.001649\n",
      "Epoch: 144, Loss: 965.38830, Residuals: -1.10871, Convergence: 0.001581\n",
      "Epoch: 145, Loss: 963.92642, Residuals: -1.10495, Convergence: 0.001517\n",
      "Epoch: 146, Loss: 962.52406, Residuals: -1.10137, Convergence: 0.001457\n",
      "Epoch: 147, Loss: 961.17745, Residuals: -1.09796, Convergence: 0.001401\n",
      "Epoch: 148, Loss: 959.88316, Residuals: -1.09472, Convergence: 0.001348\n",
      "Epoch: 149, Loss: 958.63708, Residuals: -1.09162, Convergence: 0.001300\n",
      "Epoch: 150, Loss: 957.43650, Residuals: -1.08867, Convergence: 0.001254\n",
      "Epoch: 151, Loss: 956.27821, Residuals: -1.08585, Convergence: 0.001211\n",
      "Epoch: 152, Loss: 955.15946, Residuals: -1.08315, Convergence: 0.001171\n",
      "Epoch: 153, Loss: 954.07758, Residuals: -1.08057, Convergence: 0.001134\n",
      "Epoch: 154, Loss: 953.03037, Residuals: -1.07810, Convergence: 0.001099\n",
      "Epoch: 155, Loss: 952.01514, Residuals: -1.07573, Convergence: 0.001066\n",
      "Epoch: 156, Loss: 951.02943, Residuals: -1.07345, Convergence: 0.001036\n",
      "Epoch: 157, Loss: 950.07119, Residuals: -1.07127, Convergence: 0.001009\n",
      "Epoch: 158, Loss: 949.13755, Residuals: -1.06916, Convergence: 0.000984\n",
      "Evidence 11341.168\n",
      "\n",
      "Epoch: 158, Evidence: 11341.16797, Convergence: 1.016105\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 5.77e-01\n",
      "Epoch: 158, Loss: 2381.00149, Residuals: -1.06916, Convergence:   inf\n",
      "Epoch: 159, Loss: 2340.70286, Residuals: -1.08079, Convergence: 0.017216\n",
      "Epoch: 160, Loss: 2313.22005, Residuals: -1.08018, Convergence: 0.011881\n",
      "Epoch: 161, Loss: 2290.14239, Residuals: -1.07860, Convergence: 0.010077\n",
      "Epoch: 162, Loss: 2270.51502, Residuals: -1.07657, Convergence: 0.008644\n",
      "Epoch: 163, Loss: 2253.65571, Residuals: -1.07420, Convergence: 0.007481\n",
      "Epoch: 164, Loss: 2239.04216, Residuals: -1.07157, Convergence: 0.006527\n",
      "Epoch: 165, Loss: 2226.25967, Residuals: -1.06872, Convergence: 0.005742\n",
      "Epoch: 166, Loss: 2214.97316, Residuals: -1.06571, Convergence: 0.005096\n",
      "Epoch: 167, Loss: 2204.90738, Residuals: -1.06254, Convergence: 0.004565\n",
      "Epoch: 168, Loss: 2195.83254, Residuals: -1.05926, Convergence: 0.004133\n",
      "Epoch: 169, Loss: 2187.56085, Residuals: -1.05585, Convergence: 0.003781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 170, Loss: 2179.94757, Residuals: -1.05232, Convergence: 0.003492\n",
      "Epoch: 171, Loss: 2172.89550, Residuals: -1.04868, Convergence: 0.003245\n",
      "Epoch: 172, Loss: 2166.34779, Residuals: -1.04498, Convergence: 0.003022\n",
      "Epoch: 173, Loss: 2160.27774, Residuals: -1.04124, Convergence: 0.002810\n",
      "Epoch: 174, Loss: 2154.66599, Residuals: -1.03753, Convergence: 0.002604\n",
      "Epoch: 175, Loss: 2149.49140, Residuals: -1.03389, Convergence: 0.002407\n",
      "Epoch: 176, Loss: 2144.72937, Residuals: -1.03035, Convergence: 0.002220\n",
      "Epoch: 177, Loss: 2140.34962, Residuals: -1.02694, Convergence: 0.002046\n",
      "Epoch: 178, Loss: 2136.31940, Residuals: -1.02367, Convergence: 0.001887\n",
      "Epoch: 179, Loss: 2132.60721, Residuals: -1.02055, Convergence: 0.001741\n",
      "Epoch: 180, Loss: 2129.17998, Residuals: -1.01759, Convergence: 0.001610\n",
      "Epoch: 181, Loss: 2126.00972, Residuals: -1.01478, Convergence: 0.001491\n",
      "Epoch: 182, Loss: 2123.06666, Residuals: -1.01212, Convergence: 0.001386\n",
      "Epoch: 183, Loss: 2120.32719, Residuals: -1.00960, Convergence: 0.001292\n",
      "Epoch: 184, Loss: 2117.76758, Residuals: -1.00721, Convergence: 0.001209\n",
      "Epoch: 185, Loss: 2115.36847, Residuals: -1.00496, Convergence: 0.001134\n",
      "Epoch: 186, Loss: 2113.11157, Residuals: -1.00282, Convergence: 0.001068\n",
      "Epoch: 187, Loss: 2110.98052, Residuals: -1.00080, Convergence: 0.001010\n",
      "Epoch: 188, Loss: 2108.96213, Residuals: -0.99888, Convergence: 0.000957\n",
      "Evidence 14536.599\n",
      "\n",
      "Epoch: 188, Evidence: 14536.59863, Convergence: 0.219820\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 4.40e-01\n",
      "Epoch: 188, Loss: 2507.85216, Residuals: -0.99888, Convergence:   inf\n",
      "Epoch: 189, Loss: 2493.46631, Residuals: -0.99596, Convergence: 0.005769\n",
      "Epoch: 190, Loss: 2481.88274, Residuals: -0.99216, Convergence: 0.004667\n",
      "Epoch: 191, Loss: 2471.95606, Residuals: -0.98844, Convergence: 0.004016\n",
      "Epoch: 192, Loss: 2463.36798, Residuals: -0.98495, Convergence: 0.003486\n",
      "Epoch: 193, Loss: 2455.88262, Residuals: -0.98170, Convergence: 0.003048\n",
      "Epoch: 194, Loss: 2449.31486, Residuals: -0.97869, Convergence: 0.002681\n",
      "Epoch: 195, Loss: 2443.51425, Residuals: -0.97592, Convergence: 0.002374\n",
      "Epoch: 196, Loss: 2438.35963, Residuals: -0.97334, Convergence: 0.002114\n",
      "Epoch: 197, Loss: 2433.74924, Residuals: -0.97096, Convergence: 0.001894\n",
      "Epoch: 198, Loss: 2429.60131, Residuals: -0.96876, Convergence: 0.001707\n",
      "Epoch: 199, Loss: 2425.84717, Residuals: -0.96671, Convergence: 0.001548\n",
      "Epoch: 200, Loss: 2422.43165, Residuals: -0.96481, Convergence: 0.001410\n",
      "Epoch: 201, Loss: 2419.30740, Residuals: -0.96305, Convergence: 0.001291\n",
      "Epoch: 202, Loss: 2416.43697, Residuals: -0.96141, Convergence: 0.001188\n",
      "Epoch: 203, Loss: 2413.78832, Residuals: -0.95988, Convergence: 0.001097\n",
      "Epoch: 204, Loss: 2411.33492, Residuals: -0.95846, Convergence: 0.001017\n",
      "Epoch: 205, Loss: 2409.05445, Residuals: -0.95713, Convergence: 0.000947\n",
      "Evidence 14942.545\n",
      "\n",
      "Epoch: 205, Evidence: 14942.54492, Convergence: 0.027167\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 3.36e-01\n",
      "Epoch: 205, Loss: 2512.83752, Residuals: -0.95713, Convergence:   inf\n",
      "Epoch: 206, Loss: 2506.22285, Residuals: -0.95385, Convergence: 0.002639\n",
      "Epoch: 207, Loss: 2500.76251, Residuals: -0.95081, Convergence: 0.002183\n",
      "Epoch: 208, Loss: 2496.09866, Residuals: -0.94814, Convergence: 0.001868\n",
      "Epoch: 209, Loss: 2492.04959, Residuals: -0.94580, Convergence: 0.001625\n",
      "Epoch: 210, Loss: 2488.48858, Residuals: -0.94372, Convergence: 0.001431\n",
      "Epoch: 211, Loss: 2485.32249, Residuals: -0.94187, Convergence: 0.001274\n",
      "Epoch: 212, Loss: 2482.47908, Residuals: -0.94022, Convergence: 0.001145\n",
      "Epoch: 213, Loss: 2479.90450, Residuals: -0.93874, Convergence: 0.001038\n",
      "Epoch: 214, Loss: 2477.55380, Residuals: -0.93742, Convergence: 0.000949\n",
      "Evidence 15030.300\n",
      "\n",
      "Epoch: 214, Evidence: 15030.29980, Convergence: 0.005839\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 2.63e-01\n",
      "Epoch: 214, Loss: 2514.62075, Residuals: -0.93742, Convergence:   inf\n",
      "Epoch: 215, Loss: 2510.76553, Residuals: -0.93468, Convergence: 0.001535\n",
      "Epoch: 216, Loss: 2507.52476, Residuals: -0.93237, Convergence: 0.001292\n",
      "Epoch: 217, Loss: 2504.70723, Residuals: -0.93040, Convergence: 0.001125\n",
      "Epoch: 218, Loss: 2502.21301, Residuals: -0.92870, Convergence: 0.000997\n",
      "Evidence 15058.701\n",
      "\n",
      "Epoch: 218, Evidence: 15058.70117, Convergence: 0.001886\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 2.13e-01\n",
      "Epoch: 218, Loss: 2515.85372, Residuals: -0.92870, Convergence:   inf\n",
      "Epoch: 219, Loss: 2512.95316, Residuals: -0.92637, Convergence: 0.001154\n",
      "Epoch: 220, Loss: 2510.48283, Residuals: -0.92442, Convergence: 0.000984\n",
      "Evidence 15070.498\n",
      "\n",
      "Epoch: 220, Evidence: 15070.49805, Convergence: 0.000783\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 1.78e-01\n",
      "Epoch: 220, Loss: 2516.73252, Residuals: -0.92442, Convergence:   inf\n",
      "Epoch: 221, Loss: 2512.07854, Residuals: -0.92089, Convergence: 0.001853\n",
      "Epoch: 222, Loss: 2508.48683, Residuals: -0.91815, Convergence: 0.001432\n",
      "Epoch: 223, Loss: 2505.54327, Residuals: -0.91607, Convergence: 0.001175\n",
      "Epoch: 224, Loss: 2503.03497, Residuals: -0.91464, Convergence: 0.001002\n",
      "Epoch: 225, Loss: 2500.83231, Residuals: -0.91364, Convergence: 0.000881\n",
      "Evidence 15091.250\n",
      "\n",
      "Epoch: 225, Evidence: 15091.25000, Convergence: 0.002157\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 1.48e-01\n",
      "Epoch: 225, Loss: 2516.95726, Residuals: -0.91364, Convergence:   inf\n",
      "Epoch: 226, Loss: 2514.06277, Residuals: -0.91061, Convergence: 0.001151\n",
      "Epoch: 227, Loss: 2511.73622, Residuals: -0.90881, Convergence: 0.000926\n",
      "Evidence 15102.553\n",
      "\n",
      "Epoch: 227, Evidence: 15102.55273, Convergence: 0.000748\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 1.26e-01\n",
      "Epoch: 227, Loss: 2517.27010, Residuals: -0.90881, Convergence:   inf\n",
      "Epoch: 228, Loss: 2513.00531, Residuals: -0.90443, Convergence: 0.001697\n",
      "Epoch: 229, Loss: 2509.90452, Residuals: -0.90487, Convergence: 0.001235\n",
      "Epoch: 230, Loss: 2507.33074, Residuals: -0.90570, Convergence: 0.001027\n",
      "Epoch: 231, Loss: 2505.13806, Residuals: -0.90802, Convergence: 0.000875\n",
      "Evidence 15118.041\n",
      "\n",
      "Epoch: 231, Evidence: 15118.04102, Convergence: 0.001772\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 1.12e-01\n",
      "Epoch: 231, Loss: 2516.95230, Residuals: -0.90802, Convergence:   inf\n",
      "Epoch: 232, Loss: 2515.58667, Residuals: -0.90621, Convergence: 0.000543\n",
      "Evidence 15124.095\n",
      "\n",
      "Epoch: 232, Evidence: 15124.09473, Convergence: 0.000400\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 9.12e-02\n",
      "Epoch: 232, Loss: 2517.84481, Residuals: -0.90621, Convergence:   inf\n",
      "Epoch: 233, Loss: 2568.74064, Residuals: -0.94796, Convergence: -0.019814\n",
      "Epoch: 233, Loss: 2515.64121, Residuals: -0.90299, Convergence: 0.000876\n",
      "Evidence 15128.424\n",
      "\n",
      "Epoch: 233, Evidence: 15128.42383, Convergence: 0.000686\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 8.35e-02\n",
      "Epoch: 233, Loss: 2517.43190, Residuals: -0.90299, Convergence:   inf\n",
      "Epoch: 234, Loss: 2522.04562, Residuals: -0.90318, Convergence: -0.001829\n",
      "Epoch: 234, Loss: 2517.57147, Residuals: -0.90104, Convergence: -0.000055\n",
      "Evidence 15129.765\n",
      "\n",
      "Epoch: 234, Evidence: 15129.76465, Convergence: 0.000775\n",
      "Total samples: 183, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 383.54575, Residuals: -4.54087, Convergence:   inf\n",
      "Epoch: 1, Loss: 357.86601, Residuals: -4.42084, Convergence: 0.071758\n",
      "Epoch: 2, Loss: 336.79841, Residuals: -4.25692, Convergence: 0.062553\n",
      "Epoch: 3, Loss: 320.64526, Residuals: -4.09166, Convergence: 0.050377\n",
      "Epoch: 4, Loss: 308.35587, Residuals: -3.94685, Convergence: 0.039855\n",
      "Epoch: 5, Loss: 298.61845, Residuals: -3.81930, Convergence: 0.032608\n",
      "Epoch: 6, Loss: 290.71652, Residuals: -3.70828, Convergence: 0.027181\n",
      "Epoch: 7, Loss: 284.16013, Residuals: -3.61316, Convergence: 0.023073\n",
      "Epoch: 8, Loss: 278.58730, Residuals: -3.53187, Convergence: 0.020004\n",
      "Epoch: 9, Loss: 273.74056, Residuals: -3.46209, Convergence: 0.017706\n",
      "Epoch: 10, Loss: 269.43566, Residuals: -3.40169, Convergence: 0.015977\n",
      "Epoch: 11, Loss: 265.53823, Residuals: -3.34885, Convergence: 0.014677\n",
      "Epoch: 12, Loss: 261.94968, Residuals: -3.30202, Convergence: 0.013699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Loss: 258.59834, Residuals: -3.25989, Convergence: 0.012960\n",
      "Epoch: 14, Loss: 255.43392, Residuals: -3.22131, Convergence: 0.012388\n",
      "Epoch: 15, Loss: 252.42472, Residuals: -3.18540, Convergence: 0.011921\n",
      "Epoch: 16, Loss: 249.55492, Residuals: -3.15157, Convergence: 0.011500\n",
      "Epoch: 17, Loss: 246.81409, Residuals: -3.11947, Convergence: 0.011105\n",
      "Epoch: 18, Loss: 244.18266, Residuals: -3.08876, Convergence: 0.010776\n",
      "Epoch: 19, Loss: 241.62896, Residuals: -3.05891, Convergence: 0.010569\n",
      "Epoch: 20, Loss: 239.11731, Residuals: -3.02937, Convergence: 0.010504\n",
      "Epoch: 21, Loss: 236.61774, Residuals: -2.99961, Convergence: 0.010564\n",
      "Epoch: 22, Loss: 234.10894, Residuals: -2.96932, Convergence: 0.010716\n",
      "Epoch: 23, Loss: 231.56614, Residuals: -2.93819, Convergence: 0.010981\n",
      "Epoch: 24, Loss: 228.94400, Residuals: -2.90569, Convergence: 0.011453\n",
      "Epoch: 25, Loss: 226.18440, Residuals: -2.87108, Convergence: 0.012201\n",
      "Epoch: 26, Loss: 223.28808, Residuals: -2.83426, Convergence: 0.012971\n",
      "Epoch: 27, Loss: 220.37958, Residuals: -2.79648, Convergence: 0.013198\n",
      "Epoch: 28, Loss: 217.56727, Residuals: -2.75904, Convergence: 0.012926\n",
      "Epoch: 29, Loss: 214.86600, Residuals: -2.72226, Convergence: 0.012572\n",
      "Epoch: 30, Loss: 212.25816, Residuals: -2.68607, Convergence: 0.012286\n",
      "Epoch: 31, Loss: 209.72528, Residuals: -2.65035, Convergence: 0.012077\n",
      "Epoch: 32, Loss: 207.25390, Residuals: -2.61499, Convergence: 0.011924\n",
      "Epoch: 33, Loss: 204.83554, Residuals: -2.57993, Convergence: 0.011806\n",
      "Epoch: 34, Loss: 202.46558, Residuals: -2.54510, Convergence: 0.011705\n",
      "Epoch: 35, Loss: 200.14233, Residuals: -2.51048, Convergence: 0.011608\n",
      "Epoch: 36, Loss: 197.86601, Residuals: -2.47603, Convergence: 0.011504\n",
      "Epoch: 37, Loss: 195.63807, Residuals: -2.44177, Convergence: 0.011388\n",
      "Epoch: 38, Loss: 193.46054, Residuals: -2.40767, Convergence: 0.011256\n",
      "Epoch: 39, Loss: 191.33556, Residuals: -2.37375, Convergence: 0.011106\n",
      "Epoch: 40, Loss: 189.26504, Residuals: -2.34003, Convergence: 0.010940\n",
      "Epoch: 41, Loss: 187.25051, Residuals: -2.30653, Convergence: 0.010758\n",
      "Epoch: 42, Loss: 185.29295, Residuals: -2.27326, Convergence: 0.010565\n",
      "Epoch: 43, Loss: 183.39284, Residuals: -2.24024, Convergence: 0.010361\n",
      "Epoch: 44, Loss: 181.55030, Residuals: -2.20750, Convergence: 0.010149\n",
      "Epoch: 45, Loss: 179.76517, Residuals: -2.17505, Convergence: 0.009930\n",
      "Epoch: 46, Loss: 178.03734, Residuals: -2.14292, Convergence: 0.009705\n",
      "Epoch: 47, Loss: 176.36684, Residuals: -2.11113, Convergence: 0.009472\n",
      "Epoch: 48, Loss: 174.75396, Residuals: -2.07971, Convergence: 0.009229\n",
      "Epoch: 49, Loss: 173.19919, Residuals: -2.04870, Convergence: 0.008977\n",
      "Epoch: 50, Loss: 171.70305, Residuals: -2.01813, Convergence: 0.008714\n",
      "Epoch: 51, Loss: 170.26589, Residuals: -1.98807, Convergence: 0.008441\n",
      "Epoch: 52, Loss: 168.88767, Residuals: -1.95854, Convergence: 0.008161\n",
      "Epoch: 53, Loss: 167.56791, Residuals: -1.92959, Convergence: 0.007876\n",
      "Epoch: 54, Loss: 166.30558, Residuals: -1.90127, Convergence: 0.007590\n",
      "Epoch: 55, Loss: 165.09918, Residuals: -1.87360, Convergence: 0.007307\n",
      "Epoch: 56, Loss: 163.94677, Residuals: -1.84662, Convergence: 0.007029\n",
      "Epoch: 57, Loss: 162.84600, Residuals: -1.82033, Convergence: 0.006760\n",
      "Epoch: 58, Loss: 161.79427, Residuals: -1.79475, Convergence: 0.006500\n",
      "Epoch: 59, Loss: 160.78882, Residuals: -1.76986, Convergence: 0.006253\n",
      "Epoch: 60, Loss: 159.82691, Residuals: -1.74567, Convergence: 0.006018\n",
      "Epoch: 61, Loss: 158.90591, Residuals: -1.72217, Convergence: 0.005796\n",
      "Epoch: 62, Loss: 158.02345, Residuals: -1.69932, Convergence: 0.005584\n",
      "Epoch: 63, Loss: 157.17748, Residuals: -1.67713, Convergence: 0.005382\n",
      "Epoch: 64, Loss: 156.36621, Residuals: -1.65558, Convergence: 0.005188\n",
      "Epoch: 65, Loss: 155.58818, Residuals: -1.63465, Convergence: 0.005001\n",
      "Epoch: 66, Loss: 154.84212, Residuals: -1.61435, Convergence: 0.004818\n",
      "Epoch: 67, Loss: 154.12692, Residuals: -1.59466, Convergence: 0.004640\n",
      "Epoch: 68, Loss: 153.44162, Residuals: -1.57559, Convergence: 0.004466\n",
      "Epoch: 69, Loss: 152.78530, Residuals: -1.55712, Convergence: 0.004296\n",
      "Epoch: 70, Loss: 152.15709, Residuals: -1.53926, Convergence: 0.004129\n",
      "Epoch: 71, Loss: 151.55618, Residuals: -1.52199, Convergence: 0.003965\n",
      "Epoch: 72, Loss: 150.98172, Residuals: -1.50531, Convergence: 0.003805\n",
      "Epoch: 73, Loss: 150.43291, Residuals: -1.48922, Convergence: 0.003648\n",
      "Epoch: 74, Loss: 149.90892, Residuals: -1.47369, Convergence: 0.003495\n",
      "Epoch: 75, Loss: 149.40893, Residuals: -1.45872, Convergence: 0.003346\n",
      "Epoch: 76, Loss: 148.93211, Residuals: -1.44431, Convergence: 0.003202\n",
      "Epoch: 77, Loss: 148.47765, Residuals: -1.43043, Convergence: 0.003061\n",
      "Epoch: 78, Loss: 148.04473, Residuals: -1.41707, Convergence: 0.002924\n",
      "Epoch: 79, Loss: 147.63249, Residuals: -1.40423, Convergence: 0.002792\n",
      "Epoch: 80, Loss: 147.24015, Residuals: -1.39189, Convergence: 0.002665\n",
      "Epoch: 81, Loss: 146.86685, Residuals: -1.38003, Convergence: 0.002542\n",
      "Epoch: 82, Loss: 146.51180, Residuals: -1.36865, Convergence: 0.002423\n",
      "Epoch: 83, Loss: 146.17420, Residuals: -1.35771, Convergence: 0.002310\n",
      "Epoch: 84, Loss: 145.85325, Residuals: -1.34723, Convergence: 0.002200\n",
      "Epoch: 85, Loss: 145.54819, Residuals: -1.33716, Convergence: 0.002096\n",
      "Epoch: 86, Loss: 145.25829, Residuals: -1.32751, Convergence: 0.001996\n",
      "Epoch: 87, Loss: 144.98281, Residuals: -1.31826, Convergence: 0.001900\n",
      "Epoch: 88, Loss: 144.72108, Residuals: -1.30939, Convergence: 0.001808\n",
      "Epoch: 89, Loss: 144.47244, Residuals: -1.30089, Convergence: 0.001721\n",
      "Epoch: 90, Loss: 144.23629, Residuals: -1.29275, Convergence: 0.001637\n",
      "Epoch: 91, Loss: 144.01203, Residuals: -1.28496, Convergence: 0.001557\n",
      "Epoch: 92, Loss: 143.79914, Residuals: -1.27749, Convergence: 0.001480\n",
      "Epoch: 93, Loss: 143.59711, Residuals: -1.27035, Convergence: 0.001407\n",
      "Epoch: 94, Loss: 143.40548, Residuals: -1.26351, Convergence: 0.001336\n",
      "Epoch: 95, Loss: 143.22383, Residuals: -1.25697, Convergence: 0.001268\n",
      "Epoch: 96, Loss: 143.05177, Residuals: -1.25073, Convergence: 0.001203\n",
      "Epoch: 97, Loss: 142.88894, Residuals: -1.24476, Convergence: 0.001140\n",
      "Epoch: 98, Loss: 142.73498, Residuals: -1.23907, Convergence: 0.001079\n",
      "Epoch: 99, Loss: 142.58958, Residuals: -1.23365, Convergence: 0.001020\n",
      "Epoch: 100, Loss: 142.45239, Residuals: -1.22850, Convergence: 0.000963\n",
      "Evidence -184.140\n",
      "\n",
      "Epoch: 100, Evidence: -184.14040, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.25e-01\n",
      "Epoch: 100, Loss: 1387.15975, Residuals: -1.22850, Convergence:   inf\n",
      "Epoch: 101, Loss: 1324.05464, Residuals: -1.25803, Convergence: 0.047661\n",
      "Epoch: 102, Loss: 1276.04569, Residuals: -1.28131, Convergence: 0.037623\n",
      "Epoch: 103, Loss: 1239.80573, Residuals: -1.29788, Convergence: 0.029230\n",
      "Epoch: 104, Loss: 1211.61108, Residuals: -1.30928, Convergence: 0.023270\n",
      "Epoch: 105, Loss: 1188.79203, Residuals: -1.31755, Convergence: 0.019195\n",
      "Epoch: 106, Loss: 1169.83006, Residuals: -1.32375, Convergence: 0.016209\n",
      "Epoch: 107, Loss: 1153.81421, Residuals: -1.32837, Convergence: 0.013881\n",
      "Epoch: 108, Loss: 1140.12382, Residuals: -1.33166, Convergence: 0.012008\n",
      "Epoch: 109, Loss: 1128.29934, Residuals: -1.33378, Convergence: 0.010480\n",
      "Epoch: 110, Loss: 1117.98228, Residuals: -1.33485, Convergence: 0.009228\n",
      "Epoch: 111, Loss: 1108.88527, Residuals: -1.33499, Convergence: 0.008204\n",
      "Epoch: 112, Loss: 1100.77291, Residuals: -1.33427, Convergence: 0.007370\n",
      "Epoch: 113, Loss: 1093.44988, Residuals: -1.33278, Convergence: 0.006697\n",
      "Epoch: 114, Loss: 1086.75167, Residuals: -1.33059, Convergence: 0.006164\n",
      "Epoch: 115, Loss: 1080.53813, Residuals: -1.32772, Convergence: 0.005750\n",
      "Epoch: 116, Loss: 1074.68877, Residuals: -1.32423, Convergence: 0.005443\n",
      "Epoch: 117, Loss: 1069.09933, Residuals: -1.32013, Convergence: 0.005228\n",
      "Epoch: 118, Loss: 1063.67981, Residuals: -1.31544, Convergence: 0.005095\n",
      "Epoch: 119, Loss: 1058.35167, Residuals: -1.31018, Convergence: 0.005034\n",
      "Epoch: 120, Loss: 1053.04992, Residuals: -1.30436, Convergence: 0.005035\n",
      "Epoch: 121, Loss: 1047.73038, Residuals: -1.29803, Convergence: 0.005077\n",
      "Epoch: 122, Loss: 1042.38361, Residuals: -1.29125, Convergence: 0.005129\n",
      "Epoch: 123, Loss: 1037.04487, Residuals: -1.28409, Convergence: 0.005148\n",
      "Epoch: 124, Loss: 1031.79222, Residuals: -1.27666, Convergence: 0.005091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 125, Loss: 1026.71942, Residuals: -1.26903, Convergence: 0.004941\n",
      "Epoch: 126, Loss: 1021.90321, Residuals: -1.26128, Convergence: 0.004713\n",
      "Epoch: 127, Loss: 1017.38309, Residuals: -1.25347, Convergence: 0.004443\n",
      "Epoch: 128, Loss: 1013.16589, Residuals: -1.24565, Convergence: 0.004162\n",
      "Epoch: 129, Loss: 1009.23508, Residuals: -1.23788, Convergence: 0.003895\n",
      "Epoch: 130, Loss: 1005.56542, Residuals: -1.23020, Convergence: 0.003649\n",
      "Epoch: 131, Loss: 1002.12914, Residuals: -1.22265, Convergence: 0.003429\n",
      "Epoch: 132, Loss: 998.90069, Residuals: -1.21525, Convergence: 0.003232\n",
      "Epoch: 133, Loss: 995.85746, Residuals: -1.20803, Convergence: 0.003056\n",
      "Epoch: 134, Loss: 992.98116, Residuals: -1.20102, Convergence: 0.002897\n",
      "Epoch: 135, Loss: 990.25611, Residuals: -1.19421, Convergence: 0.002752\n",
      "Epoch: 136, Loss: 987.66938, Residuals: -1.18763, Convergence: 0.002619\n",
      "Epoch: 137, Loss: 985.21044, Residuals: -1.18129, Convergence: 0.002496\n",
      "Epoch: 138, Loss: 982.86982, Residuals: -1.17519, Convergence: 0.002381\n",
      "Epoch: 139, Loss: 980.63975, Residuals: -1.16933, Convergence: 0.002274\n",
      "Epoch: 140, Loss: 978.51292, Residuals: -1.16371, Convergence: 0.002174\n",
      "Epoch: 141, Loss: 976.48262, Residuals: -1.15834, Convergence: 0.002079\n",
      "Epoch: 142, Loss: 974.54285, Residuals: -1.15320, Convergence: 0.001990\n",
      "Epoch: 143, Loss: 972.68784, Residuals: -1.14829, Convergence: 0.001907\n",
      "Epoch: 144, Loss: 970.91237, Residuals: -1.14360, Convergence: 0.001829\n",
      "Epoch: 145, Loss: 969.21108, Residuals: -1.13913, Convergence: 0.001755\n",
      "Epoch: 146, Loss: 967.57964, Residuals: -1.13487, Convergence: 0.001686\n",
      "Epoch: 147, Loss: 966.01328, Residuals: -1.13080, Convergence: 0.001621\n",
      "Epoch: 148, Loss: 964.50819, Residuals: -1.12691, Convergence: 0.001560\n",
      "Epoch: 149, Loss: 963.06018, Residuals: -1.12321, Convergence: 0.001504\n",
      "Epoch: 150, Loss: 961.66550, Residuals: -1.11967, Convergence: 0.001450\n",
      "Epoch: 151, Loss: 960.32074, Residuals: -1.11629, Convergence: 0.001400\n",
      "Epoch: 152, Loss: 959.02259, Residuals: -1.11306, Convergence: 0.001354\n",
      "Epoch: 153, Loss: 957.76775, Residuals: -1.10996, Convergence: 0.001310\n",
      "Epoch: 154, Loss: 956.55299, Residuals: -1.10700, Convergence: 0.001270\n",
      "Epoch: 155, Loss: 955.37507, Residuals: -1.10415, Convergence: 0.001233\n",
      "Epoch: 156, Loss: 954.23066, Residuals: -1.10142, Convergence: 0.001199\n",
      "Epoch: 157, Loss: 953.11624, Residuals: -1.09880, Convergence: 0.001169\n",
      "Epoch: 158, Loss: 952.02858, Residuals: -1.09626, Convergence: 0.001142\n",
      "Epoch: 159, Loss: 950.96390, Residuals: -1.09382, Convergence: 0.001120\n",
      "Epoch: 160, Loss: 949.91830, Residuals: -1.09145, Convergence: 0.001101\n",
      "Epoch: 161, Loss: 948.88819, Residuals: -1.08915, Convergence: 0.001086\n",
      "Epoch: 162, Loss: 947.86958, Residuals: -1.08690, Convergence: 0.001075\n",
      "Epoch: 163, Loss: 946.85866, Residuals: -1.08471, Convergence: 0.001068\n",
      "Epoch: 164, Loss: 945.85221, Residuals: -1.08256, Convergence: 0.001064\n",
      "Epoch: 165, Loss: 944.84684, Residuals: -1.08044, Convergence: 0.001064\n",
      "Epoch: 166, Loss: 943.84087, Residuals: -1.07834, Convergence: 0.001066\n",
      "Epoch: 167, Loss: 942.83364, Residuals: -1.07627, Convergence: 0.001068\n",
      "Epoch: 168, Loss: 941.82541, Residuals: -1.07421, Convergence: 0.001071\n",
      "Epoch: 169, Loss: 940.81869, Residuals: -1.07217, Convergence: 0.001070\n",
      "Epoch: 170, Loss: 939.81768, Residuals: -1.07016, Convergence: 0.001065\n",
      "Epoch: 171, Loss: 938.82715, Residuals: -1.06817, Convergence: 0.001055\n",
      "Epoch: 172, Loss: 937.85291, Residuals: -1.06622, Convergence: 0.001039\n",
      "Epoch: 173, Loss: 936.90063, Residuals: -1.06432, Convergence: 0.001016\n",
      "Epoch: 174, Loss: 935.97494, Residuals: -1.06247, Convergence: 0.000989\n",
      "Evidence 11219.287\n",
      "\n",
      "Epoch: 174, Evidence: 11219.28711, Convergence: 1.016413\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 5.74e-01\n",
      "Epoch: 174, Loss: 2363.64626, Residuals: -1.06247, Convergence:   inf\n",
      "Epoch: 175, Loss: 2324.67802, Residuals: -1.07098, Convergence: 0.016763\n",
      "Epoch: 176, Loss: 2298.14607, Residuals: -1.06942, Convergence: 0.011545\n",
      "Epoch: 177, Loss: 2276.11090, Residuals: -1.06712, Convergence: 0.009681\n",
      "Epoch: 178, Loss: 2257.56710, Residuals: -1.06463, Convergence: 0.008214\n",
      "Epoch: 179, Loss: 2241.82830, Residuals: -1.06207, Convergence: 0.007021\n",
      "Epoch: 180, Loss: 2228.35886, Residuals: -1.05946, Convergence: 0.006045\n",
      "Epoch: 181, Loss: 2216.72414, Residuals: -1.05684, Convergence: 0.005249\n",
      "Epoch: 182, Loss: 2206.56265, Residuals: -1.05420, Convergence: 0.004605\n",
      "Epoch: 183, Loss: 2197.57305, Residuals: -1.05153, Convergence: 0.004091\n",
      "Epoch: 184, Loss: 2189.50915, Residuals: -1.04882, Convergence: 0.003683\n",
      "Epoch: 185, Loss: 2182.17419, Residuals: -1.04605, Convergence: 0.003361\n",
      "Epoch: 186, Loss: 2175.42457, Residuals: -1.04320, Convergence: 0.003103\n",
      "Epoch: 187, Loss: 2169.16533, Residuals: -1.04028, Convergence: 0.002886\n",
      "Epoch: 188, Loss: 2163.34234, Residuals: -1.03731, Convergence: 0.002692\n",
      "Epoch: 189, Loss: 2157.92443, Residuals: -1.03432, Convergence: 0.002511\n",
      "Epoch: 190, Loss: 2152.88925, Residuals: -1.03136, Convergence: 0.002339\n",
      "Epoch: 191, Loss: 2148.21533, Residuals: -1.02845, Convergence: 0.002176\n",
      "Epoch: 192, Loss: 2143.87959, Residuals: -1.02561, Convergence: 0.002022\n",
      "Epoch: 193, Loss: 2139.85961, Residuals: -1.02286, Convergence: 0.001879\n",
      "Epoch: 194, Loss: 2136.13279, Residuals: -1.02022, Convergence: 0.001745\n",
      "Epoch: 195, Loss: 2132.67743, Residuals: -1.01767, Convergence: 0.001620\n",
      "Epoch: 196, Loss: 2129.47299, Residuals: -1.01524, Convergence: 0.001505\n",
      "Epoch: 197, Loss: 2126.50122, Residuals: -1.01292, Convergence: 0.001397\n",
      "Epoch: 198, Loss: 2123.74401, Residuals: -1.01071, Convergence: 0.001298\n",
      "Epoch: 199, Loss: 2121.18469, Residuals: -1.00860, Convergence: 0.001207\n",
      "Epoch: 200, Loss: 2118.80638, Residuals: -1.00660, Convergence: 0.001122\n",
      "Epoch: 201, Loss: 2116.59351, Residuals: -1.00469, Convergence: 0.001045\n",
      "Epoch: 202, Loss: 2114.53211, Residuals: -1.00288, Convergence: 0.000975\n",
      "Evidence 14375.140\n",
      "\n",
      "Epoch: 202, Evidence: 14375.13965, Convergence: 0.219535\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 4.35e-01\n",
      "Epoch: 202, Loss: 2488.34575, Residuals: -1.00288, Convergence:   inf\n",
      "Epoch: 203, Loss: 2474.72168, Residuals: -1.00007, Convergence: 0.005505\n",
      "Epoch: 204, Loss: 2463.59101, Residuals: -0.99677, Convergence: 0.004518\n",
      "Epoch: 205, Loss: 2454.00567, Residuals: -0.99357, Convergence: 0.003906\n",
      "Epoch: 206, Loss: 2445.69530, Residuals: -0.99057, Convergence: 0.003398\n",
      "Epoch: 207, Loss: 2438.45252, Residuals: -0.98780, Convergence: 0.002970\n",
      "Epoch: 208, Loss: 2432.10835, Residuals: -0.98525, Convergence: 0.002609\n",
      "Epoch: 209, Loss: 2426.52374, Residuals: -0.98292, Convergence: 0.002301\n",
      "Epoch: 210, Loss: 2421.58292, Residuals: -0.98077, Convergence: 0.002040\n",
      "Epoch: 211, Loss: 2417.19036, Residuals: -0.97879, Convergence: 0.001817\n",
      "Epoch: 212, Loss: 2413.26322, Residuals: -0.97696, Convergence: 0.001627\n",
      "Epoch: 213, Loss: 2409.73385, Residuals: -0.97528, Convergence: 0.001465\n",
      "Epoch: 214, Loss: 2406.54631, Residuals: -0.97373, Convergence: 0.001325\n",
      "Epoch: 215, Loss: 2403.65136, Residuals: -0.97230, Convergence: 0.001204\n",
      "Epoch: 216, Loss: 2401.00929, Residuals: -0.97098, Convergence: 0.001100\n",
      "Epoch: 217, Loss: 2398.58723, Residuals: -0.96977, Convergence: 0.001010\n",
      "Epoch: 218, Loss: 2396.35632, Residuals: -0.96866, Convergence: 0.000931\n",
      "Evidence 14748.298\n",
      "\n",
      "Epoch: 218, Evidence: 14748.29785, Convergence: 0.025302\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 3.32e-01\n",
      "Epoch: 218, Loss: 2493.40108, Residuals: -0.96866, Convergence:   inf\n",
      "Epoch: 219, Loss: 2486.72297, Residuals: -0.96550, Convergence: 0.002686\n",
      "Epoch: 220, Loss: 2481.20806, Residuals: -0.96278, Convergence: 0.002223\n",
      "Epoch: 221, Loss: 2476.53610, Residuals: -0.96050, Convergence: 0.001886\n",
      "Epoch: 222, Loss: 2472.52005, Residuals: -0.95857, Convergence: 0.001624\n",
      "Epoch: 223, Loss: 2469.02456, Residuals: -0.95692, Convergence: 0.001416\n",
      "Epoch: 224, Loss: 2465.94712, Residuals: -0.95551, Convergence: 0.001248\n",
      "Epoch: 225, Loss: 2463.20855, Residuals: -0.95429, Convergence: 0.001112\n",
      "Epoch: 226, Loss: 2460.74866, Residuals: -0.95325, Convergence: 0.001000\n",
      "Evidence 14831.683\n",
      "\n",
      "Epoch: 226, Evidence: 14831.68262, Convergence: 0.005622\n",
      "Updating hyper-parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 183, Updated regularization: 2.60e-01\n",
      "Epoch: 226, Loss: 2495.05692, Residuals: -0.95325, Convergence:   inf\n",
      "Epoch: 227, Loss: 2490.91261, Residuals: -0.95100, Convergence: 0.001664\n",
      "Epoch: 228, Loss: 2487.47555, Residuals: -0.94924, Convergence: 0.001382\n",
      "Epoch: 229, Loss: 2484.53787, Residuals: -0.94782, Convergence: 0.001182\n",
      "Epoch: 230, Loss: 2481.98118, Residuals: -0.94665, Convergence: 0.001030\n",
      "Epoch: 231, Loss: 2479.72143, Residuals: -0.94567, Convergence: 0.000911\n",
      "Evidence 14863.040\n",
      "\n",
      "Epoch: 231, Evidence: 14863.04004, Convergence: 0.002110\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.09e-01\n",
      "Epoch: 231, Loss: 2495.88224, Residuals: -0.94567, Convergence:   inf\n",
      "Epoch: 232, Loss: 2492.91144, Residuals: -0.94403, Convergence: 0.001192\n",
      "Epoch: 233, Loss: 2490.41888, Residuals: -0.94276, Convergence: 0.001001\n",
      "Epoch: 234, Loss: 2488.25922, Residuals: -0.94173, Convergence: 0.000868\n",
      "Evidence 14878.510\n",
      "\n",
      "Epoch: 234, Evidence: 14878.50977, Convergence: 0.001040\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.72e-01\n",
      "Epoch: 234, Loss: 2496.46821, Residuals: -0.94173, Convergence:   inf\n",
      "Epoch: 235, Loss: 2494.07340, Residuals: -0.94030, Convergence: 0.000960\n",
      "Evidence 14885.058\n",
      "\n",
      "Epoch: 235, Evidence: 14885.05762, Convergence: 0.000440\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.46e-01\n",
      "Epoch: 235, Loss: 2496.97907, Residuals: -0.94030, Convergence:   inf\n",
      "Epoch: 236, Loss: 2492.86879, Residuals: -0.93819, Convergence: 0.001649\n",
      "Epoch: 237, Loss: 2489.71535, Residuals: -0.93672, Convergence: 0.001267\n",
      "Epoch: 238, Loss: 2487.15831, Residuals: -0.93569, Convergence: 0.001028\n",
      "Epoch: 239, Loss: 2484.99251, Residuals: -0.93512, Convergence: 0.000872\n",
      "Evidence 14900.268\n",
      "\n",
      "Epoch: 239, Evidence: 14900.26758, Convergence: 0.001460\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.23e-01\n",
      "Epoch: 239, Loss: 2496.99588, Residuals: -0.93512, Convergence:   inf\n",
      "Epoch: 240, Loss: 2494.21185, Residuals: -0.93307, Convergence: 0.001116\n",
      "Epoch: 241, Loss: 2491.99148, Residuals: -0.93193, Convergence: 0.000891\n",
      "Evidence 14910.075\n",
      "\n",
      "Epoch: 241, Evidence: 14910.07520, Convergence: 0.000658\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.06e-01\n",
      "Epoch: 241, Loss: 2497.06225, Residuals: -0.93193, Convergence:   inf\n",
      "Epoch: 242, Loss: 2492.92701, Residuals: -0.92871, Convergence: 0.001659\n",
      "Epoch: 243, Loss: 2489.94854, Residuals: -0.92894, Convergence: 0.001196\n",
      "Epoch: 244, Loss: 2487.44097, Residuals: -0.92951, Convergence: 0.001008\n",
      "Epoch: 245, Loss: 2485.28759, Residuals: -0.93187, Convergence: 0.000866\n",
      "Evidence 14924.604\n",
      "\n",
      "Epoch: 245, Evidence: 14924.60352, Convergence: 0.001631\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 9.59e-02\n",
      "Epoch: 245, Loss: 2496.25991, Residuals: -0.93187, Convergence:   inf\n",
      "Epoch: 246, Loss: 2494.73921, Residuals: -0.93039, Convergence: 0.000610\n",
      "Evidence 14930.681\n",
      "\n",
      "Epoch: 246, Evidence: 14930.68066, Convergence: 0.000407\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.89e-02\n",
      "Epoch: 246, Loss: 2497.11438, Residuals: -0.93039, Convergence:   inf\n",
      "Epoch: 247, Loss: 2547.55656, Residuals: -0.97699, Convergence: -0.019800\n",
      "Epoch: 247, Loss: 2494.85388, Residuals: -0.92740, Convergence: 0.000906\n",
      "Evidence 14934.836\n",
      "\n",
      "Epoch: 247, Evidence: 14934.83594, Convergence: 0.000685\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.37e-02\n",
      "Epoch: 247, Loss: 2496.44657, Residuals: -0.92740, Convergence:   inf\n",
      "Epoch: 248, Loss: 2501.11535, Residuals: -0.92704, Convergence: -0.001867\n",
      "Epoch: 248, Loss: 2496.45034, Residuals: -0.92501, Convergence: -0.000002\n",
      "Evidence 14936.222\n",
      "\n",
      "Epoch: 248, Evidence: 14936.22168, Convergence: 0.000778\n",
      "Total samples: 184, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 384.66923, Residuals: -4.52357, Convergence:   inf\n",
      "Epoch: 1, Loss: 358.83460, Residuals: -4.40263, Convergence: 0.071996\n",
      "Epoch: 2, Loss: 337.72573, Residuals: -4.23866, Convergence: 0.062503\n",
      "Epoch: 3, Loss: 321.58968, Residuals: -4.07444, Convergence: 0.050176\n",
      "Epoch: 4, Loss: 309.27375, Residuals: -3.92976, Convergence: 0.039822\n",
      "Epoch: 5, Loss: 299.50012, Residuals: -3.80185, Convergence: 0.032633\n",
      "Epoch: 6, Loss: 291.56002, Residuals: -3.69042, Convergence: 0.027233\n",
      "Epoch: 7, Loss: 284.96192, Residuals: -3.59494, Convergence: 0.023154\n",
      "Epoch: 8, Loss: 279.34252, Residuals: -3.51336, Convergence: 0.020117\n",
      "Epoch: 9, Loss: 274.44504, Residuals: -3.44331, Convergence: 0.017845\n",
      "Epoch: 10, Loss: 270.08653, Residuals: -3.38269, Convergence: 0.016137\n",
      "Epoch: 11, Loss: 266.13425, Residuals: -3.32970, Convergence: 0.014851\n",
      "Epoch: 12, Loss: 262.49114, Residuals: -3.28280, Convergence: 0.013879\n",
      "Epoch: 13, Loss: 259.08691, Residuals: -3.24068, Convergence: 0.013139\n",
      "Epoch: 14, Loss: 255.87253, Residuals: -3.20219, Convergence: 0.012562\n",
      "Epoch: 15, Loss: 252.81805, Residuals: -3.16641, Convergence: 0.012082\n",
      "Epoch: 16, Loss: 249.90962, Residuals: -3.13277, Convergence: 0.011638\n",
      "Epoch: 17, Loss: 247.13732, Residuals: -3.10090, Convergence: 0.011218\n",
      "Epoch: 18, Loss: 244.48045, Residuals: -3.07040, Convergence: 0.010867\n",
      "Epoch: 19, Loss: 241.90624, Residuals: -3.04074, Convergence: 0.010641\n",
      "Epoch: 20, Loss: 239.37842, Residuals: -3.01132, Convergence: 0.010560\n",
      "Epoch: 21, Loss: 236.86661, Residuals: -2.98167, Convergence: 0.010604\n",
      "Epoch: 22, Loss: 234.35011, Residuals: -2.95148, Convergence: 0.010738\n",
      "Epoch: 23, Loss: 231.80795, Residuals: -2.92052, Convergence: 0.010967\n",
      "Epoch: 24, Loss: 229.20178, Residuals: -2.88838, Convergence: 0.011371\n",
      "Epoch: 25, Loss: 226.47547, Residuals: -2.85441, Convergence: 0.012038\n",
      "Epoch: 26, Loss: 223.60367, Residuals: -2.81823, Convergence: 0.012843\n",
      "Epoch: 27, Loss: 220.67418, Residuals: -2.78071, Convergence: 0.013275\n",
      "Epoch: 28, Loss: 217.80986, Residuals: -2.74324, Convergence: 0.013151\n",
      "Epoch: 29, Loss: 215.04733, Residuals: -2.70635, Convergence: 0.012846\n",
      "Epoch: 30, Loss: 212.37605, Residuals: -2.67003, Convergence: 0.012578\n",
      "Epoch: 31, Loss: 209.77957, Residuals: -2.63415, Convergence: 0.012377\n",
      "Epoch: 32, Loss: 207.24536, Residuals: -2.59862, Convergence: 0.012228\n",
      "Epoch: 33, Loss: 204.76550, Residuals: -2.56333, Convergence: 0.012111\n",
      "Epoch: 34, Loss: 202.33556, Residuals: -2.52823, Convergence: 0.012009\n",
      "Epoch: 35, Loss: 199.95362, Residuals: -2.49329, Convergence: 0.011912\n",
      "Epoch: 36, Loss: 197.61935, Residuals: -2.45848, Convergence: 0.011812\n",
      "Epoch: 37, Loss: 195.33338, Residuals: -2.42378, Convergence: 0.011703\n",
      "Epoch: 38, Loss: 193.09679, Residuals: -2.38919, Convergence: 0.011583\n",
      "Epoch: 39, Loss: 190.91085, Residuals: -2.35471, Convergence: 0.011450\n",
      "Epoch: 40, Loss: 188.77677, Residuals: -2.32036, Convergence: 0.011305\n",
      "Epoch: 41, Loss: 186.69566, Residuals: -2.28616, Convergence: 0.011147\n",
      "Epoch: 42, Loss: 184.66848, Residuals: -2.25211, Convergence: 0.010977\n",
      "Epoch: 43, Loss: 182.69617, Residuals: -2.21826, Convergence: 0.010796\n",
      "Epoch: 44, Loss: 180.77977, Residuals: -2.18463, Convergence: 0.010601\n",
      "Epoch: 45, Loss: 178.92044, Residuals: -2.15125, Convergence: 0.010392\n",
      "Epoch: 46, Loss: 177.11951, Residuals: -2.11816, Convergence: 0.010168\n",
      "Epoch: 47, Loss: 175.37830, Residuals: -2.08541, Convergence: 0.009928\n",
      "Epoch: 48, Loss: 173.69799, Residuals: -2.05305, Convergence: 0.009674\n",
      "Epoch: 49, Loss: 172.07936, Residuals: -2.02110, Convergence: 0.009406\n",
      "Epoch: 50, Loss: 170.52276, Residuals: -1.98963, Convergence: 0.009128\n",
      "Epoch: 51, Loss: 169.02797, Residuals: -1.95866, Convergence: 0.008843\n",
      "Epoch: 52, Loss: 167.59435, Residuals: -1.92824, Convergence: 0.008554\n",
      "Epoch: 53, Loss: 166.22086, Residuals: -1.89840, Convergence: 0.008263\n",
      "Epoch: 54, Loss: 164.90622, Residuals: -1.86916, Convergence: 0.007972\n",
      "Epoch: 55, Loss: 163.64890, Residuals: -1.84056, Convergence: 0.007683\n",
      "Epoch: 56, Loss: 162.44732, Residuals: -1.81262, Convergence: 0.007397\n",
      "Epoch: 57, Loss: 161.29982, Residuals: -1.78535, Convergence: 0.007114\n",
      "Epoch: 58, Loss: 160.20474, Residuals: -1.75878, Convergence: 0.006835\n",
      "Epoch: 59, Loss: 159.16044, Residuals: -1.73292, Convergence: 0.006561\n",
      "Epoch: 60, Loss: 158.16534, Residuals: -1.70778, Convergence: 0.006292\n",
      "Epoch: 61, Loss: 157.21788, Residuals: -1.68338, Convergence: 0.006026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62, Loss: 156.31658, Residuals: -1.65974, Convergence: 0.005766\n",
      "Epoch: 63, Loss: 155.45996, Residuals: -1.63685, Convergence: 0.005510\n",
      "Epoch: 64, Loss: 154.64658, Residuals: -1.61472, Convergence: 0.005260\n",
      "Epoch: 65, Loss: 153.87493, Residuals: -1.59336, Convergence: 0.005015\n",
      "Epoch: 66, Loss: 153.14348, Residuals: -1.57277, Convergence: 0.004776\n",
      "Epoch: 67, Loss: 152.45058, Residuals: -1.55295, Convergence: 0.004545\n",
      "Epoch: 68, Loss: 151.79446, Residuals: -1.53390, Convergence: 0.004322\n",
      "Epoch: 69, Loss: 151.17319, Residuals: -1.51559, Convergence: 0.004110\n",
      "Epoch: 70, Loss: 150.58473, Residuals: -1.49802, Convergence: 0.003908\n",
      "Epoch: 71, Loss: 150.02692, Residuals: -1.48116, Convergence: 0.003718\n",
      "Epoch: 72, Loss: 149.49761, Residuals: -1.46498, Convergence: 0.003541\n",
      "Epoch: 73, Loss: 148.99469, Residuals: -1.44946, Convergence: 0.003375\n",
      "Epoch: 74, Loss: 148.51619, Residuals: -1.43455, Convergence: 0.003222\n",
      "Epoch: 75, Loss: 148.06036, Residuals: -1.42024, Convergence: 0.003079\n",
      "Epoch: 76, Loss: 147.62566, Residuals: -1.40649, Convergence: 0.002945\n",
      "Epoch: 77, Loss: 147.21076, Residuals: -1.39327, Convergence: 0.002818\n",
      "Epoch: 78, Loss: 146.81452, Residuals: -1.38058, Convergence: 0.002699\n",
      "Epoch: 79, Loss: 146.43596, Residuals: -1.36838, Convergence: 0.002585\n",
      "Epoch: 80, Loss: 146.07421, Residuals: -1.35666, Convergence: 0.002476\n",
      "Epoch: 81, Loss: 145.72849, Residuals: -1.34541, Convergence: 0.002372\n",
      "Epoch: 82, Loss: 145.39811, Residuals: -1.33460, Convergence: 0.002272\n",
      "Epoch: 83, Loss: 145.08240, Residuals: -1.32423, Convergence: 0.002176\n",
      "Epoch: 84, Loss: 144.78073, Residuals: -1.31428, Convergence: 0.002084\n",
      "Epoch: 85, Loss: 144.49253, Residuals: -1.30473, Convergence: 0.001995\n",
      "Epoch: 86, Loss: 144.21724, Residuals: -1.29557, Convergence: 0.001909\n",
      "Epoch: 87, Loss: 143.95432, Residuals: -1.28679, Convergence: 0.001826\n",
      "Epoch: 88, Loss: 143.70328, Residuals: -1.27837, Convergence: 0.001747\n",
      "Epoch: 89, Loss: 143.46365, Residuals: -1.27030, Convergence: 0.001670\n",
      "Epoch: 90, Loss: 143.23499, Residuals: -1.26256, Convergence: 0.001596\n",
      "Epoch: 91, Loss: 143.01687, Residuals: -1.25515, Convergence: 0.001525\n",
      "Epoch: 92, Loss: 142.80892, Residuals: -1.24804, Convergence: 0.001456\n",
      "Epoch: 93, Loss: 142.61076, Residuals: -1.24124, Convergence: 0.001390\n",
      "Epoch: 94, Loss: 142.42207, Residuals: -1.23471, Convergence: 0.001325\n",
      "Epoch: 95, Loss: 142.24252, Residuals: -1.22846, Convergence: 0.001262\n",
      "Epoch: 96, Loss: 142.07185, Residuals: -1.22247, Convergence: 0.001201\n",
      "Epoch: 97, Loss: 141.90977, Residuals: -1.21673, Convergence: 0.001142\n",
      "Epoch: 98, Loss: 141.75604, Residuals: -1.21123, Convergence: 0.001084\n",
      "Epoch: 99, Loss: 141.61045, Residuals: -1.20596, Convergence: 0.001028\n",
      "Epoch: 100, Loss: 141.47279, Residuals: -1.20092, Convergence: 0.000973\n",
      "Evidence -182.733\n",
      "\n",
      "Epoch: 100, Evidence: -182.73289, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 7.24e-01\n",
      "Epoch: 100, Loss: 1377.06592, Residuals: -1.20092, Convergence:   inf\n",
      "Epoch: 101, Loss: 1314.92759, Residuals: -1.23069, Convergence: 0.047256\n",
      "Epoch: 102, Loss: 1268.25140, Residuals: -1.25404, Convergence: 0.036804\n",
      "Epoch: 103, Loss: 1233.28827, Residuals: -1.27055, Convergence: 0.028350\n",
      "Epoch: 104, Loss: 1206.14196, Residuals: -1.28194, Convergence: 0.022507\n",
      "Epoch: 105, Loss: 1184.18332, Residuals: -1.29023, Convergence: 0.018543\n",
      "Epoch: 106, Loss: 1165.95505, Residuals: -1.29650, Convergence: 0.015634\n",
      "Epoch: 107, Loss: 1150.58758, Residuals: -1.30120, Convergence: 0.013356\n",
      "Epoch: 108, Loss: 1137.48793, Residuals: -1.30459, Convergence: 0.011516\n",
      "Epoch: 109, Loss: 1126.21586, Residuals: -1.30683, Convergence: 0.010009\n",
      "Epoch: 110, Loss: 1116.42717, Residuals: -1.30807, Convergence: 0.008768\n",
      "Epoch: 111, Loss: 1107.84530, Residuals: -1.30842, Convergence: 0.007746\n",
      "Epoch: 112, Loss: 1100.24389, Residuals: -1.30798, Convergence: 0.006909\n",
      "Epoch: 113, Loss: 1093.43449, Residuals: -1.30684, Convergence: 0.006228\n",
      "Epoch: 114, Loss: 1087.25876, Residuals: -1.30507, Convergence: 0.005680\n",
      "Epoch: 115, Loss: 1081.58248, Residuals: -1.30274, Convergence: 0.005248\n",
      "Epoch: 116, Loss: 1076.29311, Residuals: -1.29989, Convergence: 0.004914\n",
      "Epoch: 117, Loss: 1071.29749, Residuals: -1.29656, Convergence: 0.004663\n",
      "Epoch: 118, Loss: 1066.52188, Residuals: -1.29278, Convergence: 0.004478\n",
      "Epoch: 119, Loss: 1061.91144, Residuals: -1.28860, Convergence: 0.004342\n",
      "Epoch: 120, Loss: 1057.42868, Residuals: -1.28402, Convergence: 0.004239\n",
      "Epoch: 121, Loss: 1053.04869, Residuals: -1.27907, Convergence: 0.004159\n",
      "Epoch: 122, Loss: 1048.75274, Residuals: -1.27377, Convergence: 0.004096\n",
      "Epoch: 123, Loss: 1044.52248, Residuals: -1.26814, Convergence: 0.004050\n",
      "Epoch: 124, Loss: 1040.33689, Residuals: -1.26220, Convergence: 0.004023\n",
      "Epoch: 125, Loss: 1036.17208, Residuals: -1.25595, Convergence: 0.004019\n",
      "Epoch: 126, Loss: 1032.00984, Residuals: -1.24944, Convergence: 0.004033\n",
      "Epoch: 127, Loss: 1027.84850, Residuals: -1.24272, Convergence: 0.004049\n",
      "Epoch: 128, Loss: 1023.71158, Residuals: -1.23584, Convergence: 0.004041\n",
      "Epoch: 129, Loss: 1019.64718, Residuals: -1.22887, Convergence: 0.003986\n",
      "Epoch: 130, Loss: 1015.71435, Residuals: -1.22189, Convergence: 0.003872\n",
      "Epoch: 131, Loss: 1011.96269, Residuals: -1.21493, Convergence: 0.003707\n",
      "Epoch: 132, Loss: 1008.42133, Residuals: -1.20804, Convergence: 0.003512\n",
      "Epoch: 133, Loss: 1005.09822, Residuals: -1.20125, Convergence: 0.003306\n",
      "Epoch: 134, Loss: 1001.98545, Residuals: -1.19459, Convergence: 0.003107\n",
      "Epoch: 135, Loss: 999.06858, Residuals: -1.18807, Convergence: 0.002920\n",
      "Epoch: 136, Loss: 996.32952, Residuals: -1.18172, Convergence: 0.002749\n",
      "Epoch: 137, Loss: 993.75164, Residuals: -1.17556, Convergence: 0.002594\n",
      "Epoch: 138, Loss: 991.31906, Residuals: -1.16958, Convergence: 0.002454\n",
      "Epoch: 139, Loss: 989.01807, Residuals: -1.16381, Convergence: 0.002327\n",
      "Epoch: 140, Loss: 986.83726, Residuals: -1.15825, Convergence: 0.002210\n",
      "Epoch: 141, Loss: 984.76613, Residuals: -1.15290, Convergence: 0.002103\n",
      "Epoch: 142, Loss: 982.79566, Residuals: -1.14776, Convergence: 0.002005\n",
      "Epoch: 143, Loss: 980.91764, Residuals: -1.14283, Convergence: 0.001915\n",
      "Epoch: 144, Loss: 979.12512, Residuals: -1.13811, Convergence: 0.001831\n",
      "Epoch: 145, Loss: 977.41163, Residuals: -1.13358, Convergence: 0.001753\n",
      "Epoch: 146, Loss: 975.77102, Residuals: -1.12925, Convergence: 0.001681\n",
      "Epoch: 147, Loss: 974.19784, Residuals: -1.12511, Convergence: 0.001615\n",
      "Epoch: 148, Loss: 972.68727, Residuals: -1.12114, Convergence: 0.001553\n",
      "Epoch: 149, Loss: 971.23447, Residuals: -1.11735, Convergence: 0.001496\n",
      "Epoch: 150, Loss: 969.83509, Residuals: -1.11372, Convergence: 0.001443\n",
      "Epoch: 151, Loss: 968.48537, Residuals: -1.11024, Convergence: 0.001394\n",
      "Epoch: 152, Loss: 967.18187, Residuals: -1.10691, Convergence: 0.001348\n",
      "Epoch: 153, Loss: 965.92070, Residuals: -1.10372, Convergence: 0.001306\n",
      "Epoch: 154, Loss: 964.69908, Residuals: -1.10066, Convergence: 0.001266\n",
      "Epoch: 155, Loss: 963.51402, Residuals: -1.09772, Convergence: 0.001230\n",
      "Epoch: 156, Loss: 962.36257, Residuals: -1.09489, Convergence: 0.001196\n",
      "Epoch: 157, Loss: 961.24195, Residuals: -1.09217, Convergence: 0.001166\n",
      "Epoch: 158, Loss: 960.14959, Residuals: -1.08954, Convergence: 0.001138\n",
      "Epoch: 159, Loss: 959.08232, Residuals: -1.08700, Convergence: 0.001113\n",
      "Epoch: 160, Loss: 958.03766, Residuals: -1.08455, Convergence: 0.001090\n",
      "Epoch: 161, Loss: 957.01249, Residuals: -1.08216, Convergence: 0.001071\n",
      "Epoch: 162, Loss: 956.00395, Residuals: -1.07984, Convergence: 0.001055\n",
      "Epoch: 163, Loss: 955.00886, Residuals: -1.07758, Convergence: 0.001042\n",
      "Epoch: 164, Loss: 954.02447, Residuals: -1.07536, Convergence: 0.001032\n",
      "Epoch: 165, Loss: 953.04772, Residuals: -1.07318, Convergence: 0.001025\n",
      "Epoch: 166, Loss: 952.07601, Residuals: -1.07103, Convergence: 0.001021\n",
      "Epoch: 167, Loss: 951.10784, Residuals: -1.06890, Convergence: 0.001018\n",
      "Epoch: 168, Loss: 950.14168, Residuals: -1.06679, Convergence: 0.001017\n",
      "Epoch: 169, Loss: 949.17795, Residuals: -1.06469, Convergence: 0.001015\n",
      "Epoch: 170, Loss: 948.21807, Residuals: -1.06261, Convergence: 0.001012\n",
      "Epoch: 171, Loss: 947.26448, Residuals: -1.06054, Convergence: 0.001007\n",
      "Epoch: 172, Loss: 946.32188, Residuals: -1.05850, Convergence: 0.000996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence 11335.848\n",
      "\n",
      "Epoch: 172, Evidence: 11335.84766, Convergence: 1.016120\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 5.73e-01\n",
      "Epoch: 172, Loss: 2359.53218, Residuals: -1.05850, Convergence:   inf\n",
      "Epoch: 173, Loss: 2319.55857, Residuals: -1.06720, Convergence: 0.017233\n",
      "Epoch: 174, Loss: 2291.21250, Residuals: -1.06546, Convergence: 0.012372\n",
      "Epoch: 175, Loss: 2267.59628, Residuals: -1.06288, Convergence: 0.010415\n",
      "Epoch: 176, Loss: 2247.70716, Residuals: -1.06011, Convergence: 0.008849\n",
      "Epoch: 177, Loss: 2230.84283, Residuals: -1.05727, Convergence: 0.007560\n",
      "Epoch: 178, Loss: 2216.45558, Residuals: -1.05441, Convergence: 0.006491\n",
      "Epoch: 179, Loss: 2204.09452, Residuals: -1.05156, Convergence: 0.005608\n",
      "Epoch: 180, Loss: 2193.37980, Residuals: -1.04872, Convergence: 0.004885\n",
      "Epoch: 181, Loss: 2183.99153, Residuals: -1.04589, Convergence: 0.004299\n",
      "Epoch: 182, Loss: 2175.65681, Residuals: -1.04307, Convergence: 0.003831\n",
      "Epoch: 183, Loss: 2168.14747, Residuals: -1.04021, Convergence: 0.003463\n",
      "Epoch: 184, Loss: 2161.27834, Residuals: -1.03730, Convergence: 0.003178\n",
      "Epoch: 185, Loss: 2154.91133, Residuals: -1.03431, Convergence: 0.002955\n",
      "Epoch: 186, Loss: 2148.95601, Residuals: -1.03123, Convergence: 0.002771\n",
      "Epoch: 187, Loss: 2143.36524, Residuals: -1.02809, Convergence: 0.002608\n",
      "Epoch: 188, Loss: 2138.11738, Residuals: -1.02491, Convergence: 0.002454\n",
      "Epoch: 189, Loss: 2133.20013, Residuals: -1.02174, Convergence: 0.002305\n",
      "Epoch: 190, Loss: 2128.60307, Residuals: -1.01862, Convergence: 0.002160\n",
      "Epoch: 191, Loss: 2124.30982, Residuals: -1.01558, Convergence: 0.002021\n",
      "Epoch: 192, Loss: 2120.30252, Residuals: -1.01263, Convergence: 0.001890\n",
      "Epoch: 193, Loss: 2116.55976, Residuals: -1.00980, Convergence: 0.001768\n",
      "Epoch: 194, Loss: 2113.06202, Residuals: -1.00709, Convergence: 0.001655\n",
      "Epoch: 195, Loss: 2109.78912, Residuals: -1.00452, Convergence: 0.001551\n",
      "Epoch: 196, Loss: 2106.72206, Residuals: -1.00207, Convergence: 0.001456\n",
      "Epoch: 197, Loss: 2103.84520, Residuals: -0.99976, Convergence: 0.001367\n",
      "Epoch: 198, Loss: 2101.14371, Residuals: -0.99757, Convergence: 0.001286\n",
      "Epoch: 199, Loss: 2098.60429, Residuals: -0.99551, Convergence: 0.001210\n",
      "Epoch: 200, Loss: 2096.21627, Residuals: -0.99357, Convergence: 0.001139\n",
      "Epoch: 201, Loss: 2093.96881, Residuals: -0.99174, Convergence: 0.001073\n",
      "Epoch: 202, Loss: 2091.85394, Residuals: -0.99002, Convergence: 0.001011\n",
      "Epoch: 203, Loss: 2089.86215, Residuals: -0.98839, Convergence: 0.000953\n",
      "Evidence 14561.438\n",
      "\n",
      "Epoch: 203, Evidence: 14561.43848, Convergence: 0.221516\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 4.34e-01\n",
      "Epoch: 203, Loss: 2475.65125, Residuals: -0.98839, Convergence:   inf\n",
      "Epoch: 204, Loss: 2461.67075, Residuals: -0.98514, Convergence: 0.005679\n",
      "Epoch: 205, Loss: 2450.31146, Residuals: -0.98138, Convergence: 0.004636\n",
      "Epoch: 206, Loss: 2440.59139, Residuals: -0.97780, Convergence: 0.003983\n",
      "Epoch: 207, Loss: 2432.21792, Residuals: -0.97448, Convergence: 0.003443\n",
      "Epoch: 208, Loss: 2424.96216, Residuals: -0.97143, Convergence: 0.002992\n",
      "Epoch: 209, Loss: 2418.63882, Residuals: -0.96865, Convergence: 0.002614\n",
      "Epoch: 210, Loss: 2413.09692, Residuals: -0.96612, Convergence: 0.002297\n",
      "Epoch: 211, Loss: 2408.21161, Residuals: -0.96382, Convergence: 0.002029\n",
      "Epoch: 212, Loss: 2403.87859, Residuals: -0.96173, Convergence: 0.001803\n",
      "Epoch: 213, Loss: 2400.01139, Residuals: -0.95981, Convergence: 0.001611\n",
      "Epoch: 214, Loss: 2396.54049, Residuals: -0.95806, Convergence: 0.001448\n",
      "Epoch: 215, Loss: 2393.40512, Residuals: -0.95645, Convergence: 0.001310\n",
      "Epoch: 216, Loss: 2390.55802, Residuals: -0.95497, Convergence: 0.001191\n",
      "Epoch: 217, Loss: 2387.95932, Residuals: -0.95361, Convergence: 0.001088\n",
      "Epoch: 218, Loss: 2385.57561, Residuals: -0.95235, Convergence: 0.000999\n",
      "Evidence 14971.854\n",
      "\n",
      "Epoch: 218, Evidence: 14971.85352, Convergence: 0.027412\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 3.31e-01\n",
      "Epoch: 218, Loss: 2481.03256, Residuals: -0.95235, Convergence:   inf\n",
      "Epoch: 219, Loss: 2474.50263, Residuals: -0.94905, Convergence: 0.002639\n",
      "Epoch: 220, Loss: 2469.11613, Residuals: -0.94616, Convergence: 0.002182\n",
      "Epoch: 221, Loss: 2464.55598, Residuals: -0.94372, Convergence: 0.001850\n",
      "Epoch: 222, Loss: 2460.63739, Residuals: -0.94163, Convergence: 0.001593\n",
      "Epoch: 223, Loss: 2457.22455, Residuals: -0.93985, Convergence: 0.001389\n",
      "Epoch: 224, Loss: 2454.21503, Residuals: -0.93833, Convergence: 0.001226\n",
      "Epoch: 225, Loss: 2451.53014, Residuals: -0.93703, Convergence: 0.001095\n",
      "Epoch: 226, Loss: 2449.11204, Residuals: -0.93591, Convergence: 0.000987\n",
      "Evidence 15052.028\n",
      "\n",
      "Epoch: 226, Evidence: 15052.02832, Convergence: 0.005327\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 2.59e-01\n",
      "Epoch: 226, Loss: 2482.71687, Residuals: -0.93591, Convergence:   inf\n",
      "Epoch: 227, Loss: 2478.80416, Residuals: -0.93337, Convergence: 0.001578\n",
      "Epoch: 228, Loss: 2475.54485, Residuals: -0.93134, Convergence: 0.001317\n",
      "Epoch: 229, Loss: 2472.75071, Residuals: -0.92970, Convergence: 0.001130\n",
      "Epoch: 230, Loss: 2470.30629, Residuals: -0.92838, Convergence: 0.000990\n",
      "Evidence 15079.822\n",
      "\n",
      "Epoch: 230, Evidence: 15079.82227, Convergence: 0.001843\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 2.09e-01\n",
      "Epoch: 230, Loss: 2483.73152, Residuals: -0.92838, Convergence:   inf\n",
      "Epoch: 231, Loss: 2480.81429, Residuals: -0.92628, Convergence: 0.001176\n",
      "Epoch: 232, Loss: 2478.35723, Residuals: -0.92465, Convergence: 0.000991\n",
      "Evidence 15091.891\n",
      "\n",
      "Epoch: 232, Evidence: 15091.89062, Convergence: 0.000800\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.74e-01\n",
      "Epoch: 232, Loss: 2484.44881, Residuals: -0.92465, Convergence:   inf\n",
      "Epoch: 233, Loss: 2479.80652, Residuals: -0.92156, Convergence: 0.001872\n",
      "Epoch: 234, Loss: 2476.27429, Residuals: -0.91958, Convergence: 0.001426\n",
      "Epoch: 235, Loss: 2473.41678, Residuals: -0.91838, Convergence: 0.001155\n",
      "Epoch: 236, Loss: 2470.99687, Residuals: -0.91782, Convergence: 0.000979\n",
      "Evidence 15109.907\n",
      "\n",
      "Epoch: 236, Evidence: 15109.90723, Convergence: 0.001991\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.44e-01\n",
      "Epoch: 236, Loss: 2484.64223, Residuals: -0.91782, Convergence:   inf\n",
      "Epoch: 237, Loss: 2481.58896, Residuals: -0.91516, Convergence: 0.001230\n",
      "Epoch: 238, Loss: 2479.19021, Residuals: -0.91392, Convergence: 0.000968\n",
      "Evidence 15121.001\n",
      "\n",
      "Epoch: 238, Evidence: 15121.00098, Convergence: 0.000734\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.22e-01\n",
      "Epoch: 238, Loss: 2484.82591, Residuals: -0.91392, Convergence:   inf\n",
      "Epoch: 239, Loss: 2480.46902, Residuals: -0.91016, Convergence: 0.001756\n",
      "Epoch: 240, Loss: 2477.35611, Residuals: -0.91142, Convergence: 0.001257\n",
      "Epoch: 241, Loss: 2474.72049, Residuals: -0.91262, Convergence: 0.001065\n",
      "Epoch: 242, Loss: 2472.39126, Residuals: -0.91538, Convergence: 0.000942\n",
      "Evidence 15136.796\n",
      "\n",
      "Epoch: 242, Evidence: 15136.79590, Convergence: 0.001776\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.07e-01\n",
      "Epoch: 242, Loss: 2484.18431, Residuals: -0.91538, Convergence:   inf\n",
      "Epoch: 243, Loss: 2482.58604, Residuals: -0.91216, Convergence: 0.000644\n",
      "Evidence 15143.533\n",
      "\n",
      "Epoch: 243, Evidence: 15143.53320, Convergence: 0.000445\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 8.83e-02\n",
      "Epoch: 243, Loss: 2485.02274, Residuals: -0.91216, Convergence:   inf\n",
      "Epoch: 244, Loss: 2524.21562, Residuals: -0.95401, Convergence: -0.015527\n",
      "Epoch: 244, Loss: 2483.19305, Residuals: -0.91006, Convergence: 0.000737\n",
      "Evidence 15147.426\n",
      "\n",
      "Epoch: 244, Evidence: 15147.42578, Convergence: 0.000702\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 8.12e-02\n",
      "Epoch: 244, Loss: 2484.51547, Residuals: -0.91006, Convergence:   inf\n",
      "Epoch: 245, Loss: 2490.11605, Residuals: -0.91363, Convergence: -0.002249\n",
      "Epoch: 245, Loss: 2484.74286, Residuals: -0.90887, Convergence: -0.000092\n",
      "Evidence 15148.919\n",
      "\n",
      "Epoch: 245, Evidence: 15148.91895, Convergence: 0.000800\n",
      "Total samples: 183, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 384.79000, Residuals: -4.55991, Convergence:   inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 358.90793, Residuals: -4.43874, Convergence: 0.072113\n",
      "Epoch: 2, Loss: 337.66412, Residuals: -4.27253, Convergence: 0.062914\n",
      "Epoch: 3, Loss: 321.45986, Residuals: -4.10633, Convergence: 0.050408\n",
      "Epoch: 4, Loss: 309.10736, Residuals: -3.96010, Convergence: 0.039962\n",
      "Epoch: 5, Loss: 299.30924, Residuals: -3.83096, Convergence: 0.032736\n",
      "Epoch: 6, Loss: 291.35092, Residuals: -3.71851, Convergence: 0.027315\n",
      "Epoch: 7, Loss: 284.74072, Residuals: -3.62220, Convergence: 0.023215\n",
      "Epoch: 8, Loss: 279.11539, Residuals: -3.53995, Convergence: 0.020154\n",
      "Epoch: 9, Loss: 274.21839, Residuals: -3.46941, Convergence: 0.017858\n",
      "Epoch: 10, Loss: 269.86745, Residuals: -3.40846, Convergence: 0.016123\n",
      "Epoch: 11, Loss: 265.93063, Residuals: -3.35529, Convergence: 0.014804\n",
      "Epoch: 12, Loss: 262.31173, Residuals: -3.30834, Convergence: 0.013796\n",
      "Epoch: 13, Loss: 258.94075, Residuals: -3.26627, Convergence: 0.013018\n",
      "Epoch: 14, Loss: 255.76766, Residuals: -3.22792, Convergence: 0.012406\n",
      "Epoch: 15, Loss: 252.75929, Residuals: -3.19233, Convergence: 0.011902\n",
      "Epoch: 16, Loss: 249.89725, Residuals: -3.15885, Convergence: 0.011453\n",
      "Epoch: 17, Loss: 247.16916, Residuals: -3.12709, Convergence: 0.011037\n",
      "Epoch: 18, Loss: 244.55501, Residuals: -3.09669, Convergence: 0.010689\n",
      "Epoch: 19, Loss: 242.02309, Residuals: -3.06716, Convergence: 0.010461\n",
      "Epoch: 20, Loss: 239.53623, Residuals: -3.03792, Convergence: 0.010382\n",
      "Epoch: 21, Loss: 237.05987, Residuals: -3.00843, Convergence: 0.010446\n",
      "Epoch: 22, Loss: 234.56634, Residuals: -2.97828, Convergence: 0.010630\n",
      "Epoch: 23, Loss: 232.02779, Residuals: -2.94714, Convergence: 0.010941\n",
      "Epoch: 24, Loss: 229.40002, Residuals: -2.91451, Convergence: 0.011455\n",
      "Epoch: 25, Loss: 226.62823, Residuals: -2.87976, Convergence: 0.012231\n",
      "Epoch: 26, Loss: 223.72181, Residuals: -2.84288, Convergence: 0.012991\n",
      "Epoch: 27, Loss: 220.80882, Residuals: -2.80521, Convergence: 0.013192\n",
      "Epoch: 28, Loss: 217.98789, Residuals: -2.76795, Convergence: 0.012941\n",
      "Epoch: 29, Loss: 215.26838, Residuals: -2.73135, Convergence: 0.012633\n",
      "Epoch: 30, Loss: 212.63219, Residuals: -2.69532, Convergence: 0.012398\n",
      "Epoch: 31, Loss: 210.06193, Residuals: -2.65970, Convergence: 0.012236\n",
      "Epoch: 32, Loss: 207.54568, Residuals: -2.62438, Convergence: 0.012124\n",
      "Epoch: 33, Loss: 205.07656, Residuals: -2.58927, Convergence: 0.012040\n",
      "Epoch: 34, Loss: 202.65151, Residuals: -2.55430, Convergence: 0.011967\n",
      "Epoch: 35, Loss: 200.27008, Residuals: -2.51946, Convergence: 0.011891\n",
      "Epoch: 36, Loss: 197.93343, Residuals: -2.48473, Convergence: 0.011805\n",
      "Epoch: 37, Loss: 195.64364, Residuals: -2.45010, Convergence: 0.011704\n",
      "Epoch: 38, Loss: 193.40319, Residuals: -2.41559, Convergence: 0.011584\n",
      "Epoch: 39, Loss: 191.21454, Residuals: -2.38121, Convergence: 0.011446\n",
      "Epoch: 40, Loss: 189.07996, Residuals: -2.34698, Convergence: 0.011289\n",
      "Epoch: 41, Loss: 187.00143, Residuals: -2.31294, Convergence: 0.011115\n",
      "Epoch: 42, Loss: 184.98060, Residuals: -2.27910, Convergence: 0.010925\n",
      "Epoch: 43, Loss: 183.01888, Residuals: -2.24551, Convergence: 0.010719\n",
      "Epoch: 44, Loss: 181.11754, Residuals: -2.21221, Convergence: 0.010498\n",
      "Epoch: 45, Loss: 179.27781, Residuals: -2.17923, Convergence: 0.010262\n",
      "Epoch: 46, Loss: 177.50095, Residuals: -2.14662, Convergence: 0.010010\n",
      "Epoch: 47, Loss: 175.78822, Residuals: -2.11443, Convergence: 0.009743\n",
      "Epoch: 48, Loss: 174.14062, Residuals: -2.08272, Convergence: 0.009461\n",
      "Epoch: 49, Loss: 172.55874, Residuals: -2.05154, Convergence: 0.009167\n",
      "Epoch: 50, Loss: 171.04257, Residuals: -2.02092, Convergence: 0.008864\n",
      "Epoch: 51, Loss: 169.59145, Residuals: -1.99090, Convergence: 0.008557\n",
      "Epoch: 52, Loss: 168.20418, Residuals: -1.96151, Convergence: 0.008248\n",
      "Epoch: 53, Loss: 166.87919, Residuals: -1.93276, Convergence: 0.007940\n",
      "Epoch: 54, Loss: 165.61464, Residuals: -1.90467, Convergence: 0.007635\n",
      "Epoch: 55, Loss: 164.40862, Residuals: -1.87724, Convergence: 0.007335\n",
      "Epoch: 56, Loss: 163.25912, Residuals: -1.85050, Convergence: 0.007041\n",
      "Epoch: 57, Loss: 162.16407, Residuals: -1.82445, Convergence: 0.006753\n",
      "Epoch: 58, Loss: 161.12130, Residuals: -1.79911, Convergence: 0.006472\n",
      "Epoch: 59, Loss: 160.12854, Residuals: -1.77449, Convergence: 0.006200\n",
      "Epoch: 60, Loss: 159.18339, Residuals: -1.75057, Convergence: 0.005938\n",
      "Epoch: 61, Loss: 158.28334, Residuals: -1.72738, Convergence: 0.005686\n",
      "Epoch: 62, Loss: 157.42582, Residuals: -1.70490, Convergence: 0.005447\n",
      "Epoch: 63, Loss: 156.60823, Residuals: -1.68311, Convergence: 0.005221\n",
      "Epoch: 64, Loss: 155.82803, Residuals: -1.66201, Convergence: 0.005007\n",
      "Epoch: 65, Loss: 155.08284, Residuals: -1.64157, Convergence: 0.004805\n",
      "Epoch: 66, Loss: 154.37044, Residuals: -1.62176, Convergence: 0.004615\n",
      "Epoch: 67, Loss: 153.68889, Residuals: -1.60258, Convergence: 0.004435\n",
      "Epoch: 68, Loss: 153.03652, Residuals: -1.58400, Convergence: 0.004263\n",
      "Epoch: 69, Loss: 152.41193, Residuals: -1.56599, Convergence: 0.004098\n",
      "Epoch: 70, Loss: 151.81390, Residuals: -1.54856, Convergence: 0.003939\n",
      "Epoch: 71, Loss: 151.24144, Residuals: -1.53169, Convergence: 0.003785\n",
      "Epoch: 72, Loss: 150.69366, Residuals: -1.51536, Convergence: 0.003635\n",
      "Epoch: 73, Loss: 150.16981, Residuals: -1.49956, Convergence: 0.003488\n",
      "Epoch: 74, Loss: 149.66913, Residuals: -1.48430, Convergence: 0.003345\n",
      "Epoch: 75, Loss: 149.19093, Residuals: -1.46956, Convergence: 0.003205\n",
      "Epoch: 76, Loss: 148.73452, Residuals: -1.45533, Convergence: 0.003069\n",
      "Epoch: 77, Loss: 148.29919, Residuals: -1.44161, Convergence: 0.002935\n",
      "Epoch: 78, Loss: 147.88422, Residuals: -1.42838, Convergence: 0.002806\n",
      "Epoch: 79, Loss: 147.48888, Residuals: -1.41564, Convergence: 0.002680\n",
      "Epoch: 80, Loss: 147.11241, Residuals: -1.40337, Convergence: 0.002559\n",
      "Epoch: 81, Loss: 146.75407, Residuals: -1.39157, Convergence: 0.002442\n",
      "Epoch: 82, Loss: 146.41309, Residuals: -1.38022, Convergence: 0.002329\n",
      "Epoch: 83, Loss: 146.08874, Residuals: -1.36931, Convergence: 0.002220\n",
      "Epoch: 84, Loss: 145.78028, Residuals: -1.35882, Convergence: 0.002116\n",
      "Epoch: 85, Loss: 145.48699, Residuals: -1.34875, Convergence: 0.002016\n",
      "Epoch: 86, Loss: 145.20820, Residuals: -1.33909, Convergence: 0.001920\n",
      "Epoch: 87, Loss: 144.94327, Residuals: -1.32981, Convergence: 0.001828\n",
      "Epoch: 88, Loss: 144.69158, Residuals: -1.32091, Convergence: 0.001739\n",
      "Epoch: 89, Loss: 144.45257, Residuals: -1.31237, Convergence: 0.001655\n",
      "Epoch: 90, Loss: 144.22570, Residuals: -1.30419, Convergence: 0.001573\n",
      "Epoch: 91, Loss: 144.01049, Residuals: -1.29634, Convergence: 0.001494\n",
      "Epoch: 92, Loss: 143.80649, Residuals: -1.28883, Convergence: 0.001419\n",
      "Epoch: 93, Loss: 143.61329, Residuals: -1.28163, Convergence: 0.001345\n",
      "Epoch: 94, Loss: 143.43052, Residuals: -1.27474, Convergence: 0.001274\n",
      "Epoch: 95, Loss: 143.25781, Residuals: -1.26816, Convergence: 0.001206\n",
      "Epoch: 96, Loss: 143.09483, Residuals: -1.26186, Convergence: 0.001139\n",
      "Epoch: 97, Loss: 142.94125, Residuals: -1.25586, Convergence: 0.001074\n",
      "Epoch: 98, Loss: 142.79676, Residuals: -1.25013, Convergence: 0.001012\n",
      "Epoch: 99, Loss: 142.66098, Residuals: -1.24468, Convergence: 0.000952\n",
      "Evidence -184.622\n",
      "\n",
      "Epoch: 99, Evidence: -184.62215, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.25e-01\n",
      "Epoch: 99, Loss: 1377.91300, Residuals: -1.24468, Convergence:   inf\n",
      "Epoch: 100, Loss: 1315.07502, Residuals: -1.27489, Convergence: 0.047783\n",
      "Epoch: 101, Loss: 1267.30669, Residuals: -1.29848, Convergence: 0.037693\n",
      "Epoch: 102, Loss: 1231.30840, Residuals: -1.31531, Convergence: 0.029236\n",
      "Epoch: 103, Loss: 1203.37303, Residuals: -1.32704, Convergence: 0.023214\n",
      "Epoch: 104, Loss: 1180.82806, Residuals: -1.33565, Convergence: 0.019093\n",
      "Epoch: 105, Loss: 1162.14433, Residuals: -1.34220, Convergence: 0.016077\n",
      "Epoch: 106, Loss: 1146.39952, Residuals: -1.34714, Convergence: 0.013734\n",
      "Epoch: 107, Loss: 1132.96583, Residuals: -1.35072, Convergence: 0.011857\n",
      "Epoch: 108, Loss: 1121.37718, Residuals: -1.35310, Convergence: 0.010334\n",
      "Epoch: 109, Loss: 1111.27174, Residuals: -1.35440, Convergence: 0.009094\n",
      "Epoch: 110, Loss: 1102.35691, Residuals: -1.35473, Convergence: 0.008087\n",
      "Epoch: 111, Loss: 1094.39154, Residuals: -1.35418, Convergence: 0.007278\n",
      "Epoch: 112, Loss: 1087.17239, Residuals: -1.35281, Convergence: 0.006640\n",
      "Epoch: 113, Loss: 1080.52248, Residuals: -1.35066, Convergence: 0.006154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 114, Loss: 1074.28597, Residuals: -1.34778, Convergence: 0.005805\n",
      "Epoch: 115, Loss: 1068.32152, Residuals: -1.34417, Convergence: 0.005583\n",
      "Epoch: 116, Loss: 1062.50393, Residuals: -1.33985, Convergence: 0.005475\n",
      "Epoch: 117, Loss: 1056.72886, Residuals: -1.33484, Convergence: 0.005465\n",
      "Epoch: 118, Loss: 1050.92610, Residuals: -1.32918, Convergence: 0.005522\n",
      "Epoch: 119, Loss: 1045.07705, Residuals: -1.32293, Convergence: 0.005597\n",
      "Epoch: 120, Loss: 1039.22682, Residuals: -1.31618, Convergence: 0.005629\n",
      "Epoch: 121, Loss: 1033.47264, Residuals: -1.30904, Convergence: 0.005568\n",
      "Epoch: 122, Loss: 1027.92779, Residuals: -1.30158, Convergence: 0.005394\n",
      "Epoch: 123, Loss: 1022.68032, Residuals: -1.29391, Convergence: 0.005131\n",
      "Epoch: 124, Loss: 1017.77607, Residuals: -1.28608, Convergence: 0.004819\n",
      "Epoch: 125, Loss: 1013.22110, Residuals: -1.27819, Convergence: 0.004496\n",
      "Epoch: 126, Loss: 1008.99689, Residuals: -1.27028, Convergence: 0.004187\n",
      "Epoch: 127, Loss: 1005.07372, Residuals: -1.26240, Convergence: 0.003903\n",
      "Epoch: 128, Loss: 1001.41942, Residuals: -1.25461, Convergence: 0.003649\n",
      "Epoch: 129, Loss: 998.00297, Residuals: -1.24694, Convergence: 0.003423\n",
      "Epoch: 130, Loss: 994.79741, Residuals: -1.23942, Convergence: 0.003222\n",
      "Epoch: 131, Loss: 991.77938, Residuals: -1.23207, Convergence: 0.003043\n",
      "Epoch: 132, Loss: 988.92956, Residuals: -1.22491, Convergence: 0.002882\n",
      "Epoch: 133, Loss: 986.23161, Residuals: -1.21795, Convergence: 0.002736\n",
      "Epoch: 134, Loss: 983.67208, Residuals: -1.21121, Convergence: 0.002602\n",
      "Epoch: 135, Loss: 981.23948, Residuals: -1.20470, Convergence: 0.002479\n",
      "Epoch: 136, Loss: 978.92467, Residuals: -1.19843, Convergence: 0.002365\n",
      "Epoch: 137, Loss: 976.71879, Residuals: -1.19239, Convergence: 0.002258\n",
      "Epoch: 138, Loss: 974.61481, Residuals: -1.18659, Convergence: 0.002159\n",
      "Epoch: 139, Loss: 972.60588, Residuals: -1.18102, Convergence: 0.002066\n",
      "Epoch: 140, Loss: 970.68601, Residuals: -1.17569, Convergence: 0.001978\n",
      "Epoch: 141, Loss: 968.84927, Residuals: -1.17059, Convergence: 0.001896\n",
      "Epoch: 142, Loss: 967.09051, Residuals: -1.16571, Convergence: 0.001819\n",
      "Epoch: 143, Loss: 965.40405, Residuals: -1.16104, Convergence: 0.001747\n",
      "Epoch: 144, Loss: 963.78564, Residuals: -1.15659, Convergence: 0.001679\n",
      "Epoch: 145, Loss: 962.23052, Residuals: -1.15233, Convergence: 0.001616\n",
      "Epoch: 146, Loss: 960.73464, Residuals: -1.14827, Convergence: 0.001557\n",
      "Epoch: 147, Loss: 959.29417, Residuals: -1.14438, Convergence: 0.001502\n",
      "Epoch: 148, Loss: 957.90538, Residuals: -1.14067, Convergence: 0.001450\n",
      "Epoch: 149, Loss: 956.56502, Residuals: -1.13711, Convergence: 0.001401\n",
      "Epoch: 150, Loss: 955.27032, Residuals: -1.13372, Convergence: 0.001355\n",
      "Epoch: 151, Loss: 954.01823, Residuals: -1.13046, Convergence: 0.001312\n",
      "Epoch: 152, Loss: 952.80620, Residuals: -1.12734, Convergence: 0.001272\n",
      "Epoch: 153, Loss: 951.63185, Residuals: -1.12435, Convergence: 0.001234\n",
      "Epoch: 154, Loss: 950.49253, Residuals: -1.12148, Convergence: 0.001199\n",
      "Epoch: 155, Loss: 949.38585, Residuals: -1.11872, Convergence: 0.001166\n",
      "Epoch: 156, Loss: 948.30956, Residuals: -1.11606, Convergence: 0.001135\n",
      "Epoch: 157, Loss: 947.26058, Residuals: -1.11349, Convergence: 0.001107\n",
      "Epoch: 158, Loss: 946.23628, Residuals: -1.11102, Convergence: 0.001082\n",
      "Epoch: 159, Loss: 945.23389, Residuals: -1.10862, Convergence: 0.001060\n",
      "Epoch: 160, Loss: 944.25017, Residuals: -1.10629, Convergence: 0.001042\n",
      "Epoch: 161, Loss: 943.28207, Residuals: -1.10402, Convergence: 0.001026\n",
      "Epoch: 162, Loss: 942.32622, Residuals: -1.10181, Convergence: 0.001014\n",
      "Epoch: 163, Loss: 941.37938, Residuals: -1.09963, Convergence: 0.001006\n",
      "Epoch: 164, Loss: 940.43842, Residuals: -1.09750, Convergence: 0.001001\n",
      "Epoch: 165, Loss: 939.50109, Residuals: -1.09539, Convergence: 0.000998\n",
      "Evidence 11169.877\n",
      "\n",
      "Epoch: 165, Evidence: 11169.87695, Convergence: 1.016529\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 5.77e-01\n",
      "Epoch: 165, Loss: 2363.96659, Residuals: -1.09539, Convergence:   inf\n",
      "Epoch: 166, Loss: 2323.17298, Residuals: -1.10309, Convergence: 0.017559\n",
      "Epoch: 167, Loss: 2295.12814, Residuals: -1.10218, Convergence: 0.012219\n",
      "Epoch: 168, Loss: 2271.75094, Residuals: -1.10010, Convergence: 0.010290\n",
      "Epoch: 169, Loss: 2251.91427, Residuals: -1.09759, Convergence: 0.008809\n",
      "Epoch: 170, Loss: 2234.92953, Residuals: -1.09482, Convergence: 0.007600\n",
      "Epoch: 171, Loss: 2220.27258, Residuals: -1.09187, Convergence: 0.006601\n",
      "Epoch: 172, Loss: 2207.51908, Residuals: -1.08880, Convergence: 0.005777\n",
      "Epoch: 173, Loss: 2196.31287, Residuals: -1.08562, Convergence: 0.005102\n",
      "Epoch: 174, Loss: 2186.35363, Residuals: -1.08236, Convergence: 0.004555\n",
      "Epoch: 175, Loss: 2177.39490, Residuals: -1.07899, Convergence: 0.004114\n",
      "Epoch: 176, Loss: 2169.24319, Residuals: -1.07551, Convergence: 0.003758\n",
      "Epoch: 177, Loss: 2161.76198, Residuals: -1.07195, Convergence: 0.003461\n",
      "Epoch: 178, Loss: 2154.86583, Residuals: -1.06831, Convergence: 0.003200\n",
      "Epoch: 179, Loss: 2148.49854, Residuals: -1.06466, Convergence: 0.002964\n",
      "Epoch: 180, Loss: 2142.62109, Residuals: -1.06103, Convergence: 0.002743\n",
      "Epoch: 181, Loss: 2137.19840, Residuals: -1.05747, Convergence: 0.002537\n",
      "Epoch: 182, Loss: 2132.19599, Residuals: -1.05402, Convergence: 0.002346\n",
      "Epoch: 183, Loss: 2127.58044, Residuals: -1.05070, Convergence: 0.002169\n",
      "Epoch: 184, Loss: 2123.31963, Residuals: -1.04752, Convergence: 0.002007\n",
      "Epoch: 185, Loss: 2119.38375, Residuals: -1.04449, Convergence: 0.001857\n",
      "Epoch: 186, Loss: 2115.74639, Residuals: -1.04162, Convergence: 0.001719\n",
      "Epoch: 187, Loss: 2112.38468, Residuals: -1.03890, Convergence: 0.001591\n",
      "Epoch: 188, Loss: 2109.27400, Residuals: -1.03634, Convergence: 0.001475\n",
      "Epoch: 189, Loss: 2106.39566, Residuals: -1.03392, Convergence: 0.001366\n",
      "Epoch: 190, Loss: 2103.72796, Residuals: -1.03164, Convergence: 0.001268\n",
      "Epoch: 191, Loss: 2101.25372, Residuals: -1.02949, Convergence: 0.001178\n",
      "Epoch: 192, Loss: 2098.95571, Residuals: -1.02745, Convergence: 0.001095\n",
      "Epoch: 193, Loss: 2096.81764, Residuals: -1.02553, Convergence: 0.001020\n",
      "Epoch: 194, Loss: 2094.82424, Residuals: -1.02372, Convergence: 0.000952\n",
      "Evidence 14338.861\n",
      "\n",
      "Epoch: 194, Evidence: 14338.86133, Convergence: 0.221007\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 4.38e-01\n",
      "Epoch: 194, Loss: 2489.75572, Residuals: -1.02372, Convergence:   inf\n",
      "Epoch: 195, Loss: 2475.39290, Residuals: -1.02089, Convergence: 0.005802\n",
      "Epoch: 196, Loss: 2463.71242, Residuals: -1.01757, Convergence: 0.004741\n",
      "Epoch: 197, Loss: 2453.71734, Residuals: -1.01431, Convergence: 0.004073\n",
      "Epoch: 198, Loss: 2445.09598, Residuals: -1.01120, Convergence: 0.003526\n",
      "Epoch: 199, Loss: 2437.61337, Residuals: -1.00832, Convergence: 0.003070\n",
      "Epoch: 200, Loss: 2431.08422, Residuals: -1.00565, Convergence: 0.002686\n",
      "Epoch: 201, Loss: 2425.35364, Residuals: -1.00321, Convergence: 0.002363\n",
      "Epoch: 202, Loss: 2420.29558, Residuals: -1.00096, Convergence: 0.002090\n",
      "Epoch: 203, Loss: 2415.80369, Residuals: -0.99890, Convergence: 0.001859\n",
      "Epoch: 204, Loss: 2411.79144, Residuals: -0.99701, Convergence: 0.001664\n",
      "Epoch: 205, Loss: 2408.18441, Residuals: -0.99528, Convergence: 0.001498\n",
      "Epoch: 206, Loss: 2404.92418, Residuals: -0.99369, Convergence: 0.001356\n",
      "Epoch: 207, Loss: 2401.95905, Residuals: -0.99223, Convergence: 0.001234\n",
      "Epoch: 208, Loss: 2399.24836, Residuals: -0.99088, Convergence: 0.001130\n",
      "Epoch: 209, Loss: 2396.75766, Residuals: -0.98965, Convergence: 0.001039\n",
      "Epoch: 210, Loss: 2394.45864, Residuals: -0.98852, Convergence: 0.000960\n",
      "Evidence 14743.008\n",
      "\n",
      "Epoch: 210, Evidence: 14743.00781, Convergence: 0.027413\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 3.34e-01\n",
      "Epoch: 210, Loss: 2494.94047, Residuals: -0.98852, Convergence:   inf\n",
      "Epoch: 211, Loss: 2488.29436, Residuals: -0.98545, Convergence: 0.002671\n",
      "Epoch: 212, Loss: 2482.83439, Residuals: -0.98279, Convergence: 0.002199\n",
      "Epoch: 213, Loss: 2478.22116, Residuals: -0.98053, Convergence: 0.001862\n",
      "Epoch: 214, Loss: 2474.26031, Residuals: -0.97863, Convergence: 0.001601\n",
      "Epoch: 215, Loss: 2470.80931, Residuals: -0.97701, Convergence: 0.001397\n",
      "Epoch: 216, Loss: 2467.76387, Residuals: -0.97564, Convergence: 0.001234\n",
      "Epoch: 217, Loss: 2465.04442, Residuals: -0.97447, Convergence: 0.001103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 218, Loss: 2462.59069, Residuals: -0.97347, Convergence: 0.000996\n",
      "Evidence 14826.956\n",
      "\n",
      "Epoch: 218, Evidence: 14826.95605, Convergence: 0.005662\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.61e-01\n",
      "Epoch: 218, Loss: 2496.53553, Residuals: -0.97347, Convergence:   inf\n",
      "Epoch: 219, Loss: 2492.57252, Residuals: -0.97123, Convergence: 0.001590\n",
      "Epoch: 220, Loss: 2489.28841, Residuals: -0.96943, Convergence: 0.001319\n",
      "Epoch: 221, Loss: 2486.47271, Residuals: -0.96797, Convergence: 0.001132\n",
      "Epoch: 222, Loss: 2484.00551, Residuals: -0.96677, Convergence: 0.000993\n",
      "Evidence 14855.115\n",
      "\n",
      "Epoch: 222, Evidence: 14855.11523, Convergence: 0.001896\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.09e-01\n",
      "Epoch: 222, Loss: 2497.50421, Residuals: -0.96677, Convergence:   inf\n",
      "Epoch: 223, Loss: 2494.57817, Residuals: -0.96495, Convergence: 0.001173\n",
      "Epoch: 224, Loss: 2492.11588, Residuals: -0.96350, Convergence: 0.000988\n",
      "Evidence 14867.310\n",
      "\n",
      "Epoch: 224, Evidence: 14867.30957, Convergence: 0.000820\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.73e-01\n",
      "Epoch: 224, Loss: 2498.22027, Residuals: -0.96350, Convergence:   inf\n",
      "Epoch: 225, Loss: 2493.59214, Residuals: -0.96093, Convergence: 0.001856\n",
      "Epoch: 226, Loss: 2490.02567, Residuals: -0.95902, Convergence: 0.001432\n",
      "Epoch: 227, Loss: 2487.11278, Residuals: -0.95778, Convergence: 0.001171\n",
      "Epoch: 228, Loss: 2484.63196, Residuals: -0.95711, Convergence: 0.000998\n",
      "Evidence 14885.384\n",
      "\n",
      "Epoch: 228, Evidence: 14885.38379, Convergence: 0.002033\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.43e-01\n",
      "Epoch: 228, Loss: 2498.31473, Residuals: -0.95711, Convergence:   inf\n",
      "Epoch: 229, Loss: 2495.22417, Residuals: -0.95461, Convergence: 0.001239\n",
      "Epoch: 230, Loss: 2492.75796, Residuals: -0.95332, Convergence: 0.000989\n",
      "Evidence 14896.418\n",
      "\n",
      "Epoch: 230, Evidence: 14896.41797, Convergence: 0.000741\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.21e-01\n",
      "Epoch: 230, Loss: 2498.51716, Residuals: -0.95332, Convergence:   inf\n",
      "Epoch: 231, Loss: 2493.98388, Residuals: -0.94946, Convergence: 0.001818\n",
      "Epoch: 232, Loss: 2490.71827, Residuals: -0.95058, Convergence: 0.001311\n",
      "Epoch: 233, Loss: 2487.97971, Residuals: -0.95161, Convergence: 0.001101\n",
      "Epoch: 234, Loss: 2485.62896, Residuals: -0.95469, Convergence: 0.000946\n",
      "Evidence 14912.469\n",
      "\n",
      "Epoch: 234, Evidence: 14912.46875, Convergence: 0.001816\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.08e-01\n",
      "Epoch: 234, Loss: 2497.78112, Residuals: -0.95469, Convergence:   inf\n",
      "Epoch: 235, Loss: 2496.03199, Residuals: -0.95287, Convergence: 0.000701\n",
      "Evidence 14919.147\n",
      "\n",
      "Epoch: 235, Evidence: 14919.14746, Convergence: 0.000448\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 8.80e-02\n",
      "Epoch: 235, Loss: 2498.78288, Residuals: -0.95287, Convergence:   inf\n",
      "Epoch: 236, Loss: 2541.74855, Residuals: -0.99615, Convergence: -0.016904\n",
      "Epoch: 236, Loss: 2496.43734, Residuals: -0.94987, Convergence: 0.000940\n",
      "Evidence 14923.492\n",
      "\n",
      "Epoch: 236, Evidence: 14923.49219, Convergence: 0.000739\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 8.13e-02\n",
      "Epoch: 236, Loss: 2498.18395, Residuals: -0.94987, Convergence:   inf\n",
      "Epoch: 237, Loss: 2503.08376, Residuals: -0.95328, Convergence: -0.001958\n",
      "Epoch: 237, Loss: 2498.08491, Residuals: -0.94892, Convergence: 0.000040\n",
      "Evidence 14925.134\n",
      "\n",
      "Epoch: 237, Evidence: 14925.13379, Convergence: 0.000849\n",
      "Total samples: 181, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 381.50944, Residuals: -4.56075, Convergence:   inf\n",
      "Epoch: 1, Loss: 355.72780, Residuals: -4.44018, Convergence: 0.072476\n",
      "Epoch: 2, Loss: 334.56171, Residuals: -4.27420, Convergence: 0.063265\n",
      "Epoch: 3, Loss: 318.41896, Residuals: -4.10792, Convergence: 0.050697\n",
      "Epoch: 4, Loss: 306.11354, Residuals: -3.96150, Convergence: 0.040199\n",
      "Epoch: 5, Loss: 296.35444, Residuals: -3.83196, Convergence: 0.032931\n",
      "Epoch: 6, Loss: 288.43298, Residuals: -3.71907, Convergence: 0.027464\n",
      "Epoch: 7, Loss: 281.85845, Residuals: -3.62237, Convergence: 0.023326\n",
      "Epoch: 8, Loss: 276.26894, Residuals: -3.53981, Convergence: 0.020232\n",
      "Epoch: 9, Loss: 271.40952, Residuals: -3.46903, Convergence: 0.017904\n",
      "Epoch: 10, Loss: 267.09920, Residuals: -3.40792, Convergence: 0.016137\n",
      "Epoch: 11, Loss: 263.20707, Residuals: -3.35467, Convergence: 0.014787\n",
      "Epoch: 12, Loss: 259.63756, Residuals: -3.30775, Convergence: 0.013748\n",
      "Epoch: 13, Loss: 256.32095, Residuals: -3.26585, Convergence: 0.012939\n",
      "Epoch: 14, Loss: 253.20686, Residuals: -3.22781, Convergence: 0.012299\n",
      "Epoch: 15, Loss: 250.26071, Residuals: -3.19265, Convergence: 0.011772\n",
      "Epoch: 16, Loss: 247.46229, Residuals: -3.15968, Convergence: 0.011308\n",
      "Epoch: 17, Loss: 244.79953, Residuals: -3.12851, Convergence: 0.010877\n",
      "Epoch: 18, Loss: 242.25580, Residuals: -3.09878, Convergence: 0.010500\n",
      "Epoch: 19, Loss: 239.80354, Residuals: -3.07007, Convergence: 0.010226\n",
      "Epoch: 20, Loss: 237.40851, Residuals: -3.04183, Convergence: 0.010088\n",
      "Epoch: 21, Loss: 235.03666, Residuals: -3.01351, Convergence: 0.010091\n",
      "Epoch: 22, Loss: 232.65926, Residuals: -2.98468, Convergence: 0.010218\n",
      "Epoch: 23, Loss: 230.25183, Residuals: -2.95497, Convergence: 0.010456\n",
      "Epoch: 24, Loss: 227.78284, Residuals: -2.92403, Convergence: 0.010839\n",
      "Epoch: 25, Loss: 225.20287, Residuals: -2.89128, Convergence: 0.011456\n",
      "Epoch: 26, Loss: 222.47005, Residuals: -2.85619, Convergence: 0.012284\n",
      "Epoch: 27, Loss: 219.63819, Residuals: -2.81926, Convergence: 0.012893\n",
      "Epoch: 28, Loss: 216.84432, Residuals: -2.78204, Convergence: 0.012884\n",
      "Epoch: 29, Loss: 214.15456, Residuals: -2.74540, Convergence: 0.012560\n",
      "Epoch: 30, Loss: 211.56569, Residuals: -2.70947, Convergence: 0.012237\n",
      "Epoch: 31, Loss: 209.05921, Residuals: -2.67414, Convergence: 0.011989\n",
      "Epoch: 32, Loss: 206.61873, Residuals: -2.63925, Convergence: 0.011811\n",
      "Epoch: 33, Loss: 204.23257, Residuals: -2.60471, Convergence: 0.011684\n",
      "Epoch: 34, Loss: 201.89321, Residuals: -2.57041, Convergence: 0.011587\n",
      "Epoch: 35, Loss: 199.59635, Residuals: -2.53627, Convergence: 0.011507\n",
      "Epoch: 36, Loss: 197.34008, Residuals: -2.50226, Convergence: 0.011433\n",
      "Epoch: 37, Loss: 195.12405, Residuals: -2.46834, Convergence: 0.011357\n",
      "Epoch: 38, Loss: 192.94901, Residuals: -2.43448, Convergence: 0.011273\n",
      "Epoch: 39, Loss: 190.81631, Residuals: -2.40068, Convergence: 0.011177\n",
      "Epoch: 40, Loss: 188.72761, Residuals: -2.36695, Convergence: 0.011067\n",
      "Epoch: 41, Loss: 186.68473, Residuals: -2.33330, Convergence: 0.010943\n",
      "Epoch: 42, Loss: 184.68951, Residuals: -2.29975, Convergence: 0.010803\n",
      "Epoch: 43, Loss: 182.74387, Residuals: -2.26632, Convergence: 0.010647\n",
      "Epoch: 44, Loss: 180.84978, Residuals: -2.23306, Convergence: 0.010473\n",
      "Epoch: 45, Loss: 179.00934, Residuals: -2.20001, Convergence: 0.010281\n",
      "Epoch: 46, Loss: 177.22475, Residuals: -2.16721, Convergence: 0.010070\n",
      "Epoch: 47, Loss: 175.49821, Residuals: -2.13473, Convergence: 0.009838\n",
      "Epoch: 48, Loss: 173.83171, Residuals: -2.10262, Convergence: 0.009587\n",
      "Epoch: 49, Loss: 172.22677, Residuals: -2.07093, Convergence: 0.009319\n",
      "Epoch: 50, Loss: 170.68428, Residuals: -2.03972, Convergence: 0.009037\n",
      "Epoch: 51, Loss: 169.20439, Residuals: -2.00904, Convergence: 0.008746\n",
      "Epoch: 52, Loss: 167.78654, Residuals: -1.97891, Convergence: 0.008450\n",
      "Epoch: 53, Loss: 166.42960, Residuals: -1.94937, Convergence: 0.008153\n",
      "Epoch: 54, Loss: 165.13203, Residuals: -1.92045, Convergence: 0.007858\n",
      "Epoch: 55, Loss: 163.89199, Residuals: -1.89217, Convergence: 0.007566\n",
      "Epoch: 56, Loss: 162.70753, Residuals: -1.86454, Convergence: 0.007280\n",
      "Epoch: 57, Loss: 161.57657, Residuals: -1.83758, Convergence: 0.007000\n",
      "Epoch: 58, Loss: 160.49700, Residuals: -1.81131, Convergence: 0.006726\n",
      "Epoch: 59, Loss: 159.46662, Residuals: -1.78573, Convergence: 0.006461\n",
      "Epoch: 60, Loss: 158.48320, Residuals: -1.76086, Convergence: 0.006205\n",
      "Epoch: 61, Loss: 157.54443, Residuals: -1.73669, Convergence: 0.005959\n",
      "Epoch: 62, Loss: 156.64802, Residuals: -1.71322, Convergence: 0.005722\n",
      "Epoch: 63, Loss: 155.79167, Residuals: -1.69045, Convergence: 0.005497\n",
      "Epoch: 64, Loss: 154.97328, Residuals: -1.66836, Convergence: 0.005281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65, Loss: 154.19089, Residuals: -1.64694, Convergence: 0.005074\n",
      "Epoch: 66, Loss: 153.44284, Residuals: -1.62618, Convergence: 0.004875\n",
      "Epoch: 67, Loss: 152.72764, Residuals: -1.60606, Convergence: 0.004683\n",
      "Epoch: 68, Loss: 152.04407, Residuals: -1.58657, Convergence: 0.004496\n",
      "Epoch: 69, Loss: 151.39106, Residuals: -1.56772, Convergence: 0.004313\n",
      "Epoch: 70, Loss: 150.76765, Residuals: -1.54949, Convergence: 0.004135\n",
      "Epoch: 71, Loss: 150.17292, Residuals: -1.53188, Convergence: 0.003960\n",
      "Epoch: 72, Loss: 149.60601, Residuals: -1.51488, Convergence: 0.003789\n",
      "Epoch: 73, Loss: 149.06605, Residuals: -1.49849, Convergence: 0.003622\n",
      "Epoch: 74, Loss: 148.55212, Residuals: -1.48270, Convergence: 0.003460\n",
      "Epoch: 75, Loss: 148.06333, Residuals: -1.46750, Convergence: 0.003301\n",
      "Epoch: 76, Loss: 147.59873, Residuals: -1.45288, Convergence: 0.003148\n",
      "Epoch: 77, Loss: 147.15739, Residuals: -1.43882, Convergence: 0.002999\n",
      "Epoch: 78, Loss: 146.73835, Residuals: -1.42532, Convergence: 0.002856\n",
      "Epoch: 79, Loss: 146.34067, Residuals: -1.41236, Convergence: 0.002718\n",
      "Epoch: 80, Loss: 145.96339, Residuals: -1.39992, Convergence: 0.002585\n",
      "Epoch: 81, Loss: 145.60562, Residuals: -1.38799, Convergence: 0.002457\n",
      "Epoch: 82, Loss: 145.26646, Residuals: -1.37655, Convergence: 0.002335\n",
      "Epoch: 83, Loss: 144.94503, Residuals: -1.36559, Convergence: 0.002218\n",
      "Epoch: 84, Loss: 144.64052, Residuals: -1.35508, Convergence: 0.002105\n",
      "Epoch: 85, Loss: 144.35215, Residuals: -1.34501, Convergence: 0.001998\n",
      "Epoch: 86, Loss: 144.07914, Residuals: -1.33537, Convergence: 0.001895\n",
      "Epoch: 87, Loss: 143.82080, Residuals: -1.32614, Convergence: 0.001796\n",
      "Epoch: 88, Loss: 143.57646, Residuals: -1.31730, Convergence: 0.001702\n",
      "Epoch: 89, Loss: 143.34549, Residuals: -1.30884, Convergence: 0.001611\n",
      "Epoch: 90, Loss: 143.12731, Residuals: -1.30074, Convergence: 0.001524\n",
      "Epoch: 91, Loss: 142.92137, Residuals: -1.29299, Convergence: 0.001441\n",
      "Epoch: 92, Loss: 142.72715, Residuals: -1.28558, Convergence: 0.001361\n",
      "Epoch: 93, Loss: 142.54417, Residuals: -1.27850, Convergence: 0.001284\n",
      "Epoch: 94, Loss: 142.37198, Residuals: -1.27173, Convergence: 0.001209\n",
      "Epoch: 95, Loss: 142.21011, Residuals: -1.26527, Convergence: 0.001138\n",
      "Epoch: 96, Loss: 142.05813, Residuals: -1.25911, Convergence: 0.001070\n",
      "Epoch: 97, Loss: 141.91558, Residuals: -1.25324, Convergence: 0.001004\n",
      "Epoch: 98, Loss: 141.78197, Residuals: -1.24765, Convergence: 0.000942\n",
      "Evidence -183.471\n",
      "\n",
      "Epoch: 98, Evidence: -183.47128, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 7.25e-01\n",
      "Epoch: 98, Loss: 1355.26872, Residuals: -1.24765, Convergence:   inf\n",
      "Epoch: 99, Loss: 1293.44158, Residuals: -1.27812, Convergence: 0.047800\n",
      "Epoch: 100, Loss: 1246.82286, Residuals: -1.30181, Convergence: 0.037390\n",
      "Epoch: 101, Loss: 1211.84968, Residuals: -1.31855, Convergence: 0.028859\n",
      "Epoch: 102, Loss: 1184.70826, Residuals: -1.33011, Convergence: 0.022910\n",
      "Epoch: 103, Loss: 1162.77107, Residuals: -1.33851, Convergence: 0.018866\n",
      "Epoch: 104, Loss: 1144.56873, Residuals: -1.34481, Convergence: 0.015903\n",
      "Epoch: 105, Loss: 1129.21991, Residuals: -1.34949, Convergence: 0.013592\n",
      "Epoch: 106, Loss: 1116.12387, Residuals: -1.35279, Convergence: 0.011734\n",
      "Epoch: 107, Loss: 1104.83257, Residuals: -1.35488, Convergence: 0.010220\n",
      "Epoch: 108, Loss: 1094.99691, Residuals: -1.35590, Convergence: 0.008982\n",
      "Epoch: 109, Loss: 1086.33656, Residuals: -1.35595, Convergence: 0.007972\n",
      "Epoch: 110, Loss: 1078.62117, Residuals: -1.35514, Convergence: 0.007153\n",
      "Epoch: 111, Loss: 1071.65902, Residuals: -1.35354, Convergence: 0.006497\n",
      "Epoch: 112, Loss: 1065.28695, Residuals: -1.35121, Convergence: 0.005982\n",
      "Epoch: 113, Loss: 1059.36401, Residuals: -1.34820, Convergence: 0.005591\n",
      "Epoch: 114, Loss: 1053.76643, Residuals: -1.34454, Convergence: 0.005312\n",
      "Epoch: 115, Loss: 1048.38445, Residuals: -1.34024, Convergence: 0.005134\n",
      "Epoch: 116, Loss: 1043.12105, Residuals: -1.33530, Convergence: 0.005046\n",
      "Epoch: 117, Loss: 1037.89215, Residuals: -1.32973, Convergence: 0.005038\n",
      "Epoch: 118, Loss: 1032.63358, Residuals: -1.32354, Convergence: 0.005092\n",
      "Epoch: 119, Loss: 1027.30896, Residuals: -1.31677, Convergence: 0.005183\n",
      "Epoch: 120, Loss: 1021.92523, Residuals: -1.30950, Convergence: 0.005268\n",
      "Epoch: 121, Loss: 1016.53869, Residuals: -1.30181, Convergence: 0.005299\n",
      "Epoch: 122, Loss: 1011.24365, Residuals: -1.29380, Convergence: 0.005236\n",
      "Epoch: 123, Loss: 1006.14140, Residuals: -1.28559, Convergence: 0.005071\n",
      "Epoch: 124, Loss: 1001.30667, Residuals: -1.27725, Convergence: 0.004828\n",
      "Epoch: 125, Loss: 996.77574, Residuals: -1.26887, Convergence: 0.004546\n",
      "Epoch: 126, Loss: 992.55179, Residuals: -1.26050, Convergence: 0.004256\n",
      "Epoch: 127, Loss: 988.61754, Residuals: -1.25221, Convergence: 0.003980\n",
      "Epoch: 128, Loss: 984.94738, Residuals: -1.24404, Convergence: 0.003726\n",
      "Epoch: 129, Loss: 981.51459, Residuals: -1.23603, Convergence: 0.003497\n",
      "Epoch: 130, Loss: 978.29438, Residuals: -1.22821, Convergence: 0.003292\n",
      "Epoch: 131, Loss: 975.26564, Residuals: -1.22060, Convergence: 0.003106\n",
      "Epoch: 132, Loss: 972.41042, Residuals: -1.21322, Convergence: 0.002936\n",
      "Epoch: 133, Loss: 969.71397, Residuals: -1.20609, Convergence: 0.002781\n",
      "Epoch: 134, Loss: 967.16434, Residuals: -1.19923, Convergence: 0.002636\n",
      "Epoch: 135, Loss: 964.75171, Residuals: -1.19264, Convergence: 0.002501\n",
      "Epoch: 136, Loss: 962.46709, Residuals: -1.18632, Convergence: 0.002374\n",
      "Epoch: 137, Loss: 960.30222, Residuals: -1.18028, Convergence: 0.002254\n",
      "Epoch: 138, Loss: 958.25033, Residuals: -1.17452, Convergence: 0.002141\n",
      "Epoch: 139, Loss: 956.30411, Residuals: -1.16904, Convergence: 0.002035\n",
      "Epoch: 140, Loss: 954.45698, Residuals: -1.16382, Convergence: 0.001935\n",
      "Epoch: 141, Loss: 952.70189, Residuals: -1.15887, Convergence: 0.001842\n",
      "Epoch: 142, Loss: 951.03260, Residuals: -1.15417, Convergence: 0.001755\n",
      "Epoch: 143, Loss: 949.44274, Residuals: -1.14971, Convergence: 0.001675\n",
      "Epoch: 144, Loss: 947.92614, Residuals: -1.14548, Convergence: 0.001600\n",
      "Epoch: 145, Loss: 946.47697, Residuals: -1.14147, Convergence: 0.001531\n",
      "Epoch: 146, Loss: 945.08970, Residuals: -1.13766, Convergence: 0.001468\n",
      "Epoch: 147, Loss: 943.75921, Residuals: -1.13405, Convergence: 0.001410\n",
      "Epoch: 148, Loss: 942.48059, Residuals: -1.13061, Convergence: 0.001357\n",
      "Epoch: 149, Loss: 941.24950, Residuals: -1.12735, Convergence: 0.001308\n",
      "Epoch: 150, Loss: 940.06180, Residuals: -1.12424, Convergence: 0.001263\n",
      "Epoch: 151, Loss: 938.91404, Residuals: -1.12128, Convergence: 0.001222\n",
      "Epoch: 152, Loss: 937.80273, Residuals: -1.11846, Convergence: 0.001185\n",
      "Epoch: 153, Loss: 936.72519, Residuals: -1.11576, Convergence: 0.001150\n",
      "Epoch: 154, Loss: 935.67861, Residuals: -1.11317, Convergence: 0.001119\n",
      "Epoch: 155, Loss: 934.66082, Residuals: -1.11070, Convergence: 0.001089\n",
      "Epoch: 156, Loss: 933.66953, Residuals: -1.10832, Convergence: 0.001062\n",
      "Epoch: 157, Loss: 932.70298, Residuals: -1.10603, Convergence: 0.001036\n",
      "Epoch: 158, Loss: 931.75969, Residuals: -1.10383, Convergence: 0.001012\n",
      "Epoch: 159, Loss: 930.83776, Residuals: -1.10170, Convergence: 0.000990\n",
      "Evidence 10972.536\n",
      "\n",
      "Epoch: 159, Evidence: 10972.53613, Convergence: 1.016721\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 5.78e-01\n",
      "Epoch: 159, Loss: 2330.29352, Residuals: -1.10170, Convergence:   inf\n",
      "Epoch: 160, Loss: 2290.68461, Residuals: -1.11088, Convergence: 0.017291\n",
      "Epoch: 161, Loss: 2262.66430, Residuals: -1.11008, Convergence: 0.012384\n",
      "Epoch: 162, Loss: 2239.10663, Residuals: -1.10820, Convergence: 0.010521\n",
      "Epoch: 163, Loss: 2219.05494, Residuals: -1.10592, Convergence: 0.009036\n",
      "Epoch: 164, Loss: 2201.84964, Residuals: -1.10337, Convergence: 0.007814\n",
      "Epoch: 165, Loss: 2186.97920, Residuals: -1.10063, Convergence: 0.006800\n",
      "Epoch: 166, Loss: 2174.02753, Residuals: -1.09771, Convergence: 0.005957\n",
      "Epoch: 167, Loss: 2162.64960, Residuals: -1.09466, Convergence: 0.005261\n",
      "Epoch: 168, Loss: 2152.55928, Residuals: -1.09148, Convergence: 0.004688\n",
      "Epoch: 169, Loss: 2143.51827, Residuals: -1.08819, Convergence: 0.004218\n",
      "Epoch: 170, Loss: 2135.32926, Residuals: -1.08481, Convergence: 0.003835\n",
      "Epoch: 171, Loss: 2127.82771, Residuals: -1.08133, Convergence: 0.003525\n",
      "Epoch: 172, Loss: 2120.88437, Residuals: -1.07777, Convergence: 0.003274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 173, Loss: 2114.40343, Residuals: -1.07412, Convergence: 0.003065\n",
      "Epoch: 174, Loss: 2108.32350, Residuals: -1.07040, Convergence: 0.002884\n",
      "Epoch: 175, Loss: 2102.61334, Residuals: -1.06666, Convergence: 0.002716\n",
      "Epoch: 176, Loss: 2097.26016, Residuals: -1.06292, Convergence: 0.002552\n",
      "Epoch: 177, Loss: 2092.25529, Residuals: -1.05923, Convergence: 0.002392\n",
      "Epoch: 178, Loss: 2087.58921, Residuals: -1.05563, Convergence: 0.002235\n",
      "Epoch: 179, Loss: 2083.24900, Residuals: -1.05215, Convergence: 0.002083\n",
      "Epoch: 180, Loss: 2079.21623, Residuals: -1.04881, Convergence: 0.001940\n",
      "Epoch: 181, Loss: 2075.46993, Residuals: -1.04563, Convergence: 0.001805\n",
      "Epoch: 182, Loss: 2071.99097, Residuals: -1.04261, Convergence: 0.001679\n",
      "Epoch: 183, Loss: 2068.75731, Residuals: -1.03976, Convergence: 0.001563\n",
      "Epoch: 184, Loss: 2065.74968, Residuals: -1.03707, Convergence: 0.001456\n",
      "Epoch: 185, Loss: 2062.94963, Residuals: -1.03455, Convergence: 0.001357\n",
      "Epoch: 186, Loss: 2060.34084, Residuals: -1.03219, Convergence: 0.001266\n",
      "Epoch: 187, Loss: 2057.90709, Residuals: -1.02997, Convergence: 0.001183\n",
      "Epoch: 188, Loss: 2055.63501, Residuals: -1.02790, Convergence: 0.001105\n",
      "Epoch: 189, Loss: 2053.51130, Residuals: -1.02596, Convergence: 0.001034\n",
      "Epoch: 190, Loss: 2051.52429, Residuals: -1.02414, Convergence: 0.000969\n",
      "Evidence 14114.099\n",
      "\n",
      "Epoch: 190, Evidence: 14114.09863, Convergence: 0.222583\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 4.40e-01\n",
      "Epoch: 190, Loss: 2457.31036, Residuals: -1.02414, Convergence:   inf\n",
      "Epoch: 191, Loss: 2442.98733, Residuals: -1.02084, Convergence: 0.005863\n",
      "Epoch: 192, Loss: 2431.32469, Residuals: -1.01714, Convergence: 0.004797\n",
      "Epoch: 193, Loss: 2421.32699, Residuals: -1.01361, Convergence: 0.004129\n",
      "Epoch: 194, Loss: 2412.69604, Residuals: -1.01032, Convergence: 0.003577\n",
      "Epoch: 195, Loss: 2405.20270, Residuals: -1.00730, Convergence: 0.003115\n",
      "Epoch: 196, Loss: 2398.65922, Residuals: -1.00456, Convergence: 0.002728\n",
      "Epoch: 197, Loss: 2392.91231, Residuals: -1.00208, Convergence: 0.002402\n",
      "Epoch: 198, Loss: 2387.83476, Residuals: -0.99983, Convergence: 0.002126\n",
      "Epoch: 199, Loss: 2383.32025, Residuals: -0.99781, Convergence: 0.001894\n",
      "Epoch: 200, Loss: 2379.28115, Residuals: -0.99598, Convergence: 0.001698\n",
      "Epoch: 201, Loss: 2375.64491, Residuals: -0.99433, Convergence: 0.001531\n",
      "Epoch: 202, Loss: 2372.35158, Residuals: -0.99284, Convergence: 0.001388\n",
      "Epoch: 203, Loss: 2369.35181, Residuals: -0.99150, Convergence: 0.001266\n",
      "Epoch: 204, Loss: 2366.60338, Residuals: -0.99028, Convergence: 0.001161\n",
      "Epoch: 205, Loss: 2364.07511, Residuals: -0.98918, Convergence: 0.001069\n",
      "Epoch: 206, Loss: 2361.73732, Residuals: -0.98818, Convergence: 0.000990\n",
      "Evidence 14539.905\n",
      "\n",
      "Epoch: 206, Evidence: 14539.90527, Convergence: 0.029285\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 3.36e-01\n",
      "Epoch: 206, Loss: 2463.07170, Residuals: -0.98818, Convergence:   inf\n",
      "Epoch: 207, Loss: 2456.50213, Residuals: -0.98485, Convergence: 0.002674\n",
      "Epoch: 208, Loss: 2451.05044, Residuals: -0.98194, Convergence: 0.002224\n",
      "Epoch: 209, Loss: 2446.41140, Residuals: -0.97949, Convergence: 0.001896\n",
      "Epoch: 210, Loss: 2442.40626, Residuals: -0.97743, Convergence: 0.001640\n",
      "Epoch: 211, Loss: 2438.90279, Residuals: -0.97568, Convergence: 0.001436\n",
      "Epoch: 212, Loss: 2435.79802, Residuals: -0.97420, Convergence: 0.001275\n",
      "Epoch: 213, Loss: 2433.01595, Residuals: -0.97295, Convergence: 0.001143\n",
      "Epoch: 214, Loss: 2430.49529, Residuals: -0.97189, Convergence: 0.001037\n",
      "Epoch: 215, Loss: 2428.19185, Residuals: -0.97098, Convergence: 0.000949\n",
      "Evidence 14625.672\n",
      "\n",
      "Epoch: 215, Evidence: 14625.67188, Convergence: 0.005864\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 2.62e-01\n",
      "Epoch: 215, Loss: 2464.56891, Residuals: -0.97098, Convergence:   inf\n",
      "Epoch: 216, Loss: 2460.70185, Residuals: -0.96841, Convergence: 0.001572\n",
      "Epoch: 217, Loss: 2457.45572, Residuals: -0.96636, Convergence: 0.001321\n",
      "Epoch: 218, Loss: 2454.65064, Residuals: -0.96472, Convergence: 0.001143\n",
      "Epoch: 219, Loss: 2452.17856, Residuals: -0.96340, Convergence: 0.001008\n",
      "Epoch: 220, Loss: 2449.96550, Residuals: -0.96233, Convergence: 0.000903\n",
      "Evidence 14656.728\n",
      "\n",
      "Epoch: 220, Evidence: 14656.72754, Convergence: 0.002119\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 2.10e-01\n",
      "Epoch: 220, Loss: 2465.45933, Residuals: -0.96233, Convergence:   inf\n",
      "Epoch: 221, Loss: 2462.66315, Residuals: -0.96030, Convergence: 0.001135\n",
      "Epoch: 222, Loss: 2460.28543, Residuals: -0.95874, Convergence: 0.000966\n",
      "Evidence 14669.188\n",
      "\n",
      "Epoch: 222, Evidence: 14669.18750, Convergence: 0.000849\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.74e-01\n",
      "Epoch: 222, Loss: 2466.15227, Residuals: -0.95874, Convergence:   inf\n",
      "Epoch: 223, Loss: 2461.61946, Residuals: -0.95583, Convergence: 0.001841\n",
      "Epoch: 224, Loss: 2458.12932, Residuals: -0.95410, Convergence: 0.001420\n",
      "Epoch: 225, Loss: 2455.25172, Residuals: -0.95301, Convergence: 0.001172\n",
      "Epoch: 226, Loss: 2452.78364, Residuals: -0.95240, Convergence: 0.001006\n",
      "Epoch: 227, Loss: 2450.60413, Residuals: -0.95209, Convergence: 0.000889\n",
      "Evidence 14689.341\n",
      "\n",
      "Epoch: 227, Evidence: 14689.34082, Convergence: 0.002220\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.44e-01\n",
      "Epoch: 227, Loss: 2466.22508, Residuals: -0.95209, Convergence:   inf\n",
      "Epoch: 228, Loss: 2463.34317, Residuals: -0.94969, Convergence: 0.001170\n",
      "Epoch: 229, Loss: 2461.02019, Residuals: -0.94861, Convergence: 0.000944\n",
      "Evidence 14700.544\n",
      "\n",
      "Epoch: 229, Evidence: 14700.54395, Convergence: 0.000762\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.22e-01\n",
      "Epoch: 229, Loss: 2466.48333, Residuals: -0.94861, Convergence:   inf\n",
      "Epoch: 230, Loss: 2462.20590, Residuals: -0.94548, Convergence: 0.001737\n",
      "Epoch: 231, Loss: 2459.03037, Residuals: -0.94673, Convergence: 0.001291\n",
      "Epoch: 232, Loss: 2456.34102, Residuals: -0.94706, Convergence: 0.001095\n",
      "Epoch: 233, Loss: 2454.00833, Residuals: -0.94997, Convergence: 0.000951\n",
      "Evidence 14716.025\n",
      "\n",
      "Epoch: 233, Evidence: 14716.02539, Convergence: 0.001813\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.08e-01\n",
      "Epoch: 233, Loss: 2465.86029, Residuals: -0.94997, Convergence:   inf\n",
      "Epoch: 234, Loss: 2464.04837, Residuals: -0.94844, Convergence: 0.000735\n",
      "Evidence 14722.408\n",
      "\n",
      "Epoch: 234, Evidence: 14722.40820, Convergence: 0.000434\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 8.80e-02\n",
      "Epoch: 234, Loss: 2466.89738, Residuals: -0.94844, Convergence:   inf\n",
      "Epoch: 235, Loss: 2509.97170, Residuals: -0.99225, Convergence: -0.017161\n",
      "Epoch: 235, Loss: 2464.59740, Residuals: -0.94652, Convergence: 0.000933\n",
      "Evidence 14726.668\n",
      "\n",
      "Epoch: 235, Evidence: 14726.66797, Convergence: 0.000723\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 8.09e-02\n",
      "Epoch: 235, Loss: 2466.34347, Residuals: -0.94652, Convergence:   inf\n",
      "Epoch: 236, Loss: 2470.67341, Residuals: -0.94865, Convergence: -0.001753\n",
      "Epoch: 236, Loss: 2466.24457, Residuals: -0.94521, Convergence: 0.000040\n",
      "Evidence 14728.316\n",
      "\n",
      "Epoch: 236, Evidence: 14728.31641, Convergence: 0.000835\n",
      "Total samples: 183, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 383.35901, Residuals: -4.52295, Convergence:   inf\n",
      "Epoch: 1, Loss: 357.71681, Residuals: -4.40334, Convergence: 0.071683\n",
      "Epoch: 2, Loss: 336.75199, Residuals: -4.24041, Convergence: 0.062256\n",
      "Epoch: 3, Loss: 320.72129, Residuals: -4.07653, Convergence: 0.049983\n",
      "Epoch: 4, Loss: 308.48604, Residuals: -3.93202, Convergence: 0.039662\n",
      "Epoch: 5, Loss: 298.77258, Residuals: -3.80418, Convergence: 0.032511\n",
      "Epoch: 6, Loss: 290.87647, Residuals: -3.69275, Convergence: 0.027146\n",
      "Epoch: 7, Loss: 284.30972, Residuals: -3.59726, Convergence: 0.023097\n",
      "Epoch: 8, Loss: 278.71135, Residuals: -3.51568, Convergence: 0.020087\n",
      "Epoch: 9, Loss: 273.82658, Residuals: -3.44570, Convergence: 0.017839\n",
      "Epoch: 10, Loss: 269.47432, Residuals: -3.38524, Convergence: 0.016151\n",
      "Epoch: 11, Loss: 265.52330, Residuals: -3.33250, Convergence: 0.014880\n",
      "Epoch: 12, Loss: 261.87761, Residuals: -3.28595, Convergence: 0.013921\n",
      "Epoch: 13, Loss: 258.46781, Residuals: -3.24426, Convergence: 0.013192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Loss: 255.24566, Residuals: -3.20625, Convergence: 0.012624\n",
      "Epoch: 15, Loss: 252.18204, Residuals: -3.17104, Convergence: 0.012148\n",
      "Epoch: 16, Loss: 249.26337, Residuals: -3.13803, Convergence: 0.011709\n",
      "Epoch: 17, Loss: 246.47889, Residuals: -3.10685, Convergence: 0.011297\n",
      "Epoch: 18, Loss: 243.80705, Residuals: -3.07708, Convergence: 0.010959\n",
      "Epoch: 19, Loss: 241.21516, Residuals: -3.04816, Convergence: 0.010745\n",
      "Epoch: 20, Loss: 238.66709, Residuals: -3.01952, Convergence: 0.010676\n",
      "Epoch: 21, Loss: 236.13067, Residuals: -2.99066, Convergence: 0.010742\n",
      "Epoch: 22, Loss: 233.58081, Residuals: -2.96121, Convergence: 0.010916\n",
      "Epoch: 23, Loss: 230.99208, Residuals: -2.93089, Convergence: 0.011207\n",
      "Epoch: 24, Loss: 228.32356, Residuals: -2.89923, Convergence: 0.011687\n",
      "Epoch: 25, Loss: 225.51917, Residuals: -2.86555, Convergence: 0.012435\n",
      "Epoch: 26, Loss: 222.56375, Residuals: -2.82955, Convergence: 0.013279\n",
      "Epoch: 27, Loss: 219.56172, Residuals: -2.79224, Convergence: 0.013673\n",
      "Epoch: 28, Loss: 216.63218, Residuals: -2.75497, Convergence: 0.013523\n",
      "Epoch: 29, Loss: 213.80279, Residuals: -2.71814, Convergence: 0.013234\n",
      "Epoch: 30, Loss: 211.06098, Residuals: -2.68169, Convergence: 0.012991\n",
      "Epoch: 31, Loss: 208.39157, Residuals: -2.64551, Convergence: 0.012810\n",
      "Epoch: 32, Loss: 205.78433, Residuals: -2.60952, Convergence: 0.012670\n",
      "Epoch: 33, Loss: 203.23395, Residuals: -2.57367, Convergence: 0.012549\n",
      "Epoch: 34, Loss: 200.73852, Residuals: -2.53798, Convergence: 0.012431\n",
      "Epoch: 35, Loss: 198.29838, Residuals: -2.50245, Convergence: 0.012305\n",
      "Epoch: 36, Loss: 195.91499, Residuals: -2.46712, Convergence: 0.012165\n",
      "Epoch: 37, Loss: 193.59033, Residuals: -2.43200, Convergence: 0.012008\n",
      "Epoch: 38, Loss: 191.32641, Residuals: -2.39713, Convergence: 0.011833\n",
      "Epoch: 39, Loss: 189.12497, Residuals: -2.36252, Convergence: 0.011640\n",
      "Epoch: 40, Loss: 186.98735, Residuals: -2.32819, Convergence: 0.011432\n",
      "Epoch: 41, Loss: 184.91438, Residuals: -2.29415, Convergence: 0.011210\n",
      "Epoch: 42, Loss: 182.90642, Residuals: -2.26043, Convergence: 0.010978\n",
      "Epoch: 43, Loss: 180.96338, Residuals: -2.22704, Convergence: 0.010737\n",
      "Epoch: 44, Loss: 179.08477, Residuals: -2.19398, Convergence: 0.010490\n",
      "Epoch: 45, Loss: 177.26982, Residuals: -2.16127, Convergence: 0.010238\n",
      "Epoch: 46, Loss: 175.51754, Residuals: -2.12892, Convergence: 0.009984\n",
      "Epoch: 47, Loss: 173.82693, Residuals: -2.09694, Convergence: 0.009726\n",
      "Epoch: 48, Loss: 172.19712, Residuals: -2.06534, Convergence: 0.009465\n",
      "Epoch: 49, Loss: 170.62754, Residuals: -2.03415, Convergence: 0.009199\n",
      "Epoch: 50, Loss: 169.11811, Residuals: -2.00339, Convergence: 0.008925\n",
      "Epoch: 51, Loss: 167.66919, Residuals: -1.97308, Convergence: 0.008642\n",
      "Epoch: 52, Loss: 166.28143, Residuals: -1.94328, Convergence: 0.008346\n",
      "Epoch: 53, Loss: 164.95554, Residuals: -1.91403, Convergence: 0.008038\n",
      "Epoch: 54, Loss: 163.69186, Residuals: -1.88538, Convergence: 0.007720\n",
      "Epoch: 55, Loss: 162.49026, Residuals: -1.85736, Convergence: 0.007395\n",
      "Epoch: 56, Loss: 161.34996, Residuals: -1.83002, Convergence: 0.007067\n",
      "Epoch: 57, Loss: 160.26961, Residuals: -1.80340, Convergence: 0.006741\n",
      "Epoch: 58, Loss: 159.24735, Residuals: -1.77751, Convergence: 0.006419\n",
      "Epoch: 59, Loss: 158.28094, Residuals: -1.75238, Convergence: 0.006106\n",
      "Epoch: 60, Loss: 157.36782, Residuals: -1.72802, Convergence: 0.005802\n",
      "Epoch: 61, Loss: 156.50524, Residuals: -1.70443, Convergence: 0.005512\n",
      "Epoch: 62, Loss: 155.69033, Residuals: -1.68162, Convergence: 0.005234\n",
      "Epoch: 63, Loss: 154.92017, Residuals: -1.65958, Convergence: 0.004971\n",
      "Epoch: 64, Loss: 154.19185, Residuals: -1.63829, Convergence: 0.004723\n",
      "Epoch: 65, Loss: 153.50256, Residuals: -1.61775, Convergence: 0.004490\n",
      "Epoch: 66, Loss: 152.84955, Residuals: -1.59794, Convergence: 0.004272\n",
      "Epoch: 67, Loss: 152.23021, Residuals: -1.57883, Convergence: 0.004068\n",
      "Epoch: 68, Loss: 151.64203, Residuals: -1.56041, Convergence: 0.003879\n",
      "Epoch: 69, Loss: 151.08260, Residuals: -1.54265, Convergence: 0.003703\n",
      "Epoch: 70, Loss: 150.54967, Residuals: -1.52553, Convergence: 0.003540\n",
      "Epoch: 71, Loss: 150.04113, Residuals: -1.50902, Convergence: 0.003389\n",
      "Epoch: 72, Loss: 149.55509, Residuals: -1.49308, Convergence: 0.003250\n",
      "Epoch: 73, Loss: 149.08992, Residuals: -1.47770, Convergence: 0.003120\n",
      "Epoch: 74, Loss: 148.64426, Residuals: -1.46284, Convergence: 0.002998\n",
      "Epoch: 75, Loss: 148.21699, Residuals: -1.44849, Convergence: 0.002883\n",
      "Epoch: 76, Loss: 147.80725, Residuals: -1.43463, Convergence: 0.002772\n",
      "Epoch: 77, Loss: 147.41435, Residuals: -1.42124, Convergence: 0.002665\n",
      "Epoch: 78, Loss: 147.03773, Residuals: -1.40832, Convergence: 0.002561\n",
      "Epoch: 79, Loss: 146.67692, Residuals: -1.39585, Convergence: 0.002460\n",
      "Epoch: 80, Loss: 146.33149, Residuals: -1.38383, Convergence: 0.002361\n",
      "Epoch: 81, Loss: 146.00106, Residuals: -1.37224, Convergence: 0.002263\n",
      "Epoch: 82, Loss: 145.68526, Residuals: -1.36107, Convergence: 0.002168\n",
      "Epoch: 83, Loss: 145.38369, Residuals: -1.35033, Convergence: 0.002074\n",
      "Epoch: 84, Loss: 145.09599, Residuals: -1.33999, Convergence: 0.001983\n",
      "Epoch: 85, Loss: 144.82176, Residuals: -1.33005, Convergence: 0.001894\n",
      "Epoch: 86, Loss: 144.56063, Residuals: -1.32051, Convergence: 0.001806\n",
      "Epoch: 87, Loss: 144.31220, Residuals: -1.31134, Convergence: 0.001721\n",
      "Epoch: 88, Loss: 144.07610, Residuals: -1.30255, Convergence: 0.001639\n",
      "Epoch: 89, Loss: 143.85195, Residuals: -1.29413, Convergence: 0.001558\n",
      "Epoch: 90, Loss: 143.63937, Residuals: -1.28606, Convergence: 0.001480\n",
      "Epoch: 91, Loss: 143.43797, Residuals: -1.27834, Convergence: 0.001404\n",
      "Epoch: 92, Loss: 143.24738, Residuals: -1.27096, Convergence: 0.001331\n",
      "Epoch: 93, Loss: 143.06716, Residuals: -1.26391, Convergence: 0.001260\n",
      "Epoch: 94, Loss: 142.89689, Residuals: -1.25720, Convergence: 0.001192\n",
      "Epoch: 95, Loss: 142.73604, Residuals: -1.25082, Convergence: 0.001127\n",
      "Epoch: 96, Loss: 142.58404, Residuals: -1.24475, Convergence: 0.001066\n",
      "Epoch: 97, Loss: 142.44022, Residuals: -1.23899, Convergence: 0.001010\n",
      "Epoch: 98, Loss: 142.30378, Residuals: -1.23354, Convergence: 0.000959\n",
      "Evidence -183.349\n",
      "\n",
      "Epoch: 98, Evidence: -183.34923, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.23e-01\n",
      "Epoch: 98, Loss: 1382.03722, Residuals: -1.23354, Convergence:   inf\n",
      "Epoch: 99, Loss: 1319.91526, Residuals: -1.26361, Convergence: 0.047065\n",
      "Epoch: 100, Loss: 1272.92999, Residuals: -1.28751, Convergence: 0.036911\n",
      "Epoch: 101, Loss: 1237.55310, Residuals: -1.30459, Convergence: 0.028586\n",
      "Epoch: 102, Loss: 1209.97673, Residuals: -1.31641, Convergence: 0.022791\n",
      "Epoch: 103, Loss: 1187.59802, Residuals: -1.32494, Convergence: 0.018844\n",
      "Epoch: 104, Loss: 1168.96973, Residuals: -1.33124, Convergence: 0.015936\n",
      "Epoch: 105, Loss: 1153.21814, Residuals: -1.33582, Convergence: 0.013659\n",
      "Epoch: 106, Loss: 1139.74306, Residuals: -1.33897, Convergence: 0.011823\n",
      "Epoch: 107, Loss: 1128.09811, Residuals: -1.34086, Convergence: 0.010323\n",
      "Epoch: 108, Loss: 1117.93516, Residuals: -1.34166, Convergence: 0.009091\n",
      "Epoch: 109, Loss: 1108.97462, Residuals: -1.34150, Convergence: 0.008080\n",
      "Epoch: 110, Loss: 1100.98796, Residuals: -1.34048, Convergence: 0.007254\n",
      "Epoch: 111, Loss: 1093.78447, Residuals: -1.33870, Convergence: 0.006586\n",
      "Epoch: 112, Loss: 1087.20208, Residuals: -1.33622, Convergence: 0.006054\n",
      "Epoch: 113, Loss: 1081.10090, Residuals: -1.33310, Convergence: 0.005643\n",
      "Epoch: 114, Loss: 1075.35759, Residuals: -1.32938, Convergence: 0.005341\n",
      "Epoch: 115, Loss: 1069.86229, Residuals: -1.32508, Convergence: 0.005136\n",
      "Epoch: 116, Loss: 1064.51839, Residuals: -1.32020, Convergence: 0.005020\n",
      "Epoch: 117, Loss: 1059.24008, Residuals: -1.31475, Convergence: 0.004983\n",
      "Epoch: 118, Loss: 1053.95760, Residuals: -1.30873, Convergence: 0.005012\n",
      "Epoch: 119, Loss: 1048.62250, Residuals: -1.30216, Convergence: 0.005088\n",
      "Epoch: 120, Loss: 1043.21934, Residuals: -1.29508, Convergence: 0.005179\n",
      "Epoch: 121, Loss: 1037.77726, Residuals: -1.28757, Convergence: 0.005244\n",
      "Epoch: 122, Loss: 1032.37005, Residuals: -1.27971, Convergence: 0.005238\n",
      "Epoch: 123, Loss: 1027.09558, Residuals: -1.27162, Convergence: 0.005135\n",
      "Epoch: 124, Loss: 1022.04374, Residuals: -1.26337, Convergence: 0.004943\n",
      "Epoch: 125, Loss: 1017.27207, Residuals: -1.25505, Convergence: 0.004691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 126, Loss: 1012.80213, Residuals: -1.24672, Convergence: 0.004413\n",
      "Epoch: 127, Loss: 1008.62858, Residuals: -1.23845, Convergence: 0.004138\n",
      "Epoch: 128, Loss: 1004.73253, Residuals: -1.23027, Convergence: 0.003878\n",
      "Epoch: 129, Loss: 1001.08907, Residuals: -1.22224, Convergence: 0.003639\n",
      "Epoch: 130, Loss: 997.67331, Residuals: -1.21438, Convergence: 0.003424\n",
      "Epoch: 131, Loss: 994.46379, Residuals: -1.20671, Convergence: 0.003227\n",
      "Epoch: 132, Loss: 991.44082, Residuals: -1.19927, Convergence: 0.003049\n",
      "Epoch: 133, Loss: 988.58884, Residuals: -1.19206, Convergence: 0.002885\n",
      "Epoch: 134, Loss: 985.89425, Residuals: -1.18510, Convergence: 0.002733\n",
      "Epoch: 135, Loss: 983.34602, Residuals: -1.17840, Convergence: 0.002591\n",
      "Epoch: 136, Loss: 980.93439, Residuals: -1.17196, Convergence: 0.002459\n",
      "Epoch: 137, Loss: 978.65095, Residuals: -1.16580, Convergence: 0.002333\n",
      "Epoch: 138, Loss: 976.48748, Residuals: -1.15990, Convergence: 0.002216\n",
      "Epoch: 139, Loss: 974.43751, Residuals: -1.15427, Convergence: 0.002104\n",
      "Epoch: 140, Loss: 972.49352, Residuals: -1.14891, Convergence: 0.001999\n",
      "Epoch: 141, Loss: 970.64908, Residuals: -1.14381, Convergence: 0.001900\n",
      "Epoch: 142, Loss: 968.89769, Residuals: -1.13896, Convergence: 0.001808\n",
      "Epoch: 143, Loss: 967.23338, Residuals: -1.13435, Convergence: 0.001721\n",
      "Epoch: 144, Loss: 965.64961, Residuals: -1.12998, Convergence: 0.001640\n",
      "Epoch: 145, Loss: 964.14119, Residuals: -1.12583, Convergence: 0.001565\n",
      "Epoch: 146, Loss: 962.70211, Residuals: -1.12190, Convergence: 0.001495\n",
      "Epoch: 147, Loss: 961.32723, Residuals: -1.11816, Convergence: 0.001430\n",
      "Epoch: 148, Loss: 960.01214, Residuals: -1.11462, Convergence: 0.001370\n",
      "Epoch: 149, Loss: 958.75197, Residuals: -1.11126, Convergence: 0.001314\n",
      "Epoch: 150, Loss: 957.54220, Residuals: -1.10806, Convergence: 0.001263\n",
      "Epoch: 151, Loss: 956.37911, Residuals: -1.10502, Convergence: 0.001216\n",
      "Epoch: 152, Loss: 955.25919, Residuals: -1.10212, Convergence: 0.001172\n",
      "Epoch: 153, Loss: 954.17874, Residuals: -1.09937, Convergence: 0.001132\n",
      "Epoch: 154, Loss: 953.13496, Residuals: -1.09673, Convergence: 0.001095\n",
      "Epoch: 155, Loss: 952.12505, Residuals: -1.09422, Convergence: 0.001061\n",
      "Epoch: 156, Loss: 951.14631, Residuals: -1.09182, Convergence: 0.001029\n",
      "Epoch: 157, Loss: 950.19627, Residuals: -1.08951, Convergence: 0.001000\n",
      "Evidence 11137.117\n",
      "\n",
      "Epoch: 157, Evidence: 11137.11719, Convergence: 1.016463\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 5.77e-01\n",
      "Epoch: 157, Loss: 2353.25147, Residuals: -1.08951, Convergence:   inf\n",
      "Epoch: 158, Loss: 2312.59203, Residuals: -1.10204, Convergence: 0.017582\n",
      "Epoch: 159, Loss: 2284.62622, Residuals: -1.10172, Convergence: 0.012241\n",
      "Epoch: 160, Loss: 2261.01309, Residuals: -1.10036, Convergence: 0.010444\n",
      "Epoch: 161, Loss: 2240.88154, Residuals: -1.09856, Convergence: 0.008984\n",
      "Epoch: 162, Loss: 2223.60372, Residuals: -1.09645, Convergence: 0.007770\n",
      "Epoch: 163, Loss: 2208.68213, Residuals: -1.09412, Convergence: 0.006756\n",
      "Epoch: 164, Loss: 2195.70727, Residuals: -1.09162, Convergence: 0.005909\n",
      "Epoch: 165, Loss: 2184.33231, Residuals: -1.08896, Convergence: 0.005208\n",
      "Epoch: 166, Loss: 2174.26238, Residuals: -1.08616, Convergence: 0.004631\n",
      "Epoch: 167, Loss: 2165.24359, Residuals: -1.08321, Convergence: 0.004165\n",
      "Epoch: 168, Loss: 2157.06164, Residuals: -1.08010, Convergence: 0.003793\n",
      "Epoch: 169, Loss: 2149.53621, Residuals: -1.07680, Convergence: 0.003501\n",
      "Epoch: 170, Loss: 2142.53053, Residuals: -1.07333, Convergence: 0.003270\n",
      "Epoch: 171, Loss: 2135.95088, Residuals: -1.06967, Convergence: 0.003080\n",
      "Epoch: 172, Loss: 2129.74618, Residuals: -1.06585, Convergence: 0.002913\n",
      "Epoch: 173, Loss: 2123.89768, Residuals: -1.06193, Convergence: 0.002754\n",
      "Epoch: 174, Loss: 2118.40247, Residuals: -1.05797, Convergence: 0.002594\n",
      "Epoch: 175, Loss: 2113.25974, Residuals: -1.05401, Convergence: 0.002434\n",
      "Epoch: 176, Loss: 2108.46227, Residuals: -1.05011, Convergence: 0.002275\n",
      "Epoch: 177, Loss: 2103.99629, Residuals: -1.04630, Convergence: 0.002123\n",
      "Epoch: 178, Loss: 2099.84050, Residuals: -1.04261, Convergence: 0.001979\n",
      "Epoch: 179, Loss: 2095.97277, Residuals: -1.03906, Convergence: 0.001845\n",
      "Epoch: 180, Loss: 2092.36768, Residuals: -1.03566, Convergence: 0.001723\n",
      "Epoch: 181, Loss: 2089.00377, Residuals: -1.03242, Convergence: 0.001610\n",
      "Epoch: 182, Loss: 2085.85994, Residuals: -1.02934, Convergence: 0.001507\n",
      "Epoch: 183, Loss: 2082.91810, Residuals: -1.02643, Convergence: 0.001412\n",
      "Epoch: 184, Loss: 2080.16188, Residuals: -1.02367, Convergence: 0.001325\n",
      "Epoch: 185, Loss: 2077.57813, Residuals: -1.02108, Convergence: 0.001244\n",
      "Epoch: 186, Loss: 2075.15453, Residuals: -1.01863, Convergence: 0.001168\n",
      "Epoch: 187, Loss: 2072.88108, Residuals: -1.01633, Convergence: 0.001097\n",
      "Epoch: 188, Loss: 2070.74610, Residuals: -1.01416, Convergence: 0.001031\n",
      "Epoch: 189, Loss: 2068.74274, Residuals: -1.01212, Convergence: 0.000968\n",
      "Evidence 14294.463\n",
      "\n",
      "Epoch: 189, Evidence: 14294.46289, Convergence: 0.220879\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 4.41e-01\n",
      "Epoch: 189, Loss: 2476.28950, Residuals: -1.01212, Convergence:   inf\n",
      "Epoch: 190, Loss: 2461.49296, Residuals: -1.00960, Convergence: 0.006011\n",
      "Epoch: 191, Loss: 2449.55300, Residuals: -1.00624, Convergence: 0.004874\n",
      "Epoch: 192, Loss: 2439.34320, Residuals: -1.00287, Convergence: 0.004185\n",
      "Epoch: 193, Loss: 2430.53197, Residuals: -0.99964, Convergence: 0.003625\n",
      "Epoch: 194, Loss: 2422.87700, Residuals: -0.99659, Convergence: 0.003159\n",
      "Epoch: 195, Loss: 2416.18724, Residuals: -0.99374, Convergence: 0.002769\n",
      "Epoch: 196, Loss: 2410.30857, Residuals: -0.99106, Convergence: 0.002439\n",
      "Epoch: 197, Loss: 2405.11297, Residuals: -0.98856, Convergence: 0.002160\n",
      "Epoch: 198, Loss: 2400.49349, Residuals: -0.98621, Convergence: 0.001924\n",
      "Epoch: 199, Loss: 2396.36117, Residuals: -0.98402, Convergence: 0.001724\n",
      "Epoch: 200, Loss: 2392.64169, Residuals: -0.98196, Convergence: 0.001555\n",
      "Epoch: 201, Loss: 2389.27331, Residuals: -0.98003, Convergence: 0.001410\n",
      "Epoch: 202, Loss: 2386.20526, Residuals: -0.97821, Convergence: 0.001286\n",
      "Epoch: 203, Loss: 2383.39425, Residuals: -0.97651, Convergence: 0.001179\n",
      "Epoch: 204, Loss: 2380.80569, Residuals: -0.97491, Convergence: 0.001087\n",
      "Epoch: 205, Loss: 2378.41017, Residuals: -0.97341, Convergence: 0.001007\n",
      "Epoch: 206, Loss: 2376.18395, Residuals: -0.97201, Convergence: 0.000937\n",
      "Evidence 14726.541\n",
      "\n",
      "Epoch: 206, Evidence: 14726.54102, Convergence: 0.029340\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 3.39e-01\n",
      "Epoch: 206, Loss: 2482.11548, Residuals: -0.97201, Convergence:   inf\n",
      "Epoch: 207, Loss: 2475.31405, Residuals: -0.96877, Convergence: 0.002748\n",
      "Epoch: 208, Loss: 2469.71726, Residuals: -0.96576, Convergence: 0.002266\n",
      "Epoch: 209, Loss: 2464.96580, Residuals: -0.96308, Convergence: 0.001928\n",
      "Epoch: 210, Loss: 2460.86961, Residuals: -0.96072, Convergence: 0.001665\n",
      "Epoch: 211, Loss: 2457.28843, Residuals: -0.95861, Convergence: 0.001457\n",
      "Epoch: 212, Loss: 2454.11968, Residuals: -0.95674, Convergence: 0.001291\n",
      "Epoch: 213, Loss: 2451.28133, Residuals: -0.95508, Convergence: 0.001158\n",
      "Epoch: 214, Loss: 2448.71445, Residuals: -0.95360, Convergence: 0.001048\n",
      "Epoch: 215, Loss: 2446.37171, Residuals: -0.95228, Convergence: 0.000958\n",
      "Evidence 14815.962\n",
      "\n",
      "Epoch: 215, Evidence: 14815.96191, Convergence: 0.006035\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.67e-01\n",
      "Epoch: 215, Loss: 2483.64747, Residuals: -0.95228, Convergence:   inf\n",
      "Epoch: 216, Loss: 2479.62282, Residuals: -0.94968, Convergence: 0.001623\n",
      "Epoch: 217, Loss: 2476.26234, Residuals: -0.94749, Convergence: 0.001357\n",
      "Epoch: 218, Loss: 2473.36796, Residuals: -0.94565, Convergence: 0.001170\n",
      "Epoch: 219, Loss: 2470.82405, Residuals: -0.94407, Convergence: 0.001030\n",
      "Epoch: 220, Loss: 2468.55443, Residuals: -0.94273, Convergence: 0.000919\n",
      "Evidence 14847.607\n",
      "\n",
      "Epoch: 220, Evidence: 14847.60742, Convergence: 0.002131\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.16e-01\n",
      "Epoch: 220, Loss: 2484.49071, Residuals: -0.94273, Convergence:   inf\n",
      "Epoch: 221, Loss: 2481.58106, Residuals: -0.94068, Convergence: 0.001172\n",
      "Epoch: 222, Loss: 2479.11719, Residuals: -0.93901, Convergence: 0.000994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence 14860.316\n",
      "\n",
      "Epoch: 222, Evidence: 14860.31641, Convergence: 0.000855\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.80e-01\n",
      "Epoch: 222, Loss: 2485.15959, Residuals: -0.93901, Convergence:   inf\n",
      "Epoch: 223, Loss: 2480.47059, Residuals: -0.93612, Convergence: 0.001890\n",
      "Epoch: 224, Loss: 2476.85646, Residuals: -0.93391, Convergence: 0.001459\n",
      "Epoch: 225, Loss: 2473.88788, Residuals: -0.93231, Convergence: 0.001200\n",
      "Epoch: 226, Loss: 2471.34819, Residuals: -0.93124, Convergence: 0.001028\n",
      "Epoch: 227, Loss: 2469.11071, Residuals: -0.93051, Convergence: 0.000906\n",
      "Evidence 14881.087\n",
      "\n",
      "Epoch: 227, Evidence: 14881.08691, Convergence: 0.002250\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.49e-01\n",
      "Epoch: 227, Loss: 2485.19358, Residuals: -0.93051, Convergence:   inf\n",
      "Epoch: 228, Loss: 2482.17080, Residuals: -0.92799, Convergence: 0.001218\n",
      "Epoch: 229, Loss: 2479.74903, Residuals: -0.92659, Convergence: 0.000977\n",
      "Evidence 14892.688\n",
      "\n",
      "Epoch: 229, Evidence: 14892.68848, Convergence: 0.000779\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.26e-01\n",
      "Epoch: 229, Loss: 2485.44492, Residuals: -0.92659, Convergence:   inf\n",
      "Epoch: 230, Loss: 2480.91687, Residuals: -0.92303, Convergence: 0.001825\n",
      "Epoch: 231, Loss: 2477.63319, Residuals: -0.92373, Convergence: 0.001325\n",
      "Epoch: 232, Loss: 2474.90554, Residuals: -0.92392, Convergence: 0.001102\n",
      "Epoch: 233, Loss: 2472.52599, Residuals: -0.92591, Convergence: 0.000962\n",
      "Evidence 14908.820\n",
      "\n",
      "Epoch: 233, Evidence: 14908.82031, Convergence: 0.001860\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.12e-01\n",
      "Epoch: 233, Loss: 2484.93463, Residuals: -0.92591, Convergence:   inf\n",
      "Epoch: 234, Loss: 2483.12244, Residuals: -0.92404, Convergence: 0.000730\n",
      "Evidence 14915.360\n",
      "\n",
      "Epoch: 234, Evidence: 14915.36035, Convergence: 0.000438\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 9.17e-02\n",
      "Epoch: 234, Loss: 2485.87485, Residuals: -0.92404, Convergence:   inf\n",
      "Epoch: 235, Loss: 2532.95854, Residuals: -0.96382, Convergence: -0.018588\n",
      "Epoch: 235, Loss: 2483.36804, Residuals: -0.92149, Convergence: 0.001009\n",
      "Epoch: 236, Loss: 2482.80317, Residuals: -0.92293, Convergence: 0.000228\n",
      "Evidence 14920.684\n",
      "\n",
      "Epoch: 236, Evidence: 14920.68359, Convergence: 0.000795\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.59e-02\n",
      "Epoch: 236, Loss: 2485.82038, Residuals: -0.92293, Convergence:   inf\n",
      "Epoch: 237, Loss: 2539.32905, Residuals: -0.96947, Convergence: -0.021072\n",
      "Epoch: 237, Loss: 2483.50303, Residuals: -0.92028, Convergence: 0.000933\n",
      "Evidence 14925.120\n",
      "\n",
      "Epoch: 237, Evidence: 14925.12012, Convergence: 0.001092\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.03e-02\n",
      "Epoch: 237, Loss: 2485.47748, Residuals: -0.92028, Convergence:   inf\n",
      "Epoch: 238, Loss: 2485.45295, Residuals: -0.91830, Convergence: 0.000010\n",
      "Evidence 14926.438\n",
      "\n",
      "Epoch: 238, Evidence: 14926.43750, Convergence: 0.000088\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 6.03e-02\n",
      "Epoch: 238, Loss: 2485.98989, Residuals: -0.91830, Convergence:   inf\n",
      "Epoch: 239, Loss: 2543.58412, Residuals: -0.97526, Convergence: -0.022643\n",
      "Epoch: 239, Loss: 2484.08036, Residuals: -0.91688, Convergence: 0.000769\n",
      "Evidence 14929.619\n",
      "\n",
      "Epoch: 239, Evidence: 14929.61914, Convergence: 0.000301\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 5.74e-02\n",
      "Epoch: 239, Loss: 2485.61902, Residuals: -0.91688, Convergence:   inf\n",
      "Epoch: 240, Loss: 2491.24066, Residuals: -0.92979, Convergence: -0.002257\n",
      "Epoch: 240, Loss: 2485.98685, Residuals: -0.91881, Convergence: -0.000148\n",
      "Evidence 14930.195\n",
      "\n",
      "Epoch: 240, Evidence: 14930.19531, Convergence: 0.000340\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 382.54121, Residuals: -4.53353, Convergence:   inf\n",
      "Epoch: 1, Loss: 356.79528, Residuals: -4.41332, Convergence: 0.072159\n",
      "Epoch: 2, Loss: 335.68956, Residuals: -4.24888, Convergence: 0.062873\n",
      "Epoch: 3, Loss: 319.57059, Residuals: -4.08418, Convergence: 0.050439\n",
      "Epoch: 4, Loss: 307.27588, Residuals: -3.93885, Convergence: 0.040012\n",
      "Epoch: 5, Loss: 297.52332, Residuals: -3.81014, Convergence: 0.032779\n",
      "Epoch: 6, Loss: 289.60840, Residuals: -3.69803, Convergence: 0.027330\n",
      "Epoch: 7, Loss: 283.04082, Residuals: -3.60210, Convergence: 0.023204\n",
      "Epoch: 8, Loss: 277.45782, Residuals: -3.52025, Convergence: 0.020122\n",
      "Epoch: 9, Loss: 272.60376, Residuals: -3.45012, Convergence: 0.017806\n",
      "Epoch: 10, Loss: 268.29680, Residuals: -3.38959, Convergence: 0.016053\n",
      "Epoch: 11, Loss: 264.40507, Residuals: -3.33683, Convergence: 0.014719\n",
      "Epoch: 12, Loss: 260.83211, Residuals: -3.29031, Convergence: 0.013698\n",
      "Epoch: 13, Loss: 257.50740, Residuals: -3.24872, Convergence: 0.012911\n",
      "Epoch: 14, Loss: 254.38004, Residuals: -3.21091, Convergence: 0.012294\n",
      "Epoch: 15, Loss: 251.41565, Residuals: -3.17589, Convergence: 0.011791\n",
      "Epoch: 16, Loss: 248.59483, Residuals: -3.14301, Convergence: 0.011347\n",
      "Epoch: 17, Loss: 245.90554, Residuals: -3.11185, Convergence: 0.010936\n",
      "Epoch: 18, Loss: 243.32918, Residuals: -3.08202, Convergence: 0.010588\n",
      "Epoch: 19, Loss: 240.83551, Residuals: -3.05304, Convergence: 0.010354\n",
      "Epoch: 20, Loss: 238.38874, Residuals: -3.02433, Convergence: 0.010264\n",
      "Epoch: 21, Loss: 235.95654, Residuals: -2.99536, Convergence: 0.010308\n",
      "Epoch: 22, Loss: 233.51540, Residuals: -2.96578, Convergence: 0.010454\n",
      "Epoch: 23, Loss: 231.04434, Residuals: -2.93535, Convergence: 0.010695\n",
      "Epoch: 24, Loss: 228.50790, Residuals: -2.90370, Convergence: 0.011100\n",
      "Epoch: 25, Loss: 225.85093, Residuals: -2.87019, Convergence: 0.011764\n",
      "Epoch: 26, Loss: 223.04107, Residuals: -2.83439, Convergence: 0.012598\n",
      "Epoch: 27, Loss: 220.15622, Residuals: -2.79708, Convergence: 0.013104\n",
      "Epoch: 28, Loss: 217.32455, Residuals: -2.75970, Convergence: 0.013030\n",
      "Epoch: 29, Loss: 214.59025, Residuals: -2.72288, Convergence: 0.012742\n",
      "Epoch: 30, Loss: 211.94452, Residuals: -2.68665, Convergence: 0.012483\n",
      "Epoch: 31, Loss: 209.37093, Residuals: -2.65089, Convergence: 0.012292\n",
      "Epoch: 32, Loss: 206.85700, Residuals: -2.61546, Convergence: 0.012153\n",
      "Epoch: 33, Loss: 204.39525, Residuals: -2.58030, Convergence: 0.012044\n",
      "Epoch: 34, Loss: 201.98203, Residuals: -2.54534, Convergence: 0.011948\n",
      "Epoch: 35, Loss: 199.61635, Residuals: -2.51055, Convergence: 0.011851\n",
      "Epoch: 36, Loss: 197.29883, Residuals: -2.47591, Convergence: 0.011746\n",
      "Epoch: 37, Loss: 195.03091, Residuals: -2.44142, Convergence: 0.011628\n",
      "Epoch: 38, Loss: 192.81437, Residuals: -2.40708, Convergence: 0.011496\n",
      "Epoch: 39, Loss: 190.65098, Residuals: -2.37290, Convergence: 0.011347\n",
      "Epoch: 40, Loss: 188.54229, Residuals: -2.33891, Convergence: 0.011184\n",
      "Epoch: 41, Loss: 186.48955, Residuals: -2.30512, Convergence: 0.011007\n",
      "Epoch: 42, Loss: 184.49369, Residuals: -2.27155, Convergence: 0.010818\n",
      "Epoch: 43, Loss: 182.55535, Residuals: -2.23822, Convergence: 0.010618\n",
      "Epoch: 44, Loss: 180.67500, Residuals: -2.20517, Convergence: 0.010407\n",
      "Epoch: 45, Loss: 178.85305, Residuals: -2.17241, Convergence: 0.010187\n",
      "Epoch: 46, Loss: 177.09005, Residuals: -2.13998, Convergence: 0.009955\n",
      "Epoch: 47, Loss: 175.38683, Residuals: -2.10792, Convergence: 0.009711\n",
      "Epoch: 48, Loss: 173.74453, Residuals: -2.07628, Convergence: 0.009452\n",
      "Epoch: 49, Loss: 172.16446, Residuals: -2.04510, Convergence: 0.009178\n",
      "Epoch: 50, Loss: 170.64790, Residuals: -2.01443, Convergence: 0.008887\n",
      "Epoch: 51, Loss: 169.19572, Residuals: -1.98433, Convergence: 0.008583\n",
      "Epoch: 52, Loss: 167.80827, Residuals: -1.95486, Convergence: 0.008268\n",
      "Epoch: 53, Loss: 166.48516, Residuals: -1.92605, Convergence: 0.007947\n",
      "Epoch: 54, Loss: 165.22532, Residuals: -1.89795, Convergence: 0.007625\n",
      "Epoch: 55, Loss: 164.02702, Residuals: -1.87058, Convergence: 0.007305\n",
      "Epoch: 56, Loss: 162.88798, Residuals: -1.84396, Convergence: 0.006993\n",
      "Epoch: 57, Loss: 161.80546, Residuals: -1.81812, Convergence: 0.006690\n",
      "Epoch: 58, Loss: 160.77628, Residuals: -1.79305, Convergence: 0.006401\n",
      "Epoch: 59, Loss: 159.79707, Residuals: -1.76875, Convergence: 0.006128\n",
      "Epoch: 60, Loss: 158.86429, Residuals: -1.74521, Convergence: 0.005872\n",
      "Epoch: 61, Loss: 157.97443, Residuals: -1.72239, Convergence: 0.005633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62, Loss: 157.12411, Residuals: -1.70027, Convergence: 0.005412\n",
      "Epoch: 63, Loss: 156.31027, Residuals: -1.67882, Convergence: 0.005207\n",
      "Epoch: 64, Loss: 155.53021, Residuals: -1.65801, Convergence: 0.005015\n",
      "Epoch: 65, Loss: 154.78168, Residuals: -1.63782, Convergence: 0.004836\n",
      "Epoch: 66, Loss: 154.06284, Residuals: -1.61821, Convergence: 0.004666\n",
      "Epoch: 67, Loss: 153.37226, Residuals: -1.59919, Convergence: 0.004503\n",
      "Epoch: 68, Loss: 152.70882, Residuals: -1.58072, Convergence: 0.004344\n",
      "Epoch: 69, Loss: 152.07171, Residuals: -1.56280, Convergence: 0.004190\n",
      "Epoch: 70, Loss: 151.46025, Residuals: -1.54542, Convergence: 0.004037\n",
      "Epoch: 71, Loss: 150.87392, Residuals: -1.52859, Convergence: 0.003886\n",
      "Epoch: 72, Loss: 150.31222, Residuals: -1.51229, Convergence: 0.003737\n",
      "Epoch: 73, Loss: 149.77466, Residuals: -1.49653, Convergence: 0.003589\n",
      "Epoch: 74, Loss: 149.26074, Residuals: -1.48129, Convergence: 0.003443\n",
      "Epoch: 75, Loss: 148.76991, Residuals: -1.46658, Convergence: 0.003299\n",
      "Epoch: 76, Loss: 148.30153, Residuals: -1.45239, Convergence: 0.003158\n",
      "Epoch: 77, Loss: 147.85495, Residuals: -1.43871, Convergence: 0.003020\n",
      "Epoch: 78, Loss: 147.42944, Residuals: -1.42554, Convergence: 0.002886\n",
      "Epoch: 79, Loss: 147.02421, Residuals: -1.41286, Convergence: 0.002756\n",
      "Epoch: 80, Loss: 146.63847, Residuals: -1.40067, Convergence: 0.002631\n",
      "Epoch: 81, Loss: 146.27140, Residuals: -1.38895, Convergence: 0.002510\n",
      "Epoch: 82, Loss: 145.92216, Residuals: -1.37770, Convergence: 0.002393\n",
      "Epoch: 83, Loss: 145.58995, Residuals: -1.36689, Convergence: 0.002282\n",
      "Epoch: 84, Loss: 145.27397, Residuals: -1.35652, Convergence: 0.002175\n",
      "Epoch: 85, Loss: 144.97343, Residuals: -1.34657, Convergence: 0.002073\n",
      "Epoch: 86, Loss: 144.68760, Residuals: -1.33702, Convergence: 0.001976\n",
      "Epoch: 87, Loss: 144.41579, Residuals: -1.32788, Convergence: 0.001882\n",
      "Epoch: 88, Loss: 144.15734, Residuals: -1.31911, Convergence: 0.001793\n",
      "Epoch: 89, Loss: 143.91166, Residuals: -1.31071, Convergence: 0.001707\n",
      "Epoch: 90, Loss: 143.67818, Residuals: -1.30266, Convergence: 0.001625\n",
      "Epoch: 91, Loss: 143.45637, Residuals: -1.29495, Convergence: 0.001546\n",
      "Epoch: 92, Loss: 143.24578, Residuals: -1.28757, Convergence: 0.001470\n",
      "Epoch: 93, Loss: 143.04599, Residuals: -1.28050, Convergence: 0.001397\n",
      "Epoch: 94, Loss: 142.85660, Residuals: -1.27375, Convergence: 0.001326\n",
      "Epoch: 95, Loss: 142.67727, Residuals: -1.26728, Convergence: 0.001257\n",
      "Epoch: 96, Loss: 142.50768, Residuals: -1.26110, Convergence: 0.001190\n",
      "Epoch: 97, Loss: 142.34753, Residuals: -1.25520, Convergence: 0.001125\n",
      "Epoch: 98, Loss: 142.19654, Residuals: -1.24957, Convergence: 0.001062\n",
      "Epoch: 99, Loss: 142.05446, Residuals: -1.24420, Convergence: 0.001000\n",
      "Epoch: 100, Loss: 141.92102, Residuals: -1.23909, Convergence: 0.000940\n",
      "Evidence -183.154\n",
      "\n",
      "Epoch: 100, Evidence: -183.15366, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.24e-01\n",
      "Epoch: 100, Loss: 1364.71055, Residuals: -1.23909, Convergence:   inf\n",
      "Epoch: 101, Loss: 1304.17535, Residuals: -1.26848, Convergence: 0.046416\n",
      "Epoch: 102, Loss: 1258.01045, Residuals: -1.29153, Convergence: 0.036697\n",
      "Epoch: 103, Loss: 1223.04689, Residuals: -1.30802, Convergence: 0.028587\n",
      "Epoch: 104, Loss: 1195.82166, Residuals: -1.31943, Convergence: 0.022767\n",
      "Epoch: 105, Loss: 1173.80068, Residuals: -1.32769, Convergence: 0.018760\n",
      "Epoch: 106, Loss: 1155.50982, Residuals: -1.33387, Convergence: 0.015829\n",
      "Epoch: 107, Loss: 1140.06100, Residuals: -1.33848, Convergence: 0.013551\n",
      "Epoch: 108, Loss: 1126.85386, Residuals: -1.34179, Convergence: 0.011720\n",
      "Epoch: 109, Loss: 1115.44417, Residuals: -1.34396, Convergence: 0.010229\n",
      "Epoch: 110, Loss: 1105.48648, Residuals: -1.34512, Convergence: 0.009008\n",
      "Epoch: 111, Loss: 1096.70265, Residuals: -1.34538, Convergence: 0.008009\n",
      "Epoch: 112, Loss: 1088.86529, Residuals: -1.34482, Convergence: 0.007198\n",
      "Epoch: 113, Loss: 1081.78209, Residuals: -1.34350, Convergence: 0.006548\n",
      "Epoch: 114, Loss: 1075.28880, Residuals: -1.34147, Convergence: 0.006039\n",
      "Epoch: 115, Loss: 1069.24117, Residuals: -1.33876, Convergence: 0.005656\n",
      "Epoch: 116, Loss: 1063.51141, Residuals: -1.33537, Convergence: 0.005388\n",
      "Epoch: 117, Loss: 1057.98498, Residuals: -1.33132, Convergence: 0.005224\n",
      "Epoch: 118, Loss: 1052.56220, Residuals: -1.32659, Convergence: 0.005152\n",
      "Epoch: 119, Loss: 1047.16252, Residuals: -1.32121, Convergence: 0.005156\n",
      "Epoch: 120, Loss: 1041.73361, Residuals: -1.31520, Convergence: 0.005211\n",
      "Epoch: 121, Loss: 1036.26483, Residuals: -1.30862, Convergence: 0.005277\n",
      "Epoch: 122, Loss: 1030.79532, Residuals: -1.30156, Convergence: 0.005306\n",
      "Epoch: 123, Loss: 1025.40627, Residuals: -1.29412, Convergence: 0.005256\n",
      "Epoch: 124, Loss: 1020.19324, Residuals: -1.28639, Convergence: 0.005110\n",
      "Epoch: 125, Loss: 1015.23273, Residuals: -1.27846, Convergence: 0.004886\n",
      "Epoch: 126, Loss: 1010.56726, Residuals: -1.27041, Convergence: 0.004617\n",
      "Epoch: 127, Loss: 1006.20560, Residuals: -1.26231, Convergence: 0.004335\n",
      "Epoch: 128, Loss: 1002.13556, Residuals: -1.25422, Convergence: 0.004061\n",
      "Epoch: 129, Loss: 998.33432, Residuals: -1.24618, Convergence: 0.003808\n",
      "Epoch: 130, Loss: 994.77630, Residuals: -1.23824, Convergence: 0.003577\n",
      "Epoch: 131, Loss: 991.43747, Residuals: -1.23044, Convergence: 0.003368\n",
      "Epoch: 132, Loss: 988.29648, Residuals: -1.22281, Convergence: 0.003178\n",
      "Epoch: 133, Loss: 985.33389, Residuals: -1.21536, Convergence: 0.003007\n",
      "Epoch: 134, Loss: 982.53410, Residuals: -1.20812, Convergence: 0.002850\n",
      "Epoch: 135, Loss: 979.88419, Residuals: -1.20110, Convergence: 0.002704\n",
      "Epoch: 136, Loss: 977.37202, Residuals: -1.19432, Convergence: 0.002570\n",
      "Epoch: 137, Loss: 974.98817, Residuals: -1.18778, Convergence: 0.002445\n",
      "Epoch: 138, Loss: 972.72397, Residuals: -1.18149, Convergence: 0.002328\n",
      "Epoch: 139, Loss: 970.57138, Residuals: -1.17544, Convergence: 0.002218\n",
      "Epoch: 140, Loss: 968.52369, Residuals: -1.16965, Convergence: 0.002114\n",
      "Epoch: 141, Loss: 966.57376, Residuals: -1.16411, Convergence: 0.002017\n",
      "Epoch: 142, Loss: 964.71563, Residuals: -1.15881, Convergence: 0.001926\n",
      "Epoch: 143, Loss: 962.94347, Residuals: -1.15376, Convergence: 0.001840\n",
      "Epoch: 144, Loss: 961.25137, Residuals: -1.14893, Convergence: 0.001760\n",
      "Epoch: 145, Loss: 959.63399, Residuals: -1.14434, Convergence: 0.001685\n",
      "Epoch: 146, Loss: 958.08621, Residuals: -1.13995, Convergence: 0.001615\n",
      "Epoch: 147, Loss: 956.60311, Residuals: -1.13578, Convergence: 0.001550\n",
      "Epoch: 148, Loss: 955.17989, Residuals: -1.13180, Convergence: 0.001490\n",
      "Epoch: 149, Loss: 953.81187, Residuals: -1.12801, Convergence: 0.001434\n",
      "Epoch: 150, Loss: 952.49475, Residuals: -1.12439, Convergence: 0.001383\n",
      "Epoch: 151, Loss: 951.22427, Residuals: -1.12094, Convergence: 0.001336\n",
      "Epoch: 152, Loss: 949.99590, Residuals: -1.11764, Convergence: 0.001293\n",
      "Epoch: 153, Loss: 948.80577, Residuals: -1.11449, Convergence: 0.001254\n",
      "Epoch: 154, Loss: 947.64914, Residuals: -1.11147, Convergence: 0.001221\n",
      "Epoch: 155, Loss: 946.52219, Residuals: -1.10857, Convergence: 0.001191\n",
      "Epoch: 156, Loss: 945.42023, Residuals: -1.10579, Convergence: 0.001166\n",
      "Epoch: 157, Loss: 944.33932, Residuals: -1.10310, Convergence: 0.001145\n",
      "Epoch: 158, Loss: 943.27467, Residuals: -1.10051, Convergence: 0.001129\n",
      "Epoch: 159, Loss: 942.22149, Residuals: -1.09800, Convergence: 0.001118\n",
      "Epoch: 160, Loss: 941.17540, Residuals: -1.09556, Convergence: 0.001111\n",
      "Epoch: 161, Loss: 940.13152, Residuals: -1.09318, Convergence: 0.001110\n",
      "Epoch: 162, Loss: 939.08524, Residuals: -1.09084, Convergence: 0.001114\n",
      "Epoch: 163, Loss: 938.03245, Residuals: -1.08853, Convergence: 0.001122\n",
      "Epoch: 164, Loss: 936.96928, Residuals: -1.08625, Convergence: 0.001135\n",
      "Epoch: 165, Loss: 935.89318, Residuals: -1.08398, Convergence: 0.001150\n",
      "Epoch: 166, Loss: 934.80237, Residuals: -1.08172, Convergence: 0.001167\n",
      "Epoch: 167, Loss: 933.69726, Residuals: -1.07947, Convergence: 0.001184\n",
      "Epoch: 168, Loss: 932.58008, Residuals: -1.07722, Convergence: 0.001198\n",
      "Epoch: 169, Loss: 931.45434, Residuals: -1.07498, Convergence: 0.001209\n",
      "Epoch: 170, Loss: 930.32576, Residuals: -1.07276, Convergence: 0.001213\n",
      "Epoch: 171, Loss: 929.20117, Residuals: -1.07057, Convergence: 0.001210\n",
      "Epoch: 172, Loss: 928.08807, Residuals: -1.06841, Convergence: 0.001199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 173, Loss: 926.99337, Residuals: -1.06631, Convergence: 0.001181\n",
      "Epoch: 174, Loss: 925.92407, Residuals: -1.06426, Convergence: 0.001155\n",
      "Epoch: 175, Loss: 924.88576, Residuals: -1.06228, Convergence: 0.001123\n",
      "Epoch: 176, Loss: 923.88329, Residuals: -1.06038, Convergence: 0.001085\n",
      "Epoch: 177, Loss: 922.91997, Residuals: -1.05855, Convergence: 0.001044\n",
      "Epoch: 178, Loss: 921.99801, Residuals: -1.05679, Convergence: 0.001000\n",
      "Evidence 11151.769\n",
      "\n",
      "Epoch: 178, Evidence: 11151.76855, Convergence: 1.016424\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.74e-01\n",
      "Epoch: 178, Loss: 2342.60832, Residuals: -1.05679, Convergence:   inf\n",
      "Epoch: 179, Loss: 2304.08049, Residuals: -1.06434, Convergence: 0.016722\n",
      "Epoch: 180, Loss: 2277.65660, Residuals: -1.06356, Convergence: 0.011601\n",
      "Epoch: 181, Loss: 2255.72051, Residuals: -1.06191, Convergence: 0.009725\n",
      "Epoch: 182, Loss: 2237.24310, Residuals: -1.06006, Convergence: 0.008259\n",
      "Epoch: 183, Loss: 2221.54598, Residuals: -1.05812, Convergence: 0.007066\n",
      "Epoch: 184, Loss: 2208.09643, Residuals: -1.05611, Convergence: 0.006091\n",
      "Epoch: 185, Loss: 2196.45666, Residuals: -1.05402, Convergence: 0.005299\n",
      "Epoch: 186, Loss: 2186.26156, Residuals: -1.05185, Convergence: 0.004663\n",
      "Epoch: 187, Loss: 2177.21251, Residuals: -1.04955, Convergence: 0.004156\n",
      "Epoch: 188, Loss: 2169.07253, Residuals: -1.04711, Convergence: 0.003753\n",
      "Epoch: 189, Loss: 2161.67146, Residuals: -1.04451, Convergence: 0.003424\n",
      "Epoch: 190, Loss: 2154.89897, Residuals: -1.04176, Convergence: 0.003143\n",
      "Epoch: 191, Loss: 2148.68587, Residuals: -1.03891, Convergence: 0.002892\n",
      "Epoch: 192, Loss: 2142.98575, Residuals: -1.03601, Convergence: 0.002660\n",
      "Epoch: 193, Loss: 2137.75730, Residuals: -1.03309, Convergence: 0.002446\n",
      "Epoch: 194, Loss: 2132.95863, Residuals: -1.03019, Convergence: 0.002250\n",
      "Epoch: 195, Loss: 2128.54917, Residuals: -1.02735, Convergence: 0.002072\n",
      "Epoch: 196, Loss: 2124.48791, Residuals: -1.02457, Convergence: 0.001912\n",
      "Epoch: 197, Loss: 2120.73924, Residuals: -1.02188, Convergence: 0.001768\n",
      "Epoch: 198, Loss: 2117.27209, Residuals: -1.01928, Convergence: 0.001638\n",
      "Epoch: 199, Loss: 2114.05949, Residuals: -1.01677, Convergence: 0.001520\n",
      "Epoch: 200, Loss: 2111.07763, Residuals: -1.01435, Convergence: 0.001412\n",
      "Epoch: 201, Loss: 2108.30666, Residuals: -1.01204, Convergence: 0.001314\n",
      "Epoch: 202, Loss: 2105.72883, Residuals: -1.00982, Convergence: 0.001224\n",
      "Epoch: 203, Loss: 2103.32680, Residuals: -1.00770, Convergence: 0.001142\n",
      "Epoch: 204, Loss: 2101.08653, Residuals: -1.00567, Convergence: 0.001066\n",
      "Epoch: 205, Loss: 2098.99287, Residuals: -1.00373, Convergence: 0.000997\n",
      "Evidence 14284.609\n",
      "\n",
      "Epoch: 205, Evidence: 14284.60938, Convergence: 0.219316\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.38e-01\n",
      "Epoch: 205, Loss: 2470.19333, Residuals: -1.00373, Convergence:   inf\n",
      "Epoch: 206, Loss: 2456.45870, Residuals: -1.00147, Convergence: 0.005591\n",
      "Epoch: 207, Loss: 2445.27092, Residuals: -0.99847, Convergence: 0.004575\n",
      "Epoch: 208, Loss: 2435.63130, Residuals: -0.99548, Convergence: 0.003958\n",
      "Epoch: 209, Loss: 2427.25568, Residuals: -0.99262, Convergence: 0.003451\n",
      "Epoch: 210, Loss: 2419.93684, Residuals: -0.98995, Convergence: 0.003024\n",
      "Epoch: 211, Loss: 2413.51007, Residuals: -0.98748, Convergence: 0.002663\n",
      "Epoch: 212, Loss: 2407.84258, Residuals: -0.98520, Convergence: 0.002354\n",
      "Epoch: 213, Loss: 2402.81992, Residuals: -0.98309, Convergence: 0.002090\n",
      "Epoch: 214, Loss: 2398.34914, Residuals: -0.98114, Convergence: 0.001864\n",
      "Epoch: 215, Loss: 2394.34934, Residuals: -0.97933, Convergence: 0.001671\n",
      "Epoch: 216, Loss: 2390.75130, Residuals: -0.97765, Convergence: 0.001505\n",
      "Epoch: 217, Loss: 2387.49828, Residuals: -0.97610, Convergence: 0.001363\n",
      "Epoch: 218, Loss: 2384.54106, Residuals: -0.97466, Convergence: 0.001240\n",
      "Epoch: 219, Loss: 2381.83919, Residuals: -0.97332, Convergence: 0.001134\n",
      "Epoch: 220, Loss: 2379.35848, Residuals: -0.97209, Convergence: 0.001043\n",
      "Epoch: 221, Loss: 2377.07019, Residuals: -0.97095, Convergence: 0.000963\n",
      "Evidence 14658.441\n",
      "\n",
      "Epoch: 221, Evidence: 14658.44141, Convergence: 0.025503\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.35e-01\n",
      "Epoch: 221, Loss: 2475.38752, Residuals: -0.97095, Convergence:   inf\n",
      "Epoch: 222, Loss: 2468.77143, Residuals: -0.96793, Convergence: 0.002680\n",
      "Epoch: 223, Loss: 2463.30019, Residuals: -0.96524, Convergence: 0.002221\n",
      "Epoch: 224, Loss: 2458.64824, Residuals: -0.96299, Convergence: 0.001892\n",
      "Epoch: 225, Loss: 2454.63821, Residuals: -0.96111, Convergence: 0.001634\n",
      "Epoch: 226, Loss: 2451.14109, Residuals: -0.95952, Convergence: 0.001427\n",
      "Epoch: 227, Loss: 2448.05633, Residuals: -0.95817, Convergence: 0.001260\n",
      "Epoch: 228, Loss: 2445.30777, Residuals: -0.95701, Convergence: 0.001124\n",
      "Epoch: 229, Loss: 2442.83370, Residuals: -0.95601, Convergence: 0.001013\n",
      "Epoch: 230, Loss: 2440.58606, Residuals: -0.95514, Convergence: 0.000921\n",
      "Evidence 14744.010\n",
      "\n",
      "Epoch: 230, Evidence: 14744.00977, Convergence: 0.005804\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.63e-01\n",
      "Epoch: 230, Loss: 2476.82764, Residuals: -0.95514, Convergence:   inf\n",
      "Epoch: 231, Loss: 2472.93628, Residuals: -0.95290, Convergence: 0.001574\n",
      "Epoch: 232, Loss: 2469.69811, Residuals: -0.95114, Convergence: 0.001311\n",
      "Epoch: 233, Loss: 2466.92160, Residuals: -0.94976, Convergence: 0.001125\n",
      "Epoch: 234, Loss: 2464.49794, Residuals: -0.94864, Convergence: 0.000983\n",
      "Evidence 14772.990\n",
      "\n",
      "Epoch: 234, Evidence: 14772.99023, Convergence: 0.001962\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.12e-01\n",
      "Epoch: 234, Loss: 2477.77491, Residuals: -0.94864, Convergence:   inf\n",
      "Epoch: 235, Loss: 2474.87256, Residuals: -0.94689, Convergence: 0.001173\n",
      "Epoch: 236, Loss: 2472.43213, Residuals: -0.94554, Convergence: 0.000987\n",
      "Evidence 14785.111\n",
      "\n",
      "Epoch: 236, Evidence: 14785.11133, Convergence: 0.000820\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.77e-01\n",
      "Epoch: 236, Loss: 2478.45907, Residuals: -0.94554, Convergence:   inf\n",
      "Epoch: 237, Loss: 2473.94328, Residuals: -0.94317, Convergence: 0.001825\n",
      "Epoch: 238, Loss: 2470.41100, Residuals: -0.94154, Convergence: 0.001430\n",
      "Epoch: 239, Loss: 2467.55198, Residuals: -0.94034, Convergence: 0.001159\n",
      "Epoch: 240, Loss: 2465.13144, Residuals: -0.93964, Convergence: 0.000982\n",
      "Evidence 14803.064\n",
      "\n",
      "Epoch: 240, Evidence: 14803.06445, Convergence: 0.002032\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.46e-01\n",
      "Epoch: 240, Loss: 2478.55220, Residuals: -0.93964, Convergence:   inf\n",
      "Epoch: 241, Loss: 2475.48981, Residuals: -0.93733, Convergence: 0.001237\n",
      "Epoch: 242, Loss: 2473.05364, Residuals: -0.93611, Convergence: 0.000985\n",
      "Evidence 14814.198\n",
      "\n",
      "Epoch: 242, Evidence: 14814.19824, Convergence: 0.000752\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.24e-01\n",
      "Epoch: 242, Loss: 2478.71249, Residuals: -0.93611, Convergence:   inf\n",
      "Epoch: 243, Loss: 2474.18421, Residuals: -0.93275, Convergence: 0.001830\n",
      "Epoch: 244, Loss: 2470.95283, Residuals: -0.93342, Convergence: 0.001308\n",
      "Epoch: 245, Loss: 2468.31065, Residuals: -0.93419, Convergence: 0.001070\n",
      "Epoch: 246, Loss: 2465.98096, Residuals: -0.93668, Convergence: 0.000945\n",
      "Evidence 14830.314\n",
      "\n",
      "Epoch: 246, Evidence: 14830.31445, Convergence: 0.001837\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.11e-01\n",
      "Epoch: 246, Loss: 2477.94467, Residuals: -0.93668, Convergence:   inf\n",
      "Epoch: 247, Loss: 2476.10634, Residuals: -0.93538, Convergence: 0.000742\n",
      "Evidence 14837.142\n",
      "\n",
      "Epoch: 247, Evidence: 14837.14160, Convergence: 0.000460\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 9.07e-02\n",
      "Epoch: 247, Loss: 2478.86829, Residuals: -0.93538, Convergence:   inf\n",
      "Epoch: 248, Loss: 2523.82481, Residuals: -0.98600, Convergence: -0.017813\n",
      "Epoch: 248, Loss: 2476.42527, Residuals: -0.93379, Convergence: 0.000987\n",
      "Evidence 14841.742\n",
      "\n",
      "Epoch: 248, Evidence: 14841.74219, Convergence: 0.000770\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.39e-02\n",
      "Epoch: 248, Loss: 2478.33635, Residuals: -0.93379, Convergence:   inf\n",
      "Epoch: 249, Loss: 2481.28924, Residuals: -0.94015, Convergence: -0.001190\n",
      "Epoch: 249, Loss: 2477.80116, Residuals: -0.93404, Convergence: 0.000216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence 14843.932\n",
      "\n",
      "Epoch: 249, Evidence: 14843.93164, Convergence: 0.000917\n",
      "Total samples: 183, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 382.56055, Residuals: -4.49876, Convergence:   inf\n",
      "Epoch: 1, Loss: 356.78866, Residuals: -4.37831, Convergence: 0.072233\n",
      "Epoch: 2, Loss: 335.66810, Residuals: -4.21400, Convergence: 0.062921\n",
      "Epoch: 3, Loss: 319.53069, Residuals: -4.04883, Convergence: 0.050503\n",
      "Epoch: 4, Loss: 307.20971, Residuals: -3.90306, Convergence: 0.040106\n",
      "Epoch: 5, Loss: 297.42565, Residuals: -3.77406, Convergence: 0.032896\n",
      "Epoch: 6, Loss: 289.47469, Residuals: -3.66186, Convergence: 0.027467\n",
      "Epoch: 7, Loss: 282.86609, Residuals: -3.56598, Convergence: 0.023363\n",
      "Epoch: 8, Loss: 277.23672, Residuals: -3.48423, Convergence: 0.020305\n",
      "Epoch: 9, Loss: 272.33065, Residuals: -3.41422, Convergence: 0.018015\n",
      "Epoch: 10, Loss: 267.96554, Residuals: -3.35382, Convergence: 0.016290\n",
      "Epoch: 11, Loss: 264.00887, Residuals: -3.30122, Convergence: 0.014987\n",
      "Epoch: 12, Loss: 260.36366, Residuals: -3.25486, Convergence: 0.014000\n",
      "Epoch: 13, Loss: 256.95931, Residuals: -3.21344, Convergence: 0.013249\n",
      "Epoch: 14, Loss: 253.74534, Residuals: -3.17580, Convergence: 0.012666\n",
      "Epoch: 15, Loss: 250.68769, Residuals: -3.14097, Convergence: 0.012197\n",
      "Epoch: 16, Loss: 247.76644, Residuals: -3.10826, Convergence: 0.011790\n",
      "Epoch: 17, Loss: 244.96873, Residuals: -3.07721, Convergence: 0.011421\n",
      "Epoch: 18, Loss: 242.27613, Residuals: -3.04744, Convergence: 0.011114\n",
      "Epoch: 19, Loss: 239.65934, Residuals: -3.01842, Convergence: 0.010919\n",
      "Epoch: 20, Loss: 237.08398, Residuals: -2.98959, Convergence: 0.010863\n",
      "Epoch: 21, Loss: 234.51982, Residuals: -2.96043, Convergence: 0.010934\n",
      "Epoch: 22, Loss: 231.94638, Residuals: -2.93062, Convergence: 0.011095\n",
      "Epoch: 23, Loss: 229.34597, Residuals: -2.89996, Convergence: 0.011338\n",
      "Epoch: 24, Loss: 226.68626, Residuals: -2.86809, Convergence: 0.011733\n",
      "Epoch: 25, Loss: 223.91397, Residuals: -2.83442, Convergence: 0.012381\n",
      "Epoch: 26, Loss: 220.98754, Residuals: -2.79839, Convergence: 0.013243\n",
      "Epoch: 27, Loss: 217.96520, Residuals: -2.76054, Convergence: 0.013866\n",
      "Epoch: 28, Loss: 214.98319, Residuals: -2.72237, Convergence: 0.013871\n",
      "Epoch: 29, Loss: 212.10482, Residuals: -2.68470, Convergence: 0.013571\n",
      "Epoch: 30, Loss: 209.32814, Residuals: -2.64763, Convergence: 0.013265\n",
      "Epoch: 31, Loss: 206.63791, Residuals: -2.61107, Convergence: 0.013019\n",
      "Epoch: 32, Loss: 204.02127, Residuals: -2.57490, Convergence: 0.012825\n",
      "Epoch: 33, Loss: 201.46980, Residuals: -2.53904, Convergence: 0.012664\n",
      "Epoch: 34, Loss: 198.97880, Residuals: -2.50342, Convergence: 0.012519\n",
      "Epoch: 35, Loss: 196.54618, Residuals: -2.46800, Convergence: 0.012377\n",
      "Epoch: 36, Loss: 194.17165, Residuals: -2.43278, Convergence: 0.012229\n",
      "Epoch: 37, Loss: 191.85600, Residuals: -2.39773, Convergence: 0.012070\n",
      "Epoch: 38, Loss: 189.60063, Residuals: -2.36287, Convergence: 0.011895\n",
      "Epoch: 39, Loss: 187.40721, Residuals: -2.32822, Convergence: 0.011704\n",
      "Epoch: 40, Loss: 185.27747, Residuals: -2.29381, Convergence: 0.011495\n",
      "Epoch: 41, Loss: 183.21303, Residuals: -2.25966, Convergence: 0.011268\n",
      "Epoch: 42, Loss: 181.21534, Residuals: -2.22583, Convergence: 0.011024\n",
      "Epoch: 43, Loss: 179.28560, Residuals: -2.19234, Convergence: 0.010764\n",
      "Epoch: 44, Loss: 177.42473, Residuals: -2.15925, Convergence: 0.010488\n",
      "Epoch: 45, Loss: 175.63334, Residuals: -2.12660, Convergence: 0.010200\n",
      "Epoch: 46, Loss: 173.91168, Residuals: -2.09442, Convergence: 0.009900\n",
      "Epoch: 47, Loss: 172.25965, Residuals: -2.06276, Convergence: 0.009590\n",
      "Epoch: 48, Loss: 170.67675, Residuals: -2.03167, Convergence: 0.009274\n",
      "Epoch: 49, Loss: 169.16209, Residuals: -2.00116, Convergence: 0.008954\n",
      "Epoch: 50, Loss: 167.71440, Residuals: -1.97128, Convergence: 0.008632\n",
      "Epoch: 51, Loss: 166.33200, Residuals: -1.94203, Convergence: 0.008311\n",
      "Epoch: 52, Loss: 165.01286, Residuals: -1.91345, Convergence: 0.007994\n",
      "Epoch: 53, Loss: 163.75461, Residuals: -1.88553, Convergence: 0.007684\n",
      "Epoch: 54, Loss: 162.55463, Residuals: -1.85828, Convergence: 0.007382\n",
      "Epoch: 55, Loss: 161.41013, Residuals: -1.83169, Convergence: 0.007091\n",
      "Epoch: 56, Loss: 160.31826, Residuals: -1.80576, Convergence: 0.006811\n",
      "Epoch: 57, Loss: 159.27620, Residuals: -1.78047, Convergence: 0.006542\n",
      "Epoch: 58, Loss: 158.28133, Residuals: -1.75581, Convergence: 0.006285\n",
      "Epoch: 59, Loss: 157.33124, Residuals: -1.73176, Convergence: 0.006039\n",
      "Epoch: 60, Loss: 156.42384, Residuals: -1.70831, Convergence: 0.005801\n",
      "Epoch: 61, Loss: 155.55732, Residuals: -1.68546, Convergence: 0.005570\n",
      "Epoch: 62, Loss: 154.73020, Residuals: -1.66319, Convergence: 0.005346\n",
      "Epoch: 63, Loss: 153.94122, Residuals: -1.64151, Convergence: 0.005125\n",
      "Epoch: 64, Loss: 153.18932, Residuals: -1.62041, Convergence: 0.004908\n",
      "Epoch: 65, Loss: 152.47354, Residuals: -1.59991, Convergence: 0.004694\n",
      "Epoch: 66, Loss: 151.79296, Residuals: -1.58000, Convergence: 0.004484\n",
      "Epoch: 67, Loss: 151.14665, Residuals: -1.56070, Convergence: 0.004276\n",
      "Epoch: 68, Loss: 150.53359, Residuals: -1.54200, Convergence: 0.004073\n",
      "Epoch: 69, Loss: 149.95273, Residuals: -1.52392, Convergence: 0.003874\n",
      "Epoch: 70, Loss: 149.40289, Residuals: -1.50646, Convergence: 0.003680\n",
      "Epoch: 71, Loss: 148.88279, Residuals: -1.48962, Convergence: 0.003493\n",
      "Epoch: 72, Loss: 148.39108, Residuals: -1.47340, Convergence: 0.003314\n",
      "Epoch: 73, Loss: 147.92627, Residuals: -1.45780, Convergence: 0.003142\n",
      "Epoch: 74, Loss: 147.48676, Residuals: -1.44282, Convergence: 0.002980\n",
      "Epoch: 75, Loss: 147.07084, Residuals: -1.42844, Convergence: 0.002828\n",
      "Epoch: 76, Loss: 146.67666, Residuals: -1.41465, Convergence: 0.002687\n",
      "Epoch: 77, Loss: 146.30230, Residuals: -1.40143, Convergence: 0.002559\n",
      "Epoch: 78, Loss: 145.94582, Residuals: -1.38875, Convergence: 0.002443\n",
      "Epoch: 79, Loss: 145.60529, Residuals: -1.37659, Convergence: 0.002339\n",
      "Epoch: 80, Loss: 145.27894, Residuals: -1.36492, Convergence: 0.002246\n",
      "Epoch: 81, Loss: 144.96521, Residuals: -1.35369, Convergence: 0.002164\n",
      "Epoch: 82, Loss: 144.66279, Residuals: -1.34289, Convergence: 0.002090\n",
      "Epoch: 83, Loss: 144.37069, Residuals: -1.33249, Convergence: 0.002023\n",
      "Epoch: 84, Loss: 144.08817, Residuals: -1.32246, Convergence: 0.001961\n",
      "Epoch: 85, Loss: 143.81476, Residuals: -1.31278, Convergence: 0.001901\n",
      "Epoch: 86, Loss: 143.55015, Residuals: -1.30344, Convergence: 0.001843\n",
      "Epoch: 87, Loss: 143.29420, Residuals: -1.29443, Convergence: 0.001786\n",
      "Epoch: 88, Loss: 143.04686, Residuals: -1.28574, Convergence: 0.001729\n",
      "Epoch: 89, Loss: 142.80811, Residuals: -1.27735, Convergence: 0.001672\n",
      "Epoch: 90, Loss: 142.57797, Residuals: -1.26927, Convergence: 0.001614\n",
      "Epoch: 91, Loss: 142.35649, Residuals: -1.26148, Convergence: 0.001556\n",
      "Epoch: 92, Loss: 142.14369, Residuals: -1.25398, Convergence: 0.001497\n",
      "Epoch: 93, Loss: 141.93958, Residuals: -1.24676, Convergence: 0.001438\n",
      "Epoch: 94, Loss: 141.74416, Residuals: -1.23982, Convergence: 0.001379\n",
      "Epoch: 95, Loss: 141.55741, Residuals: -1.23314, Convergence: 0.001319\n",
      "Epoch: 96, Loss: 141.37929, Residuals: -1.22673, Convergence: 0.001260\n",
      "Epoch: 97, Loss: 141.20975, Residuals: -1.22057, Convergence: 0.001201\n",
      "Epoch: 98, Loss: 141.04870, Residuals: -1.21466, Convergence: 0.001142\n",
      "Epoch: 99, Loss: 140.89608, Residuals: -1.20900, Convergence: 0.001083\n",
      "Epoch: 100, Loss: 140.75176, Residuals: -1.20359, Convergence: 0.001025\n",
      "Epoch: 101, Loss: 140.61561, Residuals: -1.19841, Convergence: 0.000968\n",
      "Evidence -182.127\n",
      "\n",
      "Epoch: 101, Evidence: -182.12653, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.24e-01\n",
      "Epoch: 101, Loss: 1371.02087, Residuals: -1.19841, Convergence:   inf\n",
      "Epoch: 102, Loss: 1309.14270, Residuals: -1.22895, Convergence: 0.047266\n",
      "Epoch: 103, Loss: 1261.67305, Residuals: -1.25324, Convergence: 0.037624\n",
      "Epoch: 104, Loss: 1225.48770, Residuals: -1.27125, Convergence: 0.029527\n",
      "Epoch: 105, Loss: 1197.23575, Residuals: -1.28427, Convergence: 0.023598\n",
      "Epoch: 106, Loss: 1174.38664, Residuals: -1.29402, Convergence: 0.019456\n",
      "Epoch: 107, Loss: 1155.41565, Residuals: -1.30152, Convergence: 0.016419\n",
      "Epoch: 108, Loss: 1139.39174, Residuals: -1.30726, Convergence: 0.014064\n",
      "Epoch: 109, Loss: 1125.68250, Residuals: -1.31149, Convergence: 0.012179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 110, Loss: 1113.81953, Residuals: -1.31439, Convergence: 0.010651\n",
      "Epoch: 111, Loss: 1103.43678, Residuals: -1.31608, Convergence: 0.009409\n",
      "Epoch: 112, Loss: 1094.23930, Residuals: -1.31667, Convergence: 0.008405\n",
      "Epoch: 113, Loss: 1085.98380, Residuals: -1.31627, Convergence: 0.007602\n",
      "Epoch: 114, Loss: 1078.46743, Residuals: -1.31493, Convergence: 0.006969\n",
      "Epoch: 115, Loss: 1071.51880, Residuals: -1.31274, Convergence: 0.006485\n",
      "Epoch: 116, Loss: 1064.99307, Residuals: -1.30972, Convergence: 0.006127\n",
      "Epoch: 117, Loss: 1058.76631, Residuals: -1.30592, Convergence: 0.005881\n",
      "Epoch: 118, Loss: 1052.73371, Residuals: -1.30136, Convergence: 0.005730\n",
      "Epoch: 119, Loss: 1046.80765, Residuals: -1.29608, Convergence: 0.005661\n",
      "Epoch: 120, Loss: 1040.92192, Residuals: -1.29010, Convergence: 0.005654\n",
      "Epoch: 121, Loss: 1035.04233, Residuals: -1.28351, Convergence: 0.005681\n",
      "Epoch: 122, Loss: 1029.18221, Residuals: -1.27638, Convergence: 0.005694\n",
      "Epoch: 123, Loss: 1023.40470, Residuals: -1.26884, Convergence: 0.005645\n",
      "Epoch: 124, Loss: 1017.80495, Residuals: -1.26098, Convergence: 0.005502\n",
      "Epoch: 125, Loss: 1012.47284, Residuals: -1.25292, Convergence: 0.005266\n",
      "Epoch: 126, Loss: 1007.46460, Residuals: -1.24473, Convergence: 0.004971\n",
      "Epoch: 127, Loss: 1002.79712, Residuals: -1.23649, Convergence: 0.004654\n",
      "Epoch: 128, Loss: 998.45851, Residuals: -1.22826, Convergence: 0.004345\n",
      "Epoch: 129, Loss: 994.42162, Residuals: -1.22010, Convergence: 0.004060\n",
      "Epoch: 130, Loss: 990.65478, Residuals: -1.21205, Convergence: 0.003802\n",
      "Epoch: 131, Loss: 987.12752, Residuals: -1.20414, Convergence: 0.003573\n",
      "Epoch: 132, Loss: 983.81307, Residuals: -1.19642, Convergence: 0.003369\n",
      "Epoch: 133, Loss: 980.68862, Residuals: -1.18889, Convergence: 0.003186\n",
      "Epoch: 134, Loss: 977.73576, Residuals: -1.18158, Convergence: 0.003020\n",
      "Epoch: 135, Loss: 974.93936, Residuals: -1.17451, Convergence: 0.002868\n",
      "Epoch: 136, Loss: 972.28701, Residuals: -1.16769, Convergence: 0.002728\n",
      "Epoch: 137, Loss: 969.76833, Residuals: -1.16112, Convergence: 0.002597\n",
      "Epoch: 138, Loss: 967.37438, Residuals: -1.15482, Convergence: 0.002475\n",
      "Epoch: 139, Loss: 965.09728, Residuals: -1.14878, Convergence: 0.002359\n",
      "Epoch: 140, Loss: 962.92929, Residuals: -1.14301, Convergence: 0.002251\n",
      "Epoch: 141, Loss: 960.86374, Residuals: -1.13749, Convergence: 0.002150\n",
      "Epoch: 142, Loss: 958.89384, Residuals: -1.13223, Convergence: 0.002054\n",
      "Epoch: 143, Loss: 957.01310, Residuals: -1.12722, Convergence: 0.001965\n",
      "Epoch: 144, Loss: 955.21516, Residuals: -1.12244, Convergence: 0.001882\n",
      "Epoch: 145, Loss: 953.49363, Residuals: -1.11789, Convergence: 0.001805\n",
      "Epoch: 146, Loss: 951.84266, Residuals: -1.11355, Convergence: 0.001734\n",
      "Epoch: 147, Loss: 950.25635, Residuals: -1.10942, Convergence: 0.001669\n",
      "Epoch: 148, Loss: 948.72876, Residuals: -1.10547, Convergence: 0.001610\n",
      "Epoch: 149, Loss: 947.25449, Residuals: -1.10170, Convergence: 0.001556\n",
      "Epoch: 150, Loss: 945.82807, Residuals: -1.09810, Convergence: 0.001508\n",
      "Epoch: 151, Loss: 944.44388, Residuals: -1.09464, Convergence: 0.001466\n",
      "Epoch: 152, Loss: 943.09691, Residuals: -1.09133, Convergence: 0.001428\n",
      "Epoch: 153, Loss: 941.78146, Residuals: -1.08813, Convergence: 0.001397\n",
      "Epoch: 154, Loss: 940.49234, Residuals: -1.08504, Convergence: 0.001371\n",
      "Epoch: 155, Loss: 939.22392, Residuals: -1.08205, Convergence: 0.001351\n",
      "Epoch: 156, Loss: 937.97066, Residuals: -1.07913, Convergence: 0.001336\n",
      "Epoch: 157, Loss: 936.72698, Residuals: -1.07628, Convergence: 0.001328\n",
      "Epoch: 158, Loss: 935.48832, Residuals: -1.07348, Convergence: 0.001324\n",
      "Epoch: 159, Loss: 934.25107, Residuals: -1.07071, Convergence: 0.001324\n",
      "Epoch: 160, Loss: 933.01304, Residuals: -1.06797, Convergence: 0.001327\n",
      "Epoch: 161, Loss: 931.77474, Residuals: -1.06525, Convergence: 0.001329\n",
      "Epoch: 162, Loss: 930.53898, Residuals: -1.06256, Convergence: 0.001328\n",
      "Epoch: 163, Loss: 929.31130, Residuals: -1.05990, Convergence: 0.001321\n",
      "Epoch: 164, Loss: 928.09882, Residuals: -1.05727, Convergence: 0.001306\n",
      "Epoch: 165, Loss: 926.90864, Residuals: -1.05470, Convergence: 0.001284\n",
      "Epoch: 166, Loss: 925.74855, Residuals: -1.05219, Convergence: 0.001253\n",
      "Epoch: 167, Loss: 924.62403, Residuals: -1.04976, Convergence: 0.001216\n",
      "Epoch: 168, Loss: 923.54028, Residuals: -1.04740, Convergence: 0.001173\n",
      "Epoch: 169, Loss: 922.50029, Residuals: -1.04513, Convergence: 0.001127\n",
      "Epoch: 170, Loss: 921.50623, Residuals: -1.04296, Convergence: 0.001079\n",
      "Epoch: 171, Loss: 920.55876, Residuals: -1.04087, Convergence: 0.001029\n",
      "Epoch: 172, Loss: 919.65763, Residuals: -1.03887, Convergence: 0.000980\n",
      "Evidence 11285.679\n",
      "\n",
      "Epoch: 172, Evidence: 11285.67871, Convergence: 1.016138\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 5.75e-01\n",
      "Epoch: 172, Loss: 2349.48145, Residuals: -1.03887, Convergence:   inf\n",
      "Epoch: 173, Loss: 2310.05813, Residuals: -1.04827, Convergence: 0.017066\n",
      "Epoch: 174, Loss: 2283.01561, Residuals: -1.04785, Convergence: 0.011845\n",
      "Epoch: 175, Loss: 2260.60411, Residuals: -1.04665, Convergence: 0.009914\n",
      "Epoch: 176, Loss: 2241.82177, Residuals: -1.04520, Convergence: 0.008378\n",
      "Epoch: 177, Loss: 2225.94364, Residuals: -1.04362, Convergence: 0.007133\n",
      "Epoch: 178, Loss: 2212.40371, Residuals: -1.04194, Convergence: 0.006120\n",
      "Epoch: 179, Loss: 2200.73942, Residuals: -1.04017, Convergence: 0.005300\n",
      "Epoch: 180, Loss: 2190.56790, Residuals: -1.03832, Convergence: 0.004643\n",
      "Epoch: 181, Loss: 2181.57312, Residuals: -1.03636, Convergence: 0.004123\n",
      "Epoch: 182, Loss: 2173.49450, Residuals: -1.03429, Convergence: 0.003717\n",
      "Epoch: 183, Loss: 2166.13152, Residuals: -1.03206, Convergence: 0.003399\n",
      "Epoch: 184, Loss: 2159.34223, Residuals: -1.02968, Convergence: 0.003144\n",
      "Epoch: 185, Loss: 2153.04148, Residuals: -1.02716, Convergence: 0.002926\n",
      "Epoch: 186, Loss: 2147.18474, Residuals: -1.02452, Convergence: 0.002728\n",
      "Epoch: 187, Loss: 2141.74923, Residuals: -1.02182, Convergence: 0.002538\n",
      "Epoch: 188, Loss: 2136.71529, Residuals: -1.01911, Convergence: 0.002356\n",
      "Epoch: 189, Loss: 2132.06135, Residuals: -1.01643, Convergence: 0.002183\n",
      "Epoch: 190, Loss: 2127.76130, Residuals: -1.01381, Convergence: 0.002021\n",
      "Epoch: 191, Loss: 2123.78769, Residuals: -1.01128, Convergence: 0.001871\n",
      "Epoch: 192, Loss: 2120.11009, Residuals: -1.00885, Convergence: 0.001735\n",
      "Epoch: 193, Loss: 2116.70259, Residuals: -1.00652, Convergence: 0.001610\n",
      "Epoch: 194, Loss: 2113.53751, Residuals: -1.00431, Convergence: 0.001498\n",
      "Epoch: 195, Loss: 2110.59217, Residuals: -1.00221, Convergence: 0.001396\n",
      "Epoch: 196, Loss: 2107.84404, Residuals: -1.00023, Convergence: 0.001304\n",
      "Epoch: 197, Loss: 2105.27483, Residuals: -0.99836, Convergence: 0.001220\n",
      "Epoch: 198, Loss: 2102.86904, Residuals: -0.99661, Convergence: 0.001144\n",
      "Epoch: 199, Loss: 2100.61147, Residuals: -0.99496, Convergence: 0.001075\n",
      "Epoch: 200, Loss: 2098.49111, Residuals: -0.99342, Convergence: 0.001010\n",
      "Epoch: 201, Loss: 2096.49690, Residuals: -0.99197, Convergence: 0.000951\n",
      "Evidence 14477.632\n",
      "\n",
      "Epoch: 201, Evidence: 14477.63184, Convergence: 0.220475\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 4.36e-01\n",
      "Epoch: 201, Loss: 2478.22866, Residuals: -0.99197, Convergence:   inf\n",
      "Epoch: 202, Loss: 2464.79836, Residuals: -0.98964, Convergence: 0.005449\n",
      "Epoch: 203, Loss: 2453.92882, Residuals: -0.98677, Convergence: 0.004429\n",
      "Epoch: 204, Loss: 2444.58124, Residuals: -0.98406, Convergence: 0.003824\n",
      "Epoch: 205, Loss: 2436.47783, Residuals: -0.98157, Convergence: 0.003326\n",
      "Epoch: 206, Loss: 2429.41306, Residuals: -0.97931, Convergence: 0.002908\n",
      "Epoch: 207, Loss: 2423.22003, Residuals: -0.97728, Convergence: 0.002556\n",
      "Epoch: 208, Loss: 2417.76446, Residuals: -0.97545, Convergence: 0.002256\n",
      "Epoch: 209, Loss: 2412.93472, Residuals: -0.97380, Convergence: 0.002002\n",
      "Epoch: 210, Loss: 2408.63731, Residuals: -0.97231, Convergence: 0.001784\n",
      "Epoch: 211, Loss: 2404.79379, Residuals: -0.97097, Convergence: 0.001598\n",
      "Epoch: 212, Loss: 2401.33896, Residuals: -0.96976, Convergence: 0.001439\n",
      "Epoch: 213, Loss: 2398.21773, Residuals: -0.96867, Convergence: 0.001301\n",
      "Epoch: 214, Loss: 2395.38520, Residuals: -0.96767, Convergence: 0.001182\n",
      "Epoch: 215, Loss: 2392.80137, Residuals: -0.96676, Convergence: 0.001080\n",
      "Epoch: 216, Loss: 2390.43529, Residuals: -0.96593, Convergence: 0.000990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence 14851.727\n",
      "\n",
      "Epoch: 216, Evidence: 14851.72656, Convergence: 0.025189\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 3.34e-01\n",
      "Epoch: 216, Loss: 2483.92220, Residuals: -0.96593, Convergence:   inf\n",
      "Epoch: 217, Loss: 2477.51589, Residuals: -0.96327, Convergence: 0.002586\n",
      "Epoch: 218, Loss: 2472.23487, Residuals: -0.96097, Convergence: 0.002136\n",
      "Epoch: 219, Loss: 2467.74183, Residuals: -0.95903, Convergence: 0.001821\n",
      "Epoch: 220, Loss: 2463.86486, Residuals: -0.95738, Convergence: 0.001574\n",
      "Epoch: 221, Loss: 2460.47869, Residuals: -0.95597, Convergence: 0.001376\n",
      "Epoch: 222, Loss: 2457.48877, Residuals: -0.95477, Convergence: 0.001217\n",
      "Epoch: 223, Loss: 2454.82012, Residuals: -0.95373, Convergence: 0.001087\n",
      "Epoch: 224, Loss: 2452.41566, Residuals: -0.95284, Convergence: 0.000980\n",
      "Evidence 14930.877\n",
      "\n",
      "Epoch: 224, Evidence: 14930.87695, Convergence: 0.005301\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.62e-01\n",
      "Epoch: 224, Loss: 2485.53191, Residuals: -0.95284, Convergence:   inf\n",
      "Epoch: 225, Loss: 2481.67751, Residuals: -0.95065, Convergence: 0.001553\n",
      "Epoch: 226, Loss: 2478.45844, Residuals: -0.94888, Convergence: 0.001299\n",
      "Epoch: 227, Loss: 2475.68766, Residuals: -0.94743, Convergence: 0.001119\n",
      "Epoch: 228, Loss: 2473.25812, Residuals: -0.94621, Convergence: 0.000982\n",
      "Evidence 14958.482\n",
      "\n",
      "Epoch: 228, Evidence: 14958.48242, Convergence: 0.001845\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.11e-01\n",
      "Epoch: 228, Loss: 2486.49793, Residuals: -0.94621, Convergence:   inf\n",
      "Epoch: 229, Loss: 2483.61416, Residuals: -0.94433, Convergence: 0.001161\n",
      "Epoch: 230, Loss: 2481.18143, Residuals: -0.94281, Convergence: 0.000980\n",
      "Evidence 14970.420\n",
      "\n",
      "Epoch: 230, Evidence: 14970.41992, Convergence: 0.000797\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.76e-01\n",
      "Epoch: 230, Loss: 2487.19486, Residuals: -0.94281, Convergence:   inf\n",
      "Epoch: 231, Loss: 2482.60874, Residuals: -0.94007, Convergence: 0.001847\n",
      "Epoch: 232, Loss: 2479.11498, Residuals: -0.93799, Convergence: 0.001409\n",
      "Epoch: 233, Loss: 2476.28278, Residuals: -0.93645, Convergence: 0.001144\n",
      "Epoch: 234, Loss: 2473.87768, Residuals: -0.93540, Convergence: 0.000972\n",
      "Evidence 14988.407\n",
      "\n",
      "Epoch: 234, Evidence: 14988.40723, Convergence: 0.001997\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.46e-01\n",
      "Epoch: 234, Loss: 2487.24803, Residuals: -0.93540, Convergence:   inf\n",
      "Epoch: 235, Loss: 2484.22776, Residuals: -0.93258, Convergence: 0.001216\n",
      "Epoch: 236, Loss: 2481.83107, Residuals: -0.93086, Convergence: 0.000966\n",
      "Evidence 14999.464\n",
      "\n",
      "Epoch: 236, Evidence: 14999.46387, Convergence: 0.000737\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.24e-01\n",
      "Epoch: 236, Loss: 2487.37550, Residuals: -0.93086, Convergence:   inf\n",
      "Epoch: 237, Loss: 2482.94806, Residuals: -0.92641, Convergence: 0.001783\n",
      "Epoch: 238, Loss: 2479.79174, Residuals: -0.92625, Convergence: 0.001273\n",
      "Epoch: 239, Loss: 2477.18586, Residuals: -0.92563, Convergence: 0.001052\n",
      "Epoch: 240, Loss: 2474.89502, Residuals: -0.92690, Convergence: 0.000926\n",
      "Evidence 15015.641\n",
      "\n",
      "Epoch: 240, Evidence: 15015.64062, Convergence: 0.001814\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.10e-01\n",
      "Epoch: 240, Loss: 2486.63029, Residuals: -0.92690, Convergence:   inf\n",
      "Epoch: 241, Loss: 2485.38373, Residuals: -0.92184, Convergence: 0.000502\n",
      "Evidence 15021.978\n",
      "\n",
      "Epoch: 241, Evidence: 15021.97754, Convergence: 0.000422\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 9.03e-02\n",
      "Epoch: 241, Loss: 2487.38841, Residuals: -0.92184, Convergence:   inf\n",
      "Epoch: 242, Loss: 2539.44368, Residuals: -0.94648, Convergence: -0.020499\n",
      "Epoch: 242, Loss: 2485.17213, Residuals: -0.91825, Convergence: 0.000892\n",
      "Evidence 15026.387\n",
      "\n",
      "Epoch: 242, Evidence: 15026.38672, Convergence: 0.000715\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 8.30e-02\n",
      "Epoch: 242, Loss: 2486.90969, Residuals: -0.91825, Convergence:   inf\n",
      "Epoch: 243, Loss: 2495.15761, Residuals: -0.91364, Convergence: -0.003306\n",
      "Epoch: 243, Loss: 2487.92876, Residuals: -0.91399, Convergence: -0.000410\n",
      "Evidence 15027.019\n",
      "\n",
      "Epoch: 243, Evidence: 15027.01855, Convergence: 0.000757\n",
      "Total samples: 180, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 381.14836, Residuals: -4.55376, Convergence:   inf\n",
      "Epoch: 1, Loss: 355.41758, Residuals: -4.43293, Convergence: 0.072396\n",
      "Epoch: 2, Loss: 334.36365, Residuals: -4.26802, Convergence: 0.062967\n",
      "Epoch: 3, Loss: 318.27743, Residuals: -4.10257, Convergence: 0.050541\n",
      "Epoch: 4, Loss: 306.00735, Residuals: -3.95689, Convergence: 0.040097\n",
      "Epoch: 5, Loss: 296.27198, Residuals: -3.82812, Convergence: 0.032860\n",
      "Epoch: 6, Loss: 288.36654, Residuals: -3.71598, Convergence: 0.027415\n",
      "Epoch: 7, Loss: 281.80310, Residuals: -3.61998, Convergence: 0.023291\n",
      "Epoch: 8, Loss: 276.21988, Residuals: -3.53801, Convergence: 0.020213\n",
      "Epoch: 9, Loss: 271.36124, Residuals: -3.46772, Convergence: 0.017905\n",
      "Epoch: 10, Loss: 267.04517, Residuals: -3.40697, Convergence: 0.016162\n",
      "Epoch: 11, Loss: 263.13963, Residuals: -3.35394, Convergence: 0.014842\n",
      "Epoch: 12, Loss: 259.54818, Residuals: -3.30709, Convergence: 0.013837\n",
      "Epoch: 13, Loss: 256.20084, Residuals: -3.26507, Convergence: 0.013065\n",
      "Epoch: 14, Loss: 253.04810, Residuals: -3.22673, Convergence: 0.012459\n",
      "Epoch: 15, Loss: 250.05794, Residuals: -3.19112, Convergence: 0.011958\n",
      "Epoch: 16, Loss: 247.21365, Residuals: -3.15760, Convergence: 0.011505\n",
      "Epoch: 17, Loss: 244.50494, Residuals: -3.12582, Convergence: 0.011078\n",
      "Epoch: 18, Loss: 241.91395, Residuals: -3.09541, Convergence: 0.010710\n",
      "Epoch: 19, Loss: 239.41094, Residuals: -3.06592, Convergence: 0.010455\n",
      "Epoch: 20, Loss: 236.96071, Residuals: -3.03678, Convergence: 0.010340\n",
      "Epoch: 21, Loss: 234.53081, Residuals: -3.00750, Convergence: 0.010361\n",
      "Epoch: 22, Loss: 232.09588, Residuals: -2.97770, Convergence: 0.010491\n",
      "Epoch: 23, Loss: 229.63114, Residuals: -2.94709, Convergence: 0.010733\n",
      "Epoch: 24, Loss: 227.09665, Residuals: -2.91524, Convergence: 0.011160\n",
      "Epoch: 25, Loss: 224.43609, Residuals: -2.88147, Convergence: 0.011854\n",
      "Epoch: 26, Loss: 221.63214, Residuals: -2.84550, Convergence: 0.012651\n",
      "Epoch: 27, Loss: 218.78881, Residuals: -2.80839, Convergence: 0.012996\n",
      "Epoch: 28, Loss: 216.02747, Residuals: -2.77155, Convergence: 0.012782\n",
      "Epoch: 29, Loss: 213.37551, Residuals: -2.73545, Convergence: 0.012429\n",
      "Epoch: 30, Loss: 210.81769, Residuals: -2.70003, Convergence: 0.012133\n",
      "Epoch: 31, Loss: 208.33511, Residuals: -2.66517, Convergence: 0.011916\n",
      "Epoch: 32, Loss: 205.91352, Residuals: -2.63073, Convergence: 0.011760\n",
      "Epoch: 33, Loss: 203.54355, Residuals: -2.59662, Convergence: 0.011644\n",
      "Epoch: 34, Loss: 201.21967, Residuals: -2.56278, Convergence: 0.011549\n",
      "Epoch: 35, Loss: 198.93901, Residuals: -2.52918, Convergence: 0.011464\n",
      "Epoch: 36, Loss: 196.70044, Residuals: -2.49577, Convergence: 0.011381\n",
      "Epoch: 37, Loss: 194.50394, Residuals: -2.46254, Convergence: 0.011293\n",
      "Epoch: 38, Loss: 192.35010, Residuals: -2.42948, Convergence: 0.011197\n",
      "Epoch: 39, Loss: 190.23999, Residuals: -2.39655, Convergence: 0.011092\n",
      "Epoch: 40, Loss: 188.17501, Residuals: -2.36375, Convergence: 0.010974\n",
      "Epoch: 41, Loss: 186.15699, Residuals: -2.33106, Convergence: 0.010840\n",
      "Epoch: 42, Loss: 184.18817, Residuals: -2.29847, Convergence: 0.010689\n",
      "Epoch: 43, Loss: 182.27129, Residuals: -2.26600, Convergence: 0.010517\n",
      "Epoch: 44, Loss: 180.40953, Residuals: -2.23369, Convergence: 0.010320\n",
      "Epoch: 45, Loss: 178.60652, Residuals: -2.20157, Convergence: 0.010095\n",
      "Epoch: 46, Loss: 176.86603, Residuals: -2.16973, Convergence: 0.009841\n",
      "Epoch: 47, Loss: 175.19159, Residuals: -2.13822, Convergence: 0.009558\n",
      "Epoch: 48, Loss: 173.58608, Residuals: -2.10715, Convergence: 0.009249\n",
      "Epoch: 49, Loss: 172.05132, Residuals: -2.07657, Convergence: 0.008920\n",
      "Epoch: 50, Loss: 170.58786, Residuals: -2.04656, Convergence: 0.008579\n",
      "Epoch: 51, Loss: 169.19501, Residuals: -2.01717, Convergence: 0.008232\n",
      "Epoch: 52, Loss: 167.87100, Residuals: -1.98844, Convergence: 0.007887\n",
      "Epoch: 53, Loss: 166.61314, Residuals: -1.96040, Convergence: 0.007550\n",
      "Epoch: 54, Loss: 165.41813, Residuals: -1.93306, Convergence: 0.007224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55, Loss: 164.28227, Residuals: -1.90641, Convergence: 0.006914\n",
      "Epoch: 56, Loss: 163.20169, Residuals: -1.88046, Convergence: 0.006621\n",
      "Epoch: 57, Loss: 162.17257, Residuals: -1.85518, Convergence: 0.006346\n",
      "Epoch: 58, Loss: 161.19124, Residuals: -1.83057, Convergence: 0.006088\n",
      "Epoch: 59, Loss: 160.25430, Residuals: -1.80660, Convergence: 0.005847\n",
      "Epoch: 60, Loss: 159.35863, Residuals: -1.78327, Convergence: 0.005620\n",
      "Epoch: 61, Loss: 158.50145, Residuals: -1.76055, Convergence: 0.005408\n",
      "Epoch: 62, Loss: 157.68023, Residuals: -1.73844, Convergence: 0.005208\n",
      "Epoch: 63, Loss: 156.89275, Residuals: -1.71691, Convergence: 0.005019\n",
      "Epoch: 64, Loss: 156.13702, Residuals: -1.69597, Convergence: 0.004840\n",
      "Epoch: 65, Loss: 155.41126, Residuals: -1.67560, Convergence: 0.004670\n",
      "Epoch: 66, Loss: 154.71386, Residuals: -1.65579, Convergence: 0.004508\n",
      "Epoch: 67, Loss: 154.04341, Residuals: -1.63653, Convergence: 0.004352\n",
      "Epoch: 68, Loss: 153.39863, Residuals: -1.61782, Convergence: 0.004203\n",
      "Epoch: 69, Loss: 152.77838, Residuals: -1.59964, Convergence: 0.004060\n",
      "Epoch: 70, Loss: 152.18160, Residuals: -1.58200, Convergence: 0.003921\n",
      "Epoch: 71, Loss: 151.60737, Residuals: -1.56487, Convergence: 0.003788\n",
      "Epoch: 72, Loss: 151.05486, Residuals: -1.54826, Convergence: 0.003658\n",
      "Epoch: 73, Loss: 150.52327, Residuals: -1.53216, Convergence: 0.003532\n",
      "Epoch: 74, Loss: 150.01194, Residuals: -1.51656, Convergence: 0.003409\n",
      "Epoch: 75, Loss: 149.52019, Residuals: -1.50145, Convergence: 0.003289\n",
      "Epoch: 76, Loss: 149.04744, Residuals: -1.48683, Convergence: 0.003172\n",
      "Epoch: 77, Loss: 148.59313, Residuals: -1.47268, Convergence: 0.003057\n",
      "Epoch: 78, Loss: 148.15674, Residuals: -1.45900, Convergence: 0.002946\n",
      "Epoch: 79, Loss: 147.73774, Residuals: -1.44578, Convergence: 0.002836\n",
      "Epoch: 80, Loss: 147.33566, Residuals: -1.43301, Convergence: 0.002729\n",
      "Epoch: 81, Loss: 146.95001, Residuals: -1.42069, Convergence: 0.002624\n",
      "Epoch: 82, Loss: 146.58034, Residuals: -1.40879, Convergence: 0.002522\n",
      "Epoch: 83, Loss: 146.22617, Residuals: -1.39732, Convergence: 0.002422\n",
      "Epoch: 84, Loss: 145.88704, Residuals: -1.38627, Convergence: 0.002325\n",
      "Epoch: 85, Loss: 145.56247, Residuals: -1.37561, Convergence: 0.002230\n",
      "Epoch: 86, Loss: 145.25201, Residuals: -1.36536, Convergence: 0.002137\n",
      "Epoch: 87, Loss: 144.95518, Residuals: -1.35548, Convergence: 0.002048\n",
      "Epoch: 88, Loss: 144.67152, Residuals: -1.34599, Convergence: 0.001961\n",
      "Epoch: 89, Loss: 144.40055, Residuals: -1.33685, Convergence: 0.001877\n",
      "Epoch: 90, Loss: 144.14181, Residuals: -1.32807, Convergence: 0.001795\n",
      "Epoch: 91, Loss: 143.89485, Residuals: -1.31963, Convergence: 0.001716\n",
      "Epoch: 92, Loss: 143.65922, Residuals: -1.31153, Convergence: 0.001640\n",
      "Epoch: 93, Loss: 143.43448, Residuals: -1.30374, Convergence: 0.001567\n",
      "Epoch: 94, Loss: 143.22023, Residuals: -1.29627, Convergence: 0.001496\n",
      "Epoch: 95, Loss: 143.01605, Residuals: -1.28910, Convergence: 0.001428\n",
      "Epoch: 96, Loss: 142.82159, Residuals: -1.28222, Convergence: 0.001362\n",
      "Epoch: 97, Loss: 142.63649, Residuals: -1.27562, Convergence: 0.001298\n",
      "Epoch: 98, Loss: 142.46042, Residuals: -1.26929, Convergence: 0.001236\n",
      "Epoch: 99, Loss: 142.29309, Residuals: -1.26324, Convergence: 0.001176\n",
      "Epoch: 100, Loss: 142.13424, Residuals: -1.25743, Convergence: 0.001118\n",
      "Epoch: 101, Loss: 141.98360, Residuals: -1.25188, Convergence: 0.001061\n",
      "Epoch: 102, Loss: 141.84097, Residuals: -1.24657, Convergence: 0.001006\n",
      "Epoch: 103, Loss: 141.70613, Residuals: -1.24150, Convergence: 0.000952\n",
      "Evidence -183.254\n",
      "\n",
      "Epoch: 103, Evidence: -183.25357, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 180, Updated regularization: 7.25e-01\n",
      "Epoch: 103, Loss: 1369.52069, Residuals: -1.24150, Convergence:   inf\n",
      "Epoch: 104, Loss: 1308.16289, Residuals: -1.27247, Convergence: 0.046904\n",
      "Epoch: 105, Loss: 1260.94332, Residuals: -1.29686, Convergence: 0.037448\n",
      "Epoch: 106, Loss: 1224.96119, Residuals: -1.31464, Convergence: 0.029374\n",
      "Epoch: 107, Loss: 1196.94593, Residuals: -1.32735, Convergence: 0.023406\n",
      "Epoch: 108, Loss: 1174.36544, Residuals: -1.33687, Convergence: 0.019228\n",
      "Epoch: 109, Loss: 1155.68184, Residuals: -1.34428, Convergence: 0.016167\n",
      "Epoch: 110, Loss: 1139.95103, Residuals: -1.35002, Convergence: 0.013800\n",
      "Epoch: 111, Loss: 1126.53007, Residuals: -1.35433, Convergence: 0.011914\n",
      "Epoch: 112, Loss: 1114.94248, Residuals: -1.35735, Convergence: 0.010393\n",
      "Epoch: 113, Loss: 1104.81667, Residuals: -1.35918, Convergence: 0.009165\n",
      "Epoch: 114, Loss: 1095.85175, Residuals: -1.35991, Convergence: 0.008181\n",
      "Epoch: 115, Loss: 1087.79809, Residuals: -1.35962, Convergence: 0.007404\n",
      "Epoch: 116, Loss: 1080.44263, Residuals: -1.35835, Convergence: 0.006808\n",
      "Epoch: 117, Loss: 1073.59964, Residuals: -1.35614, Convergence: 0.006374\n",
      "Epoch: 118, Loss: 1067.10620, Residuals: -1.35303, Convergence: 0.006085\n",
      "Epoch: 119, Loss: 1060.82149, Residuals: -1.34904, Convergence: 0.005924\n",
      "Epoch: 120, Loss: 1054.63407, Residuals: -1.34420, Convergence: 0.005867\n",
      "Epoch: 121, Loss: 1048.47697, Residuals: -1.33857, Convergence: 0.005872\n",
      "Epoch: 122, Loss: 1042.33862, Residuals: -1.33225, Convergence: 0.005889\n",
      "Epoch: 123, Loss: 1036.26723, Residuals: -1.32535, Convergence: 0.005859\n",
      "Epoch: 124, Loss: 1030.34822, Residuals: -1.31799, Convergence: 0.005745\n",
      "Epoch: 125, Loss: 1024.67009, Residuals: -1.31028, Convergence: 0.005541\n",
      "Epoch: 126, Loss: 1019.29381, Residuals: -1.30231, Convergence: 0.005275\n",
      "Epoch: 127, Loss: 1014.24475, Residuals: -1.29417, Convergence: 0.004978\n",
      "Epoch: 128, Loss: 1009.51781, Residuals: -1.28594, Convergence: 0.004682\n",
      "Epoch: 129, Loss: 1005.09137, Residuals: -1.27768, Convergence: 0.004404\n",
      "Epoch: 130, Loss: 1000.93586, Residuals: -1.26943, Convergence: 0.004152\n",
      "Epoch: 131, Loss: 997.02068, Residuals: -1.26125, Convergence: 0.003927\n",
      "Epoch: 132, Loss: 993.31758, Residuals: -1.25316, Convergence: 0.003728\n",
      "Epoch: 133, Loss: 989.80152, Residuals: -1.24519, Convergence: 0.003552\n",
      "Epoch: 134, Loss: 986.45071, Residuals: -1.23736, Convergence: 0.003397\n",
      "Epoch: 135, Loss: 983.24803, Residuals: -1.22970, Convergence: 0.003257\n",
      "Epoch: 136, Loss: 980.17943, Residuals: -1.22221, Convergence: 0.003131\n",
      "Epoch: 137, Loss: 977.23318, Residuals: -1.21491, Convergence: 0.003015\n",
      "Epoch: 138, Loss: 974.40109, Residuals: -1.20780, Convergence: 0.002906\n",
      "Epoch: 139, Loss: 971.67592, Residuals: -1.20090, Convergence: 0.002805\n",
      "Epoch: 140, Loss: 969.05335, Residuals: -1.19421, Convergence: 0.002706\n",
      "Epoch: 141, Loss: 966.52872, Residuals: -1.18774, Convergence: 0.002612\n",
      "Epoch: 142, Loss: 964.09986, Residuals: -1.18149, Convergence: 0.002519\n",
      "Epoch: 143, Loss: 961.76357, Residuals: -1.17545, Convergence: 0.002429\n",
      "Epoch: 144, Loss: 959.51796, Residuals: -1.16964, Convergence: 0.002340\n",
      "Epoch: 145, Loss: 957.36057, Residuals: -1.16404, Convergence: 0.002253\n",
      "Epoch: 146, Loss: 955.28932, Residuals: -1.15866, Convergence: 0.002168\n",
      "Epoch: 147, Loss: 953.30059, Residuals: -1.15350, Convergence: 0.002086\n",
      "Epoch: 148, Loss: 951.39153, Residuals: -1.14854, Convergence: 0.002007\n",
      "Epoch: 149, Loss: 949.55817, Residuals: -1.14378, Convergence: 0.001931\n",
      "Epoch: 150, Loss: 947.79610, Residuals: -1.13921, Convergence: 0.001859\n",
      "Epoch: 151, Loss: 946.10016, Residuals: -1.13483, Convergence: 0.001793\n",
      "Epoch: 152, Loss: 944.46507, Residuals: -1.13062, Convergence: 0.001731\n",
      "Epoch: 153, Loss: 942.88501, Residuals: -1.12657, Convergence: 0.001676\n",
      "Epoch: 154, Loss: 941.35312, Residuals: -1.12266, Convergence: 0.001627\n",
      "Epoch: 155, Loss: 939.86338, Residuals: -1.11889, Convergence: 0.001585\n",
      "Epoch: 156, Loss: 938.40874, Residuals: -1.11524, Convergence: 0.001550\n",
      "Epoch: 157, Loss: 936.98351, Residuals: -1.11169, Convergence: 0.001521\n",
      "Epoch: 158, Loss: 935.58248, Residuals: -1.10824, Convergence: 0.001497\n",
      "Epoch: 159, Loss: 934.20158, Residuals: -1.10487, Convergence: 0.001478\n",
      "Epoch: 160, Loss: 932.83970, Residuals: -1.10157, Convergence: 0.001460\n",
      "Epoch: 161, Loss: 931.49753, Residuals: -1.09835, Convergence: 0.001441\n",
      "Epoch: 162, Loss: 930.17822, Residuals: -1.09520, Convergence: 0.001418\n",
      "Epoch: 163, Loss: 928.88634, Residuals: -1.09213, Convergence: 0.001391\n",
      "Epoch: 164, Loss: 927.62733, Residuals: -1.08915, Convergence: 0.001357\n",
      "Epoch: 165, Loss: 926.40633, Residuals: -1.08627, Convergence: 0.001318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 166, Loss: 925.22786, Residuals: -1.08349, Convergence: 0.001274\n",
      "Epoch: 167, Loss: 924.09455, Residuals: -1.08082, Convergence: 0.001226\n",
      "Epoch: 168, Loss: 923.00792, Residuals: -1.07825, Convergence: 0.001177\n",
      "Epoch: 169, Loss: 921.96802, Residuals: -1.07578, Convergence: 0.001128\n",
      "Epoch: 170, Loss: 920.97417, Residuals: -1.07342, Convergence: 0.001079\n",
      "Epoch: 171, Loss: 920.02414, Residuals: -1.07117, Convergence: 0.001033\n",
      "Epoch: 172, Loss: 919.11631, Residuals: -1.06900, Convergence: 0.000988\n",
      "Evidence 11094.725\n",
      "\n",
      "Epoch: 172, Evidence: 11094.72461, Convergence: 1.016517\n",
      "Updating hyper-parameters...\n",
      "Total samples: 180, Updated regularization: 5.76e-01\n",
      "Epoch: 172, Loss: 2323.75037, Residuals: -1.06900, Convergence:   inf\n",
      "Epoch: 173, Loss: 2286.11497, Residuals: -1.07738, Convergence: 0.016463\n",
      "Epoch: 174, Loss: 2260.34088, Residuals: -1.07640, Convergence: 0.011403\n",
      "Epoch: 175, Loss: 2238.94538, Residuals: -1.07446, Convergence: 0.009556\n",
      "Epoch: 176, Loss: 2220.88793, Residuals: -1.07218, Convergence: 0.008131\n",
      "Epoch: 177, Loss: 2205.47701, Residuals: -1.06968, Convergence: 0.006988\n",
      "Epoch: 178, Loss: 2192.18178, Residuals: -1.06699, Convergence: 0.006065\n",
      "Epoch: 179, Loss: 2180.57693, Residuals: -1.06414, Convergence: 0.005322\n",
      "Epoch: 180, Loss: 2170.31898, Residuals: -1.06111, Convergence: 0.004726\n",
      "Epoch: 181, Loss: 2161.13990, Residuals: -1.05792, Convergence: 0.004247\n",
      "Epoch: 182, Loss: 2152.84348, Residuals: -1.05455, Convergence: 0.003854\n",
      "Epoch: 183, Loss: 2145.29158, Residuals: -1.05106, Convergence: 0.003520\n",
      "Epoch: 184, Loss: 2138.39037, Residuals: -1.04747, Convergence: 0.003227\n",
      "Epoch: 185, Loss: 2132.06930, Residuals: -1.04385, Convergence: 0.002965\n",
      "Epoch: 186, Loss: 2126.26834, Residuals: -1.04024, Convergence: 0.002728\n",
      "Epoch: 187, Loss: 2120.93366, Residuals: -1.03668, Convergence: 0.002515\n",
      "Epoch: 188, Loss: 2116.01352, Residuals: -1.03321, Convergence: 0.002325\n",
      "Epoch: 189, Loss: 2111.46155, Residuals: -1.02984, Convergence: 0.002156\n",
      "Epoch: 190, Loss: 2107.23729, Residuals: -1.02658, Convergence: 0.002005\n",
      "Epoch: 191, Loss: 2103.30569, Residuals: -1.02346, Convergence: 0.001869\n",
      "Epoch: 192, Loss: 2099.63765, Residuals: -1.02048, Convergence: 0.001747\n",
      "Epoch: 193, Loss: 2096.21006, Residuals: -1.01764, Convergence: 0.001635\n",
      "Epoch: 194, Loss: 2093.00269, Residuals: -1.01494, Convergence: 0.001532\n",
      "Epoch: 195, Loss: 2090.00065, Residuals: -1.01240, Convergence: 0.001436\n",
      "Epoch: 196, Loss: 2087.19026, Residuals: -1.01000, Convergence: 0.001346\n",
      "Epoch: 197, Loss: 2084.55971, Residuals: -1.00774, Convergence: 0.001262\n",
      "Epoch: 198, Loss: 2082.09778, Residuals: -1.00562, Convergence: 0.001182\n",
      "Epoch: 199, Loss: 2079.79504, Residuals: -1.00363, Convergence: 0.001107\n",
      "Epoch: 200, Loss: 2077.64069, Residuals: -1.00176, Convergence: 0.001037\n",
      "Epoch: 201, Loss: 2075.62562, Residuals: -1.00001, Convergence: 0.000971\n",
      "Evidence 14164.812\n",
      "\n",
      "Epoch: 201, Evidence: 14164.81152, Convergence: 0.216740\n",
      "Updating hyper-parameters...\n",
      "Total samples: 180, Updated regularization: 4.38e-01\n",
      "Epoch: 201, Loss: 2440.28977, Residuals: -1.00001, Convergence:   inf\n",
      "Epoch: 202, Loss: 2427.53404, Residuals: -0.99644, Convergence: 0.005255\n",
      "Epoch: 203, Loss: 2417.05780, Residuals: -0.99271, Convergence: 0.004334\n",
      "Epoch: 204, Loss: 2408.02478, Residuals: -0.98924, Convergence: 0.003751\n",
      "Epoch: 205, Loss: 2400.19272, Residuals: -0.98606, Convergence: 0.003263\n",
      "Epoch: 206, Loss: 2393.37406, Residuals: -0.98318, Convergence: 0.002849\n",
      "Epoch: 207, Loss: 2387.40972, Residuals: -0.98058, Convergence: 0.002498\n",
      "Epoch: 208, Loss: 2382.16814, Residuals: -0.97824, Convergence: 0.002200\n",
      "Epoch: 209, Loss: 2377.53906, Residuals: -0.97613, Convergence: 0.001947\n",
      "Epoch: 210, Loss: 2373.42911, Residuals: -0.97423, Convergence: 0.001732\n",
      "Epoch: 211, Loss: 2369.75978, Residuals: -0.97253, Convergence: 0.001548\n",
      "Epoch: 212, Loss: 2366.46693, Residuals: -0.97101, Convergence: 0.001391\n",
      "Epoch: 213, Loss: 2363.49471, Residuals: -0.96964, Convergence: 0.001258\n",
      "Epoch: 214, Loss: 2360.79952, Residuals: -0.96842, Convergence: 0.001142\n",
      "Epoch: 215, Loss: 2358.34212, Residuals: -0.96732, Convergence: 0.001042\n",
      "Epoch: 216, Loss: 2356.09069, Residuals: -0.96634, Convergence: 0.000956\n",
      "Evidence 14524.790\n",
      "\n",
      "Epoch: 216, Evidence: 14524.79004, Convergence: 0.024784\n",
      "Updating hyper-parameters...\n",
      "Total samples: 180, Updated regularization: 3.36e-01\n",
      "Epoch: 216, Loss: 2445.56916, Residuals: -0.96634, Convergence:   inf\n",
      "Epoch: 217, Loss: 2439.32149, Residuals: -0.96316, Convergence: 0.002561\n",
      "Epoch: 218, Loss: 2434.14183, Residuals: -0.96050, Convergence: 0.002128\n",
      "Epoch: 219, Loss: 2429.74521, Residuals: -0.95830, Convergence: 0.001809\n",
      "Epoch: 220, Loss: 2425.96478, Residuals: -0.95647, Convergence: 0.001558\n",
      "Epoch: 221, Loss: 2422.67474, Residuals: -0.95493, Convergence: 0.001358\n",
      "Epoch: 222, Loss: 2419.77900, Residuals: -0.95363, Convergence: 0.001197\n",
      "Epoch: 223, Loss: 2417.20122, Residuals: -0.95254, Convergence: 0.001066\n",
      "Epoch: 224, Loss: 2414.88441, Residuals: -0.95163, Convergence: 0.000959\n",
      "Evidence 14602.418\n",
      "\n",
      "Epoch: 224, Evidence: 14602.41797, Convergence: 0.005316\n",
      "Updating hyper-parameters...\n",
      "Total samples: 180, Updated regularization: 2.64e-01\n",
      "Epoch: 224, Loss: 2447.27560, Residuals: -0.95163, Convergence:   inf\n",
      "Epoch: 225, Loss: 2443.44377, Residuals: -0.94938, Convergence: 0.001568\n",
      "Epoch: 226, Loss: 2440.25910, Residuals: -0.94762, Convergence: 0.001305\n",
      "Epoch: 227, Loss: 2437.53523, Residuals: -0.94622, Convergence: 0.001117\n",
      "Epoch: 228, Loss: 2435.16189, Residuals: -0.94509, Convergence: 0.000975\n",
      "Evidence 14629.924\n",
      "\n",
      "Epoch: 228, Evidence: 14629.92383, Convergence: 0.001880\n",
      "Updating hyper-parameters...\n",
      "Total samples: 180, Updated regularization: 2.13e-01\n",
      "Epoch: 228, Loss: 2448.31360, Residuals: -0.94509, Convergence:   inf\n",
      "Epoch: 229, Loss: 2445.43803, Residuals: -0.94336, Convergence: 0.001176\n",
      "Epoch: 230, Loss: 2443.03089, Residuals: -0.94201, Convergence: 0.000985\n",
      "Evidence 14641.959\n",
      "\n",
      "Epoch: 230, Evidence: 14641.95898, Convergence: 0.000822\n",
      "Updating hyper-parameters...\n",
      "Total samples: 180, Updated regularization: 1.77e-01\n",
      "Epoch: 230, Loss: 2449.04673, Residuals: -0.94201, Convergence:   inf\n",
      "Epoch: 231, Loss: 2444.52457, Residuals: -0.93986, Convergence: 0.001850\n",
      "Epoch: 232, Loss: 2441.07572, Residuals: -0.93822, Convergence: 0.001413\n",
      "Epoch: 233, Loss: 2438.30244, Residuals: -0.93712, Convergence: 0.001137\n",
      "Epoch: 234, Loss: 2435.97110, Residuals: -0.93653, Convergence: 0.000957\n",
      "Evidence 14659.709\n",
      "\n",
      "Epoch: 234, Evidence: 14659.70898, Convergence: 0.002032\n",
      "Updating hyper-parameters...\n",
      "Total samples: 180, Updated regularization: 1.47e-01\n",
      "Epoch: 234, Loss: 2449.22160, Residuals: -0.93653, Convergence:   inf\n",
      "Epoch: 235, Loss: 2446.19385, Residuals: -0.93436, Convergence: 0.001238\n",
      "Epoch: 236, Loss: 2443.81838, Residuals: -0.93326, Convergence: 0.000972\n",
      "Evidence 14670.818\n",
      "\n",
      "Epoch: 236, Evidence: 14670.81836, Convergence: 0.000757\n",
      "Updating hyper-parameters...\n",
      "Total samples: 180, Updated regularization: 1.25e-01\n",
      "Epoch: 236, Loss: 2449.37357, Residuals: -0.93326, Convergence:   inf\n",
      "Epoch: 237, Loss: 2444.94392, Residuals: -0.93045, Convergence: 0.001812\n",
      "Epoch: 238, Loss: 2441.86592, Residuals: -0.93147, Convergence: 0.001261\n",
      "Epoch: 239, Loss: 2439.33944, Residuals: -0.93213, Convergence: 0.001036\n",
      "Epoch: 240, Loss: 2437.14640, Residuals: -0.93488, Convergence: 0.000900\n",
      "Evidence 14686.473\n",
      "\n",
      "Epoch: 240, Evidence: 14686.47266, Convergence: 0.001822\n",
      "Updating hyper-parameters...\n",
      "Total samples: 180, Updated regularization: 1.10e-01\n",
      "Epoch: 240, Loss: 2448.67051, Residuals: -0.93488, Convergence:   inf\n",
      "Epoch: 241, Loss: 2446.65177, Residuals: -0.93244, Convergence: 0.000825\n",
      "Evidence 14693.615\n",
      "\n",
      "Epoch: 241, Evidence: 14693.61523, Convergence: 0.000486\n",
      "Updating hyper-parameters...\n",
      "Total samples: 180, Updated regularization: 9.07e-02\n",
      "Epoch: 241, Loss: 2449.49704, Residuals: -0.93244, Convergence:   inf\n",
      "Epoch: 242, Loss: 2485.58906, Residuals: -0.97345, Convergence: -0.014521\n",
      "Epoch: 242, Loss: 2447.35068, Residuals: -0.93164, Convergence: 0.000877\n",
      "Evidence 14697.825\n",
      "\n",
      "Epoch: 242, Evidence: 14697.82520, Convergence: 0.000772\n",
      "Updating hyper-parameters...\n",
      "Total samples: 180, Updated regularization: 8.38e-02\n",
      "Epoch: 242, Loss: 2448.92893, Residuals: -0.93164, Convergence:   inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 243, Loss: 2453.38553, Residuals: -0.93472, Convergence: -0.001817\n",
      "Epoch: 243, Loss: 2448.75257, Residuals: -0.93042, Convergence: 0.000072\n",
      "Evidence 14699.710\n",
      "\n",
      "Epoch: 243, Evidence: 14699.70996, Convergence: 0.000901\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 381.83421, Residuals: -4.52514, Convergence:   inf\n",
      "Epoch: 1, Loss: 355.99080, Residuals: -4.40359, Convergence: 0.072596\n",
      "Epoch: 2, Loss: 334.85739, Residuals: -4.23875, Convergence: 0.063112\n",
      "Epoch: 3, Loss: 318.68601, Residuals: -4.07310, Convergence: 0.050744\n",
      "Epoch: 4, Loss: 306.34568, Residuals: -3.92706, Convergence: 0.040282\n",
      "Epoch: 5, Loss: 296.55557, Residuals: -3.79801, Convergence: 0.033013\n",
      "Epoch: 6, Loss: 288.60703, Residuals: -3.68569, Convergence: 0.027541\n",
      "Epoch: 7, Loss: 282.00942, Residuals: -3.58962, Convergence: 0.023395\n",
      "Epoch: 8, Loss: 276.39907, Residuals: -3.50768, Convergence: 0.020298\n",
      "Epoch: 9, Loss: 271.51909, Residuals: -3.43749, Convergence: 0.017973\n",
      "Epoch: 10, Loss: 267.18681, Residuals: -3.37691, Convergence: 0.016214\n",
      "Epoch: 11, Loss: 263.27015, Residuals: -3.32414, Convergence: 0.014877\n",
      "Epoch: 12, Loss: 259.67316, Residuals: -3.27766, Convergence: 0.013852\n",
      "Epoch: 13, Loss: 256.32664, Residuals: -3.23616, Convergence: 0.013056\n",
      "Epoch: 14, Loss: 253.18109, Residuals: -3.19851, Convergence: 0.012424\n",
      "Epoch: 15, Loss: 250.20179, Residuals: -3.16372, Convergence: 0.011908\n",
      "Epoch: 16, Loss: 247.36587, Residuals: -3.13106, Convergence: 0.011464\n",
      "Epoch: 17, Loss: 244.65707, Residuals: -3.10006, Convergence: 0.011072\n",
      "Epoch: 18, Loss: 242.05573, Residuals: -3.07031, Convergence: 0.010747\n",
      "Epoch: 19, Loss: 239.53298, Residuals: -3.04138, Convergence: 0.010532\n",
      "Epoch: 20, Loss: 237.05461, Residuals: -3.01273, Convergence: 0.010455\n",
      "Epoch: 21, Loss: 234.58935, Residuals: -2.98387, Convergence: 0.010509\n",
      "Epoch: 22, Loss: 232.11444, Residuals: -2.95444, Convergence: 0.010662\n",
      "Epoch: 23, Loss: 229.60947, Residuals: -2.92418, Convergence: 0.010910\n",
      "Epoch: 24, Loss: 227.03872, Residuals: -2.89271, Convergence: 0.011323\n",
      "Epoch: 25, Loss: 224.34538, Residuals: -2.85937, Convergence: 0.012005\n",
      "Epoch: 26, Loss: 221.49666, Residuals: -2.82371, Convergence: 0.012861\n",
      "Epoch: 27, Loss: 218.57894, Residuals: -2.78657, Convergence: 0.013349\n",
      "Epoch: 28, Loss: 215.72907, Residuals: -2.74945, Convergence: 0.013210\n",
      "Epoch: 29, Loss: 212.99033, Residuals: -2.71299, Convergence: 0.012859\n",
      "Epoch: 30, Loss: 210.35114, Residuals: -2.67717, Convergence: 0.012547\n",
      "Epoch: 31, Loss: 207.79281, Residuals: -2.64185, Convergence: 0.012312\n",
      "Epoch: 32, Loss: 205.30107, Residuals: -2.60691, Convergence: 0.012137\n",
      "Epoch: 33, Loss: 202.86664, Residuals: -2.57225, Convergence: 0.012000\n",
      "Epoch: 34, Loss: 200.48435, Residuals: -2.53780, Convergence: 0.011883\n",
      "Epoch: 35, Loss: 198.15197, Residuals: -2.50351, Convergence: 0.011771\n",
      "Epoch: 36, Loss: 195.86926, Residuals: -2.46938, Convergence: 0.011654\n",
      "Epoch: 37, Loss: 193.63728, Residuals: -2.43537, Convergence: 0.011527\n",
      "Epoch: 38, Loss: 191.45788, Residuals: -2.40151, Convergence: 0.011383\n",
      "Epoch: 39, Loss: 189.33327, Residuals: -2.36780, Convergence: 0.011222\n",
      "Epoch: 40, Loss: 187.26574, Residuals: -2.33427, Convergence: 0.011041\n",
      "Epoch: 41, Loss: 185.25741, Residuals: -2.30096, Convergence: 0.010841\n",
      "Epoch: 42, Loss: 183.31013, Residuals: -2.26790, Convergence: 0.010623\n",
      "Epoch: 43, Loss: 181.42534, Residuals: -2.23514, Convergence: 0.010389\n",
      "Epoch: 44, Loss: 179.60402, Residuals: -2.20270, Convergence: 0.010141\n",
      "Epoch: 45, Loss: 177.84676, Residuals: -2.17064, Convergence: 0.009881\n",
      "Epoch: 46, Loss: 176.15384, Residuals: -2.13900, Convergence: 0.009610\n",
      "Epoch: 47, Loss: 174.52531, Residuals: -2.10781, Convergence: 0.009331\n",
      "Epoch: 48, Loss: 172.96102, Residuals: -2.07713, Convergence: 0.009044\n",
      "Epoch: 49, Loss: 171.46059, Residuals: -2.04698, Convergence: 0.008751\n",
      "Epoch: 50, Loss: 170.02322, Residuals: -2.01741, Convergence: 0.008454\n",
      "Epoch: 51, Loss: 168.64759, Residuals: -1.98844, Convergence: 0.008157\n",
      "Epoch: 52, Loss: 167.33174, Residuals: -1.96010, Convergence: 0.007864\n",
      "Epoch: 53, Loss: 166.07312, Residuals: -1.93240, Convergence: 0.007579\n",
      "Epoch: 54, Loss: 164.86879, Residuals: -1.90533, Convergence: 0.007305\n",
      "Epoch: 55, Loss: 163.71561, Residuals: -1.87888, Convergence: 0.007044\n",
      "Epoch: 56, Loss: 162.61049, Residuals: -1.85304, Convergence: 0.006796\n",
      "Epoch: 57, Loss: 161.55059, Residuals: -1.82780, Convergence: 0.006561\n",
      "Epoch: 58, Loss: 160.53343, Residuals: -1.80314, Convergence: 0.006336\n",
      "Epoch: 59, Loss: 159.55684, Residuals: -1.77905, Convergence: 0.006121\n",
      "Epoch: 60, Loss: 158.61902, Residuals: -1.75553, Convergence: 0.005912\n",
      "Epoch: 61, Loss: 157.71844, Residuals: -1.73258, Convergence: 0.005710\n",
      "Epoch: 62, Loss: 156.85374, Residuals: -1.71020, Convergence: 0.005513\n",
      "Epoch: 63, Loss: 156.02369, Residuals: -1.68839, Convergence: 0.005320\n",
      "Epoch: 64, Loss: 155.22719, Residuals: -1.66716, Convergence: 0.005131\n",
      "Epoch: 65, Loss: 154.46316, Residuals: -1.64651, Convergence: 0.004946\n",
      "Epoch: 66, Loss: 153.73059, Residuals: -1.62645, Convergence: 0.004765\n",
      "Epoch: 67, Loss: 153.02845, Residuals: -1.60696, Convergence: 0.004588\n",
      "Epoch: 68, Loss: 152.35576, Residuals: -1.58806, Convergence: 0.004415\n",
      "Epoch: 69, Loss: 151.71156, Residuals: -1.56973, Convergence: 0.004246\n",
      "Epoch: 70, Loss: 151.09490, Residuals: -1.55198, Convergence: 0.004081\n",
      "Epoch: 71, Loss: 150.50482, Residuals: -1.53480, Convergence: 0.003921\n",
      "Epoch: 72, Loss: 149.94042, Residuals: -1.51818, Convergence: 0.003764\n",
      "Epoch: 73, Loss: 149.40077, Residuals: -1.50211, Convergence: 0.003612\n",
      "Epoch: 74, Loss: 148.88499, Residuals: -1.48659, Convergence: 0.003464\n",
      "Epoch: 75, Loss: 148.39220, Residuals: -1.47161, Convergence: 0.003321\n",
      "Epoch: 76, Loss: 147.92154, Residuals: -1.45715, Convergence: 0.003182\n",
      "Epoch: 77, Loss: 147.47217, Residuals: -1.44321, Convergence: 0.003047\n",
      "Epoch: 78, Loss: 147.04324, Residuals: -1.42978, Convergence: 0.002917\n",
      "Epoch: 79, Loss: 146.63395, Residuals: -1.41683, Convergence: 0.002791\n",
      "Epoch: 80, Loss: 146.24350, Residuals: -1.40437, Convergence: 0.002670\n",
      "Epoch: 81, Loss: 145.87110, Residuals: -1.39237, Convergence: 0.002553\n",
      "Epoch: 82, Loss: 145.51596, Residuals: -1.38083, Convergence: 0.002441\n",
      "Epoch: 83, Loss: 145.17737, Residuals: -1.36973, Convergence: 0.002332\n",
      "Epoch: 84, Loss: 144.85455, Residuals: -1.35905, Convergence: 0.002229\n",
      "Epoch: 85, Loss: 144.54683, Residuals: -1.34879, Convergence: 0.002129\n",
      "Epoch: 86, Loss: 144.25348, Residuals: -1.33894, Convergence: 0.002034\n",
      "Epoch: 87, Loss: 143.97388, Residuals: -1.32947, Convergence: 0.001942\n",
      "Epoch: 88, Loss: 143.70738, Residuals: -1.32037, Convergence: 0.001854\n",
      "Epoch: 89, Loss: 143.45338, Residuals: -1.31163, Convergence: 0.001771\n",
      "Epoch: 90, Loss: 143.21131, Residuals: -1.30325, Convergence: 0.001690\n",
      "Epoch: 91, Loss: 142.98064, Residuals: -1.29520, Convergence: 0.001613\n",
      "Epoch: 92, Loss: 142.76087, Residuals: -1.28747, Convergence: 0.001539\n",
      "Epoch: 93, Loss: 142.55155, Residuals: -1.28005, Convergence: 0.001468\n",
      "Epoch: 94, Loss: 142.35224, Residuals: -1.27293, Convergence: 0.001400\n",
      "Epoch: 95, Loss: 142.16256, Residuals: -1.26610, Convergence: 0.001334\n",
      "Epoch: 96, Loss: 141.98215, Residuals: -1.25955, Convergence: 0.001271\n",
      "Epoch: 97, Loss: 141.81070, Residuals: -1.25327, Convergence: 0.001209\n",
      "Epoch: 98, Loss: 141.64791, Residuals: -1.24725, Convergence: 0.001149\n",
      "Epoch: 99, Loss: 141.49352, Residuals: -1.24148, Convergence: 0.001091\n",
      "Epoch: 100, Loss: 141.34730, Residuals: -1.23595, Convergence: 0.001034\n",
      "Epoch: 101, Loss: 141.20904, Residuals: -1.23067, Convergence: 0.000979\n",
      "Evidence -182.717\n",
      "\n",
      "Epoch: 101, Evidence: -182.71651, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.25e-01\n",
      "Epoch: 101, Loss: 1373.48027, Residuals: -1.23067, Convergence:   inf\n",
      "Epoch: 102, Loss: 1311.67446, Residuals: -1.26069, Convergence: 0.047120\n",
      "Epoch: 103, Loss: 1264.48466, Residuals: -1.28431, Convergence: 0.037319\n",
      "Epoch: 104, Loss: 1228.75216, Residuals: -1.30151, Convergence: 0.029080\n",
      "Epoch: 105, Loss: 1201.01030, Residuals: -1.31375, Convergence: 0.023099\n",
      "Epoch: 106, Loss: 1178.65970, Residuals: -1.32287, Convergence: 0.018963\n",
      "Epoch: 107, Loss: 1160.16156, Residuals: -1.32987, Convergence: 0.015944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 108, Loss: 1144.58614, Residuals: -1.33523, Convergence: 0.013608\n",
      "Epoch: 109, Loss: 1131.30644, Residuals: -1.33918, Convergence: 0.011738\n",
      "Epoch: 110, Loss: 1119.85996, Residuals: -1.34186, Convergence: 0.010221\n",
      "Epoch: 111, Loss: 1109.88604, Residuals: -1.34340, Convergence: 0.008986\n",
      "Epoch: 112, Loss: 1101.09533, Residuals: -1.34390, Convergence: 0.007984\n",
      "Epoch: 113, Loss: 1093.24807, Residuals: -1.34345, Convergence: 0.007178\n",
      "Epoch: 114, Loss: 1086.14303, Residuals: -1.34213, Convergence: 0.006542\n",
      "Epoch: 115, Loss: 1079.60618, Residuals: -1.33998, Convergence: 0.006055\n",
      "Epoch: 116, Loss: 1073.48529, Residuals: -1.33704, Convergence: 0.005702\n",
      "Epoch: 117, Loss: 1067.64425, Residuals: -1.33334, Convergence: 0.005471\n",
      "Epoch: 118, Loss: 1061.96205, Residuals: -1.32891, Convergence: 0.005351\n",
      "Epoch: 119, Loss: 1056.33765, Residuals: -1.32374, Convergence: 0.005324\n",
      "Epoch: 120, Loss: 1050.69776, Residuals: -1.31789, Convergence: 0.005368\n",
      "Epoch: 121, Loss: 1045.01300, Residuals: -1.31142, Convergence: 0.005440\n",
      "Epoch: 122, Loss: 1039.30907, Residuals: -1.30441, Convergence: 0.005488\n",
      "Epoch: 123, Loss: 1033.66485, Residuals: -1.29696, Convergence: 0.005460\n",
      "Epoch: 124, Loss: 1028.18230, Residuals: -1.28919, Convergence: 0.005332\n",
      "Epoch: 125, Loss: 1022.94978, Residuals: -1.28119, Convergence: 0.005115\n",
      "Epoch: 126, Loss: 1018.01867, Residuals: -1.27304, Convergence: 0.004844\n",
      "Epoch: 127, Loss: 1013.40295, Residuals: -1.26482, Convergence: 0.004555\n",
      "Epoch: 128, Loss: 1009.09035, Residuals: -1.25659, Convergence: 0.004274\n",
      "Epoch: 129, Loss: 1005.05662, Residuals: -1.24841, Convergence: 0.004013\n",
      "Epoch: 130, Loss: 1001.27337, Residuals: -1.24033, Convergence: 0.003778\n",
      "Epoch: 131, Loss: 997.71299, Residuals: -1.23237, Convergence: 0.003569\n",
      "Epoch: 132, Loss: 994.35147, Residuals: -1.22457, Convergence: 0.003381\n",
      "Epoch: 133, Loss: 991.16821, Residuals: -1.21695, Convergence: 0.003212\n",
      "Epoch: 134, Loss: 988.14643, Residuals: -1.20953, Convergence: 0.003058\n",
      "Epoch: 135, Loss: 985.27267, Residuals: -1.20232, Convergence: 0.002917\n",
      "Epoch: 136, Loss: 982.53591, Residuals: -1.19534, Convergence: 0.002785\n",
      "Epoch: 137, Loss: 979.92720, Residuals: -1.18860, Convergence: 0.002662\n",
      "Epoch: 138, Loss: 977.43917, Residuals: -1.18210, Convergence: 0.002545\n",
      "Epoch: 139, Loss: 975.06563, Residuals: -1.17585, Convergence: 0.002434\n",
      "Epoch: 140, Loss: 972.80029, Residuals: -1.16985, Convergence: 0.002329\n",
      "Epoch: 141, Loss: 970.63822, Residuals: -1.16409, Convergence: 0.002227\n",
      "Epoch: 142, Loss: 968.57447, Residuals: -1.15859, Convergence: 0.002131\n",
      "Epoch: 143, Loss: 966.60362, Residuals: -1.15332, Convergence: 0.002039\n",
      "Epoch: 144, Loss: 964.72116, Residuals: -1.14829, Convergence: 0.001951\n",
      "Epoch: 145, Loss: 962.92177, Residuals: -1.14349, Convergence: 0.001869\n",
      "Epoch: 146, Loss: 961.20123, Residuals: -1.13891, Convergence: 0.001790\n",
      "Epoch: 147, Loss: 959.55431, Residuals: -1.13454, Convergence: 0.001716\n",
      "Epoch: 148, Loss: 957.97726, Residuals: -1.13037, Convergence: 0.001646\n",
      "Epoch: 149, Loss: 956.46488, Residuals: -1.12639, Convergence: 0.001581\n",
      "Epoch: 150, Loss: 955.01316, Residuals: -1.12258, Convergence: 0.001520\n",
      "Epoch: 151, Loss: 953.61744, Residuals: -1.11895, Convergence: 0.001464\n",
      "Epoch: 152, Loss: 952.27327, Residuals: -1.11547, Convergence: 0.001412\n",
      "Epoch: 153, Loss: 950.97647, Residuals: -1.11214, Convergence: 0.001364\n",
      "Epoch: 154, Loss: 949.72252, Residuals: -1.10895, Convergence: 0.001320\n",
      "Epoch: 155, Loss: 948.50638, Residuals: -1.10588, Convergence: 0.001282\n",
      "Epoch: 156, Loss: 947.32377, Residuals: -1.10292, Convergence: 0.001248\n",
      "Epoch: 157, Loss: 946.16945, Residuals: -1.10006, Convergence: 0.001220\n",
      "Epoch: 158, Loss: 945.03895, Residuals: -1.09729, Convergence: 0.001196\n",
      "Epoch: 159, Loss: 943.92724, Residuals: -1.09459, Convergence: 0.001178\n",
      "Epoch: 160, Loss: 942.83039, Residuals: -1.09195, Convergence: 0.001163\n",
      "Epoch: 161, Loss: 941.74522, Residuals: -1.08937, Convergence: 0.001152\n",
      "Epoch: 162, Loss: 940.66932, Residuals: -1.08683, Convergence: 0.001144\n",
      "Epoch: 163, Loss: 939.60236, Residuals: -1.08434, Convergence: 0.001136\n",
      "Epoch: 164, Loss: 938.54520, Residuals: -1.08188, Convergence: 0.001126\n",
      "Epoch: 165, Loss: 937.50053, Residuals: -1.07947, Convergence: 0.001114\n",
      "Epoch: 166, Loss: 936.47205, Residuals: -1.07710, Convergence: 0.001098\n",
      "Epoch: 167, Loss: 935.46384, Residuals: -1.07479, Convergence: 0.001078\n",
      "Epoch: 168, Loss: 934.48014, Residuals: -1.07253, Convergence: 0.001053\n",
      "Epoch: 169, Loss: 933.52486, Residuals: -1.07034, Convergence: 0.001023\n",
      "Epoch: 170, Loss: 932.60068, Residuals: -1.06823, Convergence: 0.000991\n",
      "Evidence 11202.267\n",
      "\n",
      "Epoch: 170, Evidence: 11202.26660, Convergence: 1.016311\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.75e-01\n",
      "Epoch: 170, Loss: 2351.08675, Residuals: -1.06823, Convergence:   inf\n",
      "Epoch: 171, Loss: 2314.11273, Residuals: -1.07497, Convergence: 0.015978\n",
      "Epoch: 172, Loss: 2287.66560, Residuals: -1.07339, Convergence: 0.011561\n",
      "Epoch: 173, Loss: 2265.63366, Residuals: -1.07096, Convergence: 0.009724\n",
      "Epoch: 174, Loss: 2247.05085, Residuals: -1.06832, Convergence: 0.008270\n",
      "Epoch: 175, Loss: 2231.24336, Residuals: -1.06557, Convergence: 0.007085\n",
      "Epoch: 176, Loss: 2217.69135, Residuals: -1.06277, Convergence: 0.006111\n",
      "Epoch: 177, Loss: 2205.97060, Residuals: -1.05992, Convergence: 0.005313\n",
      "Epoch: 178, Loss: 2195.72532, Residuals: -1.05704, Convergence: 0.004666\n",
      "Epoch: 179, Loss: 2186.66011, Residuals: -1.05411, Convergence: 0.004146\n",
      "Epoch: 180, Loss: 2178.52636, Residuals: -1.05111, Convergence: 0.003734\n",
      "Epoch: 181, Loss: 2171.13029, Residuals: -1.04803, Convergence: 0.003407\n",
      "Epoch: 182, Loss: 2164.32827, Residuals: -1.04485, Convergence: 0.003143\n",
      "Epoch: 183, Loss: 2158.02904, Residuals: -1.04158, Convergence: 0.002919\n",
      "Epoch: 184, Loss: 2152.18157, Residuals: -1.03826, Convergence: 0.002717\n",
      "Epoch: 185, Loss: 2146.75707, Residuals: -1.03492, Convergence: 0.002527\n",
      "Epoch: 186, Loss: 2141.73484, Residuals: -1.03161, Convergence: 0.002345\n",
      "Epoch: 187, Loss: 2137.09390, Residuals: -1.02838, Convergence: 0.002172\n",
      "Epoch: 188, Loss: 2132.81029, Residuals: -1.02524, Convergence: 0.002008\n",
      "Epoch: 189, Loss: 2128.85802, Residuals: -1.02223, Convergence: 0.001857\n",
      "Epoch: 190, Loss: 2125.20891, Residuals: -1.01935, Convergence: 0.001717\n",
      "Epoch: 191, Loss: 2121.83573, Residuals: -1.01662, Convergence: 0.001590\n",
      "Epoch: 192, Loss: 2118.71086, Residuals: -1.01403, Convergence: 0.001475\n",
      "Epoch: 193, Loss: 2115.80948, Residuals: -1.01158, Convergence: 0.001371\n",
      "Epoch: 194, Loss: 2113.10739, Residuals: -1.00928, Convergence: 0.001279\n",
      "Epoch: 195, Loss: 2110.58280, Residuals: -1.00711, Convergence: 0.001196\n",
      "Epoch: 196, Loss: 2108.21583, Residuals: -1.00507, Convergence: 0.001123\n",
      "Epoch: 197, Loss: 2105.98966, Residuals: -1.00316, Convergence: 0.001057\n",
      "Epoch: 198, Loss: 2103.88836, Residuals: -1.00136, Convergence: 0.000999\n",
      "Evidence 14334.016\n",
      "\n",
      "Epoch: 198, Evidence: 14334.01562, Convergence: 0.218484\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.36e-01\n",
      "Epoch: 198, Loss: 2474.16218, Residuals: -1.00136, Convergence:   inf\n",
      "Epoch: 199, Loss: 2460.73001, Residuals: -0.99745, Convergence: 0.005459\n",
      "Epoch: 200, Loss: 2449.72233, Residuals: -0.99342, Convergence: 0.004493\n",
      "Epoch: 201, Loss: 2440.25338, Residuals: -0.98969, Convergence: 0.003880\n",
      "Epoch: 202, Loss: 2432.05898, Residuals: -0.98628, Convergence: 0.003369\n",
      "Epoch: 203, Loss: 2424.92688, Residuals: -0.98321, Convergence: 0.002941\n",
      "Epoch: 204, Loss: 2418.68225, Residuals: -0.98047, Convergence: 0.002582\n",
      "Epoch: 205, Loss: 2413.18053, Residuals: -0.97801, Convergence: 0.002280\n",
      "Epoch: 206, Loss: 2408.30302, Residuals: -0.97582, Convergence: 0.002025\n",
      "Epoch: 207, Loss: 2403.95104, Residuals: -0.97387, Convergence: 0.001810\n",
      "Epoch: 208, Loss: 2400.04515, Residuals: -0.97212, Convergence: 0.001627\n",
      "Epoch: 209, Loss: 2396.51724, Residuals: -0.97056, Convergence: 0.001472\n",
      "Epoch: 210, Loss: 2393.31479, Residuals: -0.96915, Convergence: 0.001338\n",
      "Epoch: 211, Loss: 2390.39218, Residuals: -0.96788, Convergence: 0.001223\n",
      "Epoch: 212, Loss: 2387.71304, Residuals: -0.96674, Convergence: 0.001122\n",
      "Epoch: 213, Loss: 2385.24554, Residuals: -0.96570, Convergence: 0.001034\n",
      "Epoch: 214, Loss: 2382.96571, Residuals: -0.96476, Convergence: 0.000957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence 14696.733\n",
      "\n",
      "Epoch: 214, Evidence: 14696.73340, Convergence: 0.024680\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.30e-01\n",
      "Epoch: 214, Loss: 2479.04128, Residuals: -0.96476, Convergence:   inf\n",
      "Epoch: 215, Loss: 2472.61988, Residuals: -0.96138, Convergence: 0.002597\n",
      "Epoch: 216, Loss: 2467.32939, Residuals: -0.95854, Convergence: 0.002144\n",
      "Epoch: 217, Loss: 2462.84950, Residuals: -0.95622, Convergence: 0.001819\n",
      "Epoch: 218, Loss: 2458.99168, Residuals: -0.95430, Convergence: 0.001569\n",
      "Epoch: 219, Loss: 2455.61985, Residuals: -0.95271, Convergence: 0.001373\n",
      "Epoch: 220, Loss: 2452.63459, Residuals: -0.95139, Convergence: 0.001217\n",
      "Epoch: 221, Loss: 2449.96153, Residuals: -0.95029, Convergence: 0.001091\n",
      "Epoch: 222, Loss: 2447.54549, Residuals: -0.94937, Convergence: 0.000987\n",
      "Evidence 14777.166\n",
      "\n",
      "Epoch: 222, Evidence: 14777.16602, Convergence: 0.005443\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.57e-01\n",
      "Epoch: 222, Loss: 2480.92021, Residuals: -0.94937, Convergence:   inf\n",
      "Epoch: 223, Loss: 2477.04131, Residuals: -0.94685, Convergence: 0.001566\n",
      "Epoch: 224, Loss: 2473.81606, Residuals: -0.94491, Convergence: 0.001304\n",
      "Epoch: 225, Loss: 2471.04112, Residuals: -0.94339, Convergence: 0.001123\n",
      "Epoch: 226, Loss: 2468.60296, Residuals: -0.94218, Convergence: 0.000988\n",
      "Evidence 14804.754\n",
      "\n",
      "Epoch: 226, Evidence: 14804.75391, Convergence: 0.001863\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.06e-01\n",
      "Epoch: 226, Loss: 2482.06297, Residuals: -0.94218, Convergence:   inf\n",
      "Epoch: 227, Loss: 2479.16655, Residuals: -0.94015, Convergence: 0.001168\n",
      "Epoch: 228, Loss: 2476.72840, Residuals: -0.93861, Convergence: 0.000984\n",
      "Evidence 14816.787\n",
      "\n",
      "Epoch: 228, Evidence: 14816.78711, Convergence: 0.000812\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.70e-01\n",
      "Epoch: 228, Loss: 2482.86432, Residuals: -0.93861, Convergence:   inf\n",
      "Epoch: 229, Loss: 2478.26223, Residuals: -0.93572, Convergence: 0.001857\n",
      "Epoch: 230, Loss: 2474.75459, Residuals: -0.93386, Convergence: 0.001417\n",
      "Epoch: 231, Loss: 2471.90034, Residuals: -0.93266, Convergence: 0.001155\n",
      "Epoch: 232, Loss: 2469.47358, Residuals: -0.93202, Convergence: 0.000983\n",
      "Evidence 14834.603\n",
      "\n",
      "Epoch: 232, Evidence: 14834.60254, Convergence: 0.002012\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.40e-01\n",
      "Epoch: 232, Loss: 2483.09006, Residuals: -0.93202, Convergence:   inf\n",
      "Epoch: 233, Loss: 2480.03690, Residuals: -0.92935, Convergence: 0.001231\n",
      "Epoch: 234, Loss: 2477.62037, Residuals: -0.92804, Convergence: 0.000975\n",
      "Evidence 14845.528\n",
      "\n",
      "Epoch: 234, Evidence: 14845.52832, Convergence: 0.000736\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.18e-01\n",
      "Epoch: 234, Loss: 2483.32527, Residuals: -0.92804, Convergence:   inf\n",
      "Epoch: 235, Loss: 2478.89044, Residuals: -0.92421, Convergence: 0.001789\n",
      "Epoch: 236, Loss: 2475.71496, Residuals: -0.92520, Convergence: 0.001283\n",
      "Epoch: 237, Loss: 2473.06702, Residuals: -0.92582, Convergence: 0.001071\n",
      "Epoch: 238, Loss: 2470.76955, Residuals: -0.92880, Convergence: 0.000930\n",
      "Evidence 14861.182\n",
      "\n",
      "Epoch: 238, Evidence: 14861.18164, Convergence: 0.001788\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.05e-01\n",
      "Epoch: 238, Loss: 2482.64701, Residuals: -0.92880, Convergence:   inf\n",
      "Epoch: 239, Loss: 2480.89352, Residuals: -0.92582, Convergence: 0.000707\n",
      "Evidence 14867.923\n",
      "\n",
      "Epoch: 239, Evidence: 14867.92285, Convergence: 0.000453\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.59e-02\n",
      "Epoch: 239, Loss: 2483.67947, Residuals: -0.92582, Convergence:   inf\n",
      "Epoch: 240, Loss: 2529.89512, Residuals: -0.97081, Convergence: -0.018268\n",
      "Epoch: 240, Loss: 2481.55069, Residuals: -0.92432, Convergence: 0.000858\n",
      "Evidence 14872.037\n",
      "\n",
      "Epoch: 240, Evidence: 14872.03711, Convergence: 0.000730\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.96e-02\n",
      "Epoch: 240, Loss: 2483.04493, Residuals: -0.92432, Convergence:   inf\n",
      "Epoch: 241, Loss: 2488.26386, Residuals: -0.92564, Convergence: -0.002097\n",
      "Epoch: 241, Loss: 2483.08081, Residuals: -0.92228, Convergence: -0.000014\n",
      "Evidence 14873.722\n",
      "\n",
      "Epoch: 241, Evidence: 14873.72168, Convergence: 0.000843\n",
      "Total samples: 181, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 381.95971, Residuals: -4.54111, Convergence:   inf\n",
      "Epoch: 1, Loss: 356.29349, Residuals: -4.42088, Convergence: 0.072037\n",
      "Epoch: 2, Loss: 335.29223, Residuals: -4.25658, Convergence: 0.062636\n",
      "Epoch: 3, Loss: 319.26276, Residuals: -4.09193, Convergence: 0.050208\n",
      "Epoch: 4, Loss: 307.03435, Residuals: -3.94666, Convergence: 0.039827\n",
      "Epoch: 5, Loss: 297.33666, Residuals: -3.81795, Convergence: 0.032615\n",
      "Epoch: 6, Loss: 289.46778, Residuals: -3.70571, Convergence: 0.027184\n",
      "Epoch: 7, Loss: 282.93646, Residuals: -3.60952, Convergence: 0.023084\n",
      "Epoch: 8, Loss: 277.37818, Residuals: -3.52733, Convergence: 0.020039\n",
      "Epoch: 9, Loss: 272.53615, Residuals: -3.45682, Convergence: 0.017767\n",
      "Epoch: 10, Loss: 268.22807, Residuals: -3.39589, Convergence: 0.016061\n",
      "Epoch: 11, Loss: 264.32236, Residuals: -3.34275, Convergence: 0.014776\n",
      "Epoch: 12, Loss: 260.72358, Residuals: -3.29588, Convergence: 0.013803\n",
      "Epoch: 13, Loss: 257.36318, Residuals: -3.25399, Convergence: 0.013057\n",
      "Epoch: 14, Loss: 254.19316, Residuals: -3.21590, Convergence: 0.012471\n",
      "Epoch: 15, Loss: 251.18246, Residuals: -3.18064, Convergence: 0.011986\n",
      "Epoch: 16, Loss: 248.31454, Residuals: -3.14755, Convergence: 0.011550\n",
      "Epoch: 17, Loss: 245.57861, Residuals: -3.11619, Convergence: 0.011141\n",
      "Epoch: 18, Loss: 242.95613, Residuals: -3.08617, Convergence: 0.010794\n",
      "Epoch: 19, Loss: 240.41680, Residuals: -3.05698, Convergence: 0.010562\n",
      "Epoch: 20, Loss: 237.92572, Residuals: -3.02806, Convergence: 0.010470\n",
      "Epoch: 21, Loss: 235.45293, Residuals: -2.99892, Convergence: 0.010502\n",
      "Epoch: 22, Loss: 232.97762, Residuals: -2.96926, Convergence: 0.010625\n",
      "Epoch: 23, Loss: 230.47833, Residuals: -2.93884, Convergence: 0.010844\n",
      "Epoch: 24, Loss: 227.91529, Residuals: -2.90724, Convergence: 0.011246\n",
      "Epoch: 25, Loss: 225.22956, Residuals: -2.87379, Convergence: 0.011924\n",
      "Epoch: 26, Loss: 222.39406, Residuals: -2.83810, Convergence: 0.012750\n",
      "Epoch: 27, Loss: 219.50063, Residuals: -2.80109, Convergence: 0.013182\n",
      "Epoch: 28, Loss: 216.67471, Residuals: -2.76419, Convergence: 0.013042\n",
      "Epoch: 29, Loss: 213.95004, Residuals: -2.72790, Convergence: 0.012735\n",
      "Epoch: 30, Loss: 211.31286, Residuals: -2.69219, Convergence: 0.012480\n",
      "Epoch: 31, Loss: 208.74463, Residuals: -2.65689, Convergence: 0.012303\n",
      "Epoch: 32, Loss: 206.23197, Residuals: -2.62185, Convergence: 0.012184\n",
      "Epoch: 33, Loss: 203.76715, Residuals: -2.58698, Convergence: 0.012096\n",
      "Epoch: 34, Loss: 201.34687, Residuals: -2.55221, Convergence: 0.012020\n",
      "Epoch: 35, Loss: 198.97081, Residuals: -2.51750, Convergence: 0.011942\n",
      "Epoch: 36, Loss: 196.64044, Residuals: -2.48286, Convergence: 0.011851\n",
      "Epoch: 37, Loss: 194.35804, Residuals: -2.44829, Convergence: 0.011743\n",
      "Epoch: 38, Loss: 192.12616, Residuals: -2.41380, Convergence: 0.011617\n",
      "Epoch: 39, Loss: 189.94716, Residuals: -2.37942, Convergence: 0.011472\n",
      "Epoch: 40, Loss: 187.82319, Residuals: -2.34519, Convergence: 0.011308\n",
      "Epoch: 41, Loss: 185.75615, Residuals: -2.31112, Convergence: 0.011128\n",
      "Epoch: 42, Loss: 183.74793, Residuals: -2.27726, Convergence: 0.010929\n",
      "Epoch: 43, Loss: 181.80054, Residuals: -2.24365, Convergence: 0.010712\n",
      "Epoch: 44, Loss: 179.91619, Residuals: -2.21034, Convergence: 0.010473\n",
      "Epoch: 45, Loss: 178.09721, Residuals: -2.17740, Convergence: 0.010213\n",
      "Epoch: 46, Loss: 176.34580, Residuals: -2.14489, Convergence: 0.009932\n",
      "Epoch: 47, Loss: 174.66358, Residuals: -2.11287, Convergence: 0.009631\n",
      "Epoch: 48, Loss: 173.05137, Residuals: -2.08141, Convergence: 0.009316\n",
      "Epoch: 49, Loss: 171.50899, Residuals: -2.05056, Convergence: 0.008993\n",
      "Epoch: 50, Loss: 170.03527, Residuals: -2.02036, Convergence: 0.008667\n",
      "Epoch: 51, Loss: 168.62817, Residuals: -1.99084, Convergence: 0.008344\n",
      "Epoch: 52, Loss: 167.28501, Residuals: -1.96200, Convergence: 0.008029\n",
      "Epoch: 53, Loss: 166.00264, Residuals: -1.93386, Convergence: 0.007725\n",
      "Epoch: 54, Loss: 164.77769, Residuals: -1.90639, Convergence: 0.007434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55, Loss: 163.60673, Residuals: -1.87960, Convergence: 0.007157\n",
      "Epoch: 56, Loss: 162.48639, Residuals: -1.85346, Convergence: 0.006895\n",
      "Epoch: 57, Loss: 161.41348, Residuals: -1.82796, Convergence: 0.006647\n",
      "Epoch: 58, Loss: 160.38503, Residuals: -1.80308, Convergence: 0.006412\n",
      "Epoch: 59, Loss: 159.39837, Residuals: -1.77882, Convergence: 0.006190\n",
      "Epoch: 60, Loss: 158.45112, Residuals: -1.75516, Convergence: 0.005978\n",
      "Epoch: 61, Loss: 157.54122, Residuals: -1.73209, Convergence: 0.005776\n",
      "Epoch: 62, Loss: 156.66688, Residuals: -1.70961, Convergence: 0.005581\n",
      "Epoch: 63, Loss: 155.82661, Residuals: -1.68771, Convergence: 0.005392\n",
      "Epoch: 64, Loss: 155.01910, Residuals: -1.66640, Convergence: 0.005209\n",
      "Epoch: 65, Loss: 154.24324, Residuals: -1.64567, Convergence: 0.005030\n",
      "Epoch: 66, Loss: 153.49804, Residuals: -1.62552, Convergence: 0.004855\n",
      "Epoch: 67, Loss: 152.78262, Residuals: -1.60595, Convergence: 0.004683\n",
      "Epoch: 68, Loss: 152.09617, Residuals: -1.58695, Convergence: 0.004513\n",
      "Epoch: 69, Loss: 151.43795, Residuals: -1.56854, Convergence: 0.004346\n",
      "Epoch: 70, Loss: 150.80724, Residuals: -1.55070, Convergence: 0.004182\n",
      "Epoch: 71, Loss: 150.20331, Residuals: -1.53344, Convergence: 0.004021\n",
      "Epoch: 72, Loss: 149.62549, Residuals: -1.51673, Convergence: 0.003862\n",
      "Epoch: 73, Loss: 149.07309, Residuals: -1.50059, Convergence: 0.003706\n",
      "Epoch: 74, Loss: 148.54539, Residuals: -1.48501, Convergence: 0.003552\n",
      "Epoch: 75, Loss: 148.04169, Residuals: -1.46996, Convergence: 0.003402\n",
      "Epoch: 76, Loss: 147.56125, Residuals: -1.45546, Convergence: 0.003256\n",
      "Epoch: 77, Loss: 147.10332, Residuals: -1.44148, Convergence: 0.003113\n",
      "Epoch: 78, Loss: 146.66716, Residuals: -1.42801, Convergence: 0.002974\n",
      "Epoch: 79, Loss: 146.25196, Residuals: -1.41505, Convergence: 0.002839\n",
      "Epoch: 80, Loss: 145.85696, Residuals: -1.40259, Convergence: 0.002708\n",
      "Epoch: 81, Loss: 145.48136, Residuals: -1.39060, Convergence: 0.002582\n",
      "Epoch: 82, Loss: 145.12436, Residuals: -1.37908, Convergence: 0.002460\n",
      "Epoch: 83, Loss: 144.78518, Residuals: -1.36802, Convergence: 0.002343\n",
      "Epoch: 84, Loss: 144.46305, Residuals: -1.35740, Convergence: 0.002230\n",
      "Epoch: 85, Loss: 144.15721, Residuals: -1.34720, Convergence: 0.002122\n",
      "Epoch: 86, Loss: 143.86691, Residuals: -1.33742, Convergence: 0.002018\n",
      "Epoch: 87, Loss: 143.59147, Residuals: -1.32803, Convergence: 0.001918\n",
      "Epoch: 88, Loss: 143.33021, Residuals: -1.31904, Convergence: 0.001823\n",
      "Epoch: 89, Loss: 143.08249, Residuals: -1.31041, Convergence: 0.001731\n",
      "Epoch: 90, Loss: 142.84772, Residuals: -1.30215, Convergence: 0.001643\n",
      "Epoch: 91, Loss: 142.62535, Residuals: -1.29424, Convergence: 0.001559\n",
      "Epoch: 92, Loss: 142.41487, Residuals: -1.28666, Convergence: 0.001478\n",
      "Epoch: 93, Loss: 142.21578, Residuals: -1.27941, Convergence: 0.001400\n",
      "Epoch: 94, Loss: 142.02768, Residuals: -1.27247, Convergence: 0.001324\n",
      "Epoch: 95, Loss: 141.85014, Residuals: -1.26584, Convergence: 0.001252\n",
      "Epoch: 96, Loss: 141.68279, Residuals: -1.25951, Convergence: 0.001181\n",
      "Epoch: 97, Loss: 141.52528, Residuals: -1.25347, Convergence: 0.001113\n",
      "Epoch: 98, Loss: 141.37726, Residuals: -1.24771, Convergence: 0.001047\n",
      "Epoch: 99, Loss: 141.23839, Residuals: -1.24224, Convergence: 0.000983\n",
      "Evidence -182.181\n",
      "\n",
      "Epoch: 99, Evidence: -182.18083, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 7.24e-01\n",
      "Epoch: 99, Loss: 1364.90416, Residuals: -1.24224, Convergence:   inf\n",
      "Epoch: 100, Loss: 1304.79556, Residuals: -1.27033, Convergence: 0.046067\n",
      "Epoch: 101, Loss: 1258.84299, Residuals: -1.29216, Convergence: 0.036504\n",
      "Epoch: 102, Loss: 1223.94202, Residuals: -1.30738, Convergence: 0.028515\n",
      "Epoch: 103, Loss: 1196.63700, Residuals: -1.31754, Convergence: 0.022818\n",
      "Epoch: 104, Loss: 1174.43998, Residuals: -1.32459, Convergence: 0.018900\n",
      "Epoch: 105, Loss: 1155.93248, Residuals: -1.32963, Convergence: 0.016011\n",
      "Epoch: 106, Loss: 1140.26248, Residuals: -1.33314, Convergence: 0.013742\n",
      "Epoch: 107, Loss: 1126.84839, Residuals: -1.33538, Convergence: 0.011904\n",
      "Epoch: 108, Loss: 1115.25767, Residuals: -1.33653, Convergence: 0.010393\n",
      "Epoch: 109, Loss: 1105.15066, Residuals: -1.33672, Convergence: 0.009145\n",
      "Epoch: 110, Loss: 1096.25340, Residuals: -1.33606, Convergence: 0.008116\n",
      "Epoch: 111, Loss: 1088.33852, Residuals: -1.33465, Convergence: 0.007272\n",
      "Epoch: 112, Loss: 1081.21505, Residuals: -1.33254, Convergence: 0.006588\n",
      "Epoch: 113, Loss: 1074.71686, Residuals: -1.32981, Convergence: 0.006046\n",
      "Epoch: 114, Loss: 1068.69675, Residuals: -1.32647, Convergence: 0.005633\n",
      "Epoch: 115, Loss: 1063.01971, Residuals: -1.32255, Convergence: 0.005340\n",
      "Epoch: 116, Loss: 1057.56071, Residuals: -1.31806, Convergence: 0.005162\n",
      "Epoch: 117, Loss: 1052.20942, Residuals: -1.31300, Convergence: 0.005086\n",
      "Epoch: 118, Loss: 1046.87901, Residuals: -1.30741, Convergence: 0.005092\n",
      "Epoch: 119, Loss: 1041.52547, Residuals: -1.30132, Convergence: 0.005140\n",
      "Epoch: 120, Loss: 1036.16006, Residuals: -1.29481, Convergence: 0.005178\n",
      "Epoch: 121, Loss: 1030.84423, Residuals: -1.28796, Convergence: 0.005157\n",
      "Epoch: 122, Loss: 1025.66214, Residuals: -1.28082, Convergence: 0.005052\n",
      "Epoch: 123, Loss: 1020.68767, Residuals: -1.27348, Convergence: 0.004874\n",
      "Epoch: 124, Loss: 1015.96640, Residuals: -1.26600, Convergence: 0.004647\n",
      "Epoch: 125, Loss: 1011.51444, Residuals: -1.25842, Convergence: 0.004401\n",
      "Epoch: 126, Loss: 1007.32892, Residuals: -1.25080, Convergence: 0.004155\n",
      "Epoch: 127, Loss: 1003.39671, Residuals: -1.24321, Convergence: 0.003919\n",
      "Epoch: 128, Loss: 999.69938, Residuals: -1.23566, Convergence: 0.003698\n",
      "Epoch: 129, Loss: 996.21789, Residuals: -1.22821, Convergence: 0.003495\n",
      "Epoch: 130, Loss: 992.93370, Residuals: -1.22088, Convergence: 0.003308\n",
      "Epoch: 131, Loss: 989.82949, Residuals: -1.21369, Convergence: 0.003136\n",
      "Epoch: 132, Loss: 986.88903, Residuals: -1.20667, Convergence: 0.002980\n",
      "Epoch: 133, Loss: 984.09885, Residuals: -1.19984, Convergence: 0.002835\n",
      "Epoch: 134, Loss: 981.44607, Residuals: -1.19320, Convergence: 0.002703\n",
      "Epoch: 135, Loss: 978.92006, Residuals: -1.18677, Convergence: 0.002580\n",
      "Epoch: 136, Loss: 976.51180, Residuals: -1.18054, Convergence: 0.002466\n",
      "Epoch: 137, Loss: 974.21269, Residuals: -1.17454, Convergence: 0.002360\n",
      "Epoch: 138, Loss: 972.01560, Residuals: -1.16876, Convergence: 0.002260\n",
      "Epoch: 139, Loss: 969.91429, Residuals: -1.16319, Convergence: 0.002166\n",
      "Epoch: 140, Loss: 967.90260, Residuals: -1.15784, Convergence: 0.002078\n",
      "Epoch: 141, Loss: 965.97556, Residuals: -1.15271, Convergence: 0.001995\n",
      "Epoch: 142, Loss: 964.12778, Residuals: -1.14780, Convergence: 0.001917\n",
      "Epoch: 143, Loss: 962.35493, Residuals: -1.14309, Convergence: 0.001842\n",
      "Epoch: 144, Loss: 960.65241, Residuals: -1.13858, Convergence: 0.001772\n",
      "Epoch: 145, Loss: 959.01588, Residuals: -1.13426, Convergence: 0.001706\n",
      "Epoch: 146, Loss: 957.44076, Residuals: -1.13013, Convergence: 0.001645\n",
      "Epoch: 147, Loss: 955.92369, Residuals: -1.12618, Convergence: 0.001587\n",
      "Epoch: 148, Loss: 954.46006, Residuals: -1.12240, Convergence: 0.001533\n",
      "Epoch: 149, Loss: 953.04630, Residuals: -1.11879, Convergence: 0.001483\n",
      "Epoch: 150, Loss: 951.67837, Residuals: -1.11532, Convergence: 0.001437\n",
      "Epoch: 151, Loss: 950.35282, Residuals: -1.11199, Convergence: 0.001395\n",
      "Epoch: 152, Loss: 949.06565, Residuals: -1.10880, Convergence: 0.001356\n",
      "Epoch: 153, Loss: 947.81323, Residuals: -1.10573, Convergence: 0.001321\n",
      "Epoch: 154, Loss: 946.59167, Residuals: -1.10277, Convergence: 0.001290\n",
      "Epoch: 155, Loss: 945.39708, Residuals: -1.09991, Convergence: 0.001264\n",
      "Epoch: 156, Loss: 944.22574, Residuals: -1.09714, Convergence: 0.001241\n",
      "Epoch: 157, Loss: 943.07317, Residuals: -1.09444, Convergence: 0.001222\n",
      "Epoch: 158, Loss: 941.93567, Residuals: -1.09182, Convergence: 0.001208\n",
      "Epoch: 159, Loss: 940.80934, Residuals: -1.08925, Convergence: 0.001197\n",
      "Epoch: 160, Loss: 939.69080, Residuals: -1.08672, Convergence: 0.001190\n",
      "Epoch: 161, Loss: 938.57770, Residuals: -1.08424, Convergence: 0.001186\n",
      "Epoch: 162, Loss: 937.46885, Residuals: -1.08178, Convergence: 0.001183\n",
      "Epoch: 163, Loss: 936.36481, Residuals: -1.07935, Convergence: 0.001179\n",
      "Epoch: 164, Loss: 935.26747, Residuals: -1.07695, Convergence: 0.001173\n",
      "Epoch: 165, Loss: 934.18019, Residuals: -1.07459, Convergence: 0.001164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 166, Loss: 933.10785, Residuals: -1.07226, Convergence: 0.001149\n",
      "Epoch: 167, Loss: 932.05489, Residuals: -1.06998, Convergence: 0.001130\n",
      "Epoch: 168, Loss: 931.02583, Residuals: -1.06775, Convergence: 0.001105\n",
      "Epoch: 169, Loss: 930.02426, Residuals: -1.06559, Convergence: 0.001077\n",
      "Epoch: 170, Loss: 929.05287, Residuals: -1.06348, Convergence: 0.001046\n",
      "Epoch: 171, Loss: 928.11344, Residuals: -1.06145, Convergence: 0.001012\n",
      "Epoch: 172, Loss: 927.20650, Residuals: -1.05949, Convergence: 0.000978\n",
      "Evidence 11090.082\n",
      "\n",
      "Epoch: 172, Evidence: 11090.08203, Convergence: 1.016427\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 5.74e-01\n",
      "Epoch: 172, Loss: 2330.16035, Residuals: -1.05949, Convergence:   inf\n",
      "Epoch: 173, Loss: 2288.94927, Residuals: -1.06865, Convergence: 0.018004\n",
      "Epoch: 174, Loss: 2261.86526, Residuals: -1.06733, Convergence: 0.011974\n",
      "Epoch: 175, Loss: 2239.46456, Residuals: -1.06531, Convergence: 0.010003\n",
      "Epoch: 176, Loss: 2220.67559, Residuals: -1.06305, Convergence: 0.008461\n",
      "Epoch: 177, Loss: 2204.75982, Residuals: -1.06068, Convergence: 0.007219\n",
      "Epoch: 178, Loss: 2191.14224, Residuals: -1.05822, Convergence: 0.006215\n",
      "Epoch: 179, Loss: 2179.35620, Residuals: -1.05569, Convergence: 0.005408\n",
      "Epoch: 180, Loss: 2169.01759, Residuals: -1.05309, Convergence: 0.004766\n",
      "Epoch: 181, Loss: 2159.81392, Residuals: -1.05040, Convergence: 0.004261\n",
      "Epoch: 182, Loss: 2151.50015, Residuals: -1.04760, Convergence: 0.003864\n",
      "Epoch: 183, Loss: 2143.89994, Residuals: -1.04469, Convergence: 0.003545\n",
      "Epoch: 184, Loss: 2136.90219, Residuals: -1.04167, Convergence: 0.003275\n",
      "Epoch: 185, Loss: 2130.44417, Residuals: -1.03859, Convergence: 0.003031\n",
      "Epoch: 186, Loss: 2124.48877, Residuals: -1.03549, Convergence: 0.002803\n",
      "Epoch: 187, Loss: 2119.00347, Residuals: -1.03243, Convergence: 0.002589\n",
      "Epoch: 188, Loss: 2113.95429, Residuals: -1.02944, Convergence: 0.002389\n",
      "Epoch: 189, Loss: 2109.30592, Residuals: -1.02655, Convergence: 0.002204\n",
      "Epoch: 190, Loss: 2105.02184, Residuals: -1.02377, Convergence: 0.002035\n",
      "Epoch: 191, Loss: 2101.06597, Residuals: -1.02111, Convergence: 0.001883\n",
      "Epoch: 192, Loss: 2097.40672, Residuals: -1.01858, Convergence: 0.001745\n",
      "Epoch: 193, Loss: 2094.01523, Residuals: -1.01617, Convergence: 0.001620\n",
      "Epoch: 194, Loss: 2090.86657, Residuals: -1.01388, Convergence: 0.001506\n",
      "Epoch: 195, Loss: 2087.93960, Residuals: -1.01170, Convergence: 0.001402\n",
      "Epoch: 196, Loss: 2085.21505, Residuals: -1.00963, Convergence: 0.001307\n",
      "Epoch: 197, Loss: 2082.67671, Residuals: -1.00767, Convergence: 0.001219\n",
      "Epoch: 198, Loss: 2080.30921, Residuals: -1.00580, Convergence: 0.001138\n",
      "Epoch: 199, Loss: 2078.09910, Residuals: -1.00402, Convergence: 0.001064\n",
      "Epoch: 200, Loss: 2076.03324, Residuals: -1.00232, Convergence: 0.000995\n",
      "Evidence 14213.256\n",
      "\n",
      "Epoch: 200, Evidence: 14213.25586, Convergence: 0.219737\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 4.38e-01\n",
      "Epoch: 200, Loss: 2453.56832, Residuals: -1.00232, Convergence:   inf\n",
      "Epoch: 201, Loss: 2439.91368, Residuals: -1.00062, Convergence: 0.005596\n",
      "Epoch: 202, Loss: 2428.77638, Residuals: -0.99813, Convergence: 0.004586\n",
      "Epoch: 203, Loss: 2419.12388, Residuals: -0.99553, Convergence: 0.003990\n",
      "Epoch: 204, Loss: 2410.70155, Residuals: -0.99299, Convergence: 0.003494\n",
      "Epoch: 205, Loss: 2403.31964, Residuals: -0.99057, Convergence: 0.003072\n",
      "Epoch: 206, Loss: 2396.82573, Residuals: -0.98829, Convergence: 0.002709\n",
      "Epoch: 207, Loss: 2391.09365, Residuals: -0.98615, Convergence: 0.002397\n",
      "Epoch: 208, Loss: 2386.01415, Residuals: -0.98415, Convergence: 0.002129\n",
      "Epoch: 209, Loss: 2381.49527, Residuals: -0.98227, Convergence: 0.001897\n",
      "Epoch: 210, Loss: 2377.45639, Residuals: -0.98052, Convergence: 0.001699\n",
      "Epoch: 211, Loss: 2373.83059, Residuals: -0.97889, Convergence: 0.001527\n",
      "Epoch: 212, Loss: 2370.55939, Residuals: -0.97735, Convergence: 0.001380\n",
      "Epoch: 213, Loss: 2367.59285, Residuals: -0.97592, Convergence: 0.001253\n",
      "Epoch: 214, Loss: 2364.89028, Residuals: -0.97457, Convergence: 0.001143\n",
      "Epoch: 215, Loss: 2362.41653, Residuals: -0.97331, Convergence: 0.001047\n",
      "Epoch: 216, Loss: 2360.14116, Residuals: -0.97213, Convergence: 0.000964\n",
      "Evidence 14595.855\n",
      "\n",
      "Epoch: 216, Evidence: 14595.85547, Convergence: 0.026213\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 3.37e-01\n",
      "Epoch: 216, Loss: 2458.47527, Residuals: -0.97213, Convergence:   inf\n",
      "Epoch: 217, Loss: 2451.86351, Residuals: -0.96957, Convergence: 0.002697\n",
      "Epoch: 218, Loss: 2446.35817, Residuals: -0.96719, Convergence: 0.002250\n",
      "Epoch: 219, Loss: 2441.66960, Residuals: -0.96508, Convergence: 0.001920\n",
      "Epoch: 220, Loss: 2437.63061, Residuals: -0.96322, Convergence: 0.001657\n",
      "Epoch: 221, Loss: 2434.11275, Residuals: -0.96156, Convergence: 0.001445\n",
      "Epoch: 222, Loss: 2431.01706, Residuals: -0.96008, Convergence: 0.001273\n",
      "Epoch: 223, Loss: 2428.26337, Residuals: -0.95876, Convergence: 0.001134\n",
      "Epoch: 224, Loss: 2425.79072, Residuals: -0.95757, Convergence: 0.001019\n",
      "Epoch: 225, Loss: 2423.54983, Residuals: -0.95651, Convergence: 0.000925\n",
      "Evidence 14682.804\n",
      "\n",
      "Epoch: 225, Evidence: 14682.80371, Convergence: 0.005922\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 2.66e-01\n",
      "Epoch: 225, Loss: 2459.88259, Residuals: -0.95651, Convergence:   inf\n",
      "Epoch: 226, Loss: 2455.97408, Residuals: -0.95443, Convergence: 0.001591\n",
      "Epoch: 227, Loss: 2452.71036, Residuals: -0.95269, Convergence: 0.001331\n",
      "Epoch: 228, Loss: 2449.91928, Residuals: -0.95121, Convergence: 0.001139\n",
      "Epoch: 229, Loss: 2447.49150, Residuals: -0.94994, Convergence: 0.000992\n",
      "Evidence 14712.210\n",
      "\n",
      "Epoch: 229, Evidence: 14712.20996, Convergence: 0.001999\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 2.16e-01\n",
      "Epoch: 229, Loss: 2460.86685, Residuals: -0.94994, Convergence:   inf\n",
      "Epoch: 230, Loss: 2457.94032, Residuals: -0.94828, Convergence: 0.001191\n",
      "Epoch: 231, Loss: 2455.48889, Residuals: -0.94690, Convergence: 0.000998\n",
      "Evidence 14724.463\n",
      "\n",
      "Epoch: 231, Evidence: 14724.46289, Convergence: 0.000832\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.80e-01\n",
      "Epoch: 231, Loss: 2461.57830, Residuals: -0.94690, Convergence:   inf\n",
      "Epoch: 232, Loss: 2457.08559, Residuals: -0.94489, Convergence: 0.001828\n",
      "Epoch: 233, Loss: 2453.57905, Residuals: -0.94299, Convergence: 0.001429\n",
      "Epoch: 234, Loss: 2450.74966, Residuals: -0.94154, Convergence: 0.001155\n",
      "Epoch: 235, Loss: 2448.36527, Residuals: -0.94058, Convergence: 0.000974\n",
      "Evidence 14742.533\n",
      "\n",
      "Epoch: 235, Evidence: 14742.53320, Convergence: 0.002057\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.50e-01\n",
      "Epoch: 235, Loss: 2461.76909, Residuals: -0.94058, Convergence:   inf\n",
      "Epoch: 236, Loss: 2458.74792, Residuals: -0.93855, Convergence: 0.001229\n",
      "Epoch: 237, Loss: 2456.34415, Residuals: -0.93735, Convergence: 0.000979\n",
      "Evidence 14753.703\n",
      "\n",
      "Epoch: 237, Evidence: 14753.70312, Convergence: 0.000757\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.27e-01\n",
      "Epoch: 237, Loss: 2461.93921, Residuals: -0.93735, Convergence:   inf\n",
      "Epoch: 238, Loss: 2457.49280, Residuals: -0.93489, Convergence: 0.001809\n",
      "Epoch: 239, Loss: 2454.29649, Residuals: -0.93549, Convergence: 0.001302\n",
      "Epoch: 240, Loss: 2451.69350, Residuals: -0.93599, Convergence: 0.001062\n",
      "Epoch: 241, Loss: 2449.40480, Residuals: -0.93805, Convergence: 0.000934\n",
      "Evidence 14769.723\n",
      "\n",
      "Epoch: 241, Evidence: 14769.72266, Convergence: 0.001841\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.13e-01\n",
      "Epoch: 241, Loss: 2461.29130, Residuals: -0.93805, Convergence:   inf\n",
      "Epoch: 242, Loss: 2459.59885, Residuals: -0.93682, Convergence: 0.000688\n",
      "Evidence 14776.325\n",
      "\n",
      "Epoch: 242, Evidence: 14776.32520, Convergence: 0.000447\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 9.28e-02\n",
      "Epoch: 242, Loss: 2462.14770, Residuals: -0.93682, Convergence:   inf\n",
      "Epoch: 243, Loss: 2508.40832, Residuals: -0.98087, Convergence: -0.018442\n",
      "Epoch: 243, Loss: 2459.64649, Residuals: -0.93562, Convergence: 0.001017\n",
      "Epoch: 244, Loss: 2459.41520, Residuals: -0.93736, Convergence: 0.000094\n",
      "Evidence 14781.407\n",
      "\n",
      "Epoch: 244, Evidence: 14781.40723, Convergence: 0.000790\n",
      "Updating hyper-parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 181, Updated regularization: 7.64e-02\n",
      "Epoch: 244, Loss: 2462.01263, Residuals: -0.93736, Convergence:   inf\n",
      "Epoch: 245, Loss: 2518.06980, Residuals: -0.98383, Convergence: -0.022262\n",
      "Epoch: 245, Loss: 2459.42740, Residuals: -0.93436, Convergence: 0.001051\n",
      "Epoch: 246, Loss: 2459.23102, Residuals: -0.93317, Convergence: 0.000080\n",
      "Evidence 14786.711\n",
      "\n",
      "Epoch: 246, Evidence: 14786.71094, Convergence: 0.001149\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 6.63e-02\n",
      "Epoch: 246, Loss: 2461.66794, Residuals: -0.93317, Convergence:   inf\n",
      "Epoch: 247, Loss: 2459.39761, Residuals: -0.93098, Convergence: 0.000923\n",
      "Evidence 14791.005\n",
      "\n",
      "Epoch: 247, Evidence: 14791.00488, Convergence: 0.000290\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 6.11e-02\n",
      "Epoch: 247, Loss: 2461.65514, Residuals: -0.93098, Convergence:   inf\n",
      "Epoch: 248, Loss: 2464.65721, Residuals: -0.93434, Convergence: -0.001218\n",
      "Epoch: 248, Loss: 2461.59935, Residuals: -0.92944, Convergence: 0.000023\n",
      "Evidence 14792.199\n",
      "\n",
      "Epoch: 248, Evidence: 14792.19922, Convergence: 0.000371\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 5.41e-02\n",
      "Epoch: 248, Loss: 2461.80798, Residuals: -0.92944, Convergence:   inf\n",
      "Epoch: 249, Loss: 2520.35008, Residuals: -0.95946, Convergence: -0.023228\n",
      "Epoch: 249, Loss: 2459.70153, Residuals: -0.92738, Convergence: 0.000856\n",
      "Evidence 14795.537\n",
      "\n",
      "Epoch: 249, Evidence: 14795.53711, Convergence: 0.000597\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 381.47945, Residuals: -4.52727, Convergence:   inf\n",
      "Epoch: 1, Loss: 355.84781, Residuals: -4.40747, Convergence: 0.072030\n",
      "Epoch: 2, Loss: 334.73301, Residuals: -4.24241, Convergence: 0.063080\n",
      "Epoch: 3, Loss: 318.60704, Residuals: -4.07667, Convergence: 0.050614\n",
      "Epoch: 4, Loss: 306.30796, Residuals: -3.93071, Convergence: 0.040153\n",
      "Epoch: 5, Loss: 296.55305, Residuals: -3.80154, Convergence: 0.032894\n",
      "Epoch: 6, Loss: 288.63587, Residuals: -3.68894, Convergence: 0.027430\n",
      "Epoch: 7, Loss: 282.06397, Residuals: -3.59247, Convergence: 0.023299\n",
      "Epoch: 8, Loss: 276.47184, Residuals: -3.51008, Convergence: 0.020227\n",
      "Epoch: 9, Loss: 271.60197, Residuals: -3.43944, Convergence: 0.017930\n",
      "Epoch: 10, Loss: 267.27194, Residuals: -3.37846, Convergence: 0.016201\n",
      "Epoch: 11, Loss: 263.35037, Residuals: -3.32536, Convergence: 0.014891\n",
      "Epoch: 12, Loss: 259.74213, Residuals: -3.27862, Convergence: 0.013892\n",
      "Epoch: 13, Loss: 256.37868, Residuals: -3.23691, Convergence: 0.013119\n",
      "Epoch: 14, Loss: 253.21131, Residuals: -3.19908, Convergence: 0.012509\n",
      "Epoch: 15, Loss: 250.20715, Residuals: -3.16413, Convergence: 0.012007\n",
      "Epoch: 16, Loss: 247.34712, Residuals: -3.13135, Convergence: 0.011563\n",
      "Epoch: 17, Loss: 244.61923, Residuals: -3.10029, Convergence: 0.011152\n",
      "Epoch: 18, Loss: 242.00612, Residuals: -3.07059, Convergence: 0.010798\n",
      "Epoch: 19, Loss: 239.47967, Residuals: -3.04178, Convergence: 0.010550\n",
      "Epoch: 20, Loss: 237.00644, Residuals: -3.01331, Convergence: 0.010435\n",
      "Epoch: 21, Loss: 234.55618, Residuals: -2.98471, Convergence: 0.010446\n",
      "Epoch: 22, Loss: 232.10728, Residuals: -2.95563, Convergence: 0.010551\n",
      "Epoch: 23, Loss: 229.64197, Residuals: -2.92586, Convergence: 0.010735\n",
      "Epoch: 24, Loss: 227.13093, Residuals: -2.89508, Convergence: 0.011055\n",
      "Epoch: 25, Loss: 224.52425, Residuals: -2.86271, Convergence: 0.011610\n",
      "Epoch: 26, Loss: 221.77630, Residuals: -2.82816, Convergence: 0.012391\n",
      "Epoch: 27, Loss: 218.93051, Residuals: -2.79182, Convergence: 0.012999\n",
      "Epoch: 28, Loss: 216.12228, Residuals: -2.75515, Convergence: 0.012994\n",
      "Epoch: 29, Loss: 213.42360, Residuals: -2.71907, Convergence: 0.012645\n",
      "Epoch: 30, Loss: 210.83274, Residuals: -2.68369, Convergence: 0.012289\n",
      "Epoch: 31, Loss: 208.33075, Residuals: -2.64889, Convergence: 0.012010\n",
      "Epoch: 32, Loss: 205.90078, Residuals: -2.61453, Convergence: 0.011802\n",
      "Epoch: 33, Loss: 203.53094, Residuals: -2.58051, Convergence: 0.011644\n",
      "Epoch: 34, Loss: 201.21367, Residuals: -2.54675, Convergence: 0.011516\n",
      "Epoch: 35, Loss: 198.94471, Residuals: -2.51321, Convergence: 0.011405\n",
      "Epoch: 36, Loss: 196.72212, Residuals: -2.47985, Convergence: 0.011298\n",
      "Epoch: 37, Loss: 194.54550, Residuals: -2.44665, Convergence: 0.011188\n",
      "Epoch: 38, Loss: 192.41535, Residuals: -2.41361, Convergence: 0.011071\n",
      "Epoch: 39, Loss: 190.33261, Residuals: -2.38073, Convergence: 0.010943\n",
      "Epoch: 40, Loss: 188.29831, Residuals: -2.34800, Convergence: 0.010804\n",
      "Epoch: 41, Loss: 186.31340, Residuals: -2.31544, Convergence: 0.010654\n",
      "Epoch: 42, Loss: 184.37862, Residuals: -2.28306, Convergence: 0.010494\n",
      "Epoch: 43, Loss: 182.49453, Residuals: -2.25088, Convergence: 0.010324\n",
      "Epoch: 44, Loss: 180.66158, Residuals: -2.21890, Convergence: 0.010146\n",
      "Epoch: 45, Loss: 178.88015, Residuals: -2.18717, Convergence: 0.009959\n",
      "Epoch: 46, Loss: 177.15069, Residuals: -2.15569, Convergence: 0.009763\n",
      "Epoch: 47, Loss: 175.47366, Residuals: -2.12451, Convergence: 0.009557\n",
      "Epoch: 48, Loss: 173.84945, Residuals: -2.09364, Convergence: 0.009343\n",
      "Epoch: 49, Loss: 172.27830, Residuals: -2.06312, Convergence: 0.009120\n",
      "Epoch: 50, Loss: 170.76010, Residuals: -2.03299, Convergence: 0.008891\n",
      "Epoch: 51, Loss: 169.29441, Residuals: -2.00327, Convergence: 0.008658\n",
      "Epoch: 52, Loss: 167.88037, Residuals: -1.97398, Convergence: 0.008423\n",
      "Epoch: 53, Loss: 166.51685, Residuals: -1.94515, Convergence: 0.008189\n",
      "Epoch: 54, Loss: 165.20256, Residuals: -1.91679, Convergence: 0.007956\n",
      "Epoch: 55, Loss: 163.93623, Residuals: -1.88890, Convergence: 0.007725\n",
      "Epoch: 56, Loss: 162.71671, Residuals: -1.86151, Convergence: 0.007495\n",
      "Epoch: 57, Loss: 161.54302, Residuals: -1.83461, Convergence: 0.007266\n",
      "Epoch: 58, Loss: 160.41439, Residuals: -1.80823, Convergence: 0.007036\n",
      "Epoch: 59, Loss: 159.33022, Residuals: -1.78238, Convergence: 0.006805\n",
      "Epoch: 60, Loss: 158.28995, Residuals: -1.75709, Convergence: 0.006572\n",
      "Epoch: 61, Loss: 157.29310, Residuals: -1.73238, Convergence: 0.006338\n",
      "Epoch: 62, Loss: 156.33910, Residuals: -1.70827, Convergence: 0.006102\n",
      "Epoch: 63, Loss: 155.42738, Residuals: -1.68478, Convergence: 0.005866\n",
      "Epoch: 64, Loss: 154.55723, Residuals: -1.66194, Convergence: 0.005630\n",
      "Epoch: 65, Loss: 153.72785, Residuals: -1.63977, Convergence: 0.005395\n",
      "Epoch: 66, Loss: 152.93833, Residuals: -1.61826, Convergence: 0.005162\n",
      "Epoch: 67, Loss: 152.18768, Residuals: -1.59744, Convergence: 0.004932\n",
      "Epoch: 68, Loss: 151.47479, Residuals: -1.57731, Convergence: 0.004706\n",
      "Epoch: 69, Loss: 150.79847, Residuals: -1.55788, Convergence: 0.004485\n",
      "Epoch: 70, Loss: 150.15747, Residuals: -1.53915, Convergence: 0.004269\n",
      "Epoch: 71, Loss: 149.55049, Residuals: -1.52111, Convergence: 0.004059\n",
      "Epoch: 72, Loss: 148.97617, Residuals: -1.50375, Convergence: 0.003855\n",
      "Epoch: 73, Loss: 148.43313, Residuals: -1.48708, Convergence: 0.003658\n",
      "Epoch: 74, Loss: 147.92001, Residuals: -1.47108, Convergence: 0.003469\n",
      "Epoch: 75, Loss: 147.43540, Residuals: -1.45573, Convergence: 0.003287\n",
      "Epoch: 76, Loss: 146.97797, Residuals: -1.44102, Convergence: 0.003112\n",
      "Epoch: 77, Loss: 146.54637, Residuals: -1.42693, Convergence: 0.002945\n",
      "Epoch: 78, Loss: 146.13930, Residuals: -1.41345, Convergence: 0.002785\n",
      "Epoch: 79, Loss: 145.75549, Residuals: -1.40056, Convergence: 0.002633\n",
      "Epoch: 80, Loss: 145.39375, Residuals: -1.38824, Convergence: 0.002488\n",
      "Epoch: 81, Loss: 145.05293, Residuals: -1.37647, Convergence: 0.002350\n",
      "Epoch: 82, Loss: 144.73191, Residuals: -1.36523, Convergence: 0.002218\n",
      "Epoch: 83, Loss: 144.42968, Residuals: -1.35450, Convergence: 0.002093\n",
      "Epoch: 84, Loss: 144.14525, Residuals: -1.34427, Convergence: 0.001973\n",
      "Epoch: 85, Loss: 143.87770, Residuals: -1.33451, Convergence: 0.001860\n",
      "Epoch: 86, Loss: 143.62618, Residuals: -1.32521, Convergence: 0.001751\n",
      "Epoch: 87, Loss: 143.38987, Residuals: -1.31636, Convergence: 0.001648\n",
      "Epoch: 88, Loss: 143.16802, Residuals: -1.30793, Convergence: 0.001550\n",
      "Epoch: 89, Loss: 142.95989, Residuals: -1.29992, Convergence: 0.001456\n",
      "Epoch: 90, Loss: 142.76479, Residuals: -1.29230, Convergence: 0.001367\n",
      "Epoch: 91, Loss: 142.58203, Residuals: -1.28508, Convergence: 0.001282\n",
      "Epoch: 92, Loss: 142.41094, Residuals: -1.27824, Convergence: 0.001201\n",
      "Epoch: 93, Loss: 142.25075, Residuals: -1.27176, Convergence: 0.001126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 94, Loss: 142.10073, Residuals: -1.26563, Convergence: 0.001056\n",
      "Epoch: 95, Loss: 141.96000, Residuals: -1.25986, Convergence: 0.000991\n",
      "Evidence -183.811\n",
      "\n",
      "Epoch: 95, Evidence: -183.81146, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.24e-01\n",
      "Epoch: 95, Loss: 1358.44472, Residuals: -1.25986, Convergence:   inf\n",
      "Epoch: 96, Loss: 1295.59105, Residuals: -1.28942, Convergence: 0.048514\n",
      "Epoch: 97, Loss: 1247.60440, Residuals: -1.31228, Convergence: 0.038463\n",
      "Epoch: 98, Loss: 1211.26542, Residuals: -1.32845, Convergence: 0.030001\n",
      "Epoch: 99, Loss: 1183.01648, Residuals: -1.33950, Convergence: 0.023879\n",
      "Epoch: 100, Loss: 1160.21313, Residuals: -1.34735, Convergence: 0.019654\n",
      "Epoch: 101, Loss: 1141.30283, Residuals: -1.35309, Convergence: 0.016569\n",
      "Epoch: 102, Loss: 1125.34690, Residuals: -1.35720, Convergence: 0.014179\n",
      "Epoch: 103, Loss: 1111.70880, Residuals: -1.35996, Convergence: 0.012268\n",
      "Epoch: 104, Loss: 1099.91746, Residuals: -1.36151, Convergence: 0.010720\n",
      "Epoch: 105, Loss: 1089.60570, Residuals: -1.36198, Convergence: 0.009464\n",
      "Epoch: 106, Loss: 1080.47650, Residuals: -1.36147, Convergence: 0.008449\n",
      "Epoch: 107, Loss: 1072.28311, Residuals: -1.36006, Convergence: 0.007641\n",
      "Epoch: 108, Loss: 1064.81509, Residuals: -1.35779, Convergence: 0.007013\n",
      "Epoch: 109, Loss: 1057.88831, Residuals: -1.35471, Convergence: 0.006548\n",
      "Epoch: 110, Loss: 1051.34086, Residuals: -1.35083, Convergence: 0.006228\n",
      "Epoch: 111, Loss: 1045.03244, Residuals: -1.34618, Convergence: 0.006037\n",
      "Epoch: 112, Loss: 1038.85177, Residuals: -1.34077, Convergence: 0.005950\n",
      "Epoch: 113, Loss: 1032.73141, Residuals: -1.33466, Convergence: 0.005926\n",
      "Epoch: 114, Loss: 1026.66139, Residuals: -1.32793, Convergence: 0.005912\n",
      "Epoch: 115, Loss: 1020.69061, Residuals: -1.32067, Convergence: 0.005850\n",
      "Epoch: 116, Loss: 1014.90217, Residuals: -1.31300, Convergence: 0.005703\n",
      "Epoch: 117, Loss: 1009.37887, Residuals: -1.30501, Convergence: 0.005472\n",
      "Epoch: 118, Loss: 1004.17569, Residuals: -1.29681, Convergence: 0.005182\n",
      "Epoch: 119, Loss: 999.31167, Residuals: -1.28847, Convergence: 0.004867\n",
      "Epoch: 120, Loss: 994.77974, Residuals: -1.28008, Convergence: 0.004556\n",
      "Epoch: 121, Loss: 990.55820, Residuals: -1.27170, Convergence: 0.004262\n",
      "Epoch: 122, Loss: 986.61872, Residuals: -1.26338, Convergence: 0.003993\n",
      "Epoch: 123, Loss: 982.93322, Residuals: -1.25516, Convergence: 0.003749\n",
      "Epoch: 124, Loss: 979.47510, Residuals: -1.24709, Convergence: 0.003531\n",
      "Epoch: 125, Loss: 976.22135, Residuals: -1.23919, Convergence: 0.003333\n",
      "Epoch: 126, Loss: 973.15154, Residuals: -1.23148, Convergence: 0.003155\n",
      "Epoch: 127, Loss: 970.24825, Residuals: -1.22399, Convergence: 0.002992\n",
      "Epoch: 128, Loss: 967.49762, Residuals: -1.21672, Convergence: 0.002843\n",
      "Epoch: 129, Loss: 964.88662, Residuals: -1.20969, Convergence: 0.002706\n",
      "Epoch: 130, Loss: 962.40501, Residuals: -1.20290, Convergence: 0.002579\n",
      "Epoch: 131, Loss: 960.04352, Residuals: -1.19636, Convergence: 0.002460\n",
      "Epoch: 132, Loss: 957.79459, Residuals: -1.19007, Convergence: 0.002348\n",
      "Epoch: 133, Loss: 955.65123, Residuals: -1.18404, Convergence: 0.002243\n",
      "Epoch: 134, Loss: 953.60682, Residuals: -1.17825, Convergence: 0.002144\n",
      "Epoch: 135, Loss: 951.65576, Residuals: -1.17272, Convergence: 0.002050\n",
      "Epoch: 136, Loss: 949.79301, Residuals: -1.16743, Convergence: 0.001961\n",
      "Epoch: 137, Loss: 948.01312, Residuals: -1.16237, Convergence: 0.001877\n",
      "Epoch: 138, Loss: 946.31145, Residuals: -1.15755, Convergence: 0.001798\n",
      "Epoch: 139, Loss: 944.68346, Residuals: -1.15295, Convergence: 0.001723\n",
      "Epoch: 140, Loss: 943.12452, Residuals: -1.14856, Convergence: 0.001653\n",
      "Epoch: 141, Loss: 941.63101, Residuals: -1.14438, Convergence: 0.001586\n",
      "Epoch: 142, Loss: 940.19793, Residuals: -1.14040, Convergence: 0.001524\n",
      "Epoch: 143, Loss: 938.82224, Residuals: -1.13661, Convergence: 0.001465\n",
      "Epoch: 144, Loss: 937.49994, Residuals: -1.13300, Convergence: 0.001410\n",
      "Epoch: 145, Loss: 936.22729, Residuals: -1.12956, Convergence: 0.001359\n",
      "Epoch: 146, Loss: 935.00104, Residuals: -1.12627, Convergence: 0.001311\n",
      "Epoch: 147, Loss: 933.81805, Residuals: -1.12314, Convergence: 0.001267\n",
      "Epoch: 148, Loss: 932.67558, Residuals: -1.12016, Convergence: 0.001225\n",
      "Epoch: 149, Loss: 931.57000, Residuals: -1.11731, Convergence: 0.001187\n",
      "Epoch: 150, Loss: 930.49931, Residuals: -1.11458, Convergence: 0.001151\n",
      "Epoch: 151, Loss: 929.46024, Residuals: -1.11197, Convergence: 0.001118\n",
      "Epoch: 152, Loss: 928.45048, Residuals: -1.10948, Convergence: 0.001088\n",
      "Epoch: 153, Loss: 927.46741, Residuals: -1.10708, Convergence: 0.001060\n",
      "Epoch: 154, Loss: 926.50859, Residuals: -1.10478, Convergence: 0.001035\n",
      "Epoch: 155, Loss: 925.57136, Residuals: -1.10256, Convergence: 0.001013\n",
      "Epoch: 156, Loss: 924.65307, Residuals: -1.10042, Convergence: 0.000993\n",
      "Evidence 10979.575\n",
      "\n",
      "Epoch: 156, Evidence: 10979.57520, Convergence: 1.016741\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.79e-01\n",
      "Epoch: 156, Loss: 2334.96783, Residuals: -1.10042, Convergence:   inf\n",
      "Epoch: 157, Loss: 2294.43569, Residuals: -1.10852, Convergence: 0.017665\n",
      "Epoch: 158, Loss: 2265.90695, Residuals: -1.10789, Convergence: 0.012590\n",
      "Epoch: 159, Loss: 2241.92659, Residuals: -1.10604, Convergence: 0.010696\n",
      "Epoch: 160, Loss: 2221.45420, Residuals: -1.10367, Convergence: 0.009216\n",
      "Epoch: 161, Loss: 2203.80633, Residuals: -1.10093, Convergence: 0.008008\n",
      "Epoch: 162, Loss: 2188.46596, Residuals: -1.09790, Convergence: 0.007010\n",
      "Epoch: 163, Loss: 2175.02053, Residuals: -1.09466, Convergence: 0.006182\n",
      "Epoch: 164, Loss: 2163.13357, Residuals: -1.09125, Convergence: 0.005495\n",
      "Epoch: 165, Loss: 2152.52493, Residuals: -1.08771, Convergence: 0.004928\n",
      "Epoch: 166, Loss: 2142.96074, Residuals: -1.08405, Convergence: 0.004463\n",
      "Epoch: 167, Loss: 2134.25218, Residuals: -1.08028, Convergence: 0.004080\n",
      "Epoch: 168, Loss: 2126.25500, Residuals: -1.07641, Convergence: 0.003761\n",
      "Epoch: 169, Loss: 2118.87557, Residuals: -1.07247, Convergence: 0.003483\n",
      "Epoch: 170, Loss: 2112.05649, Residuals: -1.06848, Convergence: 0.003229\n",
      "Epoch: 171, Loss: 2105.76438, Residuals: -1.06451, Convergence: 0.002988\n",
      "Epoch: 172, Loss: 2099.97297, Residuals: -1.06059, Convergence: 0.002758\n",
      "Epoch: 173, Loss: 2094.65411, Residuals: -1.05677, Convergence: 0.002539\n",
      "Epoch: 174, Loss: 2089.77496, Residuals: -1.05309, Convergence: 0.002335\n",
      "Epoch: 175, Loss: 2085.30081, Residuals: -1.04955, Convergence: 0.002146\n",
      "Epoch: 176, Loss: 2081.19376, Residuals: -1.04619, Convergence: 0.001973\n",
      "Epoch: 177, Loss: 2077.41891, Residuals: -1.04299, Convergence: 0.001817\n",
      "Epoch: 178, Loss: 2073.94077, Residuals: -1.03996, Convergence: 0.001677\n",
      "Epoch: 179, Loss: 2070.72686, Residuals: -1.03709, Convergence: 0.001552\n",
      "Epoch: 180, Loss: 2067.74785, Residuals: -1.03439, Convergence: 0.001441\n",
      "Epoch: 181, Loss: 2064.97656, Residuals: -1.03184, Convergence: 0.001342\n",
      "Epoch: 182, Loss: 2062.39001, Residuals: -1.02944, Convergence: 0.001254\n",
      "Epoch: 183, Loss: 2059.96745, Residuals: -1.02718, Convergence: 0.001176\n",
      "Epoch: 184, Loss: 2057.69090, Residuals: -1.02505, Convergence: 0.001106\n",
      "Epoch: 185, Loss: 2055.54581, Residuals: -1.02304, Convergence: 0.001044\n",
      "Epoch: 186, Loss: 2053.51914, Residuals: -1.02115, Convergence: 0.000987\n",
      "Evidence 14197.770\n",
      "\n",
      "Epoch: 186, Evidence: 14197.76953, Convergence: 0.226669\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.42e-01\n",
      "Epoch: 186, Loss: 2465.77029, Residuals: -1.02115, Convergence:   inf\n",
      "Epoch: 187, Loss: 2450.97015, Residuals: -1.01806, Convergence: 0.006038\n",
      "Epoch: 188, Loss: 2438.98343, Residuals: -1.01434, Convergence: 0.004915\n",
      "Epoch: 189, Loss: 2428.72293, Residuals: -1.01062, Convergence: 0.004225\n",
      "Epoch: 190, Loss: 2419.85836, Residuals: -1.00705, Convergence: 0.003663\n",
      "Epoch: 191, Loss: 2412.14824, Residuals: -1.00371, Convergence: 0.003196\n",
      "Epoch: 192, Loss: 2405.40270, Residuals: -1.00061, Convergence: 0.002804\n",
      "Epoch: 193, Loss: 2399.46321, Residuals: -0.99775, Convergence: 0.002475\n",
      "Epoch: 194, Loss: 2394.20121, Residuals: -0.99511, Convergence: 0.002198\n",
      "Epoch: 195, Loss: 2389.51002, Residuals: -0.99267, Convergence: 0.001963\n",
      "Epoch: 196, Loss: 2385.30224, Residuals: -0.99043, Convergence: 0.001764\n",
      "Epoch: 197, Loss: 2381.50542, Residuals: -0.98836, Convergence: 0.001594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 198, Loss: 2378.05934, Residuals: -0.98645, Convergence: 0.001449\n",
      "Epoch: 199, Loss: 2374.91576, Residuals: -0.98468, Convergence: 0.001324\n",
      "Epoch: 200, Loss: 2372.03333, Residuals: -0.98304, Convergence: 0.001215\n",
      "Epoch: 201, Loss: 2369.37852, Residuals: -0.98152, Convergence: 0.001120\n",
      "Epoch: 202, Loss: 2366.92298, Residuals: -0.98010, Convergence: 0.001037\n",
      "Epoch: 203, Loss: 2364.64495, Residuals: -0.97879, Convergence: 0.000963\n",
      "Evidence 14630.570\n",
      "\n",
      "Epoch: 203, Evidence: 14630.57031, Convergence: 0.029582\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.38e-01\n",
      "Epoch: 203, Loss: 2471.30201, Residuals: -0.97879, Convergence:   inf\n",
      "Epoch: 204, Loss: 2464.72042, Residuals: -0.97540, Convergence: 0.002670\n",
      "Epoch: 205, Loss: 2459.26578, Residuals: -0.97227, Convergence: 0.002218\n",
      "Epoch: 206, Loss: 2454.61352, Residuals: -0.96952, Convergence: 0.001895\n",
      "Epoch: 207, Loss: 2450.58737, Residuals: -0.96713, Convergence: 0.001643\n",
      "Epoch: 208, Loss: 2447.05793, Residuals: -0.96506, Convergence: 0.001442\n",
      "Epoch: 209, Loss: 2443.92727, Residuals: -0.96325, Convergence: 0.001281\n",
      "Epoch: 210, Loss: 2441.12055, Residuals: -0.96169, Convergence: 0.001150\n",
      "Epoch: 211, Loss: 2438.57977, Residuals: -0.96033, Convergence: 0.001042\n",
      "Epoch: 212, Loss: 2436.25998, Residuals: -0.95915, Convergence: 0.000952\n",
      "Evidence 14719.111\n",
      "\n",
      "Epoch: 212, Evidence: 14719.11133, Convergence: 0.006015\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.64e-01\n",
      "Epoch: 212, Loss: 2473.07993, Residuals: -0.95915, Convergence:   inf\n",
      "Epoch: 213, Loss: 2469.21372, Residuals: -0.95647, Convergence: 0.001566\n",
      "Epoch: 214, Loss: 2465.96252, Residuals: -0.95429, Convergence: 0.001318\n",
      "Epoch: 215, Loss: 2463.14943, Residuals: -0.95251, Convergence: 0.001142\n",
      "Epoch: 216, Loss: 2460.67080, Residuals: -0.95107, Convergence: 0.001007\n",
      "Epoch: 217, Loss: 2458.45499, Residuals: -0.94990, Convergence: 0.000901\n",
      "Evidence 14750.297\n",
      "\n",
      "Epoch: 217, Evidence: 14750.29688, Convergence: 0.002114\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.13e-01\n",
      "Epoch: 217, Loss: 2474.15152, Residuals: -0.94990, Convergence:   inf\n",
      "Epoch: 218, Loss: 2471.35598, Residuals: -0.94789, Convergence: 0.001131\n",
      "Epoch: 219, Loss: 2468.97912, Residuals: -0.94635, Convergence: 0.000963\n",
      "Evidence 14762.834\n",
      "\n",
      "Epoch: 219, Evidence: 14762.83398, Convergence: 0.000849\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.76e-01\n",
      "Epoch: 219, Loss: 2474.96139, Residuals: -0.94635, Convergence:   inf\n",
      "Epoch: 220, Loss: 2470.44812, Residuals: -0.94364, Convergence: 0.001827\n",
      "Epoch: 221, Loss: 2466.98469, Residuals: -0.94210, Convergence: 0.001404\n",
      "Epoch: 222, Loss: 2464.14155, Residuals: -0.94118, Convergence: 0.001154\n",
      "Epoch: 223, Loss: 2461.70761, Residuals: -0.94072, Convergence: 0.000989\n",
      "Evidence 14780.775\n",
      "\n",
      "Epoch: 223, Evidence: 14780.77539, Convergence: 0.002062\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.46e-01\n",
      "Epoch: 223, Loss: 2475.30792, Residuals: -0.94072, Convergence:   inf\n",
      "Epoch: 224, Loss: 2472.32530, Residuals: -0.93888, Convergence: 0.001206\n",
      "Epoch: 225, Loss: 2469.92517, Residuals: -0.93814, Convergence: 0.000972\n",
      "Evidence 14791.558\n",
      "\n",
      "Epoch: 225, Evidence: 14791.55762, Convergence: 0.000729\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.24e-01\n",
      "Epoch: 225, Loss: 2475.61485, Residuals: -0.93814, Convergence:   inf\n",
      "Epoch: 226, Loss: 2471.21491, Residuals: -0.93628, Convergence: 0.001780\n",
      "Epoch: 227, Loss: 2467.95102, Residuals: -0.93777, Convergence: 0.001323\n",
      "Epoch: 228, Loss: 2465.28614, Residuals: -0.93848, Convergence: 0.001081\n",
      "Epoch: 229, Loss: 2462.97539, Residuals: -0.94103, Convergence: 0.000938\n",
      "Evidence 14807.304\n",
      "\n",
      "Epoch: 229, Evidence: 14807.30371, Convergence: 0.001792\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.10e-01\n",
      "Epoch: 229, Loss: 2475.10903, Residuals: -0.94103, Convergence:   inf\n",
      "Epoch: 230, Loss: 2473.60008, Residuals: -0.94137, Convergence: 0.000610\n",
      "Evidence 14813.408\n",
      "\n",
      "Epoch: 230, Evidence: 14813.40820, Convergence: 0.000412\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.96e-02\n",
      "Epoch: 230, Loss: 2476.15005, Residuals: -0.94137, Convergence:   inf\n",
      "Epoch: 231, Loss: 2526.42678, Residuals: -0.98524, Convergence: -0.019900\n",
      "Epoch: 231, Loss: 2473.68063, Residuals: -0.94029, Convergence: 0.000998\n",
      "Evidence 14817.937\n",
      "\n",
      "Epoch: 231, Evidence: 14817.93652, Convergence: 0.000718\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.25e-02\n",
      "Epoch: 231, Loss: 2475.62515, Residuals: -0.94029, Convergence:   inf\n",
      "Epoch: 232, Loss: 2479.14831, Residuals: -0.94463, Convergence: -0.001421\n",
      "Epoch: 232, Loss: 2475.35475, Residuals: -0.94026, Convergence: 0.000109\n",
      "Evidence 14819.788\n",
      "\n",
      "Epoch: 232, Evidence: 14819.78809, Convergence: 0.000842\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 383.85956, Residuals: -4.54136, Convergence:   inf\n",
      "Epoch: 1, Loss: 358.03401, Residuals: -4.42098, Convergence: 0.072132\n",
      "Epoch: 2, Loss: 336.92982, Residuals: -4.25667, Convergence: 0.062637\n",
      "Epoch: 3, Loss: 320.80238, Residuals: -4.09188, Convergence: 0.050272\n",
      "Epoch: 4, Loss: 308.51462, Residuals: -3.94693, Convergence: 0.039829\n",
      "Epoch: 5, Loss: 298.77577, Residuals: -3.81903, Convergence: 0.032596\n",
      "Epoch: 6, Loss: 290.86964, Residuals: -3.70759, Convergence: 0.027181\n",
      "Epoch: 7, Loss: 284.30793, Residuals: -3.61204, Convergence: 0.023080\n",
      "Epoch: 8, Loss: 278.72836, Residuals: -3.53037, Convergence: 0.020018\n",
      "Epoch: 9, Loss: 273.87353, Residuals: -3.46027, Convergence: 0.017727\n",
      "Epoch: 10, Loss: 269.55999, Residuals: -3.39966, Convergence: 0.016002\n",
      "Epoch: 11, Loss: 265.65475, Residuals: -3.34675, Convergence: 0.014700\n",
      "Epoch: 12, Loss: 262.06077, Residuals: -3.30005, Convergence: 0.013714\n",
      "Epoch: 13, Loss: 258.70782, Residuals: -3.25825, Convergence: 0.012960\n",
      "Epoch: 14, Loss: 255.54630, Residuals: -3.22022, Convergence: 0.012372\n",
      "Epoch: 15, Loss: 252.54410, Residuals: -3.18501, Convergence: 0.011888\n",
      "Epoch: 16, Loss: 249.68458, Residuals: -3.15196, Convergence: 0.011453\n",
      "Epoch: 17, Loss: 246.95730, Residuals: -3.12069, Convergence: 0.011044\n",
      "Epoch: 18, Loss: 244.34333, Residuals: -3.09082, Convergence: 0.010698\n",
      "Epoch: 19, Loss: 241.81127, Residuals: -3.06185, Convergence: 0.010471\n",
      "Epoch: 20, Loss: 239.32492, Residuals: -3.03317, Convergence: 0.010389\n",
      "Epoch: 21, Loss: 236.85309, Residuals: -3.00428, Convergence: 0.010436\n",
      "Epoch: 22, Loss: 234.37414, Residuals: -2.97482, Convergence: 0.010577\n",
      "Epoch: 23, Loss: 231.86689, Residuals: -2.94452, Convergence: 0.010813\n",
      "Epoch: 24, Loss: 229.29319, Residuals: -2.91296, Convergence: 0.011224\n",
      "Epoch: 25, Loss: 226.59750, Residuals: -2.87946, Convergence: 0.011896\n",
      "Epoch: 26, Loss: 223.75839, Residuals: -2.84370, Convergence: 0.012688\n",
      "Epoch: 27, Loss: 220.87005, Residuals: -2.80661, Convergence: 0.013077\n",
      "Epoch: 28, Loss: 218.05375, Residuals: -2.76958, Convergence: 0.012916\n",
      "Epoch: 29, Loss: 215.34201, Residuals: -2.73313, Convergence: 0.012593\n",
      "Epoch: 30, Loss: 212.72246, Residuals: -2.69726, Convergence: 0.012314\n",
      "Epoch: 31, Loss: 210.17763, Residuals: -2.66185, Convergence: 0.012108\n",
      "Epoch: 32, Loss: 207.69403, Residuals: -2.62680, Convergence: 0.011958\n",
      "Epoch: 33, Loss: 205.26282, Residuals: -2.59201, Convergence: 0.011844\n",
      "Epoch: 34, Loss: 202.87890, Residuals: -2.55743, Convergence: 0.011750\n",
      "Epoch: 35, Loss: 200.54000, Residuals: -2.52302, Convergence: 0.011663\n",
      "Epoch: 36, Loss: 198.24580, Residuals: -2.48874, Convergence: 0.011572\n",
      "Epoch: 37, Loss: 195.99732, Residuals: -2.45458, Convergence: 0.011472\n",
      "Epoch: 38, Loss: 193.79631, Residuals: -2.42053, Convergence: 0.011357\n",
      "Epoch: 39, Loss: 191.64491, Residuals: -2.38659, Convergence: 0.011226\n",
      "Epoch: 40, Loss: 189.54540, Residuals: -2.35280, Convergence: 0.011077\n",
      "Epoch: 41, Loss: 187.50003, Residuals: -2.31916, Convergence: 0.010909\n",
      "Epoch: 42, Loss: 185.51103, Residuals: -2.28571, Convergence: 0.010722\n",
      "Epoch: 43, Loss: 183.58063, Residuals: -2.25249, Convergence: 0.010515\n",
      "Epoch: 44, Loss: 181.71101, Residuals: -2.21956, Convergence: 0.010289\n",
      "Epoch: 45, Loss: 179.90431, Residuals: -2.18698, Convergence: 0.010043\n",
      "Epoch: 46, Loss: 178.16236, Residuals: -2.15480, Convergence: 0.009777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47, Loss: 176.48645, Residuals: -2.12310, Convergence: 0.009496\n",
      "Epoch: 48, Loss: 174.87704, Residuals: -2.09191, Convergence: 0.009203\n",
      "Epoch: 49, Loss: 173.33363, Residuals: -2.06130, Convergence: 0.008904\n",
      "Epoch: 50, Loss: 171.85478, Residuals: -2.03128, Convergence: 0.008605\n",
      "Epoch: 51, Loss: 170.43831, Residuals: -2.00187, Convergence: 0.008311\n",
      "Epoch: 52, Loss: 169.08152, Residuals: -1.97307, Convergence: 0.008024\n",
      "Epoch: 53, Loss: 167.78147, Residuals: -1.94488, Convergence: 0.007748\n",
      "Epoch: 54, Loss: 166.53521, Residuals: -1.91728, Convergence: 0.007483\n",
      "Epoch: 55, Loss: 165.33994, Residuals: -1.89027, Convergence: 0.007229\n",
      "Epoch: 56, Loss: 164.19310, Residuals: -1.86384, Convergence: 0.006985\n",
      "Epoch: 57, Loss: 163.09239, Residuals: -1.83799, Convergence: 0.006749\n",
      "Epoch: 58, Loss: 162.03576, Residuals: -1.81272, Convergence: 0.006521\n",
      "Epoch: 59, Loss: 161.02143, Residuals: -1.78803, Convergence: 0.006299\n",
      "Epoch: 60, Loss: 160.04779, Residuals: -1.76393, Convergence: 0.006083\n",
      "Epoch: 61, Loss: 159.11339, Residuals: -1.74043, Convergence: 0.005873\n",
      "Epoch: 62, Loss: 158.21691, Residuals: -1.71752, Convergence: 0.005666\n",
      "Epoch: 63, Loss: 157.35711, Residuals: -1.69521, Convergence: 0.005464\n",
      "Epoch: 64, Loss: 156.53282, Residuals: -1.67352, Convergence: 0.005266\n",
      "Epoch: 65, Loss: 155.74293, Residuals: -1.65243, Convergence: 0.005072\n",
      "Epoch: 66, Loss: 154.98637, Residuals: -1.63196, Convergence: 0.004881\n",
      "Epoch: 67, Loss: 154.26210, Residuals: -1.61210, Convergence: 0.004695\n",
      "Epoch: 68, Loss: 153.56912, Residuals: -1.59285, Convergence: 0.004513\n",
      "Epoch: 69, Loss: 152.90642, Residuals: -1.57421, Convergence: 0.004334\n",
      "Epoch: 70, Loss: 152.27307, Residuals: -1.55618, Convergence: 0.004159\n",
      "Epoch: 71, Loss: 151.66808, Residuals: -1.53875, Convergence: 0.003989\n",
      "Epoch: 72, Loss: 151.09052, Residuals: -1.52191, Convergence: 0.003823\n",
      "Epoch: 73, Loss: 150.53945, Residuals: -1.50566, Convergence: 0.003661\n",
      "Epoch: 74, Loss: 150.01392, Residuals: -1.48999, Convergence: 0.003503\n",
      "Epoch: 75, Loss: 149.51300, Residuals: -1.47489, Convergence: 0.003350\n",
      "Epoch: 76, Loss: 149.03573, Residuals: -1.46035, Convergence: 0.003202\n",
      "Epoch: 77, Loss: 148.58116, Residuals: -1.44636, Convergence: 0.003059\n",
      "Epoch: 78, Loss: 148.14837, Residuals: -1.43290, Convergence: 0.002921\n",
      "Epoch: 79, Loss: 147.73637, Residuals: -1.41996, Convergence: 0.002789\n",
      "Epoch: 80, Loss: 147.34424, Residuals: -1.40754, Convergence: 0.002661\n",
      "Epoch: 81, Loss: 146.97103, Residuals: -1.39561, Convergence: 0.002539\n",
      "Epoch: 82, Loss: 146.61584, Residuals: -1.38415, Convergence: 0.002423\n",
      "Epoch: 83, Loss: 146.27776, Residuals: -1.37317, Convergence: 0.002311\n",
      "Epoch: 84, Loss: 145.95591, Residuals: -1.36263, Convergence: 0.002205\n",
      "Epoch: 85, Loss: 145.64947, Residuals: -1.35252, Convergence: 0.002104\n",
      "Epoch: 86, Loss: 145.35763, Residuals: -1.34283, Convergence: 0.002008\n",
      "Epoch: 87, Loss: 145.07962, Residuals: -1.33354, Convergence: 0.001916\n",
      "Epoch: 88, Loss: 144.81475, Residuals: -1.32464, Convergence: 0.001829\n",
      "Epoch: 89, Loss: 144.56233, Residuals: -1.31612, Convergence: 0.001746\n",
      "Epoch: 90, Loss: 144.32175, Residuals: -1.30794, Convergence: 0.001667\n",
      "Epoch: 91, Loss: 144.09244, Residuals: -1.30011, Convergence: 0.001591\n",
      "Epoch: 92, Loss: 143.87387, Residuals: -1.29261, Convergence: 0.001519\n",
      "Epoch: 93, Loss: 143.66558, Residuals: -1.28543, Convergence: 0.001450\n",
      "Epoch: 94, Loss: 143.46712, Residuals: -1.27854, Convergence: 0.001383\n",
      "Epoch: 95, Loss: 143.27813, Residuals: -1.27195, Convergence: 0.001319\n",
      "Epoch: 96, Loss: 143.09824, Residuals: -1.26564, Convergence: 0.001257\n",
      "Epoch: 97, Loss: 142.92715, Residuals: -1.25959, Convergence: 0.001197\n",
      "Epoch: 98, Loss: 142.76459, Residuals: -1.25381, Convergence: 0.001139\n",
      "Epoch: 99, Loss: 142.61032, Residuals: -1.24827, Convergence: 0.001082\n",
      "Epoch: 100, Loss: 142.46412, Residuals: -1.24298, Convergence: 0.001026\n",
      "Epoch: 101, Loss: 142.32578, Residuals: -1.23792, Convergence: 0.000972\n",
      "Evidence -184.082\n",
      "\n",
      "Epoch: 101, Evidence: -184.08173, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.25e-01\n",
      "Epoch: 101, Loss: 1373.35738, Residuals: -1.23792, Convergence:   inf\n",
      "Epoch: 102, Loss: 1311.24827, Residuals: -1.26909, Convergence: 0.047366\n",
      "Epoch: 103, Loss: 1264.04109, Residuals: -1.29332, Convergence: 0.037346\n",
      "Epoch: 104, Loss: 1228.39744, Residuals: -1.31062, Convergence: 0.029016\n",
      "Epoch: 105, Loss: 1200.69527, Residuals: -1.32271, Convergence: 0.023072\n",
      "Epoch: 106, Loss: 1178.30769, Residuals: -1.33162, Convergence: 0.019000\n",
      "Epoch: 107, Loss: 1159.71916, Residuals: -1.33840, Convergence: 0.016028\n",
      "Epoch: 108, Loss: 1144.01887, Residuals: -1.34353, Convergence: 0.013724\n",
      "Epoch: 109, Loss: 1130.58951, Residuals: -1.34723, Convergence: 0.011878\n",
      "Epoch: 110, Loss: 1118.97245, Residuals: -1.34966, Convergence: 0.010382\n",
      "Epoch: 111, Loss: 1108.81042, Residuals: -1.35093, Convergence: 0.009165\n",
      "Epoch: 112, Loss: 1099.81542, Residuals: -1.35115, Convergence: 0.008179\n",
      "Epoch: 113, Loss: 1091.75005, Residuals: -1.35041, Convergence: 0.007388\n",
      "Epoch: 114, Loss: 1084.41688, Residuals: -1.34878, Convergence: 0.006762\n",
      "Epoch: 115, Loss: 1077.64927, Residuals: -1.34631, Convergence: 0.006280\n",
      "Epoch: 116, Loss: 1071.30519, Residuals: -1.34307, Convergence: 0.005922\n",
      "Epoch: 117, Loss: 1065.26385, Residuals: -1.33909, Convergence: 0.005671\n",
      "Epoch: 118, Loss: 1059.42112, Residuals: -1.33439, Convergence: 0.005515\n",
      "Epoch: 119, Loss: 1053.68865, Residuals: -1.32900, Convergence: 0.005440\n",
      "Epoch: 120, Loss: 1047.99570, Residuals: -1.32296, Convergence: 0.005432\n",
      "Epoch: 121, Loss: 1042.29903, Residuals: -1.31631, Convergence: 0.005465\n",
      "Epoch: 122, Loss: 1036.59599, Residuals: -1.30912, Convergence: 0.005502\n",
      "Epoch: 123, Loss: 1030.93212, Residuals: -1.30149, Convergence: 0.005494\n",
      "Epoch: 124, Loss: 1025.38980, Residuals: -1.29352, Convergence: 0.005405\n",
      "Epoch: 125, Loss: 1020.05837, Residuals: -1.28531, Convergence: 0.005227\n",
      "Epoch: 126, Loss: 1015.00365, Residuals: -1.27695, Convergence: 0.004980\n",
      "Epoch: 127, Loss: 1010.25686, Residuals: -1.26853, Convergence: 0.004699\n",
      "Epoch: 128, Loss: 1005.81825, Residuals: -1.26010, Convergence: 0.004413\n",
      "Epoch: 129, Loss: 1001.66999, Residuals: -1.25172, Convergence: 0.004141\n",
      "Epoch: 130, Loss: 997.78664, Residuals: -1.24344, Convergence: 0.003892\n",
      "Epoch: 131, Loss: 994.14133, Residuals: -1.23530, Convergence: 0.003667\n",
      "Epoch: 132, Loss: 990.70886, Residuals: -1.22733, Convergence: 0.003465\n",
      "Epoch: 133, Loss: 987.46785, Residuals: -1.21955, Convergence: 0.003282\n",
      "Epoch: 134, Loss: 984.40044, Residuals: -1.21198, Convergence: 0.003116\n",
      "Epoch: 135, Loss: 981.49144, Residuals: -1.20465, Convergence: 0.002964\n",
      "Epoch: 136, Loss: 978.72911, Residuals: -1.19756, Convergence: 0.002822\n",
      "Epoch: 137, Loss: 976.10286, Residuals: -1.19072, Convergence: 0.002691\n",
      "Epoch: 138, Loss: 973.60374, Residuals: -1.18413, Convergence: 0.002567\n",
      "Epoch: 139, Loss: 971.22462, Residuals: -1.17781, Convergence: 0.002450\n",
      "Epoch: 140, Loss: 968.95778, Residuals: -1.17175, Convergence: 0.002339\n",
      "Epoch: 141, Loss: 966.79696, Residuals: -1.16594, Convergence: 0.002235\n",
      "Epoch: 142, Loss: 964.73591, Residuals: -1.16039, Convergence: 0.002136\n",
      "Epoch: 143, Loss: 962.76860, Residuals: -1.15509, Convergence: 0.002043\n",
      "Epoch: 144, Loss: 960.88975, Residuals: -1.15002, Convergence: 0.001955\n",
      "Epoch: 145, Loss: 959.09392, Residuals: -1.14519, Convergence: 0.001872\n",
      "Epoch: 146, Loss: 957.37613, Residuals: -1.14057, Convergence: 0.001794\n",
      "Epoch: 147, Loss: 955.73161, Residuals: -1.13617, Convergence: 0.001721\n",
      "Epoch: 148, Loss: 954.15586, Residuals: -1.13197, Convergence: 0.001651\n",
      "Epoch: 149, Loss: 952.64474, Residuals: -1.12795, Convergence: 0.001586\n",
      "Epoch: 150, Loss: 951.19378, Residuals: -1.12412, Convergence: 0.001525\n",
      "Epoch: 151, Loss: 949.79974, Residuals: -1.12045, Convergence: 0.001468\n",
      "Epoch: 152, Loss: 948.45821, Residuals: -1.11695, Convergence: 0.001414\n",
      "Epoch: 153, Loss: 947.16567, Residuals: -1.11359, Convergence: 0.001365\n",
      "Epoch: 154, Loss: 945.91827, Residuals: -1.11038, Convergence: 0.001319\n",
      "Epoch: 155, Loss: 944.71236, Residuals: -1.10729, Convergence: 0.001276\n",
      "Epoch: 156, Loss: 943.54393, Residuals: -1.10432, Convergence: 0.001238\n",
      "Epoch: 157, Loss: 942.40903, Residuals: -1.10147, Convergence: 0.001204\n",
      "Epoch: 158, Loss: 941.30350, Residuals: -1.09871, Convergence: 0.001174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 159, Loss: 940.22381, Residuals: -1.09604, Convergence: 0.001148\n",
      "Epoch: 160, Loss: 939.16523, Residuals: -1.09346, Convergence: 0.001127\n",
      "Epoch: 161, Loss: 938.12425, Residuals: -1.09094, Convergence: 0.001110\n",
      "Epoch: 162, Loss: 937.09701, Residuals: -1.08848, Convergence: 0.001096\n",
      "Epoch: 163, Loss: 936.08018, Residuals: -1.08607, Convergence: 0.001086\n",
      "Epoch: 164, Loss: 935.07126, Residuals: -1.08369, Convergence: 0.001079\n",
      "Epoch: 165, Loss: 934.06843, Residuals: -1.08136, Convergence: 0.001074\n",
      "Epoch: 166, Loss: 933.07154, Residuals: -1.07905, Convergence: 0.001068\n",
      "Epoch: 167, Loss: 932.08138, Residuals: -1.07678, Convergence: 0.001062\n",
      "Epoch: 168, Loss: 931.10004, Residuals: -1.07453, Convergence: 0.001054\n",
      "Epoch: 169, Loss: 930.13133, Residuals: -1.07232, Convergence: 0.001041\n",
      "Epoch: 170, Loss: 929.17941, Residuals: -1.07015, Convergence: 0.001024\n",
      "Epoch: 171, Loss: 928.24886, Residuals: -1.06803, Convergence: 0.001002\n",
      "Epoch: 172, Loss: 927.34369, Residuals: -1.06597, Convergence: 0.000976\n",
      "Evidence 11147.456\n",
      "\n",
      "Epoch: 172, Evidence: 11147.45605, Convergence: 1.016513\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.75e-01\n",
      "Epoch: 172, Loss: 2344.06511, Residuals: -1.06597, Convergence:   inf\n",
      "Epoch: 173, Loss: 2306.17978, Residuals: -1.07411, Convergence: 0.016428\n",
      "Epoch: 174, Loss: 2280.19561, Residuals: -1.07285, Convergence: 0.011396\n",
      "Epoch: 175, Loss: 2258.65757, Residuals: -1.07065, Convergence: 0.009536\n",
      "Epoch: 176, Loss: 2240.53010, Residuals: -1.06822, Convergence: 0.008091\n",
      "Epoch: 177, Loss: 2225.13948, Residuals: -1.06567, Convergence: 0.006917\n",
      "Epoch: 178, Loss: 2211.96602, Residuals: -1.06305, Convergence: 0.005956\n",
      "Epoch: 179, Loss: 2200.58416, Residuals: -1.06037, Convergence: 0.005172\n",
      "Epoch: 180, Loss: 2190.63780, Residuals: -1.05764, Convergence: 0.004540\n",
      "Epoch: 181, Loss: 2181.83017, Residuals: -1.05484, Convergence: 0.004037\n",
      "Epoch: 182, Loss: 2173.92191, Residuals: -1.05195, Convergence: 0.003638\n",
      "Epoch: 183, Loss: 2166.72837, Residuals: -1.04896, Convergence: 0.003320\n",
      "Epoch: 184, Loss: 2160.11813, Residuals: -1.04588, Convergence: 0.003060\n",
      "Epoch: 185, Loss: 2154.00838, Residuals: -1.04271, Convergence: 0.002836\n",
      "Epoch: 186, Loss: 2148.34625, Residuals: -1.03950, Convergence: 0.002636\n",
      "Epoch: 187, Loss: 2143.09541, Residuals: -1.03629, Convergence: 0.002450\n",
      "Epoch: 188, Loss: 2138.22524, Residuals: -1.03312, Convergence: 0.002278\n",
      "Epoch: 189, Loss: 2133.70432, Residuals: -1.03001, Convergence: 0.002119\n",
      "Epoch: 190, Loss: 2129.50411, Residuals: -1.02700, Convergence: 0.001972\n",
      "Epoch: 191, Loss: 2125.59762, Residuals: -1.02410, Convergence: 0.001838\n",
      "Epoch: 192, Loss: 2121.95775, Residuals: -1.02132, Convergence: 0.001715\n",
      "Epoch: 193, Loss: 2118.56192, Residuals: -1.01868, Convergence: 0.001603\n",
      "Epoch: 194, Loss: 2115.39010, Residuals: -1.01617, Convergence: 0.001499\n",
      "Epoch: 195, Loss: 2112.42335, Residuals: -1.01380, Convergence: 0.001404\n",
      "Epoch: 196, Loss: 2109.64735, Residuals: -1.01157, Convergence: 0.001316\n",
      "Epoch: 197, Loss: 2107.04712, Residuals: -1.00946, Convergence: 0.001234\n",
      "Epoch: 198, Loss: 2104.61151, Residuals: -1.00749, Convergence: 0.001157\n",
      "Epoch: 199, Loss: 2102.32828, Residuals: -1.00563, Convergence: 0.001086\n",
      "Epoch: 200, Loss: 2100.18742, Residuals: -1.00389, Convergence: 0.001019\n",
      "Epoch: 201, Loss: 2098.17930, Residuals: -1.00224, Convergence: 0.000957\n",
      "Evidence 14250.404\n",
      "\n",
      "Epoch: 201, Evidence: 14250.40430, Convergence: 0.217745\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.35e-01\n",
      "Epoch: 201, Loss: 2468.83516, Residuals: -1.00224, Convergence:   inf\n",
      "Epoch: 202, Loss: 2455.44364, Residuals: -0.99904, Convergence: 0.005454\n",
      "Epoch: 203, Loss: 2444.49839, Residuals: -0.99543, Convergence: 0.004478\n",
      "Epoch: 204, Loss: 2435.07483, Residuals: -0.99197, Convergence: 0.003870\n",
      "Epoch: 205, Loss: 2426.90940, Residuals: -0.98874, Convergence: 0.003365\n",
      "Epoch: 206, Loss: 2419.79528, Residuals: -0.98576, Convergence: 0.002940\n",
      "Epoch: 207, Loss: 2413.56502, Residuals: -0.98304, Convergence: 0.002581\n",
      "Epoch: 208, Loss: 2408.07965, Residuals: -0.98056, Convergence: 0.002278\n",
      "Epoch: 209, Loss: 2403.22475, Residuals: -0.97830, Convergence: 0.002020\n",
      "Epoch: 210, Loss: 2398.90364, Residuals: -0.97624, Convergence: 0.001801\n",
      "Epoch: 211, Loss: 2395.03645, Residuals: -0.97437, Convergence: 0.001615\n",
      "Epoch: 212, Loss: 2391.55686, Residuals: -0.97266, Convergence: 0.001455\n",
      "Epoch: 213, Loss: 2388.40922, Residuals: -0.97111, Convergence: 0.001318\n",
      "Epoch: 214, Loss: 2385.54701, Residuals: -0.96970, Convergence: 0.001200\n",
      "Epoch: 215, Loss: 2382.93187, Residuals: -0.96841, Convergence: 0.001097\n",
      "Epoch: 216, Loss: 2380.53061, Residuals: -0.96724, Convergence: 0.001009\n",
      "Epoch: 217, Loss: 2378.31786, Residuals: -0.96618, Convergence: 0.000930\n",
      "Evidence 14619.207\n",
      "\n",
      "Epoch: 217, Evidence: 14619.20703, Convergence: 0.025227\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.31e-01\n",
      "Epoch: 217, Loss: 2473.93461, Residuals: -0.96618, Convergence:   inf\n",
      "Epoch: 218, Loss: 2467.49463, Residuals: -0.96296, Convergence: 0.002610\n",
      "Epoch: 219, Loss: 2462.17664, Residuals: -0.96019, Convergence: 0.002160\n",
      "Epoch: 220, Loss: 2457.67863, Residuals: -0.95786, Convergence: 0.001830\n",
      "Epoch: 221, Loss: 2453.81617, Residuals: -0.95591, Convergence: 0.001574\n",
      "Epoch: 222, Loss: 2450.45262, Residuals: -0.95427, Convergence: 0.001373\n",
      "Epoch: 223, Loss: 2447.48564, Residuals: -0.95289, Convergence: 0.001212\n",
      "Epoch: 224, Loss: 2444.83811, Residuals: -0.95174, Convergence: 0.001083\n",
      "Epoch: 225, Loss: 2442.44957, Residuals: -0.95077, Convergence: 0.000978\n",
      "Evidence 14700.438\n",
      "\n",
      "Epoch: 225, Evidence: 14700.43750, Convergence: 0.005526\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.59e-01\n",
      "Epoch: 225, Loss: 2475.64636, Residuals: -0.95077, Convergence:   inf\n",
      "Epoch: 226, Loss: 2471.73681, Residuals: -0.94849, Convergence: 0.001582\n",
      "Epoch: 227, Loss: 2468.49181, Residuals: -0.94668, Convergence: 0.001315\n",
      "Epoch: 228, Loss: 2465.71400, Residuals: -0.94525, Convergence: 0.001127\n",
      "Epoch: 229, Loss: 2463.28503, Residuals: -0.94411, Convergence: 0.000986\n",
      "Evidence 14728.330\n",
      "\n",
      "Epoch: 229, Evidence: 14728.33008, Convergence: 0.001894\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.08e-01\n",
      "Epoch: 229, Loss: 2476.66294, Residuals: -0.94411, Convergence:   inf\n",
      "Epoch: 230, Loss: 2473.74526, Residuals: -0.94234, Convergence: 0.001179\n",
      "Epoch: 231, Loss: 2471.29757, Residuals: -0.94097, Convergence: 0.000990\n",
      "Evidence 14740.511\n",
      "\n",
      "Epoch: 231, Evidence: 14740.51074, Convergence: 0.000826\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.72e-01\n",
      "Epoch: 231, Loss: 2477.39589, Residuals: -0.94097, Convergence:   inf\n",
      "Epoch: 232, Loss: 2472.79268, Residuals: -0.93865, Convergence: 0.001862\n",
      "Epoch: 233, Loss: 2469.27255, Residuals: -0.93704, Convergence: 0.001426\n",
      "Epoch: 234, Loss: 2466.41727, Residuals: -0.93602, Convergence: 0.001158\n",
      "Epoch: 235, Loss: 2463.99731, Residuals: -0.93550, Convergence: 0.000982\n",
      "Evidence 14758.419\n",
      "\n",
      "Epoch: 235, Evidence: 14758.41895, Convergence: 0.002039\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.42e-01\n",
      "Epoch: 235, Loss: 2477.47112, Residuals: -0.93550, Convergence:   inf\n",
      "Epoch: 236, Loss: 2474.39099, Residuals: -0.93327, Convergence: 0.001245\n",
      "Epoch: 237, Loss: 2471.95708, Residuals: -0.93221, Convergence: 0.000985\n",
      "Evidence 14769.562\n",
      "\n",
      "Epoch: 237, Evidence: 14769.56250, Convergence: 0.000754\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.20e-01\n",
      "Epoch: 237, Loss: 2477.64802, Residuals: -0.93221, Convergence:   inf\n",
      "Epoch: 238, Loss: 2473.16518, Residuals: -0.92895, Convergence: 0.001813\n",
      "Epoch: 239, Loss: 2469.96214, Residuals: -0.93021, Convergence: 0.001297\n",
      "Epoch: 240, Loss: 2467.25463, Residuals: -0.93096, Convergence: 0.001097\n",
      "Epoch: 241, Loss: 2464.88400, Residuals: -0.93370, Convergence: 0.000962\n",
      "Evidence 14785.481\n",
      "\n",
      "Epoch: 241, Evidence: 14785.48145, Convergence: 0.001830\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.07e-01\n",
      "Epoch: 241, Loss: 2476.81752, Residuals: -0.93370, Convergence:   inf\n",
      "Epoch: 242, Loss: 2474.85689, Residuals: -0.93146, Convergence: 0.000792\n",
      "Evidence 14792.553\n",
      "\n",
      "Epoch: 242, Evidence: 14792.55273, Convergence: 0.000478\n",
      "Updating hyper-parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 182, Updated regularization: 8.72e-02\n",
      "Epoch: 242, Loss: 2477.79906, Residuals: -0.93146, Convergence:   inf\n",
      "Epoch: 243, Loss: 2515.30610, Residuals: -0.97308, Convergence: -0.014912\n",
      "Epoch: 243, Loss: 2475.54978, Residuals: -0.92934, Convergence: 0.000909\n",
      "Evidence 14796.704\n",
      "\n",
      "Epoch: 243, Evidence: 14796.70410, Convergence: 0.000758\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.07e-02\n",
      "Epoch: 243, Loss: 2477.16458, Residuals: -0.92934, Convergence:   inf\n",
      "Epoch: 244, Loss: 2481.07208, Residuals: -0.93261, Convergence: -0.001575\n",
      "Epoch: 244, Loss: 2476.76980, Residuals: -0.92845, Convergence: 0.000159\n",
      "Evidence 14798.773\n",
      "\n",
      "Epoch: 244, Evidence: 14798.77344, Convergence: 0.000898\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 382.58970, Residuals: -4.54048, Convergence:   inf\n",
      "Epoch: 1, Loss: 356.87234, Residuals: -4.41869, Convergence: 0.072063\n",
      "Epoch: 2, Loss: 335.68478, Residuals: -4.25168, Convergence: 0.063117\n",
      "Epoch: 3, Loss: 319.45690, Residuals: -4.08381, Convergence: 0.050798\n",
      "Epoch: 4, Loss: 307.11051, Residuals: -3.93690, Convergence: 0.040202\n",
      "Epoch: 5, Loss: 297.32396, Residuals: -3.80745, Convergence: 0.032915\n",
      "Epoch: 6, Loss: 289.38008, Residuals: -3.69479, Convergence: 0.027451\n",
      "Epoch: 7, Loss: 282.78813, Residuals: -3.59833, Convergence: 0.023311\n",
      "Epoch: 8, Loss: 277.18590, Residuals: -3.51597, Convergence: 0.020211\n",
      "Epoch: 9, Loss: 272.31648, Residuals: -3.44533, Convergence: 0.017881\n",
      "Epoch: 10, Loss: 267.99630, Residuals: -3.38425, Convergence: 0.016120\n",
      "Epoch: 11, Loss: 264.09163, Residuals: -3.33087, Convergence: 0.014785\n",
      "Epoch: 12, Loss: 260.50439, Residuals: -3.28365, Convergence: 0.013770\n",
      "Epoch: 13, Loss: 257.16298, Residuals: -3.24130, Convergence: 0.012993\n",
      "Epoch: 14, Loss: 254.01584, Residuals: -3.20270, Convergence: 0.012390\n",
      "Epoch: 15, Loss: 251.02732, Residuals: -3.16694, Convergence: 0.011905\n",
      "Epoch: 16, Loss: 248.17554, Residuals: -3.13333, Convergence: 0.011491\n",
      "Epoch: 17, Loss: 245.44746, Residuals: -3.10146, Convergence: 0.011115\n",
      "Epoch: 18, Loss: 242.82765, Residuals: -3.07100, Convergence: 0.010789\n",
      "Epoch: 19, Loss: 240.29100, Residuals: -3.04153, Convergence: 0.010557\n",
      "Epoch: 20, Loss: 237.80606, Residuals: -3.01253, Convergence: 0.010449\n",
      "Epoch: 21, Loss: 235.34269, Residuals: -2.98347, Convergence: 0.010467\n",
      "Epoch: 22, Loss: 232.87746, Residuals: -2.95396, Convergence: 0.010586\n",
      "Epoch: 23, Loss: 230.39084, Residuals: -2.92369, Convergence: 0.010793\n",
      "Epoch: 24, Loss: 227.85414, Residuals: -2.89234, Convergence: 0.011133\n",
      "Epoch: 25, Loss: 225.21886, Residuals: -2.85933, Convergence: 0.011701\n",
      "Epoch: 26, Loss: 222.43563, Residuals: -2.82405, Convergence: 0.012513\n",
      "Epoch: 27, Loss: 219.53766, Residuals: -2.78675, Convergence: 0.013200\n",
      "Epoch: 28, Loss: 216.66490, Residuals: -2.74890, Convergence: 0.013259\n",
      "Epoch: 29, Loss: 213.90371, Residuals: -2.71159, Convergence: 0.012909\n",
      "Epoch: 30, Loss: 211.25823, Residuals: -2.67501, Convergence: 0.012522\n",
      "Epoch: 31, Loss: 208.71095, Residuals: -2.63910, Convergence: 0.012205\n",
      "Epoch: 32, Loss: 206.24524, Residuals: -2.60377, Convergence: 0.011955\n",
      "Epoch: 33, Loss: 203.84890, Residuals: -2.56894, Convergence: 0.011755\n",
      "Epoch: 34, Loss: 201.51367, Residuals: -2.53453, Convergence: 0.011588\n",
      "Epoch: 35, Loss: 199.23434, Residuals: -2.50050, Convergence: 0.011440\n",
      "Epoch: 36, Loss: 197.00800, Residuals: -2.46680, Convergence: 0.011301\n",
      "Epoch: 37, Loss: 194.83333, Residuals: -2.43340, Convergence: 0.011162\n",
      "Epoch: 38, Loss: 192.71012, Residuals: -2.40026, Convergence: 0.011018\n",
      "Epoch: 39, Loss: 190.63876, Residuals: -2.36737, Convergence: 0.010865\n",
      "Epoch: 40, Loss: 188.61997, Residuals: -2.33474, Convergence: 0.010703\n",
      "Epoch: 41, Loss: 186.65439, Residuals: -2.30236, Convergence: 0.010531\n",
      "Epoch: 42, Loss: 184.74252, Residuals: -2.27025, Convergence: 0.010349\n",
      "Epoch: 43, Loss: 182.88451, Residuals: -2.23842, Convergence: 0.010159\n",
      "Epoch: 44, Loss: 181.08010, Residuals: -2.20689, Convergence: 0.009965\n",
      "Epoch: 45, Loss: 179.32866, Residuals: -2.17566, Convergence: 0.009767\n",
      "Epoch: 46, Loss: 177.62924, Residuals: -2.14476, Convergence: 0.009567\n",
      "Epoch: 47, Loss: 175.98070, Residuals: -2.11419, Convergence: 0.009368\n",
      "Epoch: 48, Loss: 174.38187, Residuals: -2.08395, Convergence: 0.009169\n",
      "Epoch: 49, Loss: 172.83170, Residuals: -2.05406, Convergence: 0.008969\n",
      "Epoch: 50, Loss: 171.32949, Residuals: -2.02452, Convergence: 0.008768\n",
      "Epoch: 51, Loss: 169.87491, Residuals: -1.99533, Convergence: 0.008563\n",
      "Epoch: 52, Loss: 168.46800, Residuals: -1.96653, Convergence: 0.008351\n",
      "Epoch: 53, Loss: 167.10903, Residuals: -1.93811, Convergence: 0.008132\n",
      "Epoch: 54, Loss: 165.79832, Residuals: -1.91013, Convergence: 0.007905\n",
      "Epoch: 55, Loss: 164.53608, Residuals: -1.88259, Convergence: 0.007672\n",
      "Epoch: 56, Loss: 163.32229, Residuals: -1.85555, Convergence: 0.007432\n",
      "Epoch: 57, Loss: 162.15663, Residuals: -1.82902, Convergence: 0.007188\n",
      "Epoch: 58, Loss: 161.03851, Residuals: -1.80305, Convergence: 0.006943\n",
      "Epoch: 59, Loss: 159.96709, Residuals: -1.77764, Convergence: 0.006698\n",
      "Epoch: 60, Loss: 158.94130, Residuals: -1.75283, Convergence: 0.006454\n",
      "Epoch: 61, Loss: 157.95993, Residuals: -1.72863, Convergence: 0.006213\n",
      "Epoch: 62, Loss: 157.02166, Residuals: -1.70506, Convergence: 0.005975\n",
      "Epoch: 63, Loss: 156.12512, Residuals: -1.68213, Convergence: 0.005742\n",
      "Epoch: 64, Loss: 155.26889, Residuals: -1.65985, Convergence: 0.005514\n",
      "Epoch: 65, Loss: 154.45154, Residuals: -1.63821, Convergence: 0.005292\n",
      "Epoch: 66, Loss: 153.67163, Residuals: -1.61723, Convergence: 0.005075\n",
      "Epoch: 67, Loss: 152.92777, Residuals: -1.59690, Convergence: 0.004864\n",
      "Epoch: 68, Loss: 152.21858, Residuals: -1.57722, Convergence: 0.004659\n",
      "Epoch: 69, Loss: 151.54270, Residuals: -1.55819, Convergence: 0.004460\n",
      "Epoch: 70, Loss: 150.89885, Residuals: -1.53981, Convergence: 0.004267\n",
      "Epoch: 71, Loss: 150.28571, Residuals: -1.52206, Convergence: 0.004080\n",
      "Epoch: 72, Loss: 149.70203, Residuals: -1.50494, Convergence: 0.003899\n",
      "Epoch: 73, Loss: 149.14660, Residuals: -1.48844, Convergence: 0.003724\n",
      "Epoch: 74, Loss: 148.61821, Residuals: -1.47255, Convergence: 0.003555\n",
      "Epoch: 75, Loss: 148.11570, Residuals: -1.45725, Convergence: 0.003393\n",
      "Epoch: 76, Loss: 147.63792, Residuals: -1.44254, Convergence: 0.003236\n",
      "Epoch: 77, Loss: 147.18374, Residuals: -1.42840, Convergence: 0.003086\n",
      "Epoch: 78, Loss: 146.75205, Residuals: -1.41482, Convergence: 0.002942\n",
      "Epoch: 79, Loss: 146.34182, Residuals: -1.40178, Convergence: 0.002803\n",
      "Epoch: 80, Loss: 145.95198, Residuals: -1.38926, Convergence: 0.002671\n",
      "Epoch: 81, Loss: 145.58155, Residuals: -1.37726, Convergence: 0.002545\n",
      "Epoch: 82, Loss: 145.22955, Residuals: -1.36575, Convergence: 0.002424\n",
      "Epoch: 83, Loss: 144.89506, Residuals: -1.35471, Convergence: 0.002308\n",
      "Epoch: 84, Loss: 144.57720, Residuals: -1.34414, Convergence: 0.002199\n",
      "Epoch: 85, Loss: 144.27514, Residuals: -1.33401, Convergence: 0.002094\n",
      "Epoch: 86, Loss: 143.98807, Residuals: -1.32431, Convergence: 0.001994\n",
      "Epoch: 87, Loss: 143.71525, Residuals: -1.31501, Convergence: 0.001898\n",
      "Epoch: 88, Loss: 143.45599, Residuals: -1.30611, Convergence: 0.001807\n",
      "Epoch: 89, Loss: 143.20963, Residuals: -1.29759, Convergence: 0.001720\n",
      "Epoch: 90, Loss: 142.97554, Residuals: -1.28943, Convergence: 0.001637\n",
      "Epoch: 91, Loss: 142.75318, Residuals: -1.28162, Convergence: 0.001558\n",
      "Epoch: 92, Loss: 142.54200, Residuals: -1.27414, Convergence: 0.001481\n",
      "Epoch: 93, Loss: 142.34153, Residuals: -1.26697, Convergence: 0.001408\n",
      "Epoch: 94, Loss: 142.15130, Residuals: -1.26012, Convergence: 0.001338\n",
      "Epoch: 95, Loss: 141.97092, Residuals: -1.25355, Convergence: 0.001271\n",
      "Epoch: 96, Loss: 141.79999, Residuals: -1.24727, Convergence: 0.001205\n",
      "Epoch: 97, Loss: 141.63817, Residuals: -1.24125, Convergence: 0.001142\n",
      "Epoch: 98, Loss: 141.48515, Residuals: -1.23549, Convergence: 0.001082\n",
      "Epoch: 99, Loss: 141.34062, Residuals: -1.22998, Convergence: 0.001023\n",
      "Epoch: 100, Loss: 141.20432, Residuals: -1.22471, Convergence: 0.000965\n",
      "Evidence -182.879\n",
      "\n",
      "Epoch: 100, Evidence: -182.87900, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.24e-01\n",
      "Epoch: 100, Loss: 1366.55596, Residuals: -1.22471, Convergence:   inf\n",
      "Epoch: 101, Loss: 1305.19331, Residuals: -1.25542, Convergence: 0.047014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 102, Loss: 1258.61254, Residuals: -1.27931, Convergence: 0.037010\n",
      "Epoch: 103, Loss: 1223.38681, Residuals: -1.29642, Convergence: 0.028794\n",
      "Epoch: 104, Loss: 1195.94201, Residuals: -1.30841, Convergence: 0.022948\n",
      "Epoch: 105, Loss: 1173.72089, Residuals: -1.31720, Convergence: 0.018932\n",
      "Epoch: 106, Loss: 1155.23982, Residuals: -1.32383, Convergence: 0.015998\n",
      "Epoch: 107, Loss: 1139.60597, Residuals: -1.32879, Convergence: 0.013719\n",
      "Epoch: 108, Loss: 1126.21588, Residuals: -1.33232, Convergence: 0.011889\n",
      "Epoch: 109, Loss: 1114.62385, Residuals: -1.33457, Convergence: 0.010400\n",
      "Epoch: 110, Loss: 1104.48380, Residuals: -1.33568, Convergence: 0.009181\n",
      "Epoch: 111, Loss: 1095.51671, Residuals: -1.33575, Convergence: 0.008185\n",
      "Epoch: 112, Loss: 1087.49492, Residuals: -1.33489, Convergence: 0.007376\n",
      "Epoch: 113, Loss: 1080.22870, Residuals: -1.33318, Convergence: 0.006727\n",
      "Epoch: 114, Loss: 1073.55881, Residuals: -1.33068, Convergence: 0.006213\n",
      "Epoch: 115, Loss: 1067.35000, Residuals: -1.32746, Convergence: 0.005817\n",
      "Epoch: 116, Loss: 1061.48863, Residuals: -1.32356, Convergence: 0.005522\n",
      "Epoch: 117, Loss: 1055.87913, Residuals: -1.31902, Convergence: 0.005313\n",
      "Epoch: 118, Loss: 1050.44045, Residuals: -1.31386, Convergence: 0.005178\n",
      "Epoch: 119, Loss: 1045.10382, Residuals: -1.30812, Convergence: 0.005106\n",
      "Epoch: 120, Loss: 1039.81266, Residuals: -1.30182, Convergence: 0.005089\n",
      "Epoch: 121, Loss: 1034.52676, Residuals: -1.29501, Convergence: 0.005109\n",
      "Epoch: 122, Loss: 1029.23407, Residuals: -1.28775, Convergence: 0.005142\n",
      "Epoch: 123, Loss: 1023.96122, Residuals: -1.28012, Convergence: 0.005149\n",
      "Epoch: 124, Loss: 1018.77471, Residuals: -1.27222, Convergence: 0.005091\n",
      "Epoch: 125, Loss: 1013.75929, Residuals: -1.26412, Convergence: 0.004947\n",
      "Epoch: 126, Loss: 1008.98725, Residuals: -1.25591, Convergence: 0.004730\n",
      "Epoch: 127, Loss: 1004.49935, Residuals: -1.24767, Convergence: 0.004468\n",
      "Epoch: 128, Loss: 1000.30366, Residuals: -1.23943, Convergence: 0.004194\n",
      "Epoch: 129, Loss: 996.38679, Residuals: -1.23127, Convergence: 0.003931\n",
      "Epoch: 130, Loss: 992.72476, Residuals: -1.22322, Convergence: 0.003689\n",
      "Epoch: 131, Loss: 989.29131, Residuals: -1.21532, Convergence: 0.003471\n",
      "Epoch: 132, Loss: 986.06214, Residuals: -1.20760, Convergence: 0.003275\n",
      "Epoch: 133, Loss: 983.01618, Residuals: -1.20008, Convergence: 0.003099\n",
      "Epoch: 134, Loss: 980.13563, Residuals: -1.19278, Convergence: 0.002939\n",
      "Epoch: 135, Loss: 977.40607, Residuals: -1.18572, Convergence: 0.002793\n",
      "Epoch: 136, Loss: 974.81539, Residuals: -1.17890, Convergence: 0.002658\n",
      "Epoch: 137, Loss: 972.35331, Residuals: -1.17234, Convergence: 0.002532\n",
      "Epoch: 138, Loss: 970.01120, Residuals: -1.16604, Convergence: 0.002415\n",
      "Epoch: 139, Loss: 967.78101, Residuals: -1.16000, Convergence: 0.002304\n",
      "Epoch: 140, Loss: 965.65600, Residuals: -1.15422, Convergence: 0.002201\n",
      "Epoch: 141, Loss: 963.62932, Residuals: -1.14869, Convergence: 0.002103\n",
      "Epoch: 142, Loss: 961.69502, Residuals: -1.14341, Convergence: 0.002011\n",
      "Epoch: 143, Loss: 959.84737, Residuals: -1.13837, Convergence: 0.001925\n",
      "Epoch: 144, Loss: 958.08084, Residuals: -1.13356, Convergence: 0.001844\n",
      "Epoch: 145, Loss: 956.39018, Residuals: -1.12898, Convergence: 0.001768\n",
      "Epoch: 146, Loss: 954.77053, Residuals: -1.12461, Convergence: 0.001696\n",
      "Epoch: 147, Loss: 953.21723, Residuals: -1.12044, Convergence: 0.001630\n",
      "Epoch: 148, Loss: 951.72563, Residuals: -1.11646, Convergence: 0.001567\n",
      "Epoch: 149, Loss: 950.29197, Residuals: -1.11266, Convergence: 0.001509\n",
      "Epoch: 150, Loss: 948.91201, Residuals: -1.10903, Convergence: 0.001454\n",
      "Epoch: 151, Loss: 947.58226, Residuals: -1.10556, Convergence: 0.001403\n",
      "Epoch: 152, Loss: 946.29932, Residuals: -1.10225, Convergence: 0.001356\n",
      "Epoch: 153, Loss: 945.06017, Residuals: -1.09907, Convergence: 0.001311\n",
      "Epoch: 154, Loss: 943.86145, Residuals: -1.09603, Convergence: 0.001270\n",
      "Epoch: 155, Loss: 942.70017, Residuals: -1.09311, Convergence: 0.001232\n",
      "Epoch: 156, Loss: 941.57337, Residuals: -1.09031, Convergence: 0.001197\n",
      "Epoch: 157, Loss: 940.47792, Residuals: -1.08761, Convergence: 0.001165\n",
      "Epoch: 158, Loss: 939.41124, Residuals: -1.08501, Convergence: 0.001135\n",
      "Epoch: 159, Loss: 938.36969, Residuals: -1.08251, Convergence: 0.001110\n",
      "Epoch: 160, Loss: 937.35040, Residuals: -1.08008, Convergence: 0.001087\n",
      "Epoch: 161, Loss: 936.35014, Residuals: -1.07773, Convergence: 0.001068\n",
      "Epoch: 162, Loss: 935.36494, Residuals: -1.07545, Convergence: 0.001053\n",
      "Epoch: 163, Loss: 934.39145, Residuals: -1.07321, Convergence: 0.001042\n",
      "Epoch: 164, Loss: 933.42644, Residuals: -1.07103, Convergence: 0.001034\n",
      "Epoch: 165, Loss: 932.46632, Residuals: -1.06888, Convergence: 0.001030\n",
      "Epoch: 166, Loss: 931.50817, Residuals: -1.06677, Convergence: 0.001029\n",
      "Epoch: 167, Loss: 930.55032, Residuals: -1.06467, Convergence: 0.001029\n",
      "Epoch: 168, Loss: 929.59133, Residuals: -1.06260, Convergence: 0.001032\n",
      "Epoch: 169, Loss: 928.63283, Residuals: -1.06053, Convergence: 0.001032\n",
      "Epoch: 170, Loss: 927.67637, Residuals: -1.05849, Convergence: 0.001031\n",
      "Epoch: 171, Loss: 926.72636, Residuals: -1.05646, Convergence: 0.001025\n",
      "Epoch: 172, Loss: 925.78780, Residuals: -1.05447, Convergence: 0.001014\n",
      "Epoch: 173, Loss: 924.86653, Residuals: -1.05251, Convergence: 0.000996\n",
      "Evidence 11176.451\n",
      "\n",
      "Epoch: 173, Evidence: 11176.45117, Convergence: 1.016363\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.74e-01\n",
      "Epoch: 173, Loss: 2340.89637, Residuals: -1.05251, Convergence:   inf\n",
      "Epoch: 174, Loss: 2304.92056, Residuals: -1.06174, Convergence: 0.015608\n",
      "Epoch: 175, Loss: 2278.59667, Residuals: -1.06043, Convergence: 0.011553\n",
      "Epoch: 176, Loss: 2256.58098, Residuals: -1.05814, Convergence: 0.009756\n",
      "Epoch: 177, Loss: 2238.00233, Residuals: -1.05563, Convergence: 0.008301\n",
      "Epoch: 178, Loss: 2222.19738, Residuals: -1.05303, Convergence: 0.007112\n",
      "Epoch: 179, Loss: 2208.64926, Residuals: -1.05037, Convergence: 0.006134\n",
      "Epoch: 180, Loss: 2196.93084, Residuals: -1.04765, Convergence: 0.005334\n",
      "Epoch: 181, Loss: 2186.68159, Residuals: -1.04488, Convergence: 0.004687\n",
      "Epoch: 182, Loss: 2177.59949, Residuals: -1.04204, Convergence: 0.004171\n",
      "Epoch: 183, Loss: 2169.43067, Residuals: -1.03910, Convergence: 0.003765\n",
      "Epoch: 184, Loss: 2161.97580, Residuals: -1.03604, Convergence: 0.003448\n",
      "Epoch: 185, Loss: 2155.08736, Residuals: -1.03284, Convergence: 0.003196\n",
      "Epoch: 186, Loss: 2148.67581, Residuals: -1.02951, Convergence: 0.002984\n",
      "Epoch: 187, Loss: 2142.69258, Residuals: -1.02609, Convergence: 0.002792\n",
      "Epoch: 188, Loss: 2137.11340, Residuals: -1.02262, Convergence: 0.002611\n",
      "Epoch: 189, Loss: 2131.92334, Residuals: -1.01918, Convergence: 0.002434\n",
      "Epoch: 190, Loss: 2127.10382, Residuals: -1.01579, Convergence: 0.002266\n",
      "Epoch: 191, Loss: 2122.63516, Residuals: -1.01251, Convergence: 0.002105\n",
      "Epoch: 192, Loss: 2118.49488, Residuals: -1.00934, Convergence: 0.001954\n",
      "Epoch: 193, Loss: 2114.65815, Residuals: -1.00632, Convergence: 0.001814\n",
      "Epoch: 194, Loss: 2111.10139, Residuals: -1.00344, Convergence: 0.001685\n",
      "Epoch: 195, Loss: 2107.80100, Residuals: -1.00072, Convergence: 0.001566\n",
      "Epoch: 196, Loss: 2104.73617, Residuals: -0.99815, Convergence: 0.001456\n",
      "Epoch: 197, Loss: 2101.88435, Residuals: -0.99573, Convergence: 0.001357\n",
      "Epoch: 198, Loss: 2099.22848, Residuals: -0.99345, Convergence: 0.001265\n",
      "Epoch: 199, Loss: 2096.75100, Residuals: -0.99130, Convergence: 0.001182\n",
      "Epoch: 200, Loss: 2094.43569, Residuals: -0.98928, Convergence: 0.001105\n",
      "Epoch: 201, Loss: 2092.26891, Residuals: -0.98738, Convergence: 0.001036\n",
      "Epoch: 202, Loss: 2090.23745, Residuals: -0.98560, Convergence: 0.000972\n",
      "Evidence 14382.497\n",
      "\n",
      "Epoch: 202, Evidence: 14382.49707, Convergence: 0.222913\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.35e-01\n",
      "Epoch: 202, Loss: 2470.54607, Residuals: -0.98560, Convergence:   inf\n",
      "Epoch: 203, Loss: 2457.23260, Residuals: -0.98218, Convergence: 0.005418\n",
      "Epoch: 204, Loss: 2446.31293, Residuals: -0.97831, Convergence: 0.004464\n",
      "Epoch: 205, Loss: 2436.92981, Residuals: -0.97467, Convergence: 0.003850\n",
      "Epoch: 206, Loss: 2428.81566, Residuals: -0.97130, Convergence: 0.003341\n",
      "Epoch: 207, Loss: 2421.75819, Residuals: -0.96823, Convergence: 0.002914\n",
      "Epoch: 208, Loss: 2415.58363, Residuals: -0.96544, Convergence: 0.002556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 209, Loss: 2410.14983, Residuals: -0.96293, Convergence: 0.002255\n",
      "Epoch: 210, Loss: 2405.33877, Residuals: -0.96067, Convergence: 0.002000\n",
      "Epoch: 211, Loss: 2401.05459, Residuals: -0.95863, Convergence: 0.001784\n",
      "Epoch: 212, Loss: 2397.21649, Residuals: -0.95679, Convergence: 0.001601\n",
      "Epoch: 213, Loss: 2393.75831, Residuals: -0.95514, Convergence: 0.001445\n",
      "Epoch: 214, Loss: 2390.62555, Residuals: -0.95366, Convergence: 0.001310\n",
      "Epoch: 215, Loss: 2387.77196, Residuals: -0.95233, Convergence: 0.001195\n",
      "Epoch: 216, Loss: 2385.16013, Residuals: -0.95114, Convergence: 0.001095\n",
      "Epoch: 217, Loss: 2382.75794, Residuals: -0.95007, Convergence: 0.001008\n",
      "Epoch: 218, Loss: 2380.53982, Residuals: -0.94911, Convergence: 0.000932\n",
      "Evidence 14755.854\n",
      "\n",
      "Epoch: 218, Evidence: 14755.85449, Convergence: 0.025302\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.32e-01\n",
      "Epoch: 218, Loss: 2475.85705, Residuals: -0.94911, Convergence:   inf\n",
      "Epoch: 219, Loss: 2469.50634, Residuals: -0.94581, Convergence: 0.002572\n",
      "Epoch: 220, Loss: 2464.24033, Residuals: -0.94301, Convergence: 0.002137\n",
      "Epoch: 221, Loss: 2459.77331, Residuals: -0.94068, Convergence: 0.001816\n",
      "Epoch: 222, Loss: 2455.92926, Residuals: -0.93876, Convergence: 0.001565\n",
      "Epoch: 223, Loss: 2452.57707, Residuals: -0.93717, Convergence: 0.001367\n",
      "Epoch: 224, Loss: 2449.61674, Residuals: -0.93586, Convergence: 0.001208\n",
      "Epoch: 225, Loss: 2446.97205, Residuals: -0.93478, Convergence: 0.001081\n",
      "Epoch: 226, Loss: 2444.58514, Residuals: -0.93389, Convergence: 0.000976\n",
      "Evidence 14836.409\n",
      "\n",
      "Epoch: 226, Evidence: 14836.40918, Convergence: 0.005430\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.60e-01\n",
      "Epoch: 226, Loss: 2477.67730, Residuals: -0.93389, Convergence:   inf\n",
      "Epoch: 227, Loss: 2473.76612, Residuals: -0.93158, Convergence: 0.001581\n",
      "Epoch: 228, Loss: 2470.50114, Residuals: -0.92980, Convergence: 0.001322\n",
      "Epoch: 229, Loss: 2467.69882, Residuals: -0.92843, Convergence: 0.001136\n",
      "Epoch: 230, Loss: 2465.24482, Residuals: -0.92737, Convergence: 0.000995\n",
      "Evidence 14864.399\n",
      "\n",
      "Epoch: 230, Evidence: 14864.39941, Convergence: 0.001883\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.10e-01\n",
      "Epoch: 230, Loss: 2478.76504, Residuals: -0.92737, Convergence:   inf\n",
      "Epoch: 231, Loss: 2475.80778, Residuals: -0.92563, Convergence: 0.001194\n",
      "Epoch: 232, Loss: 2473.31770, Residuals: -0.92436, Convergence: 0.001007\n",
      "Epoch: 233, Loss: 2471.15387, Residuals: -0.92342, Convergence: 0.000876\n",
      "Evidence 14879.064\n",
      "\n",
      "Epoch: 233, Evidence: 14879.06445, Convergence: 0.000986\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.74e-01\n",
      "Epoch: 233, Loss: 2479.48255, Residuals: -0.92342, Convergence:   inf\n",
      "Epoch: 234, Loss: 2475.00708, Residuals: -0.92131, Convergence: 0.001808\n",
      "Epoch: 235, Loss: 2471.59105, Residuals: -0.92031, Convergence: 0.001382\n",
      "Epoch: 236, Loss: 2468.80489, Residuals: -0.91985, Convergence: 0.001129\n",
      "Epoch: 237, Loss: 2466.43142, Residuals: -0.91979, Convergence: 0.000962\n",
      "Evidence 14897.203\n",
      "\n",
      "Epoch: 237, Evidence: 14897.20312, Convergence: 0.002202\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.44e-01\n",
      "Epoch: 237, Loss: 2479.56473, Residuals: -0.91979, Convergence:   inf\n",
      "Epoch: 238, Loss: 2476.54720, Residuals: -0.91830, Convergence: 0.001218\n",
      "Epoch: 239, Loss: 2474.14989, Residuals: -0.91796, Convergence: 0.000969\n",
      "Evidence 14908.227\n",
      "\n",
      "Epoch: 239, Evidence: 14908.22656, Convergence: 0.000739\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.22e-01\n",
      "Epoch: 239, Loss: 2479.78238, Residuals: -0.91796, Convergence:   inf\n",
      "Epoch: 240, Loss: 2475.35045, Residuals: -0.91664, Convergence: 0.001790\n",
      "Epoch: 241, Loss: 2472.11940, Residuals: -0.91898, Convergence: 0.001307\n",
      "Epoch: 242, Loss: 2469.45108, Residuals: -0.91998, Convergence: 0.001081\n",
      "Epoch: 243, Loss: 2467.08182, Residuals: -0.92295, Convergence: 0.000960\n",
      "Evidence 14924.150\n",
      "\n",
      "Epoch: 243, Evidence: 14924.15039, Convergence: 0.001806\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.09e-01\n",
      "Epoch: 243, Loss: 2479.07316, Residuals: -0.92295, Convergence:   inf\n",
      "Epoch: 244, Loss: 2476.76774, Residuals: -0.92238, Convergence: 0.000931\n",
      "Evidence 14931.367\n",
      "\n",
      "Epoch: 244, Evidence: 14931.36719, Convergence: 0.000483\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.92e-02\n",
      "Epoch: 244, Loss: 2479.98765, Residuals: -0.92238, Convergence:   inf\n",
      "Epoch: 245, Loss: 2511.64665, Residuals: -0.96375, Convergence: -0.012605\n",
      "Epoch: 245, Loss: 2477.62271, Residuals: -0.92322, Convergence: 0.000955\n",
      "Evidence 14935.584\n",
      "\n",
      "Epoch: 245, Evidence: 14935.58398, Convergence: 0.000766\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.27e-02\n",
      "Epoch: 245, Loss: 2479.36777, Residuals: -0.92322, Convergence:   inf\n",
      "Epoch: 246, Loss: 2482.29845, Residuals: -0.92948, Convergence: -0.001181\n",
      "Epoch: 246, Loss: 2478.65839, Residuals: -0.92423, Convergence: 0.000286\n",
      "Evidence 14937.914\n",
      "\n",
      "Epoch: 246, Evidence: 14937.91406, Convergence: 0.000921\n",
      "Total samples: 183, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 380.96800, Residuals: -4.51253, Convergence:   inf\n",
      "Epoch: 1, Loss: 355.34805, Residuals: -4.39190, Convergence: 0.072098\n",
      "Epoch: 2, Loss: 334.45123, Residuals: -4.22876, Convergence: 0.062481\n",
      "Epoch: 3, Loss: 318.50705, Residuals: -4.06548, Convergence: 0.050059\n",
      "Epoch: 4, Loss: 306.35131, Residuals: -3.92162, Convergence: 0.039679\n",
      "Epoch: 5, Loss: 296.71843, Residuals: -3.79464, Convergence: 0.032465\n",
      "Epoch: 6, Loss: 288.90229, Residuals: -3.68405, Convergence: 0.027055\n",
      "Epoch: 7, Loss: 282.41565, Residuals: -3.58932, Convergence: 0.022968\n",
      "Epoch: 8, Loss: 276.89810, Residuals: -3.50842, Convergence: 0.019926\n",
      "Epoch: 9, Loss: 272.09466, Residuals: -3.43906, Convergence: 0.017654\n",
      "Epoch: 10, Loss: 267.82374, Residuals: -3.37914, Convergence: 0.015947\n",
      "Epoch: 11, Loss: 263.95350, Residuals: -3.32689, Convergence: 0.014663\n",
      "Epoch: 12, Loss: 260.38740, Residuals: -3.28076, Convergence: 0.013695\n",
      "Epoch: 13, Loss: 257.05518, Residuals: -3.23945, Convergence: 0.012963\n",
      "Epoch: 14, Loss: 253.90735, Residuals: -3.20177, Convergence: 0.012398\n",
      "Epoch: 15, Loss: 250.91275, Residuals: -3.16680, Convergence: 0.011935\n",
      "Epoch: 16, Loss: 248.05576, Residuals: -3.13391, Convergence: 0.011518\n",
      "Epoch: 17, Loss: 245.32499, Residuals: -3.10269, Convergence: 0.011131\n",
      "Epoch: 18, Loss: 242.69909, Residuals: -3.07272, Convergence: 0.010820\n",
      "Epoch: 19, Loss: 240.14492, Residuals: -3.04345, Convergence: 0.010636\n",
      "Epoch: 20, Loss: 237.62614, Residuals: -3.01428, Convergence: 0.010600\n",
      "Epoch: 21, Loss: 235.11264, Residuals: -2.98474, Convergence: 0.010691\n",
      "Epoch: 22, Loss: 232.58294, Residuals: -2.95453, Convergence: 0.010877\n",
      "Epoch: 23, Loss: 230.01134, Residuals: -2.92339, Convergence: 0.011180\n",
      "Epoch: 24, Loss: 227.35052, Residuals: -2.89079, Convergence: 0.011704\n",
      "Epoch: 25, Loss: 224.53978, Residuals: -2.85601, Convergence: 0.012518\n",
      "Epoch: 26, Loss: 221.57853, Residuals: -2.81893, Convergence: 0.013364\n",
      "Epoch: 27, Loss: 218.59324, Residuals: -2.78084, Convergence: 0.013657\n",
      "Epoch: 28, Loss: 215.69293, Residuals: -2.74304, Convergence: 0.013446\n",
      "Epoch: 29, Loss: 212.89178, Residuals: -2.70582, Convergence: 0.013158\n",
      "Epoch: 30, Loss: 210.17235, Residuals: -2.66909, Convergence: 0.012939\n",
      "Epoch: 31, Loss: 207.51741, Residuals: -2.63270, Convergence: 0.012794\n",
      "Epoch: 32, Loss: 204.91559, Residuals: -2.59653, Convergence: 0.012697\n",
      "Epoch: 33, Loss: 202.36096, Residuals: -2.56052, Convergence: 0.012624\n",
      "Epoch: 34, Loss: 199.85159, Residuals: -2.52460, Convergence: 0.012556\n",
      "Epoch: 35, Loss: 197.38809, Residuals: -2.48878, Convergence: 0.012481\n",
      "Epoch: 36, Loss: 194.97246, Residuals: -2.45306, Convergence: 0.012390\n",
      "Epoch: 37, Loss: 192.60727, Residuals: -2.41744, Convergence: 0.012280\n",
      "Epoch: 38, Loss: 190.29502, Residuals: -2.38195, Convergence: 0.012151\n",
      "Epoch: 39, Loss: 188.03792, Residuals: -2.34660, Convergence: 0.012003\n",
      "Epoch: 40, Loss: 185.83781, Residuals: -2.31143, Convergence: 0.011839\n",
      "Epoch: 41, Loss: 183.69630, Residuals: -2.27644, Convergence: 0.011658\n",
      "Epoch: 42, Loss: 181.61505, Residuals: -2.24168, Convergence: 0.011460\n",
      "Epoch: 43, Loss: 179.59603, Residuals: -2.20717, Convergence: 0.011242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44, Loss: 177.64181, Residuals: -2.17295, Convergence: 0.011001\n",
      "Epoch: 45, Loss: 175.75554, Residuals: -2.13909, Convergence: 0.010732\n",
      "Epoch: 46, Loss: 173.94065, Residuals: -2.10564, Convergence: 0.010434\n",
      "Epoch: 47, Loss: 172.20039, Residuals: -2.07268, Convergence: 0.010106\n",
      "Epoch: 48, Loss: 170.53741, Residuals: -2.04028, Convergence: 0.009751\n",
      "Epoch: 49, Loss: 168.95344, Residuals: -2.00850, Convergence: 0.009375\n",
      "Epoch: 50, Loss: 167.44927, Residuals: -1.97742, Convergence: 0.008983\n",
      "Epoch: 51, Loss: 166.02479, Residuals: -1.94707, Convergence: 0.008580\n",
      "Epoch: 52, Loss: 164.67914, Residuals: -1.91752, Convergence: 0.008171\n",
      "Epoch: 53, Loss: 163.41083, Residuals: -1.88878, Convergence: 0.007762\n",
      "Epoch: 54, Loss: 162.21784, Residuals: -1.86089, Convergence: 0.007354\n",
      "Epoch: 55, Loss: 161.09774, Residuals: -1.83387, Convergence: 0.006953\n",
      "Epoch: 56, Loss: 160.04773, Residuals: -1.80774, Convergence: 0.006561\n",
      "Epoch: 57, Loss: 159.06463, Residuals: -1.78249, Convergence: 0.006181\n",
      "Epoch: 58, Loss: 158.14497, Residuals: -1.75814, Convergence: 0.005815\n",
      "Epoch: 59, Loss: 157.28497, Residuals: -1.73467, Convergence: 0.005468\n",
      "Epoch: 60, Loss: 156.48057, Residuals: -1.71206, Convergence: 0.005141\n",
      "Epoch: 61, Loss: 155.72754, Residuals: -1.69031, Convergence: 0.004836\n",
      "Epoch: 62, Loss: 155.02149, Residuals: -1.66938, Convergence: 0.004555\n",
      "Epoch: 63, Loss: 154.35802, Residuals: -1.64923, Convergence: 0.004298\n",
      "Epoch: 64, Loss: 153.73280, Residuals: -1.62984, Convergence: 0.004067\n",
      "Epoch: 65, Loss: 153.14166, Residuals: -1.61115, Convergence: 0.003860\n",
      "Epoch: 66, Loss: 152.58066, Residuals: -1.59312, Convergence: 0.003677\n",
      "Epoch: 67, Loss: 152.04618, Residuals: -1.57572, Convergence: 0.003515\n",
      "Epoch: 68, Loss: 151.53493, Residuals: -1.55890, Convergence: 0.003374\n",
      "Epoch: 69, Loss: 151.04396, Residuals: -1.54261, Convergence: 0.003250\n",
      "Epoch: 70, Loss: 150.57081, Residuals: -1.52682, Convergence: 0.003142\n",
      "Epoch: 71, Loss: 150.11339, Residuals: -1.51149, Convergence: 0.003047\n",
      "Epoch: 72, Loss: 149.67004, Residuals: -1.49659, Convergence: 0.002962\n",
      "Epoch: 73, Loss: 149.23948, Residuals: -1.48210, Convergence: 0.002885\n",
      "Epoch: 74, Loss: 148.82077, Residuals: -1.46799, Convergence: 0.002813\n",
      "Epoch: 75, Loss: 148.41327, Residuals: -1.45425, Convergence: 0.002746\n",
      "Epoch: 76, Loss: 148.01651, Residuals: -1.44088, Convergence: 0.002681\n",
      "Epoch: 77, Loss: 147.63021, Residuals: -1.42786, Convergence: 0.002617\n",
      "Epoch: 78, Loss: 147.25424, Residuals: -1.41518, Convergence: 0.002553\n",
      "Epoch: 79, Loss: 146.88853, Residuals: -1.40285, Convergence: 0.002490\n",
      "Epoch: 80, Loss: 146.53306, Residuals: -1.39087, Convergence: 0.002426\n",
      "Epoch: 81, Loss: 146.18785, Residuals: -1.37922, Convergence: 0.002361\n",
      "Epoch: 82, Loss: 145.85295, Residuals: -1.36791, Convergence: 0.002296\n",
      "Epoch: 83, Loss: 145.52838, Residuals: -1.35695, Convergence: 0.002230\n",
      "Epoch: 84, Loss: 145.21419, Residuals: -1.34631, Convergence: 0.002164\n",
      "Epoch: 85, Loss: 144.91038, Residuals: -1.33602, Convergence: 0.002097\n",
      "Epoch: 86, Loss: 144.61695, Residuals: -1.32606, Convergence: 0.002029\n",
      "Epoch: 87, Loss: 144.33387, Residuals: -1.31643, Convergence: 0.001961\n",
      "Epoch: 88, Loss: 144.06108, Residuals: -1.30712, Convergence: 0.001894\n",
      "Epoch: 89, Loss: 143.79849, Residuals: -1.29814, Convergence: 0.001826\n",
      "Epoch: 90, Loss: 143.54601, Residuals: -1.28948, Convergence: 0.001759\n",
      "Epoch: 91, Loss: 143.30347, Residuals: -1.28114, Convergence: 0.001692\n",
      "Epoch: 92, Loss: 143.07073, Residuals: -1.27311, Convergence: 0.001627\n",
      "Epoch: 93, Loss: 142.84761, Residuals: -1.26538, Convergence: 0.001562\n",
      "Epoch: 94, Loss: 142.63390, Residuals: -1.25795, Convergence: 0.001498\n",
      "Epoch: 95, Loss: 142.42941, Residuals: -1.25081, Convergence: 0.001436\n",
      "Epoch: 96, Loss: 142.23391, Residuals: -1.24395, Convergence: 0.001375\n",
      "Epoch: 97, Loss: 142.04717, Residuals: -1.23738, Convergence: 0.001315\n",
      "Epoch: 98, Loss: 141.86898, Residuals: -1.23107, Convergence: 0.001256\n",
      "Epoch: 99, Loss: 141.69912, Residuals: -1.22503, Convergence: 0.001199\n",
      "Epoch: 100, Loss: 141.53737, Residuals: -1.21925, Convergence: 0.001143\n",
      "Epoch: 101, Loss: 141.38351, Residuals: -1.21371, Convergence: 0.001088\n",
      "Epoch: 102, Loss: 141.23736, Residuals: -1.20843, Convergence: 0.001035\n",
      "Epoch: 103, Loss: 141.09871, Residuals: -1.20338, Convergence: 0.000983\n",
      "Evidence -182.740\n",
      "\n",
      "Epoch: 103, Evidence: -182.73984, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.25e-01\n",
      "Epoch: 103, Loss: 1383.31006, Residuals: -1.20338, Convergence:   inf\n",
      "Epoch: 104, Loss: 1318.73425, Residuals: -1.23331, Convergence: 0.048968\n",
      "Epoch: 105, Loss: 1268.08726, Residuals: -1.25757, Convergence: 0.039940\n",
      "Epoch: 106, Loss: 1228.89294, Residuals: -1.27591, Convergence: 0.031894\n",
      "Epoch: 107, Loss: 1198.16605, Residuals: -1.28931, Convergence: 0.025645\n",
      "Epoch: 108, Loss: 1173.40802, Residuals: -1.29939, Convergence: 0.021099\n",
      "Epoch: 109, Loss: 1153.00016, Residuals: -1.30719, Convergence: 0.017700\n",
      "Epoch: 110, Loss: 1135.90631, Residuals: -1.31325, Convergence: 0.015049\n",
      "Epoch: 111, Loss: 1121.40918, Residuals: -1.31783, Convergence: 0.012928\n",
      "Epoch: 112, Loss: 1108.97815, Residuals: -1.32113, Convergence: 0.011209\n",
      "Epoch: 113, Loss: 1098.20248, Residuals: -1.32327, Convergence: 0.009812\n",
      "Epoch: 114, Loss: 1088.75671, Residuals: -1.32438, Convergence: 0.008676\n",
      "Epoch: 115, Loss: 1080.37810, Residuals: -1.32454, Convergence: 0.007755\n",
      "Epoch: 116, Loss: 1072.85183, Residuals: -1.32384, Convergence: 0.007015\n",
      "Epoch: 117, Loss: 1066.00190, Residuals: -1.32234, Convergence: 0.006426\n",
      "Epoch: 118, Loss: 1059.68329, Residuals: -1.32011, Convergence: 0.005963\n",
      "Epoch: 119, Loss: 1053.77603, Residuals: -1.31721, Convergence: 0.005606\n",
      "Epoch: 120, Loss: 1048.18031, Residuals: -1.31366, Convergence: 0.005339\n",
      "Epoch: 121, Loss: 1042.81090, Residuals: -1.30951, Convergence: 0.005149\n",
      "Epoch: 122, Loss: 1037.59301, Residuals: -1.30477, Convergence: 0.005029\n",
      "Epoch: 123, Loss: 1032.45675, Residuals: -1.29947, Convergence: 0.004975\n",
      "Epoch: 124, Loss: 1027.33745, Residuals: -1.29363, Convergence: 0.004983\n",
      "Epoch: 125, Loss: 1022.18205, Residuals: -1.28727, Convergence: 0.005044\n",
      "Epoch: 126, Loss: 1016.96391, Residuals: -1.28045, Convergence: 0.005131\n",
      "Epoch: 127, Loss: 1011.69911, Residuals: -1.27326, Convergence: 0.005204\n",
      "Epoch: 128, Loss: 1006.44952, Residuals: -1.26578, Convergence: 0.005216\n",
      "Epoch: 129, Loss: 1001.30618, Residuals: -1.25812, Convergence: 0.005137\n",
      "Epoch: 130, Loss: 996.35772, Residuals: -1.25035, Convergence: 0.004967\n",
      "Epoch: 131, Loss: 991.66426, Residuals: -1.24254, Convergence: 0.004733\n",
      "Epoch: 132, Loss: 987.25268, Residuals: -1.23473, Convergence: 0.004469\n",
      "Epoch: 133, Loss: 983.12287, Residuals: -1.22697, Convergence: 0.004201\n",
      "Epoch: 134, Loss: 979.25863, Residuals: -1.21929, Convergence: 0.003946\n",
      "Epoch: 135, Loss: 975.63880, Residuals: -1.21173, Convergence: 0.003710\n",
      "Epoch: 136, Loss: 972.23987, Residuals: -1.20433, Convergence: 0.003496\n",
      "Epoch: 137, Loss: 969.04047, Residuals: -1.19710, Convergence: 0.003302\n",
      "Epoch: 138, Loss: 966.02242, Residuals: -1.19007, Convergence: 0.003124\n",
      "Epoch: 139, Loss: 963.16947, Residuals: -1.18325, Convergence: 0.002962\n",
      "Epoch: 140, Loss: 960.46813, Residuals: -1.17665, Convergence: 0.002813\n",
      "Epoch: 141, Loss: 957.90656, Residuals: -1.17029, Convergence: 0.002674\n",
      "Epoch: 142, Loss: 955.47461, Residuals: -1.16416, Convergence: 0.002545\n",
      "Epoch: 143, Loss: 953.16337, Residuals: -1.15826, Convergence: 0.002425\n",
      "Epoch: 144, Loss: 950.96467, Residuals: -1.15261, Convergence: 0.002312\n",
      "Epoch: 145, Loss: 948.87082, Residuals: -1.14719, Convergence: 0.002207\n",
      "Epoch: 146, Loss: 946.87469, Residuals: -1.14199, Convergence: 0.002108\n",
      "Epoch: 147, Loss: 944.96977, Residuals: -1.13703, Convergence: 0.002016\n",
      "Epoch: 148, Loss: 943.14951, Residuals: -1.13227, Convergence: 0.001930\n",
      "Epoch: 149, Loss: 941.40767, Residuals: -1.12773, Convergence: 0.001850\n",
      "Epoch: 150, Loss: 939.73837, Residuals: -1.12339, Convergence: 0.001776\n",
      "Epoch: 151, Loss: 938.13548, Residuals: -1.11924, Convergence: 0.001709\n",
      "Epoch: 152, Loss: 936.59302, Residuals: -1.11526, Convergence: 0.001647\n",
      "Epoch: 153, Loss: 935.10556, Residuals: -1.11146, Convergence: 0.001591\n",
      "Epoch: 154, Loss: 933.66691, Residuals: -1.10781, Convergence: 0.001541\n",
      "Epoch: 155, Loss: 932.27143, Residuals: -1.10431, Convergence: 0.001497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 156, Loss: 930.91310, Residuals: -1.10095, Convergence: 0.001459\n",
      "Epoch: 157, Loss: 929.58582, Residuals: -1.09771, Convergence: 0.001428\n",
      "Epoch: 158, Loss: 928.28369, Residuals: -1.09459, Convergence: 0.001403\n",
      "Epoch: 159, Loss: 927.00064, Residuals: -1.09156, Convergence: 0.001384\n",
      "Epoch: 160, Loss: 925.73119, Residuals: -1.08862, Convergence: 0.001371\n",
      "Epoch: 161, Loss: 924.47045, Residuals: -1.08575, Convergence: 0.001364\n",
      "Epoch: 162, Loss: 923.21388, Residuals: -1.08295, Convergence: 0.001361\n",
      "Epoch: 163, Loss: 921.95965, Residuals: -1.08020, Convergence: 0.001360\n",
      "Epoch: 164, Loss: 920.70716, Residuals: -1.07751, Convergence: 0.001360\n",
      "Epoch: 165, Loss: 919.45876, Residuals: -1.07486, Convergence: 0.001358\n",
      "Epoch: 166, Loss: 918.21894, Residuals: -1.07226, Convergence: 0.001350\n",
      "Epoch: 167, Loss: 916.99447, Residuals: -1.06972, Convergence: 0.001335\n",
      "Epoch: 168, Loss: 915.79373, Residuals: -1.06723, Convergence: 0.001311\n",
      "Epoch: 169, Loss: 914.62516, Residuals: -1.06483, Convergence: 0.001278\n",
      "Epoch: 170, Loss: 913.49604, Residuals: -1.06250, Convergence: 0.001236\n",
      "Epoch: 171, Loss: 912.41277, Residuals: -1.06027, Convergence: 0.001187\n",
      "Epoch: 172, Loss: 911.37962, Residuals: -1.05813, Convergence: 0.001134\n",
      "Epoch: 173, Loss: 910.39918, Residuals: -1.05609, Convergence: 0.001077\n",
      "Epoch: 174, Loss: 909.47161, Residuals: -1.05414, Convergence: 0.001020\n",
      "Epoch: 175, Loss: 908.59597, Residuals: -1.05229, Convergence: 0.000964\n",
      "Evidence 11317.314\n",
      "\n",
      "Epoch: 175, Evidence: 11317.31445, Convergence: 1.016147\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 5.77e-01\n",
      "Epoch: 175, Loss: 2351.61258, Residuals: -1.05229, Convergence:   inf\n",
      "Epoch: 176, Loss: 2312.66115, Residuals: -1.06370, Convergence: 0.016843\n",
      "Epoch: 177, Loss: 2287.36907, Residuals: -1.06359, Convergence: 0.011057\n",
      "Epoch: 178, Loss: 2266.50751, Residuals: -1.06252, Convergence: 0.009204\n",
      "Epoch: 179, Loss: 2248.98976, Residuals: -1.06109, Convergence: 0.007789\n",
      "Epoch: 180, Loss: 2234.10804, Residuals: -1.05945, Convergence: 0.006661\n",
      "Epoch: 181, Loss: 2221.33005, Residuals: -1.05767, Convergence: 0.005752\n",
      "Epoch: 182, Loss: 2210.23391, Residuals: -1.05575, Convergence: 0.005020\n",
      "Epoch: 183, Loss: 2200.47823, Residuals: -1.05370, Convergence: 0.004433\n",
      "Epoch: 184, Loss: 2191.78695, Residuals: -1.05152, Convergence: 0.003965\n",
      "Epoch: 185, Loss: 2183.94850, Residuals: -1.04919, Convergence: 0.003589\n",
      "Epoch: 186, Loss: 2176.80592, Residuals: -1.04672, Convergence: 0.003281\n",
      "Epoch: 187, Loss: 2170.25029, Residuals: -1.04413, Convergence: 0.003021\n",
      "Epoch: 188, Loss: 2164.20921, Residuals: -1.04145, Convergence: 0.002791\n",
      "Epoch: 189, Loss: 2158.63124, Residuals: -1.03873, Convergence: 0.002584\n",
      "Epoch: 190, Loss: 2153.47327, Residuals: -1.03599, Convergence: 0.002395\n",
      "Epoch: 191, Loss: 2148.69849, Residuals: -1.03327, Convergence: 0.002222\n",
      "Epoch: 192, Loss: 2144.27271, Residuals: -1.03059, Convergence: 0.002064\n",
      "Epoch: 193, Loss: 2140.16469, Residuals: -1.02798, Convergence: 0.001919\n",
      "Epoch: 194, Loss: 2136.34766, Residuals: -1.02544, Convergence: 0.001787\n",
      "Epoch: 195, Loss: 2132.79702, Residuals: -1.02299, Convergence: 0.001665\n",
      "Epoch: 196, Loss: 2129.49049, Residuals: -1.02062, Convergence: 0.001553\n",
      "Epoch: 197, Loss: 2126.41042, Residuals: -1.01835, Convergence: 0.001448\n",
      "Epoch: 198, Loss: 2123.53827, Residuals: -1.01616, Convergence: 0.001353\n",
      "Epoch: 199, Loss: 2120.85940, Residuals: -1.01407, Convergence: 0.001263\n",
      "Epoch: 200, Loss: 2118.35867, Residuals: -1.01206, Convergence: 0.001181\n",
      "Epoch: 201, Loss: 2116.02223, Residuals: -1.01014, Convergence: 0.001104\n",
      "Epoch: 202, Loss: 2113.83868, Residuals: -1.00829, Convergence: 0.001033\n",
      "Epoch: 203, Loss: 2111.79559, Residuals: -1.00652, Convergence: 0.000967\n",
      "Evidence 14480.038\n",
      "\n",
      "Epoch: 203, Evidence: 14480.03809, Convergence: 0.218420\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 4.41e-01\n",
      "Epoch: 203, Loss: 2475.38832, Residuals: -1.00652, Convergence:   inf\n",
      "Epoch: 204, Loss: 2462.19631, Residuals: -1.00474, Convergence: 0.005358\n",
      "Epoch: 205, Loss: 2451.53162, Residuals: -1.00200, Convergence: 0.004350\n",
      "Epoch: 206, Loss: 2442.34048, Residuals: -0.99925, Convergence: 0.003763\n",
      "Epoch: 207, Loss: 2434.36442, Residuals: -0.99663, Convergence: 0.003276\n",
      "Epoch: 208, Loss: 2427.41122, Residuals: -0.99419, Convergence: 0.002864\n",
      "Epoch: 209, Loss: 2421.32459, Residuals: -0.99193, Convergence: 0.002514\n",
      "Epoch: 210, Loss: 2415.97226, Residuals: -0.98983, Convergence: 0.002215\n",
      "Epoch: 211, Loss: 2411.24211, Residuals: -0.98788, Convergence: 0.001962\n",
      "Epoch: 212, Loss: 2407.04183, Residuals: -0.98605, Convergence: 0.001745\n",
      "Epoch: 213, Loss: 2403.29140, Residuals: -0.98435, Convergence: 0.001561\n",
      "Epoch: 214, Loss: 2399.92356, Residuals: -0.98276, Convergence: 0.001403\n",
      "Epoch: 215, Loss: 2396.88353, Residuals: -0.98127, Convergence: 0.001268\n",
      "Epoch: 216, Loss: 2394.12443, Residuals: -0.97987, Convergence: 0.001152\n",
      "Epoch: 217, Loss: 2391.60729, Residuals: -0.97856, Convergence: 0.001052\n",
      "Epoch: 218, Loss: 2389.29945, Residuals: -0.97733, Convergence: 0.000966\n",
      "Evidence 14831.030\n",
      "\n",
      "Epoch: 218, Evidence: 14831.03027, Convergence: 0.023666\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 3.40e-01\n",
      "Epoch: 218, Loss: 2480.55920, Residuals: -0.97733, Convergence:   inf\n",
      "Epoch: 219, Loss: 2473.88602, Residuals: -0.97455, Convergence: 0.002697\n",
      "Epoch: 220, Loss: 2468.34166, Residuals: -0.97194, Convergence: 0.002246\n",
      "Epoch: 221, Loss: 2463.61082, Residuals: -0.96965, Convergence: 0.001920\n",
      "Epoch: 222, Loss: 2459.52655, Residuals: -0.96766, Convergence: 0.001661\n",
      "Epoch: 223, Loss: 2455.96572, Residuals: -0.96591, Convergence: 0.001450\n",
      "Epoch: 224, Loss: 2452.83185, Residuals: -0.96436, Convergence: 0.001278\n",
      "Epoch: 225, Loss: 2450.04875, Residuals: -0.96297, Convergence: 0.001136\n",
      "Epoch: 226, Loss: 2447.55477, Residuals: -0.96173, Convergence: 0.001019\n",
      "Epoch: 227, Loss: 2445.30215, Residuals: -0.96060, Convergence: 0.000921\n",
      "Evidence 14914.729\n",
      "\n",
      "Epoch: 227, Evidence: 14914.72852, Convergence: 0.005612\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.69e-01\n",
      "Epoch: 227, Loss: 2481.90830, Residuals: -0.96060, Convergence:   inf\n",
      "Epoch: 228, Loss: 2477.79708, Residuals: -0.95835, Convergence: 0.001659\n",
      "Epoch: 229, Loss: 2474.35687, Residuals: -0.95647, Convergence: 0.001390\n",
      "Epoch: 230, Loss: 2471.40678, Residuals: -0.95491, Convergence: 0.001194\n",
      "Epoch: 231, Loss: 2468.84219, Residuals: -0.95361, Convergence: 0.001039\n",
      "Epoch: 232, Loss: 2466.58450, Residuals: -0.95250, Convergence: 0.000915\n",
      "Evidence 14947.615\n",
      "\n",
      "Epoch: 232, Evidence: 14947.61523, Convergence: 0.002200\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.18e-01\n",
      "Epoch: 232, Loss: 2482.81308, Residuals: -0.95250, Convergence:   inf\n",
      "Epoch: 233, Loss: 2479.78897, Residuals: -0.95082, Convergence: 0.001220\n",
      "Epoch: 234, Loss: 2477.24977, Residuals: -0.94951, Convergence: 0.001025\n",
      "Epoch: 235, Loss: 2475.06166, Residuals: -0.94845, Convergence: 0.000884\n",
      "Evidence 14963.417\n",
      "\n",
      "Epoch: 235, Evidence: 14963.41699, Convergence: 0.001056\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.81e-01\n",
      "Epoch: 235, Loss: 2483.46603, Residuals: -0.94845, Convergence:   inf\n",
      "Epoch: 236, Loss: 2481.01217, Residuals: -0.94713, Convergence: 0.000989\n",
      "Evidence 14970.096\n",
      "\n",
      "Epoch: 236, Evidence: 14970.09570, Convergence: 0.000446\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.55e-01\n",
      "Epoch: 236, Loss: 2483.98630, Residuals: -0.94713, Convergence:   inf\n",
      "Epoch: 237, Loss: 2479.82112, Residuals: -0.94561, Convergence: 0.001680\n",
      "Epoch: 238, Loss: 2476.65818, Residuals: -0.94460, Convergence: 0.001277\n",
      "Epoch: 239, Loss: 2474.12658, Residuals: -0.94371, Convergence: 0.001023\n",
      "Epoch: 240, Loss: 2472.01019, Residuals: -0.94310, Convergence: 0.000856\n",
      "Evidence 14985.971\n",
      "\n",
      "Epoch: 240, Evidence: 14985.97070, Convergence: 0.001505\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.31e-01\n",
      "Epoch: 240, Loss: 2484.25834, Residuals: -0.94310, Convergence:   inf\n",
      "Epoch: 241, Loss: 2481.49674, Residuals: -0.94178, Convergence: 0.001113\n",
      "Epoch: 242, Loss: 2479.31604, Residuals: -0.94107, Convergence: 0.000880\n",
      "Evidence 14996.098\n",
      "\n",
      "Epoch: 242, Evidence: 14996.09766, Convergence: 0.000675\n",
      "Updating hyper-parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 183, Updated regularization: 1.13e-01\n",
      "Epoch: 242, Loss: 2484.38077, Residuals: -0.94107, Convergence:   inf\n",
      "Epoch: 243, Loss: 2480.35873, Residuals: -0.93985, Convergence: 0.001622\n",
      "Epoch: 244, Loss: 2477.40671, Residuals: -0.94024, Convergence: 0.001192\n",
      "Epoch: 245, Loss: 2474.99633, Residuals: -0.94024, Convergence: 0.000974\n",
      "Evidence 15008.717\n",
      "\n",
      "Epoch: 245, Evidence: 15008.71680, Convergence: 0.001516\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 9.42e-02\n",
      "Epoch: 245, Loss: 2484.40845, Residuals: -0.94024, Convergence:   inf\n",
      "Epoch: 246, Loss: 2481.75553, Residuals: -0.93902, Convergence: 0.001069\n",
      "Epoch: 247, Loss: 2480.39297, Residuals: -0.94088, Convergence: 0.000549\n",
      "Evidence 15016.564\n",
      "\n",
      "Epoch: 247, Evidence: 15016.56445, Convergence: 0.000523\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.83e-02\n",
      "Epoch: 247, Loss: 2484.35846, Residuals: -0.94088, Convergence:   inf\n",
      "Epoch: 248, Loss: 2532.44368, Residuals: -0.97633, Convergence: -0.018988\n",
      "Epoch: 248, Loss: 2482.07165, Residuals: -0.93740, Convergence: 0.000921\n",
      "Evidence 15021.372\n",
      "\n",
      "Epoch: 248, Evidence: 15021.37207, Convergence: 0.000842\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.12e-02\n",
      "Epoch: 248, Loss: 2484.21480, Residuals: -0.93740, Convergence:   inf\n",
      "Epoch: 249, Loss: 2483.56053, Residuals: -0.93344, Convergence: 0.000263\n",
      "Evidence 15023.469\n",
      "\n",
      "Epoch: 249, Evidence: 15023.46875, Convergence: 0.000982\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 382.10683, Residuals: -4.54140, Convergence:   inf\n",
      "Epoch: 1, Loss: 356.39844, Residuals: -4.42167, Convergence: 0.072134\n",
      "Epoch: 2, Loss: 335.33091, Residuals: -4.25731, Convergence: 0.062826\n",
      "Epoch: 3, Loss: 319.25878, Residuals: -4.09271, Convergence: 0.050342\n",
      "Epoch: 4, Loss: 307.00371, Residuals: -3.94757, Convergence: 0.039918\n",
      "Epoch: 5, Loss: 297.28487, Residuals: -3.81913, Convergence: 0.032692\n",
      "Epoch: 6, Loss: 289.39619, Residuals: -3.70713, Convergence: 0.027259\n",
      "Epoch: 7, Loss: 282.85019, Residuals: -3.61116, Convergence: 0.023143\n",
      "Epoch: 8, Loss: 277.28542, Residuals: -3.52922, Convergence: 0.020069\n",
      "Epoch: 9, Loss: 272.44672, Residuals: -3.45900, Convergence: 0.017760\n",
      "Epoch: 10, Loss: 268.15307, Residuals: -3.39840, Convergence: 0.016012\n",
      "Epoch: 11, Loss: 264.27364, Residuals: -3.34562, Convergence: 0.014680\n",
      "Epoch: 12, Loss: 260.71319, Residuals: -3.29913, Convergence: 0.013657\n",
      "Epoch: 13, Loss: 257.40238, Residuals: -3.25760, Convergence: 0.012862\n",
      "Epoch: 14, Loss: 254.29112, Residuals: -3.21984, Convergence: 0.012235\n",
      "Epoch: 15, Loss: 251.34490, Residuals: -3.18487, Convergence: 0.011722\n",
      "Epoch: 16, Loss: 248.54363, Residuals: -3.15201, Convergence: 0.011271\n",
      "Epoch: 17, Loss: 245.87557, Residuals: -3.12085, Convergence: 0.010851\n",
      "Epoch: 18, Loss: 243.32405, Residuals: -3.09111, Convergence: 0.010486\n",
      "Epoch: 19, Loss: 240.86045, Residuals: -3.06235, Convergence: 0.010228\n",
      "Epoch: 20, Loss: 238.44866, Residuals: -3.03403, Convergence: 0.010114\n",
      "Epoch: 21, Loss: 236.05277, Residuals: -3.00559, Convergence: 0.010150\n",
      "Epoch: 22, Loss: 233.64284, Residuals: -2.97653, Convergence: 0.010315\n",
      "Epoch: 23, Loss: 231.19403, Residuals: -2.94652, Convergence: 0.010592\n",
      "Epoch: 24, Loss: 228.67409, Residuals: -2.91514, Convergence: 0.011020\n",
      "Epoch: 25, Loss: 226.03287, Residuals: -2.88182, Convergence: 0.011685\n",
      "Epoch: 26, Loss: 223.23449, Residuals: -2.84610, Convergence: 0.012536\n",
      "Epoch: 27, Loss: 220.34829, Residuals: -2.80863, Convergence: 0.013098\n",
      "Epoch: 28, Loss: 217.51136, Residuals: -2.77098, Convergence: 0.013043\n",
      "Epoch: 29, Loss: 214.77949, Residuals: -2.73396, Convergence: 0.012719\n",
      "Epoch: 30, Loss: 212.14615, Residuals: -2.69765, Convergence: 0.012413\n",
      "Epoch: 31, Loss: 209.59305, Residuals: -2.66194, Convergence: 0.012181\n",
      "Epoch: 32, Loss: 207.10472, Residuals: -2.62672, Convergence: 0.012015\n",
      "Epoch: 33, Loss: 204.67033, Residuals: -2.59185, Convergence: 0.011894\n",
      "Epoch: 34, Loss: 202.28313, Residuals: -2.55726, Convergence: 0.011801\n",
      "Epoch: 35, Loss: 199.93949, Residuals: -2.52285, Convergence: 0.011722\n",
      "Epoch: 36, Loss: 197.63798, Residuals: -2.48859, Convergence: 0.011645\n",
      "Epoch: 37, Loss: 195.37871, Residuals: -2.45443, Convergence: 0.011564\n",
      "Epoch: 38, Loss: 193.16268, Residuals: -2.42034, Convergence: 0.011472\n",
      "Epoch: 39, Loss: 190.99146, Residuals: -2.38633, Convergence: 0.011368\n",
      "Epoch: 40, Loss: 188.86686, Residuals: -2.35238, Convergence: 0.011249\n",
      "Epoch: 41, Loss: 186.79082, Residuals: -2.31852, Convergence: 0.011114\n",
      "Epoch: 42, Loss: 184.76542, Residuals: -2.28477, Convergence: 0.010962\n",
      "Epoch: 43, Loss: 182.79287, Residuals: -2.25115, Convergence: 0.010791\n",
      "Epoch: 44, Loss: 180.87559, Residuals: -2.21771, Convergence: 0.010600\n",
      "Epoch: 45, Loss: 179.01623, Residuals: -2.18450, Convergence: 0.010387\n",
      "Epoch: 46, Loss: 177.21756, Residuals: -2.15158, Convergence: 0.010150\n",
      "Epoch: 47, Loss: 175.48218, Residuals: -2.11902, Convergence: 0.009889\n",
      "Epoch: 48, Loss: 173.81224, Residuals: -2.08689, Convergence: 0.009608\n",
      "Epoch: 49, Loss: 172.20914, Residuals: -2.05525, Convergence: 0.009309\n",
      "Epoch: 50, Loss: 170.67332, Residuals: -2.02417, Convergence: 0.008999\n",
      "Epoch: 51, Loss: 169.20422, Residuals: -1.99370, Convergence: 0.008682\n",
      "Epoch: 52, Loss: 167.80031, Residuals: -1.96388, Convergence: 0.008367\n",
      "Epoch: 53, Loss: 166.45920, Residuals: -1.93474, Convergence: 0.008057\n",
      "Epoch: 54, Loss: 165.17788, Residuals: -1.90628, Convergence: 0.007757\n",
      "Epoch: 55, Loss: 163.95296, Residuals: -1.87851, Convergence: 0.007471\n",
      "Epoch: 56, Loss: 162.78090, Residuals: -1.85142, Convergence: 0.007200\n",
      "Epoch: 57, Loss: 161.65833, Residuals: -1.82498, Convergence: 0.006944\n",
      "Epoch: 58, Loss: 160.58220, Residuals: -1.79919, Convergence: 0.006701\n",
      "Epoch: 59, Loss: 159.54989, Residuals: -1.77403, Convergence: 0.006470\n",
      "Epoch: 60, Loss: 158.55932, Residuals: -1.74948, Convergence: 0.006247\n",
      "Epoch: 61, Loss: 157.60879, Residuals: -1.72554, Convergence: 0.006031\n",
      "Epoch: 62, Loss: 156.69702, Residuals: -1.70221, Convergence: 0.005819\n",
      "Epoch: 63, Loss: 155.82297, Residuals: -1.67950, Convergence: 0.005609\n",
      "Epoch: 64, Loss: 154.98577, Residuals: -1.65740, Convergence: 0.005402\n",
      "Epoch: 65, Loss: 154.18464, Residuals: -1.63593, Convergence: 0.005196\n",
      "Epoch: 66, Loss: 153.41886, Residuals: -1.61510, Convergence: 0.004991\n",
      "Epoch: 67, Loss: 152.68765, Residuals: -1.59490, Convergence: 0.004789\n",
      "Epoch: 68, Loss: 151.99022, Residuals: -1.57535, Convergence: 0.004589\n",
      "Epoch: 69, Loss: 151.32572, Residuals: -1.55644, Convergence: 0.004391\n",
      "Epoch: 70, Loss: 150.69321, Residuals: -1.53818, Convergence: 0.004197\n",
      "Epoch: 71, Loss: 150.09169, Residuals: -1.52055, Convergence: 0.004008\n",
      "Epoch: 72, Loss: 149.52013, Residuals: -1.50357, Convergence: 0.003823\n",
      "Epoch: 73, Loss: 148.97739, Residuals: -1.48721, Convergence: 0.003643\n",
      "Epoch: 74, Loss: 148.46232, Residuals: -1.47148, Convergence: 0.003469\n",
      "Epoch: 75, Loss: 147.97378, Residuals: -1.45635, Convergence: 0.003302\n",
      "Epoch: 76, Loss: 147.51054, Residuals: -1.44181, Convergence: 0.003140\n",
      "Epoch: 77, Loss: 147.07144, Residuals: -1.42786, Convergence: 0.002986\n",
      "Epoch: 78, Loss: 146.65528, Residuals: -1.41447, Convergence: 0.002838\n",
      "Epoch: 79, Loss: 146.26095, Residuals: -1.40162, Convergence: 0.002696\n",
      "Epoch: 80, Loss: 145.88733, Residuals: -1.38931, Convergence: 0.002561\n",
      "Epoch: 81, Loss: 145.53332, Residuals: -1.37750, Convergence: 0.002432\n",
      "Epoch: 82, Loss: 145.19792, Residuals: -1.36619, Convergence: 0.002310\n",
      "Epoch: 83, Loss: 144.88015, Residuals: -1.35535, Convergence: 0.002193\n",
      "Epoch: 84, Loss: 144.57908, Residuals: -1.34496, Convergence: 0.002082\n",
      "Epoch: 85, Loss: 144.29384, Residuals: -1.33502, Convergence: 0.001977\n",
      "Epoch: 86, Loss: 144.02361, Residuals: -1.32549, Convergence: 0.001876\n",
      "Epoch: 87, Loss: 143.76763, Residuals: -1.31636, Convergence: 0.001781\n",
      "Epoch: 88, Loss: 143.52517, Residuals: -1.30762, Convergence: 0.001689\n",
      "Epoch: 89, Loss: 143.29557, Residuals: -1.29925, Convergence: 0.001602\n",
      "Epoch: 90, Loss: 143.07823, Residuals: -1.29123, Convergence: 0.001519\n",
      "Epoch: 91, Loss: 142.87255, Residuals: -1.28354, Convergence: 0.001440\n",
      "Epoch: 92, Loss: 142.67801, Residuals: -1.27618, Convergence: 0.001363\n",
      "Epoch: 93, Loss: 142.49414, Residuals: -1.26913, Convergence: 0.001290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 94, Loss: 142.32047, Residuals: -1.26238, Convergence: 0.001220\n",
      "Epoch: 95, Loss: 142.15659, Residuals: -1.25591, Convergence: 0.001153\n",
      "Epoch: 96, Loss: 142.00213, Residuals: -1.24972, Convergence: 0.001088\n",
      "Epoch: 97, Loss: 141.85674, Residuals: -1.24379, Convergence: 0.001025\n",
      "Epoch: 98, Loss: 141.72007, Residuals: -1.23813, Convergence: 0.000964\n",
      "Evidence -183.819\n",
      "\n",
      "Epoch: 98, Evidence: -183.81873, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.25e-01\n",
      "Epoch: 98, Loss: 1355.11906, Residuals: -1.23813, Convergence:   inf\n",
      "Epoch: 99, Loss: 1292.26612, Residuals: -1.26958, Convergence: 0.048638\n",
      "Epoch: 100, Loss: 1245.02480, Residuals: -1.29392, Convergence: 0.037944\n",
      "Epoch: 101, Loss: 1209.74835, Residuals: -1.31098, Convergence: 0.029160\n",
      "Epoch: 102, Loss: 1182.47574, Residuals: -1.32267, Convergence: 0.023064\n",
      "Epoch: 103, Loss: 1160.47078, Residuals: -1.33116, Convergence: 0.018962\n",
      "Epoch: 104, Loss: 1142.21998, Residuals: -1.33757, Convergence: 0.015978\n",
      "Epoch: 105, Loss: 1126.82613, Residuals: -1.34236, Convergence: 0.013661\n",
      "Epoch: 106, Loss: 1113.68115, Residuals: -1.34576, Convergence: 0.011803\n",
      "Epoch: 107, Loss: 1102.33312, Residuals: -1.34794, Convergence: 0.010295\n",
      "Epoch: 108, Loss: 1092.43028, Residuals: -1.34901, Convergence: 0.009065\n",
      "Epoch: 109, Loss: 1083.68942, Residuals: -1.34909, Convergence: 0.008066\n",
      "Epoch: 110, Loss: 1075.87887, Residuals: -1.34826, Convergence: 0.007260\n",
      "Epoch: 111, Loss: 1068.80680, Residuals: -1.34660, Convergence: 0.006617\n",
      "Epoch: 112, Loss: 1062.31122, Residuals: -1.34417, Convergence: 0.006115\n",
      "Epoch: 113, Loss: 1056.25716, Residuals: -1.34103, Convergence: 0.005732\n",
      "Epoch: 114, Loss: 1050.53134, Residuals: -1.33722, Convergence: 0.005450\n",
      "Epoch: 115, Loss: 1045.04030, Residuals: -1.33276, Convergence: 0.005254\n",
      "Epoch: 116, Loss: 1039.70867, Residuals: -1.32768, Convergence: 0.005128\n",
      "Epoch: 117, Loss: 1034.47696, Residuals: -1.32201, Convergence: 0.005057\n",
      "Epoch: 118, Loss: 1029.30100, Residuals: -1.31577, Convergence: 0.005029\n",
      "Epoch: 119, Loss: 1024.15359, Residuals: -1.30900, Convergence: 0.005026\n",
      "Epoch: 120, Loss: 1019.03112, Residuals: -1.30177, Convergence: 0.005027\n",
      "Epoch: 121, Loss: 1013.95827, Residuals: -1.29415, Convergence: 0.005003\n",
      "Epoch: 122, Loss: 1008.98809, Residuals: -1.28623, Convergence: 0.004926\n",
      "Epoch: 123, Loss: 1004.18603, Residuals: -1.27812, Convergence: 0.004782\n",
      "Epoch: 124, Loss: 999.61203, Residuals: -1.26989, Convergence: 0.004576\n",
      "Epoch: 125, Loss: 995.30330, Residuals: -1.26163, Convergence: 0.004329\n",
      "Epoch: 126, Loss: 991.27126, Residuals: -1.25339, Convergence: 0.004068\n",
      "Epoch: 127, Loss: 987.50800, Residuals: -1.24524, Convergence: 0.003811\n",
      "Epoch: 128, Loss: 983.99477, Residuals: -1.23720, Convergence: 0.003570\n",
      "Epoch: 129, Loss: 980.70813, Residuals: -1.22931, Convergence: 0.003351\n",
      "Epoch: 130, Loss: 977.62483, Residuals: -1.22161, Convergence: 0.003154\n",
      "Epoch: 131, Loss: 974.72435, Residuals: -1.21410, Convergence: 0.002976\n",
      "Epoch: 132, Loss: 971.98889, Residuals: -1.20681, Convergence: 0.002814\n",
      "Epoch: 133, Loss: 969.40282, Residuals: -1.19976, Convergence: 0.002668\n",
      "Epoch: 134, Loss: 966.95382, Residuals: -1.19294, Convergence: 0.002533\n",
      "Epoch: 135, Loss: 964.63121, Residuals: -1.18638, Convergence: 0.002408\n",
      "Epoch: 136, Loss: 962.42588, Residuals: -1.18007, Convergence: 0.002291\n",
      "Epoch: 137, Loss: 960.32956, Residuals: -1.17402, Convergence: 0.002183\n",
      "Epoch: 138, Loss: 958.33556, Residuals: -1.16823, Convergence: 0.002081\n",
      "Epoch: 139, Loss: 956.43653, Residuals: -1.16270, Convergence: 0.001986\n",
      "Epoch: 140, Loss: 954.62693, Residuals: -1.15742, Convergence: 0.001896\n",
      "Epoch: 141, Loss: 952.90027, Residuals: -1.15239, Convergence: 0.001812\n",
      "Epoch: 142, Loss: 951.25127, Residuals: -1.14759, Convergence: 0.001734\n",
      "Epoch: 143, Loss: 949.67472, Residuals: -1.14303, Convergence: 0.001660\n",
      "Epoch: 144, Loss: 948.16520, Residuals: -1.13868, Convergence: 0.001592\n",
      "Epoch: 145, Loss: 946.71831, Residuals: -1.13455, Convergence: 0.001528\n",
      "Epoch: 146, Loss: 945.32933, Residuals: -1.13062, Convergence: 0.001469\n",
      "Epoch: 147, Loss: 943.99432, Residuals: -1.12688, Convergence: 0.001414\n",
      "Epoch: 148, Loss: 942.70928, Residuals: -1.12331, Convergence: 0.001363\n",
      "Epoch: 149, Loss: 941.47071, Residuals: -1.11992, Convergence: 0.001316\n",
      "Epoch: 150, Loss: 940.27562, Residuals: -1.11669, Convergence: 0.001271\n",
      "Epoch: 151, Loss: 939.12082, Residuals: -1.11360, Convergence: 0.001230\n",
      "Epoch: 152, Loss: 938.00421, Residuals: -1.11065, Convergence: 0.001190\n",
      "Epoch: 153, Loss: 936.92277, Residuals: -1.10784, Convergence: 0.001154\n",
      "Epoch: 154, Loss: 935.87474, Residuals: -1.10515, Convergence: 0.001120\n",
      "Epoch: 155, Loss: 934.85777, Residuals: -1.10257, Convergence: 0.001088\n",
      "Epoch: 156, Loss: 933.87013, Residuals: -1.10010, Convergence: 0.001058\n",
      "Epoch: 157, Loss: 932.91002, Residuals: -1.09773, Convergence: 0.001029\n",
      "Epoch: 158, Loss: 931.97579, Residuals: -1.09545, Convergence: 0.001002\n",
      "Epoch: 159, Loss: 931.06519, Residuals: -1.09325, Convergence: 0.000978\n",
      "Evidence 11022.450\n",
      "\n",
      "Epoch: 159, Evidence: 11022.45020, Convergence: 1.016677\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.79e-01\n",
      "Epoch: 159, Loss: 2337.29032, Residuals: -1.09325, Convergence:   inf\n",
      "Epoch: 160, Loss: 2297.62881, Residuals: -1.10270, Convergence: 0.017262\n",
      "Epoch: 161, Loss: 2269.48430, Residuals: -1.10198, Convergence: 0.012401\n",
      "Epoch: 162, Loss: 2245.91841, Residuals: -1.09990, Convergence: 0.010493\n",
      "Epoch: 163, Loss: 2225.86406, Residuals: -1.09737, Convergence: 0.009010\n",
      "Epoch: 164, Loss: 2208.63056, Residuals: -1.09454, Convergence: 0.007803\n",
      "Epoch: 165, Loss: 2193.68828, Residuals: -1.09147, Convergence: 0.006811\n",
      "Epoch: 166, Loss: 2180.61391, Residuals: -1.08820, Convergence: 0.005996\n",
      "Epoch: 167, Loss: 2169.06421, Residuals: -1.08476, Convergence: 0.005325\n",
      "Epoch: 168, Loss: 2158.75758, Residuals: -1.08117, Convergence: 0.004774\n",
      "Epoch: 169, Loss: 2149.46012, Residuals: -1.07747, Convergence: 0.004325\n",
      "Epoch: 170, Loss: 2140.98162, Residuals: -1.07365, Convergence: 0.003960\n",
      "Epoch: 171, Loss: 2133.17519, Residuals: -1.06972, Convergence: 0.003660\n",
      "Epoch: 172, Loss: 2125.93407, Residuals: -1.06570, Convergence: 0.003406\n",
      "Epoch: 173, Loss: 2119.19521, Residuals: -1.06162, Convergence: 0.003180\n",
      "Epoch: 174, Loss: 2112.92533, Residuals: -1.05751, Convergence: 0.002967\n",
      "Epoch: 175, Loss: 2107.10503, Residuals: -1.05345, Convergence: 0.002762\n",
      "Epoch: 176, Loss: 2101.71854, Residuals: -1.04946, Convergence: 0.002563\n",
      "Epoch: 177, Loss: 2096.74581, Residuals: -1.04559, Convergence: 0.002372\n",
      "Epoch: 178, Loss: 2092.16258, Residuals: -1.04187, Convergence: 0.002191\n",
      "Epoch: 179, Loss: 2087.94134, Residuals: -1.03833, Convergence: 0.002022\n",
      "Epoch: 180, Loss: 2084.05302, Residuals: -1.03496, Convergence: 0.001866\n",
      "Epoch: 181, Loss: 2080.46756, Residuals: -1.03177, Convergence: 0.001723\n",
      "Epoch: 182, Loss: 2077.15811, Residuals: -1.02877, Convergence: 0.001593\n",
      "Epoch: 183, Loss: 2074.09817, Residuals: -1.02595, Convergence: 0.001475\n",
      "Epoch: 184, Loss: 2071.26412, Residuals: -1.02330, Convergence: 0.001368\n",
      "Epoch: 185, Loss: 2068.63395, Residuals: -1.02082, Convergence: 0.001271\n",
      "Epoch: 186, Loss: 2066.18905, Residuals: -1.01850, Convergence: 0.001183\n",
      "Epoch: 187, Loss: 2063.91186, Residuals: -1.01633, Convergence: 0.001103\n",
      "Epoch: 188, Loss: 2061.78723, Residuals: -1.01430, Convergence: 0.001030\n",
      "Epoch: 189, Loss: 2059.80086, Residuals: -1.01240, Convergence: 0.000964\n",
      "Evidence 14225.363\n",
      "\n",
      "Epoch: 189, Evidence: 14225.36328, Convergence: 0.225155\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.41e-01\n",
      "Epoch: 189, Loss: 2466.02633, Residuals: -1.01240, Convergence:   inf\n",
      "Epoch: 190, Loss: 2451.51703, Residuals: -1.00951, Convergence: 0.005918\n",
      "Epoch: 191, Loss: 2439.80023, Residuals: -1.00573, Convergence: 0.004802\n",
      "Epoch: 192, Loss: 2429.79892, Residuals: -1.00198, Convergence: 0.004116\n",
      "Epoch: 193, Loss: 2421.17072, Residuals: -0.99841, Convergence: 0.003564\n",
      "Epoch: 194, Loss: 2413.66704, Residuals: -0.99506, Convergence: 0.003109\n",
      "Epoch: 195, Loss: 2407.09585, Residuals: -0.99194, Convergence: 0.002730\n",
      "Epoch: 196, Loss: 2401.30440, Residuals: -0.98903, Convergence: 0.002412\n",
      "Epoch: 197, Loss: 2396.16859, Residuals: -0.98634, Convergence: 0.002143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 198, Loss: 2391.58566, Residuals: -0.98383, Convergence: 0.001916\n",
      "Epoch: 199, Loss: 2387.47277, Residuals: -0.98150, Convergence: 0.001723\n",
      "Epoch: 200, Loss: 2383.75955, Residuals: -0.97933, Convergence: 0.001558\n",
      "Epoch: 201, Loss: 2380.38922, Residuals: -0.97731, Convergence: 0.001416\n",
      "Epoch: 202, Loss: 2377.31354, Residuals: -0.97542, Convergence: 0.001294\n",
      "Epoch: 203, Loss: 2374.49329, Residuals: -0.97365, Convergence: 0.001188\n",
      "Epoch: 204, Loss: 2371.89565, Residuals: -0.97200, Convergence: 0.001095\n",
      "Epoch: 205, Loss: 2369.49350, Residuals: -0.97045, Convergence: 0.001014\n",
      "Epoch: 206, Loss: 2367.26202, Residuals: -0.96900, Convergence: 0.000943\n",
      "Evidence 14659.744\n",
      "\n",
      "Epoch: 206, Evidence: 14659.74414, Convergence: 0.029631\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.37e-01\n",
      "Epoch: 206, Loss: 2471.54426, Residuals: -0.96900, Convergence:   inf\n",
      "Epoch: 207, Loss: 2465.05187, Residuals: -0.96562, Convergence: 0.002634\n",
      "Epoch: 208, Loss: 2459.72237, Residuals: -0.96243, Convergence: 0.002167\n",
      "Epoch: 209, Loss: 2455.19535, Residuals: -0.95957, Convergence: 0.001844\n",
      "Epoch: 210, Loss: 2451.27844, Residuals: -0.95703, Convergence: 0.001598\n",
      "Epoch: 211, Loss: 2447.84019, Residuals: -0.95475, Convergence: 0.001405\n",
      "Epoch: 212, Loss: 2444.78454, Residuals: -0.95271, Convergence: 0.001250\n",
      "Epoch: 213, Loss: 2442.04025, Residuals: -0.95088, Convergence: 0.001124\n",
      "Epoch: 214, Loss: 2439.55151, Residuals: -0.94923, Convergence: 0.001020\n",
      "Epoch: 215, Loss: 2437.27651, Residuals: -0.94773, Convergence: 0.000933\n",
      "Evidence 14746.847\n",
      "\n",
      "Epoch: 215, Evidence: 14746.84668, Convergence: 0.005907\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.64e-01\n",
      "Epoch: 215, Loss: 2473.22534, Residuals: -0.94773, Convergence:   inf\n",
      "Epoch: 216, Loss: 2469.45171, Residuals: -0.94493, Convergence: 0.001528\n",
      "Epoch: 217, Loss: 2466.29929, Residuals: -0.94251, Convergence: 0.001278\n",
      "Epoch: 218, Loss: 2463.56635, Residuals: -0.94041, Convergence: 0.001109\n",
      "Epoch: 219, Loss: 2461.14818, Residuals: -0.93859, Convergence: 0.000983\n",
      "Evidence 14775.031\n",
      "\n",
      "Epoch: 219, Evidence: 14775.03125, Convergence: 0.001908\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.13e-01\n",
      "Epoch: 219, Loss: 2474.37470, Residuals: -0.93859, Convergence:   inf\n",
      "Epoch: 220, Loss: 2471.53538, Residuals: -0.93622, Convergence: 0.001149\n",
      "Epoch: 221, Loss: 2469.12704, Residuals: -0.93420, Convergence: 0.000975\n",
      "Evidence 14786.863\n",
      "\n",
      "Epoch: 221, Evidence: 14786.86328, Convergence: 0.000800\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.77e-01\n",
      "Epoch: 221, Loss: 2475.21217, Residuals: -0.93420, Convergence:   inf\n",
      "Epoch: 222, Loss: 2470.65530, Residuals: -0.93068, Convergence: 0.001844\n",
      "Epoch: 223, Loss: 2467.12744, Residuals: -0.92790, Convergence: 0.001430\n",
      "Epoch: 224, Loss: 2464.22228, Residuals: -0.92579, Convergence: 0.001179\n",
      "Epoch: 225, Loss: 2461.73990, Residuals: -0.92433, Convergence: 0.001008\n",
      "Epoch: 226, Loss: 2459.55967, Residuals: -0.92335, Convergence: 0.000886\n",
      "Evidence 14807.413\n",
      "\n",
      "Epoch: 226, Evidence: 14807.41309, Convergence: 0.002187\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.47e-01\n",
      "Epoch: 226, Loss: 2475.32031, Residuals: -0.92335, Convergence:   inf\n",
      "Epoch: 227, Loss: 2472.44521, Residuals: -0.92053, Convergence: 0.001163\n",
      "Epoch: 228, Loss: 2470.12689, Residuals: -0.91888, Convergence: 0.000939\n",
      "Evidence 14818.727\n",
      "\n",
      "Epoch: 228, Evidence: 14818.72656, Convergence: 0.000763\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.24e-01\n",
      "Epoch: 228, Loss: 2475.60682, Residuals: -0.91888, Convergence:   inf\n",
      "Epoch: 229, Loss: 2471.35464, Residuals: -0.91518, Convergence: 0.001721\n",
      "Epoch: 230, Loss: 2468.23753, Residuals: -0.91570, Convergence: 0.001263\n",
      "Epoch: 231, Loss: 2465.68961, Residuals: -0.91655, Convergence: 0.001033\n",
      "Epoch: 232, Loss: 2463.49757, Residuals: -0.91936, Convergence: 0.000890\n",
      "Evidence 14834.137\n",
      "\n",
      "Epoch: 232, Evidence: 14834.13672, Convergence: 0.001801\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.11e-01\n",
      "Epoch: 232, Loss: 2475.07800, Residuals: -0.91936, Convergence:   inf\n",
      "Epoch: 233, Loss: 2473.55194, Residuals: -0.91789, Convergence: 0.000617\n",
      "Evidence 14840.394\n",
      "\n",
      "Epoch: 233, Evidence: 14840.39355, Convergence: 0.000422\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 9.01e-02\n",
      "Epoch: 233, Loss: 2476.01474, Residuals: -0.91789, Convergence:   inf\n",
      "Epoch: 234, Loss: 2519.93014, Residuals: -0.95746, Convergence: -0.017427\n",
      "Epoch: 234, Loss: 2473.76419, Residuals: -0.91536, Convergence: 0.000910\n",
      "Evidence 14844.723\n",
      "\n",
      "Epoch: 234, Evidence: 14844.72266, Convergence: 0.000713\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.29e-02\n",
      "Epoch: 234, Loss: 2475.49937, Residuals: -0.91536, Convergence:   inf\n",
      "Epoch: 235, Loss: 2480.98846, Residuals: -0.91760, Convergence: -0.002212\n",
      "Epoch: 235, Loss: 2475.74626, Residuals: -0.91432, Convergence: -0.000100\n",
      "Evidence 14845.994\n",
      "\n",
      "Epoch: 235, Evidence: 14845.99414, Convergence: 0.000799\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 380.69305, Residuals: -4.51042, Convergence:   inf\n",
      "Epoch: 1, Loss: 355.00113, Residuals: -4.39015, Convergence: 0.072371\n",
      "Epoch: 2, Loss: 333.99282, Residuals: -4.22600, Convergence: 0.062900\n",
      "Epoch: 3, Loss: 317.91873, Residuals: -4.06093, Convergence: 0.050560\n",
      "Epoch: 4, Loss: 305.66382, Residuals: -3.91544, Convergence: 0.040093\n",
      "Epoch: 5, Loss: 295.94757, Residuals: -3.78677, Convergence: 0.032831\n",
      "Epoch: 6, Loss: 288.06228, Residuals: -3.67462, Convergence: 0.027374\n",
      "Epoch: 7, Loss: 281.51801, Residuals: -3.57855, Convergence: 0.023246\n",
      "Epoch: 8, Loss: 275.95268, Residuals: -3.49653, Convergence: 0.020168\n",
      "Epoch: 9, Loss: 271.11109, Residuals: -3.42624, Convergence: 0.017858\n",
      "Epoch: 10, Loss: 266.81224, Residuals: -3.36560, Convergence: 0.016112\n",
      "Epoch: 11, Loss: 262.92566, Residuals: -3.31282, Convergence: 0.014782\n",
      "Epoch: 12, Loss: 259.35693, Residuals: -3.26640, Convergence: 0.013760\n",
      "Epoch: 13, Loss: 256.03815, Residuals: -3.22504, Convergence: 0.012962\n",
      "Epoch: 14, Loss: 252.92106, Residuals: -3.18760, Convergence: 0.012324\n",
      "Epoch: 15, Loss: 249.97247, Residuals: -3.15309, Convergence: 0.011796\n",
      "Epoch: 16, Loss: 247.17160, Residuals: -3.12080, Convergence: 0.011332\n",
      "Epoch: 17, Loss: 244.50413, Residuals: -3.09024, Convergence: 0.010910\n",
      "Epoch: 18, Loss: 241.95133, Residuals: -3.06103, Convergence: 0.010551\n",
      "Epoch: 19, Loss: 239.48443, Residuals: -3.03270, Convergence: 0.010301\n",
      "Epoch: 20, Loss: 237.06856, Residuals: -3.00473, Convergence: 0.010191\n",
      "Epoch: 21, Loss: 234.67003, Residuals: -2.97659, Convergence: 0.010221\n",
      "Epoch: 22, Loss: 232.26132, Residuals: -2.94787, Convergence: 0.010371\n",
      "Epoch: 23, Loss: 229.81733, Residuals: -2.91827, Convergence: 0.010635\n",
      "Epoch: 24, Loss: 227.30065, Residuals: -2.88738, Convergence: 0.011072\n",
      "Epoch: 25, Loss: 224.65520, Residuals: -2.85457, Convergence: 0.011776\n",
      "Epoch: 26, Loss: 221.85127, Residuals: -2.81944, Convergence: 0.012639\n",
      "Epoch: 27, Loss: 218.97741, Residuals: -2.78286, Convergence: 0.013124\n",
      "Epoch: 28, Loss: 216.16524, Residuals: -2.74629, Convergence: 0.013009\n",
      "Epoch: 29, Loss: 213.45399, Residuals: -2.71030, Convergence: 0.012702\n",
      "Epoch: 30, Loss: 210.83150, Residuals: -2.67487, Convergence: 0.012439\n",
      "Epoch: 31, Loss: 208.27949, Residuals: -2.63988, Convergence: 0.012253\n",
      "Epoch: 32, Loss: 205.78450, Residuals: -2.60517, Convergence: 0.012124\n",
      "Epoch: 33, Loss: 203.33846, Residuals: -2.57068, Convergence: 0.012029\n",
      "Epoch: 34, Loss: 200.93763, Residuals: -2.53633, Convergence: 0.011948\n",
      "Epoch: 35, Loss: 198.58123, Residuals: -2.50209, Convergence: 0.011866\n",
      "Epoch: 36, Loss: 196.27040, Residuals: -2.46795, Convergence: 0.011774\n",
      "Epoch: 37, Loss: 194.00723, Residuals: -2.43392, Convergence: 0.011665\n",
      "Epoch: 38, Loss: 191.79429, Residuals: -2.40000, Convergence: 0.011538\n",
      "Epoch: 39, Loss: 189.63407, Residuals: -2.36621, Convergence: 0.011392\n",
      "Epoch: 40, Loss: 187.52885, Residuals: -2.33258, Convergence: 0.011226\n",
      "Epoch: 41, Loss: 185.48053, Residuals: -2.29913, Convergence: 0.011043\n",
      "Epoch: 42, Loss: 183.49060, Residuals: -2.26589, Convergence: 0.010845\n",
      "Epoch: 43, Loss: 181.56021, Residuals: -2.23288, Convergence: 0.010632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44, Loss: 179.69024, Residuals: -2.20015, Convergence: 0.010407\n",
      "Epoch: 45, Loss: 177.88143, Residuals: -2.16773, Convergence: 0.010169\n",
      "Epoch: 46, Loss: 176.13450, Residuals: -2.13566, Convergence: 0.009918\n",
      "Epoch: 47, Loss: 174.45019, Residuals: -2.10397, Convergence: 0.009655\n",
      "Epoch: 48, Loss: 172.82925, Residuals: -2.07273, Convergence: 0.009379\n",
      "Epoch: 49, Loss: 171.27230, Residuals: -2.04199, Convergence: 0.009091\n",
      "Epoch: 50, Loss: 169.77954, Residuals: -2.01178, Convergence: 0.008792\n",
      "Epoch: 51, Loss: 168.35060, Residuals: -1.98217, Convergence: 0.008488\n",
      "Epoch: 52, Loss: 166.98441, Residuals: -1.95318, Convergence: 0.008182\n",
      "Epoch: 53, Loss: 165.67916, Residuals: -1.92485, Convergence: 0.007878\n",
      "Epoch: 54, Loss: 164.43248, Residuals: -1.89717, Convergence: 0.007582\n",
      "Epoch: 55, Loss: 163.24156, Residuals: -1.87016, Convergence: 0.007295\n",
      "Epoch: 56, Loss: 162.10340, Residuals: -1.84382, Convergence: 0.007021\n",
      "Epoch: 57, Loss: 161.01498, Residuals: -1.81812, Convergence: 0.006760\n",
      "Epoch: 58, Loss: 159.97338, Residuals: -1.79307, Convergence: 0.006511\n",
      "Epoch: 59, Loss: 158.97588, Residuals: -1.76864, Convergence: 0.006275\n",
      "Epoch: 60, Loss: 158.02004, Residuals: -1.74484, Convergence: 0.006049\n",
      "Epoch: 61, Loss: 157.10370, Residuals: -1.72164, Convergence: 0.005833\n",
      "Epoch: 62, Loss: 156.22499, Residuals: -1.69905, Convergence: 0.005625\n",
      "Epoch: 63, Loss: 155.38229, Residuals: -1.67705, Convergence: 0.005423\n",
      "Epoch: 64, Loss: 154.57422, Residuals: -1.65565, Convergence: 0.005228\n",
      "Epoch: 65, Loss: 153.79956, Residuals: -1.63484, Convergence: 0.005037\n",
      "Epoch: 66, Loss: 153.05724, Residuals: -1.61462, Convergence: 0.004850\n",
      "Epoch: 67, Loss: 152.34628, Residuals: -1.59499, Convergence: 0.004667\n",
      "Epoch: 68, Loss: 151.66574, Residuals: -1.57596, Convergence: 0.004487\n",
      "Epoch: 69, Loss: 151.01477, Residuals: -1.55750, Convergence: 0.004311\n",
      "Epoch: 70, Loss: 150.39251, Residuals: -1.53963, Convergence: 0.004138\n",
      "Epoch: 71, Loss: 149.79812, Residuals: -1.52234, Convergence: 0.003968\n",
      "Epoch: 72, Loss: 149.23075, Residuals: -1.50563, Convergence: 0.003802\n",
      "Epoch: 73, Loss: 148.68958, Residuals: -1.48948, Convergence: 0.003640\n",
      "Epoch: 74, Loss: 148.17373, Residuals: -1.47389, Convergence: 0.003481\n",
      "Epoch: 75, Loss: 147.68235, Residuals: -1.45885, Convergence: 0.003327\n",
      "Epoch: 76, Loss: 147.21453, Residuals: -1.44436, Convergence: 0.003178\n",
      "Epoch: 77, Loss: 146.76942, Residuals: -1.43040, Convergence: 0.003033\n",
      "Epoch: 78, Loss: 146.34609, Residuals: -1.41696, Convergence: 0.002893\n",
      "Epoch: 79, Loss: 145.94366, Residuals: -1.40403, Convergence: 0.002757\n",
      "Epoch: 80, Loss: 145.56122, Residuals: -1.39159, Convergence: 0.002627\n",
      "Epoch: 81, Loss: 145.19789, Residuals: -1.37964, Convergence: 0.002502\n",
      "Epoch: 82, Loss: 144.85280, Residuals: -1.36816, Convergence: 0.002382\n",
      "Epoch: 83, Loss: 144.52508, Residuals: -1.35713, Convergence: 0.002268\n",
      "Epoch: 84, Loss: 144.21392, Residuals: -1.34655, Convergence: 0.002158\n",
      "Epoch: 85, Loss: 143.91851, Residuals: -1.33639, Convergence: 0.002053\n",
      "Epoch: 86, Loss: 143.63811, Residuals: -1.32665, Convergence: 0.001952\n",
      "Epoch: 87, Loss: 143.37199, Residuals: -1.31731, Convergence: 0.001856\n",
      "Epoch: 88, Loss: 143.11947, Residuals: -1.30836, Convergence: 0.001764\n",
      "Epoch: 89, Loss: 142.87993, Residuals: -1.29977, Convergence: 0.001677\n",
      "Epoch: 90, Loss: 142.65277, Residuals: -1.29155, Convergence: 0.001592\n",
      "Epoch: 91, Loss: 142.43746, Residuals: -1.28367, Convergence: 0.001512\n",
      "Epoch: 92, Loss: 142.23351, Residuals: -1.27613, Convergence: 0.001434\n",
      "Epoch: 93, Loss: 142.04045, Residuals: -1.26891, Convergence: 0.001359\n",
      "Epoch: 94, Loss: 141.85788, Residuals: -1.26200, Convergence: 0.001287\n",
      "Epoch: 95, Loss: 141.68540, Residuals: -1.25540, Convergence: 0.001217\n",
      "Epoch: 96, Loss: 141.52265, Residuals: -1.24910, Convergence: 0.001150\n",
      "Epoch: 97, Loss: 141.36932, Residuals: -1.24309, Convergence: 0.001085\n",
      "Epoch: 98, Loss: 141.22505, Residuals: -1.23736, Convergence: 0.001022\n",
      "Epoch: 99, Loss: 141.08951, Residuals: -1.23191, Convergence: 0.000961\n",
      "Evidence -182.685\n",
      "\n",
      "Epoch: 99, Evidence: -182.68451, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.25e-01\n",
      "Epoch: 99, Loss: 1366.71488, Residuals: -1.23191, Convergence:   inf\n",
      "Epoch: 100, Loss: 1304.09815, Residuals: -1.26037, Convergence: 0.048015\n",
      "Epoch: 101, Loss: 1256.32064, Residuals: -1.28280, Convergence: 0.038030\n",
      "Epoch: 102, Loss: 1220.17211, Residuals: -1.29893, Convergence: 0.029626\n",
      "Epoch: 103, Loss: 1192.07312, Residuals: -1.31017, Convergence: 0.023572\n",
      "Epoch: 104, Loss: 1169.39141, Residuals: -1.31832, Convergence: 0.019396\n",
      "Epoch: 105, Loss: 1150.58984, Residuals: -1.32442, Convergence: 0.016341\n",
      "Epoch: 106, Loss: 1134.73689, Residuals: -1.32893, Convergence: 0.013971\n",
      "Epoch: 107, Loss: 1121.19984, Residuals: -1.33209, Convergence: 0.012074\n",
      "Epoch: 108, Loss: 1109.51094, Residuals: -1.33406, Convergence: 0.010535\n",
      "Epoch: 109, Loss: 1099.30621, Residuals: -1.33499, Convergence: 0.009283\n",
      "Epoch: 110, Loss: 1090.29379, Residuals: -1.33496, Convergence: 0.008266\n",
      "Epoch: 111, Loss: 1082.23379, Residuals: -1.33408, Convergence: 0.007448\n",
      "Epoch: 112, Loss: 1074.92451, Residuals: -1.33240, Convergence: 0.006800\n",
      "Epoch: 113, Loss: 1068.19402, Residuals: -1.32998, Convergence: 0.006301\n",
      "Epoch: 114, Loss: 1061.89138, Residuals: -1.32685, Convergence: 0.005935\n",
      "Epoch: 115, Loss: 1055.88447, Residuals: -1.32302, Convergence: 0.005689\n",
      "Epoch: 116, Loss: 1050.05625, Residuals: -1.31851, Convergence: 0.005550\n",
      "Epoch: 117, Loss: 1044.31079, Residuals: -1.31331, Convergence: 0.005502\n",
      "Epoch: 118, Loss: 1038.57989, Residuals: -1.30746, Convergence: 0.005518\n",
      "Epoch: 119, Loss: 1032.83801, Residuals: -1.30101, Convergence: 0.005559\n",
      "Epoch: 120, Loss: 1027.11057, Residuals: -1.29405, Convergence: 0.005576\n",
      "Epoch: 121, Loss: 1021.46947, Residuals: -1.28668, Convergence: 0.005523\n",
      "Epoch: 122, Loss: 1016.00713, Residuals: -1.27901, Convergence: 0.005376\n",
      "Epoch: 123, Loss: 1010.80504, Residuals: -1.27115, Convergence: 0.005146\n",
      "Epoch: 124, Loss: 1005.91257, Residuals: -1.26316, Convergence: 0.004864\n",
      "Epoch: 125, Loss: 1001.34528, Residuals: -1.25514, Convergence: 0.004561\n",
      "Epoch: 126, Loss: 997.09454, Residuals: -1.24714, Convergence: 0.004263\n",
      "Epoch: 127, Loss: 993.13851, Residuals: -1.23921, Convergence: 0.003983\n",
      "Epoch: 128, Loss: 989.45049, Residuals: -1.23139, Convergence: 0.003727\n",
      "Epoch: 129, Loss: 986.00327, Residuals: -1.22372, Convergence: 0.003496\n",
      "Epoch: 130, Loss: 982.77208, Residuals: -1.21623, Convergence: 0.003288\n",
      "Epoch: 131, Loss: 979.73491, Residuals: -1.20893, Convergence: 0.003100\n",
      "Epoch: 132, Loss: 976.87318, Residuals: -1.20184, Convergence: 0.002929\n",
      "Epoch: 133, Loss: 974.17039, Residuals: -1.19497, Convergence: 0.002774\n",
      "Epoch: 134, Loss: 971.61288, Residuals: -1.18833, Convergence: 0.002632\n",
      "Epoch: 135, Loss: 969.18918, Residuals: -1.18194, Convergence: 0.002501\n",
      "Epoch: 136, Loss: 966.88884, Residuals: -1.17578, Convergence: 0.002379\n",
      "Epoch: 137, Loss: 964.70300, Residuals: -1.16988, Convergence: 0.002266\n",
      "Epoch: 138, Loss: 962.62377, Residuals: -1.16422, Convergence: 0.002160\n",
      "Epoch: 139, Loss: 960.64416, Residuals: -1.15881, Convergence: 0.002061\n",
      "Epoch: 140, Loss: 958.75750, Residuals: -1.15363, Convergence: 0.001968\n",
      "Epoch: 141, Loss: 956.95767, Residuals: -1.14870, Convergence: 0.001881\n",
      "Epoch: 142, Loss: 955.23888, Residuals: -1.14399, Convergence: 0.001799\n",
      "Epoch: 143, Loss: 953.59624, Residuals: -1.13950, Convergence: 0.001723\n",
      "Epoch: 144, Loss: 952.02453, Residuals: -1.13523, Convergence: 0.001651\n",
      "Epoch: 145, Loss: 950.51910, Residuals: -1.13117, Convergence: 0.001584\n",
      "Epoch: 146, Loss: 949.07531, Residuals: -1.12729, Convergence: 0.001521\n",
      "Epoch: 147, Loss: 947.68862, Residuals: -1.12361, Convergence: 0.001463\n",
      "Epoch: 148, Loss: 946.35581, Residuals: -1.12010, Convergence: 0.001408\n",
      "Epoch: 149, Loss: 945.07220, Residuals: -1.11675, Convergence: 0.001358\n",
      "Epoch: 150, Loss: 943.83462, Residuals: -1.11356, Convergence: 0.001311\n",
      "Epoch: 151, Loss: 942.63911, Residuals: -1.11051, Convergence: 0.001268\n",
      "Epoch: 152, Loss: 941.48252, Residuals: -1.10760, Convergence: 0.001228\n",
      "Epoch: 153, Loss: 940.36162, Residuals: -1.10481, Convergence: 0.001192\n",
      "Epoch: 154, Loss: 939.27316, Residuals: -1.10214, Convergence: 0.001159\n",
      "Epoch: 155, Loss: 938.21424, Residuals: -1.09958, Convergence: 0.001129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 156, Loss: 937.18157, Residuals: -1.09711, Convergence: 0.001102\n",
      "Epoch: 157, Loss: 936.17257, Residuals: -1.09473, Convergence: 0.001078\n",
      "Epoch: 158, Loss: 935.18412, Residuals: -1.09243, Convergence: 0.001057\n",
      "Epoch: 159, Loss: 934.21357, Residuals: -1.09020, Convergence: 0.001039\n",
      "Epoch: 160, Loss: 933.25812, Residuals: -1.08803, Convergence: 0.001024\n",
      "Epoch: 161, Loss: 932.31460, Residuals: -1.08592, Convergence: 0.001012\n",
      "Epoch: 162, Loss: 931.38008, Residuals: -1.08384, Convergence: 0.001003\n",
      "Epoch: 163, Loss: 930.45206, Residuals: -1.08181, Convergence: 0.000997\n",
      "Evidence 11140.008\n",
      "\n",
      "Epoch: 163, Evidence: 11140.00781, Convergence: 1.016399\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.77e-01\n",
      "Epoch: 163, Loss: 2348.56197, Residuals: -1.08181, Convergence:   inf\n",
      "Epoch: 164, Loss: 2306.79262, Residuals: -1.09018, Convergence: 0.018107\n",
      "Epoch: 165, Loss: 2278.52374, Residuals: -1.08901, Convergence: 0.012407\n",
      "Epoch: 166, Loss: 2254.82717, Residuals: -1.08670, Convergence: 0.010509\n",
      "Epoch: 167, Loss: 2234.65534, Residuals: -1.08398, Convergence: 0.009027\n",
      "Epoch: 168, Loss: 2217.35145, Residuals: -1.08102, Convergence: 0.007804\n",
      "Epoch: 169, Loss: 2202.41338, Residuals: -1.07789, Convergence: 0.006783\n",
      "Epoch: 170, Loss: 2189.42958, Residuals: -1.07465, Convergence: 0.005930\n",
      "Epoch: 171, Loss: 2178.05265, Residuals: -1.07133, Convergence: 0.005223\n",
      "Epoch: 172, Loss: 2167.98743, Residuals: -1.06793, Convergence: 0.004643\n",
      "Epoch: 173, Loss: 2158.98494, Residuals: -1.06445, Convergence: 0.004170\n",
      "Epoch: 174, Loss: 2150.84823, Residuals: -1.06090, Convergence: 0.003783\n",
      "Epoch: 175, Loss: 2143.43357, Residuals: -1.05727, Convergence: 0.003459\n",
      "Epoch: 176, Loss: 2136.64725, Residuals: -1.05359, Convergence: 0.003176\n",
      "Epoch: 177, Loss: 2130.43098, Residuals: -1.04992, Convergence: 0.002918\n",
      "Epoch: 178, Loss: 2124.74410, Residuals: -1.04629, Convergence: 0.002677\n",
      "Epoch: 179, Loss: 2119.54802, Residuals: -1.04276, Convergence: 0.002452\n",
      "Epoch: 180, Loss: 2114.80475, Residuals: -1.03935, Convergence: 0.002243\n",
      "Epoch: 181, Loss: 2110.47239, Residuals: -1.03610, Convergence: 0.002053\n",
      "Epoch: 182, Loss: 2106.51044, Residuals: -1.03300, Convergence: 0.001881\n",
      "Epoch: 183, Loss: 2102.87842, Residuals: -1.03007, Convergence: 0.001727\n",
      "Epoch: 184, Loss: 2099.53814, Residuals: -1.02731, Convergence: 0.001591\n",
      "Epoch: 185, Loss: 2096.45511, Residuals: -1.02470, Convergence: 0.001471\n",
      "Epoch: 186, Loss: 2093.59717, Residuals: -1.02223, Convergence: 0.001365\n",
      "Epoch: 187, Loss: 2090.93577, Residuals: -1.01990, Convergence: 0.001273\n",
      "Epoch: 188, Loss: 2088.44687, Residuals: -1.01769, Convergence: 0.001192\n",
      "Epoch: 189, Loss: 2086.10892, Residuals: -1.01560, Convergence: 0.001121\n",
      "Epoch: 190, Loss: 2083.90150, Residuals: -1.01359, Convergence: 0.001059\n",
      "Epoch: 191, Loss: 2081.81096, Residuals: -1.01168, Convergence: 0.001004\n",
      "Epoch: 192, Loss: 2079.82385, Residuals: -1.00985, Convergence: 0.000955\n",
      "Evidence 14293.614\n",
      "\n",
      "Epoch: 192, Evidence: 14293.61426, Convergence: 0.220630\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.41e-01\n",
      "Epoch: 192, Loss: 2474.66563, Residuals: -1.00985, Convergence:   inf\n",
      "Epoch: 193, Loss: 2460.91232, Residuals: -1.00758, Convergence: 0.005589\n",
      "Epoch: 194, Loss: 2449.49336, Residuals: -1.00451, Convergence: 0.004662\n",
      "Epoch: 195, Loss: 2439.42135, Residuals: -1.00136, Convergence: 0.004129\n",
      "Epoch: 196, Loss: 2430.48495, Residuals: -0.99826, Convergence: 0.003677\n",
      "Epoch: 197, Loss: 2422.53565, Residuals: -0.99529, Convergence: 0.003281\n",
      "Epoch: 198, Loss: 2415.45015, Residuals: -0.99246, Convergence: 0.002933\n",
      "Epoch: 199, Loss: 2409.12683, Residuals: -0.98980, Convergence: 0.002625\n",
      "Epoch: 200, Loss: 2403.47465, Residuals: -0.98731, Convergence: 0.002352\n",
      "Epoch: 201, Loss: 2398.41179, Residuals: -0.98498, Convergence: 0.002111\n",
      "Epoch: 202, Loss: 2393.86489, Residuals: -0.98280, Convergence: 0.001899\n",
      "Epoch: 203, Loss: 2389.76884, Residuals: -0.98077, Convergence: 0.001714\n",
      "Epoch: 204, Loss: 2386.06633, Residuals: -0.97887, Convergence: 0.001552\n",
      "Epoch: 205, Loss: 2382.70580, Residuals: -0.97710, Convergence: 0.001410\n",
      "Epoch: 206, Loss: 2379.64430, Residuals: -0.97545, Convergence: 0.001287\n",
      "Epoch: 207, Loss: 2376.84422, Residuals: -0.97392, Convergence: 0.001178\n",
      "Epoch: 208, Loss: 2374.27170, Residuals: -0.97248, Convergence: 0.001084\n",
      "Epoch: 209, Loss: 2371.89925, Residuals: -0.97114, Convergence: 0.001000\n",
      "Epoch: 210, Loss: 2369.70331, Residuals: -0.96989, Convergence: 0.000927\n",
      "Evidence 14704.812\n",
      "\n",
      "Epoch: 210, Evidence: 14704.81250, Convergence: 0.027964\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.38e-01\n",
      "Epoch: 210, Loss: 2479.69373, Residuals: -0.96989, Convergence:   inf\n",
      "Epoch: 211, Loss: 2472.47754, Residuals: -0.96659, Convergence: 0.002919\n",
      "Epoch: 212, Loss: 2466.46819, Residuals: -0.96369, Convergence: 0.002436\n",
      "Epoch: 213, Loss: 2461.35254, Residuals: -0.96121, Convergence: 0.002078\n",
      "Epoch: 214, Loss: 2456.94647, Residuals: -0.95909, Convergence: 0.001793\n",
      "Epoch: 215, Loss: 2453.10976, Residuals: -0.95726, Convergence: 0.001564\n",
      "Epoch: 216, Loss: 2449.73357, Residuals: -0.95568, Convergence: 0.001378\n",
      "Epoch: 217, Loss: 2446.73156, Residuals: -0.95431, Convergence: 0.001227\n",
      "Epoch: 218, Loss: 2444.03777, Residuals: -0.95313, Convergence: 0.001102\n",
      "Epoch: 219, Loss: 2441.59842, Residuals: -0.95209, Convergence: 0.000999\n",
      "Evidence 14801.073\n",
      "\n",
      "Epoch: 219, Evidence: 14801.07324, Convergence: 0.006504\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.65e-01\n",
      "Epoch: 219, Loss: 2481.18033, Residuals: -0.95209, Convergence:   inf\n",
      "Epoch: 220, Loss: 2476.84611, Residuals: -0.94964, Convergence: 0.001750\n",
      "Epoch: 221, Loss: 2473.22909, Residuals: -0.94768, Convergence: 0.001462\n",
      "Epoch: 222, Loss: 2470.13550, Residuals: -0.94610, Convergence: 0.001252\n",
      "Epoch: 223, Loss: 2467.44255, Residuals: -0.94480, Convergence: 0.001091\n",
      "Epoch: 224, Loss: 2465.06422, Residuals: -0.94372, Convergence: 0.000965\n",
      "Evidence 14835.143\n",
      "\n",
      "Epoch: 224, Evidence: 14835.14258, Convergence: 0.002297\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.14e-01\n",
      "Epoch: 224, Loss: 2482.01262, Residuals: -0.94372, Convergence:   inf\n",
      "Epoch: 225, Loss: 2478.92875, Residuals: -0.94200, Convergence: 0.001244\n",
      "Epoch: 226, Loss: 2476.34807, Residuals: -0.94066, Convergence: 0.001042\n",
      "Epoch: 227, Loss: 2474.12054, Residuals: -0.93958, Convergence: 0.000900\n",
      "Evidence 14851.061\n",
      "\n",
      "Epoch: 227, Evidence: 14851.06055, Convergence: 0.001072\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.76e-01\n",
      "Epoch: 227, Loss: 2482.60999, Residuals: -0.93958, Convergence:   inf\n",
      "Epoch: 228, Loss: 2480.17742, Residuals: -0.93820, Convergence: 0.000981\n",
      "Evidence 14857.756\n",
      "\n",
      "Epoch: 228, Evidence: 14857.75586, Convergence: 0.000451\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.50e-01\n",
      "Epoch: 228, Loss: 2483.11158, Residuals: -0.93820, Convergence:   inf\n",
      "Epoch: 229, Loss: 2479.04967, Residuals: -0.93644, Convergence: 0.001638\n",
      "Epoch: 230, Loss: 2475.86973, Residuals: -0.93490, Convergence: 0.001284\n",
      "Epoch: 231, Loss: 2473.27041, Residuals: -0.93378, Convergence: 0.001051\n",
      "Epoch: 232, Loss: 2471.04850, Residuals: -0.93311, Convergence: 0.000899\n",
      "Evidence 14873.078\n",
      "\n",
      "Epoch: 232, Evidence: 14873.07812, Convergence: 0.001480\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.26e-01\n",
      "Epoch: 232, Loss: 2483.21174, Residuals: -0.93311, Convergence:   inf\n",
      "Epoch: 233, Loss: 2480.48790, Residuals: -0.93116, Convergence: 0.001098\n",
      "Epoch: 234, Loss: 2478.28835, Residuals: -0.93008, Convergence: 0.000888\n",
      "Evidence 14882.601\n",
      "\n",
      "Epoch: 234, Evidence: 14882.60059, Convergence: 0.000640\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.09e-01\n",
      "Epoch: 234, Loss: 2483.31008, Residuals: -0.93008, Convergence:   inf\n",
      "Epoch: 235, Loss: 2479.23974, Residuals: -0.92702, Convergence: 0.001642\n",
      "Epoch: 236, Loss: 2476.24961, Residuals: -0.92779, Convergence: 0.001208\n",
      "Epoch: 237, Loss: 2473.71080, Residuals: -0.92823, Convergence: 0.001026\n",
      "Epoch: 238, Loss: 2471.51675, Residuals: -0.93078, Convergence: 0.000888\n",
      "Evidence 14897.071\n",
      "\n",
      "Epoch: 238, Evidence: 14897.07129, Convergence: 0.001611\n",
      "Updating hyper-parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 182, Updated regularization: 9.84e-02\n",
      "Epoch: 238, Loss: 2482.59930, Residuals: -0.93078, Convergence:   inf\n",
      "Epoch: 239, Loss: 2481.02660, Residuals: -0.92934, Convergence: 0.000634\n",
      "Evidence 14902.794\n",
      "\n",
      "Epoch: 239, Evidence: 14902.79395, Convergence: 0.000384\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.10e-02\n",
      "Epoch: 239, Loss: 2483.58896, Residuals: -0.92934, Convergence:   inf\n",
      "Epoch: 240, Loss: 2535.42500, Residuals: -0.97364, Convergence: -0.020445\n",
      "Epoch: 240, Loss: 2481.24856, Residuals: -0.92696, Convergence: 0.000943\n",
      "Evidence 14906.938\n",
      "\n",
      "Epoch: 240, Evidence: 14906.93750, Convergence: 0.000662\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.52e-02\n",
      "Epoch: 240, Loss: 2482.97520, Residuals: -0.92696, Convergence:   inf\n",
      "Epoch: 241, Loss: 2485.65413, Residuals: -0.92739, Convergence: -0.001078\n",
      "Epoch: 241, Loss: 2482.46576, Residuals: -0.92491, Convergence: 0.000205\n",
      "Evidence 14908.890\n",
      "\n",
      "Epoch: 241, Evidence: 14908.88965, Convergence: 0.000793\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 381.70970, Residuals: -4.53456, Convergence:   inf\n",
      "Epoch: 1, Loss: 356.00872, Residuals: -4.41490, Convergence: 0.072192\n",
      "Epoch: 2, Loss: 334.91787, Residuals: -4.25024, Convergence: 0.062973\n",
      "Epoch: 3, Loss: 318.83182, Residuals: -4.08522, Convergence: 0.050453\n",
      "Epoch: 4, Loss: 306.56407, Residuals: -3.93960, Convergence: 0.040017\n",
      "Epoch: 5, Loss: 296.83395, Residuals: -3.81066, Convergence: 0.032780\n",
      "Epoch: 6, Loss: 288.93463, Residuals: -3.69816, Convergence: 0.027339\n",
      "Epoch: 7, Loss: 282.37699, Residuals: -3.60169, Convergence: 0.023223\n",
      "Epoch: 8, Loss: 276.79783, Residuals: -3.51924, Convergence: 0.020156\n",
      "Epoch: 9, Loss: 271.94068, Residuals: -3.44850, Convergence: 0.017861\n",
      "Epoch: 10, Loss: 267.62354, Residuals: -3.38736, Convergence: 0.016131\n",
      "Epoch: 11, Loss: 263.71510, Residuals: -3.33405, Convergence: 0.014821\n",
      "Epoch: 12, Loss: 260.12012, Residuals: -3.28707, Convergence: 0.013820\n",
      "Epoch: 13, Loss: 256.76988, Residuals: -3.24510, Convergence: 0.013048\n",
      "Epoch: 14, Loss: 253.61563, Residuals: -3.20697, Convergence: 0.012437\n",
      "Epoch: 15, Loss: 250.62491, Residuals: -3.17173, Convergence: 0.011933\n",
      "Epoch: 16, Loss: 247.77973, Residuals: -3.13869, Convergence: 0.011483\n",
      "Epoch: 17, Loss: 245.06904, Residuals: -3.10742, Convergence: 0.011061\n",
      "Epoch: 18, Loss: 242.47496, Residuals: -3.07758, Convergence: 0.010698\n",
      "Epoch: 19, Loss: 239.96753, Residuals: -3.04867, Convergence: 0.010449\n",
      "Epoch: 20, Loss: 237.51099, Residuals: -3.02012, Convergence: 0.010343\n",
      "Epoch: 21, Loss: 235.07293, Residuals: -2.99140, Convergence: 0.010371\n",
      "Epoch: 22, Loss: 232.62995, Residuals: -2.96214, Convergence: 0.010502\n",
      "Epoch: 23, Loss: 230.16189, Residuals: -2.93209, Convergence: 0.010723\n",
      "Epoch: 24, Loss: 227.63549, Residuals: -2.90087, Convergence: 0.011098\n",
      "Epoch: 25, Loss: 224.99880, Residuals: -2.86790, Convergence: 0.011719\n",
      "Epoch: 26, Loss: 222.21992, Residuals: -2.83275, Convergence: 0.012505\n",
      "Epoch: 27, Loss: 219.36993, Residuals: -2.79611, Convergence: 0.012992\n",
      "Epoch: 28, Loss: 216.57207, Residuals: -2.75939, Convergence: 0.012919\n",
      "Epoch: 29, Loss: 213.87071, Residuals: -2.72325, Convergence: 0.012631\n",
      "Epoch: 30, Loss: 211.25665, Residuals: -2.68770, Convergence: 0.012374\n",
      "Epoch: 31, Loss: 208.71199, Residuals: -2.65262, Convergence: 0.012192\n",
      "Epoch: 32, Loss: 206.22265, Residuals: -2.61787, Convergence: 0.012071\n",
      "Epoch: 33, Loss: 203.77977, Residuals: -2.58335, Convergence: 0.011988\n",
      "Epoch: 34, Loss: 201.37886, Residuals: -2.54898, Convergence: 0.011922\n",
      "Epoch: 35, Loss: 199.01864, Residuals: -2.51470, Convergence: 0.011859\n",
      "Epoch: 36, Loss: 196.69988, Residuals: -2.48051, Convergence: 0.011788\n",
      "Epoch: 37, Loss: 194.42447, Residuals: -2.44637, Convergence: 0.011703\n",
      "Epoch: 38, Loss: 192.19482, Residuals: -2.41231, Convergence: 0.011601\n",
      "Epoch: 39, Loss: 190.01340, Residuals: -2.37832, Convergence: 0.011480\n",
      "Epoch: 40, Loss: 187.88257, Residuals: -2.34443, Convergence: 0.011341\n",
      "Epoch: 41, Loss: 185.80456, Residuals: -2.31066, Convergence: 0.011184\n",
      "Epoch: 42, Loss: 183.78148, Residuals: -2.27703, Convergence: 0.011008\n",
      "Epoch: 43, Loss: 181.81552, Residuals: -2.24358, Convergence: 0.010813\n",
      "Epoch: 44, Loss: 179.90893, Residuals: -2.21034, Convergence: 0.010598\n",
      "Epoch: 45, Loss: 178.06403, Residuals: -2.17738, Convergence: 0.010361\n",
      "Epoch: 46, Loss: 176.28299, Residuals: -2.14475, Convergence: 0.010103\n",
      "Epoch: 47, Loss: 174.56763, Residuals: -2.11250, Convergence: 0.009826\n",
      "Epoch: 48, Loss: 172.91915, Residuals: -2.08070, Convergence: 0.009533\n",
      "Epoch: 49, Loss: 171.33797, Residuals: -2.04940, Convergence: 0.009228\n",
      "Epoch: 50, Loss: 169.82375, Residuals: -2.01865, Convergence: 0.008916\n",
      "Epoch: 51, Loss: 168.37535, Residuals: -1.98849, Convergence: 0.008602\n",
      "Epoch: 52, Loss: 166.99094, Residuals: -1.95895, Convergence: 0.008290\n",
      "Epoch: 53, Loss: 165.66821, Residuals: -1.93006, Convergence: 0.007984\n",
      "Epoch: 54, Loss: 164.40451, Residuals: -1.90183, Convergence: 0.007687\n",
      "Epoch: 55, Loss: 163.19699, Residuals: -1.87425, Convergence: 0.007399\n",
      "Epoch: 56, Loss: 162.04277, Residuals: -1.84734, Convergence: 0.007123\n",
      "Epoch: 57, Loss: 160.93907, Residuals: -1.82108, Convergence: 0.006858\n",
      "Epoch: 58, Loss: 159.88321, Residuals: -1.79548, Convergence: 0.006604\n",
      "Epoch: 59, Loss: 158.87273, Residuals: -1.77051, Convergence: 0.006360\n",
      "Epoch: 60, Loss: 157.90535, Residuals: -1.74618, Convergence: 0.006126\n",
      "Epoch: 61, Loss: 156.97898, Residuals: -1.72248, Convergence: 0.005901\n",
      "Epoch: 62, Loss: 156.09181, Residuals: -1.69940, Convergence: 0.005684\n",
      "Epoch: 63, Loss: 155.24220, Residuals: -1.67694, Convergence: 0.005473\n",
      "Epoch: 64, Loss: 154.42879, Residuals: -1.65510, Convergence: 0.005267\n",
      "Epoch: 65, Loss: 153.65036, Residuals: -1.63387, Convergence: 0.005066\n",
      "Epoch: 66, Loss: 152.90588, Residuals: -1.61326, Convergence: 0.004869\n",
      "Epoch: 67, Loss: 152.19440, Residuals: -1.59326, Convergence: 0.004675\n",
      "Epoch: 68, Loss: 151.51503, Residuals: -1.57387, Convergence: 0.004484\n",
      "Epoch: 69, Loss: 150.86693, Residuals: -1.55511, Convergence: 0.004296\n",
      "Epoch: 70, Loss: 150.24922, Residuals: -1.53696, Convergence: 0.004111\n",
      "Epoch: 71, Loss: 149.66099, Residuals: -1.51943, Convergence: 0.003930\n",
      "Epoch: 72, Loss: 149.10130, Residuals: -1.50251, Convergence: 0.003754\n",
      "Epoch: 73, Loss: 148.56919, Residuals: -1.48620, Convergence: 0.003582\n",
      "Epoch: 74, Loss: 148.06362, Residuals: -1.47049, Convergence: 0.003415\n",
      "Epoch: 75, Loss: 147.58354, Residuals: -1.45537, Convergence: 0.003253\n",
      "Epoch: 76, Loss: 147.12789, Residuals: -1.44082, Convergence: 0.003097\n",
      "Epoch: 77, Loss: 146.69559, Residuals: -1.42685, Convergence: 0.002947\n",
      "Epoch: 78, Loss: 146.28557, Residuals: -1.41343, Convergence: 0.002803\n",
      "Epoch: 79, Loss: 145.89680, Residuals: -1.40054, Convergence: 0.002665\n",
      "Epoch: 80, Loss: 145.52825, Residuals: -1.38818, Convergence: 0.002532\n",
      "Epoch: 81, Loss: 145.17895, Residuals: -1.37633, Convergence: 0.002406\n",
      "Epoch: 82, Loss: 144.84797, Residuals: -1.36497, Convergence: 0.002285\n",
      "Epoch: 83, Loss: 144.53443, Residuals: -1.35409, Convergence: 0.002169\n",
      "Epoch: 84, Loss: 144.23751, Residuals: -1.34366, Convergence: 0.002059\n",
      "Epoch: 85, Loss: 143.95643, Residuals: -1.33368, Convergence: 0.001953\n",
      "Epoch: 86, Loss: 143.69049, Residuals: -1.32413, Convergence: 0.001851\n",
      "Epoch: 87, Loss: 143.43903, Residuals: -1.31499, Convergence: 0.001753\n",
      "Epoch: 88, Loss: 143.20143, Residuals: -1.30626, Convergence: 0.001659\n",
      "Epoch: 89, Loss: 142.97712, Residuals: -1.29792, Convergence: 0.001569\n",
      "Epoch: 90, Loss: 142.76555, Residuals: -1.28995, Convergence: 0.001482\n",
      "Epoch: 91, Loss: 142.56623, Residuals: -1.28235, Convergence: 0.001398\n",
      "Epoch: 92, Loss: 142.37864, Residuals: -1.27511, Convergence: 0.001318\n",
      "Epoch: 93, Loss: 142.20226, Residuals: -1.26822, Convergence: 0.001240\n",
      "Epoch: 94, Loss: 142.03658, Residuals: -1.26168, Convergence: 0.001166\n",
      "Epoch: 95, Loss: 141.88100, Residuals: -1.25546, Convergence: 0.001097\n",
      "Epoch: 96, Loss: 141.73486, Residuals: -1.24958, Convergence: 0.001031\n",
      "Epoch: 97, Loss: 141.59743, Residuals: -1.24401, Convergence: 0.000971\n",
      "Evidence -183.202\n",
      "\n",
      "Epoch: 97, Evidence: -183.20157, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.25e-01\n",
      "Epoch: 97, Loss: 1362.15142, Residuals: -1.24401, Convergence:   inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 98, Loss: 1299.27039, Residuals: -1.27382, Convergence: 0.048397\n",
      "Epoch: 99, Loss: 1251.63569, Residuals: -1.29732, Convergence: 0.038058\n",
      "Epoch: 100, Loss: 1215.86451, Residuals: -1.31400, Convergence: 0.029420\n",
      "Epoch: 101, Loss: 1188.09656, Residuals: -1.32550, Convergence: 0.023372\n",
      "Epoch: 102, Loss: 1165.65486, Residuals: -1.33382, Convergence: 0.019252\n",
      "Epoch: 103, Loss: 1147.04163, Residuals: -1.34003, Convergence: 0.016227\n",
      "Epoch: 104, Loss: 1131.35012, Residuals: -1.34463, Convergence: 0.013870\n",
      "Epoch: 105, Loss: 1117.95703, Residuals: -1.34786, Convergence: 0.011980\n",
      "Epoch: 106, Loss: 1106.39950, Residuals: -1.34990, Convergence: 0.010446\n",
      "Epoch: 107, Loss: 1096.31510, Residuals: -1.35087, Convergence: 0.009198\n",
      "Epoch: 108, Loss: 1087.41103, Residuals: -1.35087, Convergence: 0.008188\n",
      "Epoch: 109, Loss: 1079.44461, Residuals: -1.34999, Convergence: 0.007380\n",
      "Epoch: 110, Loss: 1072.21015, Residuals: -1.34828, Convergence: 0.006747\n",
      "Epoch: 111, Loss: 1065.52630, Residuals: -1.34578, Convergence: 0.006273\n",
      "Epoch: 112, Loss: 1059.23150, Residuals: -1.34252, Convergence: 0.005943\n",
      "Epoch: 113, Loss: 1053.17972, Residuals: -1.33850, Convergence: 0.005746\n",
      "Epoch: 114, Loss: 1047.24370, Residuals: -1.33375, Convergence: 0.005668\n",
      "Epoch: 115, Loss: 1041.32735, Residuals: -1.32830, Convergence: 0.005682\n",
      "Epoch: 116, Loss: 1035.38397, Residuals: -1.32219, Convergence: 0.005740\n",
      "Epoch: 117, Loss: 1029.43182, Residuals: -1.31552, Convergence: 0.005782\n",
      "Epoch: 118, Loss: 1023.54978, Residuals: -1.30839, Convergence: 0.005747\n",
      "Epoch: 119, Loss: 1017.84689, Residuals: -1.30089, Convergence: 0.005603\n",
      "Epoch: 120, Loss: 1012.42025, Residuals: -1.29311, Convergence: 0.005360\n",
      "Epoch: 121, Loss: 1007.32870, Residuals: -1.28514, Convergence: 0.005055\n",
      "Epoch: 122, Loss: 1002.59047, Residuals: -1.27706, Convergence: 0.004726\n",
      "Epoch: 123, Loss: 998.19536, Residuals: -1.26894, Convergence: 0.004403\n",
      "Epoch: 124, Loss: 994.11841, Residuals: -1.26083, Convergence: 0.004101\n",
      "Epoch: 125, Loss: 990.32853, Residuals: -1.25278, Convergence: 0.003827\n",
      "Epoch: 126, Loss: 986.79511, Residuals: -1.24483, Convergence: 0.003581\n",
      "Epoch: 127, Loss: 983.48963, Residuals: -1.23703, Convergence: 0.003361\n",
      "Epoch: 128, Loss: 980.38699, Residuals: -1.22939, Convergence: 0.003165\n",
      "Epoch: 129, Loss: 977.46605, Residuals: -1.22193, Convergence: 0.002988\n",
      "Epoch: 130, Loss: 974.70901, Residuals: -1.21469, Convergence: 0.002829\n",
      "Epoch: 131, Loss: 972.10010, Residuals: -1.20766, Convergence: 0.002684\n",
      "Epoch: 132, Loss: 969.62700, Residuals: -1.20087, Convergence: 0.002551\n",
      "Epoch: 133, Loss: 967.27896, Residuals: -1.19431, Convergence: 0.002427\n",
      "Epoch: 134, Loss: 965.04671, Residuals: -1.18800, Convergence: 0.002313\n",
      "Epoch: 135, Loss: 962.92190, Residuals: -1.18194, Convergence: 0.002207\n",
      "Epoch: 136, Loss: 960.89704, Residuals: -1.17612, Convergence: 0.002107\n",
      "Epoch: 137, Loss: 958.96610, Residuals: -1.17056, Convergence: 0.002014\n",
      "Epoch: 138, Loss: 957.12241, Residuals: -1.16523, Convergence: 0.001926\n",
      "Epoch: 139, Loss: 955.36049, Residuals: -1.16014, Convergence: 0.001844\n",
      "Epoch: 140, Loss: 953.67467, Residuals: -1.15528, Convergence: 0.001768\n",
      "Epoch: 141, Loss: 952.06005, Residuals: -1.15065, Convergence: 0.001696\n",
      "Epoch: 142, Loss: 950.51175, Residuals: -1.14622, Convergence: 0.001629\n",
      "Epoch: 143, Loss: 949.02508, Residuals: -1.14201, Convergence: 0.001567\n",
      "Epoch: 144, Loss: 947.59610, Residuals: -1.13799, Convergence: 0.001508\n",
      "Epoch: 145, Loss: 946.22043, Residuals: -1.13416, Convergence: 0.001454\n",
      "Epoch: 146, Loss: 944.89413, Residuals: -1.13050, Convergence: 0.001404\n",
      "Epoch: 147, Loss: 943.61404, Residuals: -1.12701, Convergence: 0.001357\n",
      "Epoch: 148, Loss: 942.37677, Residuals: -1.12368, Convergence: 0.001313\n",
      "Epoch: 149, Loss: 941.17894, Residuals: -1.12050, Convergence: 0.001273\n",
      "Epoch: 150, Loss: 940.01764, Residuals: -1.11745, Convergence: 0.001235\n",
      "Epoch: 151, Loss: 938.89017, Residuals: -1.11454, Convergence: 0.001201\n",
      "Epoch: 152, Loss: 937.79372, Residuals: -1.11174, Convergence: 0.001169\n",
      "Epoch: 153, Loss: 936.72594, Residuals: -1.10906, Convergence: 0.001140\n",
      "Epoch: 154, Loss: 935.68405, Residuals: -1.10649, Convergence: 0.001114\n",
      "Epoch: 155, Loss: 934.66540, Residuals: -1.10401, Convergence: 0.001090\n",
      "Epoch: 156, Loss: 933.66772, Residuals: -1.10162, Convergence: 0.001069\n",
      "Epoch: 157, Loss: 932.68813, Residuals: -1.09930, Convergence: 0.001050\n",
      "Epoch: 158, Loss: 931.72423, Residuals: -1.09706, Convergence: 0.001035\n",
      "Epoch: 159, Loss: 930.77277, Residuals: -1.09489, Convergence: 0.001022\n",
      "Epoch: 160, Loss: 929.83112, Residuals: -1.09276, Convergence: 0.001013\n",
      "Epoch: 161, Loss: 928.89638, Residuals: -1.09069, Convergence: 0.001006\n",
      "Epoch: 162, Loss: 927.96564, Residuals: -1.08865, Convergence: 0.001003\n",
      "Epoch: 163, Loss: 927.03706, Residuals: -1.08664, Convergence: 0.001002\n",
      "Epoch: 164, Loss: 926.10850, Residuals: -1.08465, Convergence: 0.001003\n",
      "Epoch: 165, Loss: 925.17977, Residuals: -1.08268, Convergence: 0.001004\n",
      "Epoch: 166, Loss: 924.25124, Residuals: -1.08073, Convergence: 0.001005\n",
      "Epoch: 167, Loss: 923.32453, Residuals: -1.07879, Convergence: 0.001004\n",
      "Epoch: 168, Loss: 922.40303, Residuals: -1.07687, Convergence: 0.000999\n",
      "Evidence 11065.708\n",
      "\n",
      "Epoch: 168, Evidence: 11065.70801, Convergence: 1.016556\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.77e-01\n",
      "Epoch: 168, Loss: 2341.63821, Residuals: -1.07687, Convergence:   inf\n",
      "Epoch: 169, Loss: 2303.31258, Residuals: -1.08534, Convergence: 0.016639\n",
      "Epoch: 170, Loss: 2275.85197, Residuals: -1.08443, Convergence: 0.012066\n",
      "Epoch: 171, Loss: 2252.92988, Residuals: -1.08257, Convergence: 0.010174\n",
      "Epoch: 172, Loss: 2233.59704, Residuals: -1.08040, Convergence: 0.008655\n",
      "Epoch: 173, Loss: 2217.15911, Residuals: -1.07807, Convergence: 0.007414\n",
      "Epoch: 174, Loss: 2203.07519, Residuals: -1.07562, Convergence: 0.006393\n",
      "Epoch: 175, Loss: 2190.90046, Residuals: -1.07309, Convergence: 0.005557\n",
      "Epoch: 176, Loss: 2180.26467, Residuals: -1.07049, Convergence: 0.004878\n",
      "Epoch: 177, Loss: 2170.85334, Residuals: -1.06781, Convergence: 0.004335\n",
      "Epoch: 178, Loss: 2162.40893, Residuals: -1.06503, Convergence: 0.003905\n",
      "Epoch: 179, Loss: 2154.72452, Residuals: -1.06214, Convergence: 0.003566\n",
      "Epoch: 180, Loss: 2147.65044, Residuals: -1.05912, Convergence: 0.003294\n",
      "Epoch: 181, Loss: 2141.08930, Residuals: -1.05599, Convergence: 0.003064\n",
      "Epoch: 182, Loss: 2134.98619, Residuals: -1.05279, Convergence: 0.002859\n",
      "Epoch: 183, Loss: 2129.30999, Residuals: -1.04955, Convergence: 0.002666\n",
      "Epoch: 184, Loss: 2124.03758, Residuals: -1.04633, Convergence: 0.002482\n",
      "Epoch: 185, Loss: 2119.14745, Residuals: -1.04317, Convergence: 0.002308\n",
      "Epoch: 186, Loss: 2114.61717, Residuals: -1.04010, Convergence: 0.002142\n",
      "Epoch: 187, Loss: 2110.42445, Residuals: -1.03714, Convergence: 0.001987\n",
      "Epoch: 188, Loss: 2106.54697, Residuals: -1.03432, Convergence: 0.001841\n",
      "Epoch: 189, Loss: 2102.96373, Residuals: -1.03163, Convergence: 0.001704\n",
      "Epoch: 190, Loss: 2099.65285, Residuals: -1.02909, Convergence: 0.001577\n",
      "Epoch: 191, Loss: 2096.59313, Residuals: -1.02669, Convergence: 0.001459\n",
      "Epoch: 192, Loss: 2093.76487, Residuals: -1.02443, Convergence: 0.001351\n",
      "Epoch: 193, Loss: 2091.14832, Residuals: -1.02230, Convergence: 0.001251\n",
      "Epoch: 194, Loss: 2088.72516, Residuals: -1.02029, Convergence: 0.001160\n",
      "Epoch: 195, Loss: 2086.47778, Residuals: -1.01839, Convergence: 0.001077\n",
      "Epoch: 196, Loss: 2084.38927, Residuals: -1.01660, Convergence: 0.001002\n",
      "Epoch: 197, Loss: 2082.44521, Residuals: -1.01492, Convergence: 0.000934\n",
      "Evidence 14261.705\n",
      "\n",
      "Epoch: 197, Evidence: 14261.70508, Convergence: 0.224096\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.38e-01\n",
      "Epoch: 197, Loss: 2471.53993, Residuals: -1.01492, Convergence:   inf\n",
      "Epoch: 198, Loss: 2457.60364, Residuals: -1.01238, Convergence: 0.005671\n",
      "Epoch: 199, Loss: 2446.23908, Residuals: -1.00931, Convergence: 0.004646\n",
      "Epoch: 200, Loss: 2436.51597, Residuals: -1.00628, Convergence: 0.003991\n",
      "Epoch: 201, Loss: 2428.13047, Residuals: -1.00341, Convergence: 0.003453\n",
      "Epoch: 202, Loss: 2420.85246, Residuals: -1.00075, Convergence: 0.003006\n",
      "Epoch: 203, Loss: 2414.50122, Residuals: -0.99830, Convergence: 0.002630\n",
      "Epoch: 204, Loss: 2408.92811, Residuals: -0.99605, Convergence: 0.002314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 205, Loss: 2404.00952, Residuals: -0.99399, Convergence: 0.002046\n",
      "Epoch: 206, Loss: 2399.64381, Residuals: -0.99210, Convergence: 0.001819\n",
      "Epoch: 207, Loss: 2395.74487, Residuals: -0.99037, Convergence: 0.001627\n",
      "Epoch: 208, Loss: 2392.24277, Residuals: -0.98877, Convergence: 0.001464\n",
      "Epoch: 209, Loss: 2389.07858, Residuals: -0.98730, Convergence: 0.001324\n",
      "Epoch: 210, Loss: 2386.20252, Residuals: -0.98594, Convergence: 0.001205\n",
      "Epoch: 211, Loss: 2383.57549, Residuals: -0.98469, Convergence: 0.001102\n",
      "Epoch: 212, Loss: 2381.16262, Residuals: -0.98353, Convergence: 0.001013\n",
      "Epoch: 213, Loss: 2378.93712, Residuals: -0.98246, Convergence: 0.000935\n",
      "Evidence 14661.061\n",
      "\n",
      "Epoch: 213, Evidence: 14661.06055, Convergence: 0.027239\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.34e-01\n",
      "Epoch: 213, Loss: 2476.59461, Residuals: -0.98246, Convergence:   inf\n",
      "Epoch: 214, Loss: 2470.15971, Residuals: -0.97948, Convergence: 0.002605\n",
      "Epoch: 215, Loss: 2464.86082, Residuals: -0.97687, Convergence: 0.002150\n",
      "Epoch: 216, Loss: 2460.37840, Residuals: -0.97464, Convergence: 0.001822\n",
      "Epoch: 217, Loss: 2456.52792, Residuals: -0.97274, Convergence: 0.001567\n",
      "Epoch: 218, Loss: 2453.17286, Residuals: -0.97113, Convergence: 0.001368\n",
      "Epoch: 219, Loss: 2450.21236, Residuals: -0.96975, Convergence: 0.001208\n",
      "Epoch: 220, Loss: 2447.56963, Residuals: -0.96858, Convergence: 0.001080\n",
      "Epoch: 221, Loss: 2445.18525, Residuals: -0.96758, Convergence: 0.000975\n",
      "Evidence 14743.281\n",
      "\n",
      "Epoch: 221, Evidence: 14743.28125, Convergence: 0.005577\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.61e-01\n",
      "Epoch: 221, Loss: 2478.21618, Residuals: -0.96758, Convergence:   inf\n",
      "Epoch: 222, Loss: 2474.34783, Residuals: -0.96528, Convergence: 0.001563\n",
      "Epoch: 223, Loss: 2471.13394, Residuals: -0.96344, Convergence: 0.001301\n",
      "Epoch: 224, Loss: 2468.37832, Residuals: -0.96195, Convergence: 0.001116\n",
      "Epoch: 225, Loss: 2465.96584, Residuals: -0.96074, Convergence: 0.000978\n",
      "Evidence 14770.999\n",
      "\n",
      "Epoch: 225, Evidence: 14770.99902, Convergence: 0.001876\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.10e-01\n",
      "Epoch: 225, Loss: 2479.19246, Residuals: -0.96074, Convergence:   inf\n",
      "Epoch: 226, Loss: 2476.31016, Residuals: -0.95886, Convergence: 0.001164\n",
      "Epoch: 227, Loss: 2473.88596, Residuals: -0.95738, Convergence: 0.000980\n",
      "Evidence 14783.060\n",
      "\n",
      "Epoch: 227, Evidence: 14783.05957, Convergence: 0.000816\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.75e-01\n",
      "Epoch: 227, Loss: 2479.90498, Residuals: -0.95738, Convergence:   inf\n",
      "Epoch: 228, Loss: 2475.31961, Residuals: -0.95475, Convergence: 0.001852\n",
      "Epoch: 229, Loss: 2471.81934, Residuals: -0.95287, Convergence: 0.001416\n",
      "Epoch: 230, Loss: 2468.97576, Residuals: -0.95167, Convergence: 0.001152\n",
      "Epoch: 231, Loss: 2466.56168, Residuals: -0.95104, Convergence: 0.000979\n",
      "Evidence 14800.927\n",
      "\n",
      "Epoch: 231, Evidence: 14800.92676, Convergence: 0.002022\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.44e-01\n",
      "Epoch: 231, Loss: 2480.01379, Residuals: -0.95104, Convergence:   inf\n",
      "Epoch: 232, Loss: 2476.95331, Residuals: -0.94856, Convergence: 0.001236\n",
      "Epoch: 233, Loss: 2474.52529, Residuals: -0.94728, Convergence: 0.000981\n",
      "Evidence 14811.957\n",
      "\n",
      "Epoch: 233, Evidence: 14811.95703, Convergence: 0.000745\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.22e-01\n",
      "Epoch: 233, Loss: 2480.19557, Residuals: -0.94728, Convergence:   inf\n",
      "Epoch: 234, Loss: 2475.68582, Residuals: -0.94358, Convergence: 0.001822\n",
      "Epoch: 235, Loss: 2472.47913, Residuals: -0.94450, Convergence: 0.001297\n",
      "Epoch: 236, Loss: 2469.80735, Residuals: -0.94543, Convergence: 0.001082\n",
      "Epoch: 237, Loss: 2467.46168, Residuals: -0.94812, Convergence: 0.000951\n",
      "Evidence 14827.946\n",
      "\n",
      "Epoch: 237, Evidence: 14827.94629, Convergence: 0.001822\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.09e-01\n",
      "Epoch: 237, Loss: 2479.47976, Residuals: -0.94812, Convergence:   inf\n",
      "Epoch: 238, Loss: 2477.73324, Residuals: -0.94574, Convergence: 0.000705\n",
      "Evidence 14834.647\n",
      "\n",
      "Epoch: 238, Evidence: 14834.64746, Convergence: 0.000452\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.91e-02\n",
      "Epoch: 238, Loss: 2480.36712, Residuals: -0.94574, Convergence:   inf\n",
      "Epoch: 239, Loss: 2521.95369, Residuals: -0.98347, Convergence: -0.016490\n",
      "Epoch: 239, Loss: 2477.95829, Residuals: -0.94320, Convergence: 0.000972\n",
      "Evidence 14839.073\n",
      "\n",
      "Epoch: 239, Evidence: 14839.07324, Convergence: 0.000750\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.22e-02\n",
      "Epoch: 239, Loss: 2479.80915, Residuals: -0.94320, Convergence:   inf\n",
      "Epoch: 240, Loss: 2484.82629, Residuals: -0.94579, Convergence: -0.002019\n",
      "Epoch: 240, Loss: 2479.76212, Residuals: -0.94217, Convergence: 0.000019\n",
      "Evidence 14840.693\n",
      "\n",
      "Epoch: 240, Evidence: 14840.69336, Convergence: 0.000859\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 384.94925, Residuals: -4.54888, Convergence:   inf\n",
      "Epoch: 1, Loss: 359.21946, Residuals: -4.42831, Convergence: 0.071627\n",
      "Epoch: 2, Loss: 338.18933, Residuals: -4.26440, Convergence: 0.062185\n",
      "Epoch: 3, Loss: 322.11212, Residuals: -4.10019, Convergence: 0.049912\n",
      "Epoch: 4, Loss: 309.84659, Residuals: -3.95550, Convergence: 0.039586\n",
      "Epoch: 5, Loss: 300.11636, Residuals: -3.82759, Convergence: 0.032422\n",
      "Epoch: 6, Loss: 292.21320, Residuals: -3.71616, Convergence: 0.027046\n",
      "Epoch: 7, Loss: 285.64659, Residuals: -3.62069, Convergence: 0.022989\n",
      "Epoch: 8, Loss: 280.05449, Residuals: -3.53912, Convergence: 0.019968\n",
      "Epoch: 9, Loss: 275.18142, Residuals: -3.46910, Convergence: 0.017709\n",
      "Epoch: 10, Loss: 270.84569, Residuals: -3.40853, Convergence: 0.016008\n",
      "Epoch: 11, Loss: 266.91566, Residuals: -3.35560, Convergence: 0.014724\n",
      "Epoch: 12, Loss: 263.29519, Residuals: -3.30877, Convergence: 0.013751\n",
      "Epoch: 13, Loss: 259.91456, Residuals: -3.26673, Convergence: 0.013007\n",
      "Epoch: 14, Loss: 256.72475, Residuals: -3.22833, Convergence: 0.012425\n",
      "Epoch: 15, Loss: 253.69475, Residuals: -3.19263, Convergence: 0.011943\n",
      "Epoch: 16, Loss: 250.80893, Residuals: -3.15904, Convergence: 0.011506\n",
      "Epoch: 17, Loss: 248.05639, Residuals: -3.12717, Convergence: 0.011096\n",
      "Epoch: 18, Loss: 245.41665, Residuals: -3.09666, Convergence: 0.010756\n",
      "Epoch: 19, Loss: 242.85712, Residuals: -3.06699, Convergence: 0.010539\n",
      "Epoch: 20, Loss: 240.34080, Residuals: -3.03756, Convergence: 0.010470\n",
      "Epoch: 21, Loss: 237.83491, Residuals: -3.00785, Convergence: 0.010536\n",
      "Epoch: 22, Loss: 235.31455, Residuals: -2.97752, Convergence: 0.010711\n",
      "Epoch: 23, Loss: 232.75373, Residuals: -2.94624, Convergence: 0.011002\n",
      "Epoch: 24, Loss: 230.10991, Residuals: -2.91355, Convergence: 0.011489\n",
      "Epoch: 25, Loss: 227.33101, Residuals: -2.87882, Convergence: 0.012224\n",
      "Epoch: 26, Loss: 224.42221, Residuals: -2.84201, Convergence: 0.012961\n",
      "Epoch: 27, Loss: 221.49708, Residuals: -2.80432, Convergence: 0.013206\n",
      "Epoch: 28, Loss: 218.64894, Residuals: -2.76688, Convergence: 0.013026\n",
      "Epoch: 29, Loss: 215.88913, Residuals: -2.72996, Convergence: 0.012783\n",
      "Epoch: 30, Loss: 213.20172, Residuals: -2.69344, Convergence: 0.012605\n",
      "Epoch: 31, Loss: 210.57108, Residuals: -2.65720, Convergence: 0.012493\n",
      "Epoch: 32, Loss: 207.98693, Residuals: -2.62111, Convergence: 0.012425\n",
      "Epoch: 33, Loss: 205.44397, Residuals: -2.58509, Convergence: 0.012378\n",
      "Epoch: 34, Loss: 202.94063, Residuals: -2.54910, Convergence: 0.012335\n",
      "Epoch: 35, Loss: 200.47782, Residuals: -2.51312, Convergence: 0.012285\n",
      "Epoch: 36, Loss: 198.05794, Residuals: -2.47715, Convergence: 0.012218\n",
      "Epoch: 37, Loss: 195.68421, Residuals: -2.44120, Convergence: 0.012130\n",
      "Epoch: 38, Loss: 193.36009, Residuals: -2.40528, Convergence: 0.012020\n",
      "Epoch: 39, Loss: 191.08903, Residuals: -2.36945, Convergence: 0.011885\n",
      "Epoch: 40, Loss: 188.87424, Residuals: -2.33372, Convergence: 0.011726\n",
      "Epoch: 41, Loss: 186.71859, Residuals: -2.29816, Convergence: 0.011545\n",
      "Epoch: 42, Loss: 184.62462, Residuals: -2.26279, Convergence: 0.011342\n",
      "Epoch: 43, Loss: 182.59453, Residuals: -2.22768, Convergence: 0.011118\n",
      "Epoch: 44, Loss: 180.63031, Residuals: -2.19288, Convergence: 0.010874\n",
      "Epoch: 45, Loss: 178.73372, Residuals: -2.15844, Convergence: 0.010611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46, Loss: 176.90626, Residuals: -2.12442, Convergence: 0.010330\n",
      "Epoch: 47, Loss: 175.14905, Residuals: -2.09087, Convergence: 0.010033\n",
      "Epoch: 48, Loss: 173.46270, Residuals: -2.05786, Convergence: 0.009722\n",
      "Epoch: 49, Loss: 171.84712, Residuals: -2.02543, Convergence: 0.009401\n",
      "Epoch: 50, Loss: 170.30157, Residuals: -1.99362, Convergence: 0.009075\n",
      "Epoch: 51, Loss: 168.82466, Residuals: -1.96246, Convergence: 0.008748\n",
      "Epoch: 52, Loss: 167.41450, Residuals: -1.93199, Convergence: 0.008423\n",
      "Epoch: 53, Loss: 166.06885, Residuals: -1.90222, Convergence: 0.008103\n",
      "Epoch: 54, Loss: 164.78518, Residuals: -1.87319, Convergence: 0.007790\n",
      "Epoch: 55, Loss: 163.56080, Residuals: -1.84490, Convergence: 0.007486\n",
      "Epoch: 56, Loss: 162.39285, Residuals: -1.81738, Convergence: 0.007192\n",
      "Epoch: 57, Loss: 161.27840, Residuals: -1.79061, Convergence: 0.006910\n",
      "Epoch: 58, Loss: 160.21446, Residuals: -1.76461, Convergence: 0.006641\n",
      "Epoch: 59, Loss: 159.19807, Residuals: -1.73936, Convergence: 0.006384\n",
      "Epoch: 60, Loss: 158.22641, Residuals: -1.71485, Convergence: 0.006141\n",
      "Epoch: 61, Loss: 157.29690, Residuals: -1.69106, Convergence: 0.005909\n",
      "Epoch: 62, Loss: 156.40725, Residuals: -1.66798, Convergence: 0.005688\n",
      "Epoch: 63, Loss: 155.55549, Residuals: -1.64559, Convergence: 0.005476\n",
      "Epoch: 64, Loss: 154.73995, Residuals: -1.62388, Convergence: 0.005270\n",
      "Epoch: 65, Loss: 153.95922, Residuals: -1.60284, Convergence: 0.005071\n",
      "Epoch: 66, Loss: 153.21206, Residuals: -1.58246, Convergence: 0.004877\n",
      "Epoch: 67, Loss: 152.49741, Residuals: -1.56273, Convergence: 0.004686\n",
      "Epoch: 68, Loss: 151.81425, Residuals: -1.54365, Convergence: 0.004500\n",
      "Epoch: 69, Loss: 151.16161, Residuals: -1.52521, Convergence: 0.004318\n",
      "Epoch: 70, Loss: 150.53854, Residuals: -1.50741, Convergence: 0.004139\n",
      "Epoch: 71, Loss: 149.94410, Residuals: -1.49023, Convergence: 0.003964\n",
      "Epoch: 72, Loss: 149.37732, Residuals: -1.47368, Convergence: 0.003794\n",
      "Epoch: 73, Loss: 148.83724, Residuals: -1.45774, Convergence: 0.003629\n",
      "Epoch: 74, Loss: 148.32286, Residuals: -1.44239, Convergence: 0.003468\n",
      "Epoch: 75, Loss: 147.83319, Residuals: -1.42763, Convergence: 0.003312\n",
      "Epoch: 76, Loss: 147.36722, Residuals: -1.41345, Convergence: 0.003162\n",
      "Epoch: 77, Loss: 146.92395, Residuals: -1.39983, Convergence: 0.003017\n",
      "Epoch: 78, Loss: 146.50235, Residuals: -1.38676, Convergence: 0.002878\n",
      "Epoch: 79, Loss: 146.10143, Residuals: -1.37422, Convergence: 0.002744\n",
      "Epoch: 80, Loss: 145.72020, Residuals: -1.36219, Convergence: 0.002616\n",
      "Epoch: 81, Loss: 145.35768, Residuals: -1.35066, Convergence: 0.002494\n",
      "Epoch: 82, Loss: 145.01293, Residuals: -1.33961, Convergence: 0.002377\n",
      "Epoch: 83, Loss: 144.68503, Residuals: -1.32903, Convergence: 0.002266\n",
      "Epoch: 84, Loss: 144.37309, Residuals: -1.31889, Convergence: 0.002161\n",
      "Epoch: 85, Loss: 144.07626, Residuals: -1.30919, Convergence: 0.002060\n",
      "Epoch: 86, Loss: 143.79374, Residuals: -1.29990, Convergence: 0.001965\n",
      "Epoch: 87, Loss: 143.52476, Residuals: -1.29101, Convergence: 0.001874\n",
      "Epoch: 88, Loss: 143.26861, Residuals: -1.28250, Convergence: 0.001788\n",
      "Epoch: 89, Loss: 143.02459, Residuals: -1.27435, Convergence: 0.001706\n",
      "Epoch: 90, Loss: 142.79209, Residuals: -1.26656, Convergence: 0.001628\n",
      "Epoch: 91, Loss: 142.57052, Residuals: -1.25910, Convergence: 0.001554\n",
      "Epoch: 92, Loss: 142.35932, Residuals: -1.25195, Convergence: 0.001484\n",
      "Epoch: 93, Loss: 142.15802, Residuals: -1.24512, Convergence: 0.001416\n",
      "Epoch: 94, Loss: 141.96614, Residuals: -1.23857, Convergence: 0.001352\n",
      "Epoch: 95, Loss: 141.78327, Residuals: -1.23230, Convergence: 0.001290\n",
      "Epoch: 96, Loss: 141.60903, Residuals: -1.22630, Convergence: 0.001230\n",
      "Epoch: 97, Loss: 141.44307, Residuals: -1.22056, Convergence: 0.001173\n",
      "Epoch: 98, Loss: 141.28509, Residuals: -1.21506, Convergence: 0.001118\n",
      "Epoch: 99, Loss: 141.13480, Residuals: -1.20979, Convergence: 0.001065\n",
      "Epoch: 100, Loss: 140.99196, Residuals: -1.20474, Convergence: 0.001013\n",
      "Epoch: 101, Loss: 140.85634, Residuals: -1.19991, Convergence: 0.000963\n",
      "Evidence -182.439\n",
      "\n",
      "Epoch: 101, Evidence: -182.43930, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.25e-01\n",
      "Epoch: 101, Loss: 1355.89373, Residuals: -1.19991, Convergence:   inf\n",
      "Epoch: 102, Loss: 1294.00544, Residuals: -1.23041, Convergence: 0.047827\n",
      "Epoch: 103, Loss: 1247.16964, Residuals: -1.25433, Convergence: 0.037554\n",
      "Epoch: 104, Loss: 1212.02317, Residuals: -1.27137, Convergence: 0.028998\n",
      "Epoch: 105, Loss: 1184.85807, Residuals: -1.28323, Convergence: 0.022927\n",
      "Epoch: 106, Loss: 1162.99621, Residuals: -1.29193, Convergence: 0.018798\n",
      "Epoch: 107, Loss: 1144.90672, Residuals: -1.29859, Convergence: 0.015800\n",
      "Epoch: 108, Loss: 1129.68271, Residuals: -1.30366, Convergence: 0.013476\n",
      "Epoch: 109, Loss: 1116.71554, Residuals: -1.30739, Convergence: 0.011612\n",
      "Epoch: 110, Loss: 1105.55877, Residuals: -1.30995, Convergence: 0.010092\n",
      "Epoch: 111, Loss: 1095.86493, Residuals: -1.31147, Convergence: 0.008846\n",
      "Epoch: 112, Loss: 1087.35743, Residuals: -1.31205, Convergence: 0.007824\n",
      "Epoch: 113, Loss: 1079.80967, Residuals: -1.31179, Convergence: 0.006990\n",
      "Epoch: 114, Loss: 1073.03495, Residuals: -1.31079, Convergence: 0.006314\n",
      "Epoch: 115, Loss: 1066.87620, Residuals: -1.30911, Convergence: 0.005773\n",
      "Epoch: 116, Loss: 1061.20266, Residuals: -1.30682, Convergence: 0.005346\n",
      "Epoch: 117, Loss: 1055.90435, Residuals: -1.30397, Convergence: 0.005018\n",
      "Epoch: 118, Loss: 1050.89337, Residuals: -1.30060, Convergence: 0.004768\n",
      "Epoch: 119, Loss: 1046.10111, Residuals: -1.29675, Convergence: 0.004581\n",
      "Epoch: 120, Loss: 1041.47960, Residuals: -1.29247, Convergence: 0.004437\n",
      "Epoch: 121, Loss: 1036.99584, Residuals: -1.28779, Convergence: 0.004324\n",
      "Epoch: 122, Loss: 1032.62965, Residuals: -1.28274, Convergence: 0.004228\n",
      "Epoch: 123, Loss: 1028.36551, Residuals: -1.27736, Convergence: 0.004147\n",
      "Epoch: 124, Loss: 1024.18743, Residuals: -1.27165, Convergence: 0.004079\n",
      "Epoch: 125, Loss: 1020.07604, Residuals: -1.26565, Convergence: 0.004030\n",
      "Epoch: 126, Loss: 1016.00841, Residuals: -1.25937, Convergence: 0.004004\n",
      "Epoch: 127, Loss: 1011.96362, Residuals: -1.25285, Convergence: 0.003997\n",
      "Epoch: 128, Loss: 1007.93071, Residuals: -1.24612, Convergence: 0.004001\n",
      "Epoch: 129, Loss: 1003.91886, Residuals: -1.23924, Convergence: 0.003996\n",
      "Epoch: 130, Loss: 999.96093, Residuals: -1.23228, Convergence: 0.003958\n",
      "Epoch: 131, Loss: 996.10544, Residuals: -1.22528, Convergence: 0.003871\n",
      "Epoch: 132, Loss: 992.40307, Residuals: -1.21832, Convergence: 0.003731\n",
      "Epoch: 133, Loss: 988.88958, Residuals: -1.21142, Convergence: 0.003553\n",
      "Epoch: 134, Loss: 985.58173, Residuals: -1.20463, Convergence: 0.003356\n",
      "Epoch: 135, Loss: 982.47957, Residuals: -1.19796, Convergence: 0.003157\n",
      "Epoch: 136, Loss: 979.57255, Residuals: -1.19144, Convergence: 0.002968\n",
      "Epoch: 137, Loss: 976.84557, Residuals: -1.18508, Convergence: 0.002792\n",
      "Epoch: 138, Loss: 974.28196, Residuals: -1.17891, Convergence: 0.002631\n",
      "Epoch: 139, Loss: 971.86660, Residuals: -1.17294, Convergence: 0.002485\n",
      "Epoch: 140, Loss: 969.58476, Residuals: -1.16717, Convergence: 0.002353\n",
      "Epoch: 141, Loss: 967.42497, Residuals: -1.16162, Convergence: 0.002233\n",
      "Epoch: 142, Loss: 965.37555, Residuals: -1.15628, Convergence: 0.002123\n",
      "Epoch: 143, Loss: 963.42747, Residuals: -1.15115, Convergence: 0.002022\n",
      "Epoch: 144, Loss: 961.57163, Residuals: -1.14625, Convergence: 0.001930\n",
      "Epoch: 145, Loss: 959.80081, Residuals: -1.14155, Convergence: 0.001845\n",
      "Epoch: 146, Loss: 958.10781, Residuals: -1.13706, Convergence: 0.001767\n",
      "Epoch: 147, Loss: 956.48641, Residuals: -1.13277, Convergence: 0.001695\n",
      "Epoch: 148, Loss: 954.93105, Residuals: -1.12867, Convergence: 0.001629\n",
      "Epoch: 149, Loss: 953.43618, Residuals: -1.12475, Convergence: 0.001568\n",
      "Epoch: 150, Loss: 951.99754, Residuals: -1.12101, Convergence: 0.001511\n",
      "Epoch: 151, Loss: 950.61064, Residuals: -1.11743, Convergence: 0.001459\n",
      "Epoch: 152, Loss: 949.27177, Residuals: -1.11402, Convergence: 0.001410\n",
      "Epoch: 153, Loss: 947.97749, Residuals: -1.11074, Convergence: 0.001365\n",
      "Epoch: 154, Loss: 946.72461, Residuals: -1.10761, Convergence: 0.001323\n",
      "Epoch: 155, Loss: 945.51018, Residuals: -1.10461, Convergence: 0.001284\n",
      "Epoch: 156, Loss: 944.33165, Residuals: -1.10173, Convergence: 0.001248\n",
      "Epoch: 157, Loss: 943.18640, Residuals: -1.09896, Convergence: 0.001214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 158, Loss: 942.07223, Residuals: -1.09630, Convergence: 0.001183\n",
      "Epoch: 159, Loss: 940.98633, Residuals: -1.09373, Convergence: 0.001154\n",
      "Epoch: 160, Loss: 939.92664, Residuals: -1.09126, Convergence: 0.001127\n",
      "Epoch: 161, Loss: 938.89020, Residuals: -1.08886, Convergence: 0.001104\n",
      "Epoch: 162, Loss: 937.87465, Residuals: -1.08653, Convergence: 0.001083\n",
      "Epoch: 163, Loss: 936.87686, Residuals: -1.08426, Convergence: 0.001065\n",
      "Epoch: 164, Loss: 935.89385, Residuals: -1.08205, Convergence: 0.001050\n",
      "Epoch: 165, Loss: 934.92273, Residuals: -1.07989, Convergence: 0.001039\n",
      "Epoch: 166, Loss: 933.95981, Residuals: -1.07775, Convergence: 0.001031\n",
      "Epoch: 167, Loss: 933.00322, Residuals: -1.07565, Convergence: 0.001025\n",
      "Epoch: 168, Loss: 932.05005, Residuals: -1.07357, Convergence: 0.001023\n",
      "Epoch: 169, Loss: 931.09937, Residuals: -1.07150, Convergence: 0.001021\n",
      "Epoch: 170, Loss: 930.15089, Residuals: -1.06944, Convergence: 0.001020\n",
      "Epoch: 171, Loss: 929.20592, Residuals: -1.06739, Convergence: 0.001017\n",
      "Epoch: 172, Loss: 928.26733, Residuals: -1.06535, Convergence: 0.001011\n",
      "Epoch: 173, Loss: 927.33908, Residuals: -1.06334, Convergence: 0.001001\n",
      "Epoch: 174, Loss: 926.42578, Residuals: -1.06135, Convergence: 0.000986\n",
      "Evidence 11195.278\n",
      "\n",
      "Epoch: 174, Evidence: 11195.27832, Convergence: 1.016296\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.73e-01\n",
      "Epoch: 174, Loss: 2334.65402, Residuals: -1.06135, Convergence:   inf\n",
      "Epoch: 175, Loss: 2292.54243, Residuals: -1.06908, Convergence: 0.018369\n",
      "Epoch: 176, Loss: 2264.01805, Residuals: -1.06722, Convergence: 0.012599\n",
      "Epoch: 177, Loss: 2240.38531, Residuals: -1.06441, Convergence: 0.010549\n",
      "Epoch: 178, Loss: 2220.49573, Residuals: -1.06144, Convergence: 0.008957\n",
      "Epoch: 179, Loss: 2203.64711, Residuals: -1.05844, Convergence: 0.007646\n",
      "Epoch: 180, Loss: 2189.29432, Residuals: -1.05546, Convergence: 0.006556\n",
      "Epoch: 181, Loss: 2176.98804, Residuals: -1.05252, Convergence: 0.005653\n",
      "Epoch: 182, Loss: 2166.35099, Residuals: -1.04961, Convergence: 0.004910\n",
      "Epoch: 183, Loss: 2157.06198, Residuals: -1.04675, Convergence: 0.004306\n",
      "Epoch: 184, Loss: 2148.85136, Residuals: -1.04392, Convergence: 0.003821\n",
      "Epoch: 185, Loss: 2141.49400, Residuals: -1.04109, Convergence: 0.003436\n",
      "Epoch: 186, Loss: 2134.81118, Residuals: -1.03824, Convergence: 0.003130\n",
      "Epoch: 187, Loss: 2128.67330, Residuals: -1.03535, Convergence: 0.002883\n",
      "Epoch: 188, Loss: 2122.99671, Residuals: -1.03243, Convergence: 0.002674\n",
      "Epoch: 189, Loss: 2117.73473, Residuals: -1.02950, Convergence: 0.002485\n",
      "Epoch: 190, Loss: 2112.85950, Residuals: -1.02658, Convergence: 0.002307\n",
      "Epoch: 191, Loss: 2108.35057, Residuals: -1.02371, Convergence: 0.002139\n",
      "Epoch: 192, Loss: 2104.18238, Residuals: -1.02091, Convergence: 0.001981\n",
      "Epoch: 193, Loss: 2100.32798, Residuals: -1.01821, Convergence: 0.001835\n",
      "Epoch: 194, Loss: 2096.75569, Residuals: -1.01561, Convergence: 0.001704\n",
      "Epoch: 195, Loss: 2093.43403, Residuals: -1.01312, Convergence: 0.001587\n",
      "Epoch: 196, Loss: 2090.33162, Residuals: -1.01074, Convergence: 0.001484\n",
      "Epoch: 197, Loss: 2087.42070, Residuals: -1.00847, Convergence: 0.001395\n",
      "Epoch: 198, Loss: 2084.67481, Residuals: -1.00630, Convergence: 0.001317\n",
      "Epoch: 199, Loss: 2082.07225, Residuals: -1.00422, Convergence: 0.001250\n",
      "Epoch: 200, Loss: 2079.59433, Residuals: -1.00224, Convergence: 0.001192\n",
      "Epoch: 201, Loss: 2077.22734, Residuals: -1.00034, Convergence: 0.001139\n",
      "Epoch: 202, Loss: 2074.95987, Residuals: -0.99853, Convergence: 0.001093\n",
      "Epoch: 203, Loss: 2072.78421, Residuals: -0.99680, Convergence: 0.001050\n",
      "Epoch: 204, Loss: 2070.69497, Residuals: -0.99515, Convergence: 0.001009\n",
      "Epoch: 205, Loss: 2068.68958, Residuals: -0.99357, Convergence: 0.000969\n",
      "Evidence 14418.572\n",
      "\n",
      "Epoch: 205, Evidence: 14418.57227, Convergence: 0.223552\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.33e-01\n",
      "Epoch: 205, Loss: 2450.51799, Residuals: -0.99357, Convergence:   inf\n",
      "Epoch: 206, Loss: 2436.71964, Residuals: -0.99033, Convergence: 0.005663\n",
      "Epoch: 207, Loss: 2425.36490, Residuals: -0.98652, Convergence: 0.004682\n",
      "Epoch: 208, Loss: 2415.49205, Residuals: -0.98288, Convergence: 0.004087\n",
      "Epoch: 209, Loss: 2406.87199, Residuals: -0.97947, Convergence: 0.003581\n",
      "Epoch: 210, Loss: 2399.31981, Residuals: -0.97633, Convergence: 0.003148\n",
      "Epoch: 211, Loss: 2392.67997, Residuals: -0.97348, Convergence: 0.002775\n",
      "Epoch: 212, Loss: 2386.82071, Residuals: -0.97089, Convergence: 0.002455\n",
      "Epoch: 213, Loss: 2381.63027, Residuals: -0.96855, Convergence: 0.002179\n",
      "Epoch: 214, Loss: 2377.01190, Residuals: -0.96643, Convergence: 0.001943\n",
      "Epoch: 215, Loss: 2372.88537, Residuals: -0.96451, Convergence: 0.001739\n",
      "Epoch: 216, Loss: 2369.18038, Residuals: -0.96275, Convergence: 0.001564\n",
      "Epoch: 217, Loss: 2365.83944, Residuals: -0.96115, Convergence: 0.001412\n",
      "Epoch: 218, Loss: 2362.81204, Residuals: -0.95967, Convergence: 0.001281\n",
      "Epoch: 219, Loss: 2360.05660, Residuals: -0.95831, Convergence: 0.001168\n",
      "Epoch: 220, Loss: 2357.53762, Residuals: -0.95705, Convergence: 0.001068\n",
      "Epoch: 221, Loss: 2355.22508, Residuals: -0.95588, Convergence: 0.000982\n",
      "Evidence 14828.688\n",
      "\n",
      "Epoch: 221, Evidence: 14828.68750, Convergence: 0.027657\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.29e-01\n",
      "Epoch: 221, Loss: 2455.44235, Residuals: -0.95588, Convergence:   inf\n",
      "Epoch: 222, Loss: 2448.58980, Residuals: -0.95246, Convergence: 0.002799\n",
      "Epoch: 223, Loss: 2442.93153, Residuals: -0.94954, Convergence: 0.002316\n",
      "Epoch: 224, Loss: 2438.14328, Residuals: -0.94709, Convergence: 0.001964\n",
      "Epoch: 225, Loss: 2434.03179, Residuals: -0.94503, Convergence: 0.001689\n",
      "Epoch: 226, Loss: 2430.45615, Residuals: -0.94329, Convergence: 0.001471\n",
      "Epoch: 227, Loss: 2427.31040, Residuals: -0.94183, Convergence: 0.001296\n",
      "Epoch: 228, Loss: 2424.51235, Residuals: -0.94059, Convergence: 0.001154\n",
      "Epoch: 229, Loss: 2421.99937, Residuals: -0.93953, Convergence: 0.001038\n",
      "Epoch: 230, Loss: 2419.72255, Residuals: -0.93862, Convergence: 0.000941\n",
      "Evidence 14916.631\n",
      "\n",
      "Epoch: 230, Evidence: 14916.63086, Convergence: 0.005896\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.55e-01\n",
      "Epoch: 230, Loss: 2456.99319, Residuals: -0.93862, Convergence:   inf\n",
      "Epoch: 231, Loss: 2453.04247, Residuals: -0.93604, Convergence: 0.001611\n",
      "Epoch: 232, Loss: 2449.76699, Residuals: -0.93404, Convergence: 0.001337\n",
      "Epoch: 233, Loss: 2446.96877, Residuals: -0.93246, Convergence: 0.001144\n",
      "Epoch: 234, Loss: 2444.53095, Residuals: -0.93121, Convergence: 0.000997\n",
      "Evidence 14946.004\n",
      "\n",
      "Epoch: 234, Evidence: 14946.00391, Convergence: 0.001965\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.04e-01\n",
      "Epoch: 234, Loss: 2458.03001, Residuals: -0.93121, Convergence:   inf\n",
      "Epoch: 235, Loss: 2455.11564, Residuals: -0.92917, Convergence: 0.001187\n",
      "Epoch: 236, Loss: 2452.68476, Residuals: -0.92761, Convergence: 0.000991\n",
      "Evidence 14958.116\n",
      "\n",
      "Epoch: 236, Evidence: 14958.11621, Convergence: 0.000810\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.69e-01\n",
      "Epoch: 236, Loss: 2458.73951, Residuals: -0.92761, Convergence:   inf\n",
      "Epoch: 237, Loss: 2454.29603, Residuals: -0.92488, Convergence: 0.001810\n",
      "Epoch: 238, Loss: 2450.85598, Residuals: -0.92290, Convergence: 0.001404\n",
      "Epoch: 239, Loss: 2448.07709, Residuals: -0.92161, Convergence: 0.001135\n",
      "Epoch: 240, Loss: 2445.72115, Residuals: -0.92094, Convergence: 0.000963\n",
      "Evidence 14975.541\n",
      "\n",
      "Epoch: 240, Evidence: 14975.54102, Convergence: 0.001972\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.39e-01\n",
      "Epoch: 240, Loss: 2458.91967, Residuals: -0.92094, Convergence:   inf\n",
      "Epoch: 241, Loss: 2456.00634, Residuals: -0.91811, Convergence: 0.001186\n",
      "Epoch: 242, Loss: 2453.71857, Residuals: -0.91665, Convergence: 0.000932\n",
      "Evidence 14986.256\n",
      "\n",
      "Epoch: 242, Evidence: 14986.25586, Convergence: 0.000715\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.18e-01\n",
      "Epoch: 242, Loss: 2459.09536, Residuals: -0.91665, Convergence:   inf\n",
      "Epoch: 243, Loss: 2455.01829, Residuals: -0.91233, Convergence: 0.001661\n",
      "Epoch: 244, Loss: 2452.08179, Residuals: -0.91336, Convergence: 0.001198\n",
      "Epoch: 245, Loss: 2449.48607, Residuals: -0.91448, Convergence: 0.001060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 246, Loss: 2447.27111, Residuals: -0.91747, Convergence: 0.000905\n",
      "Evidence 15001.244\n",
      "\n",
      "Epoch: 246, Evidence: 15001.24414, Convergence: 0.001713\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.04e-01\n",
      "Epoch: 246, Loss: 2458.43531, Residuals: -0.91747, Convergence:   inf\n",
      "Epoch: 247, Loss: 2457.05739, Residuals: -0.91371, Convergence: 0.000561\n",
      "Evidence 15007.739\n",
      "\n",
      "Epoch: 247, Evidence: 15007.73926, Convergence: 0.000433\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.51e-02\n",
      "Epoch: 247, Loss: 2459.37128, Residuals: -0.91371, Convergence:   inf\n",
      "Epoch: 248, Loss: 2500.08169, Residuals: -0.95609, Convergence: -0.016284\n",
      "Epoch: 248, Loss: 2457.95790, Residuals: -0.91123, Convergence: 0.000575\n",
      "Evidence 15011.194\n",
      "\n",
      "Epoch: 248, Evidence: 15011.19434, Convergence: 0.000663\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.83e-02\n",
      "Epoch: 248, Loss: 2458.78423, Residuals: -0.91123, Convergence:   inf\n",
      "Epoch: 249, Loss: 2465.50139, Residuals: -0.91268, Convergence: -0.002724\n",
      "Epoch: 249, Loss: 2459.41662, Residuals: -0.90898, Convergence: -0.000257\n",
      "Evidence 15012.354\n",
      "\n",
      "Epoch: 249, Evidence: 15012.35352, Convergence: 0.000740\n",
      "Total samples: 183, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 384.84316, Residuals: -4.53833, Convergence:   inf\n",
      "Epoch: 1, Loss: 359.02308, Residuals: -4.41770, Convergence: 0.071918\n",
      "Epoch: 2, Loss: 337.88305, Residuals: -4.25342, Convergence: 0.062566\n",
      "Epoch: 3, Loss: 321.72864, Residuals: -4.08882, Convergence: 0.050211\n",
      "Epoch: 4, Loss: 309.40270, Residuals: -3.94388, Convergence: 0.039838\n",
      "Epoch: 5, Loss: 299.62081, Residuals: -3.81584, Convergence: 0.032648\n",
      "Epoch: 6, Loss: 291.67274, Residuals: -3.70437, Convergence: 0.027250\n",
      "Epoch: 7, Loss: 285.06847, Residuals: -3.60891, Convergence: 0.023167\n",
      "Epoch: 8, Loss: 279.44548, Residuals: -3.52737, Convergence: 0.020122\n",
      "Epoch: 9, Loss: 274.54729, Residuals: -3.45739, Convergence: 0.017841\n",
      "Epoch: 10, Loss: 270.19108, Residuals: -3.39684, Convergence: 0.016123\n",
      "Epoch: 11, Loss: 266.24399, Residuals: -3.34391, Convergence: 0.014825\n",
      "Epoch: 12, Loss: 262.60885, Residuals: -3.29706, Convergence: 0.013842\n",
      "Epoch: 13, Loss: 259.21513, Residuals: -3.25498, Convergence: 0.013092\n",
      "Epoch: 14, Loss: 256.01319, Residuals: -3.21651, Convergence: 0.012507\n",
      "Epoch: 15, Loss: 252.97147, Residuals: -3.18072, Convergence: 0.012024\n",
      "Epoch: 16, Loss: 250.07384, Residuals: -3.14701, Convergence: 0.011587\n",
      "Epoch: 17, Loss: 247.30919, Residuals: -3.11502, Convergence: 0.011179\n",
      "Epoch: 18, Loss: 244.65719, Residuals: -3.08435, Convergence: 0.010840\n",
      "Epoch: 19, Loss: 242.08568, Residuals: -3.05451, Convergence: 0.010622\n",
      "Epoch: 20, Loss: 239.55881, Residuals: -3.02491, Convergence: 0.010548\n",
      "Epoch: 21, Loss: 237.04678, Residuals: -2.99509, Convergence: 0.010597\n",
      "Epoch: 22, Loss: 234.52968, Residuals: -2.96473, Convergence: 0.010733\n",
      "Epoch: 23, Loss: 231.98684, Residuals: -2.93362, Convergence: 0.010961\n",
      "Epoch: 24, Loss: 229.37877, Residuals: -2.90132, Convergence: 0.011370\n",
      "Epoch: 25, Loss: 226.64678, Residuals: -2.86714, Convergence: 0.012054\n",
      "Epoch: 26, Loss: 223.76426, Residuals: -2.83069, Convergence: 0.012882\n",
      "Epoch: 27, Loss: 220.82503, Residuals: -2.79290, Convergence: 0.013310\n",
      "Epoch: 28, Loss: 217.95858, Residuals: -2.75522, Convergence: 0.013151\n",
      "Epoch: 29, Loss: 215.20170, Residuals: -2.71821, Convergence: 0.012811\n",
      "Epoch: 30, Loss: 212.54217, Residuals: -2.68184, Convergence: 0.012513\n",
      "Epoch: 31, Loss: 209.96191, Residuals: -2.64598, Convergence: 0.012289\n",
      "Epoch: 32, Loss: 207.44707, Residuals: -2.61051, Convergence: 0.012123\n",
      "Epoch: 33, Loss: 204.98865, Residuals: -2.57533, Convergence: 0.011993\n",
      "Epoch: 34, Loss: 202.58154, Residuals: -2.54037, Convergence: 0.011882\n",
      "Epoch: 35, Loss: 200.22336, Residuals: -2.50560, Convergence: 0.011778\n",
      "Epoch: 36, Loss: 197.91355, Residuals: -2.47096, Convergence: 0.011671\n",
      "Epoch: 37, Loss: 195.65272, Residuals: -2.43647, Convergence: 0.011555\n",
      "Epoch: 38, Loss: 193.44210, Residuals: -2.40210, Convergence: 0.011428\n",
      "Epoch: 39, Loss: 191.28315, Residuals: -2.36786, Convergence: 0.011287\n",
      "Epoch: 40, Loss: 189.17739, Residuals: -2.33376, Convergence: 0.011131\n",
      "Epoch: 41, Loss: 187.12619, Residuals: -2.29983, Convergence: 0.010962\n",
      "Epoch: 42, Loss: 185.13081, Residuals: -2.26609, Convergence: 0.010778\n",
      "Epoch: 43, Loss: 183.19236, Residuals: -2.23257, Convergence: 0.010581\n",
      "Epoch: 44, Loss: 181.31191, Residuals: -2.19930, Convergence: 0.010371\n",
      "Epoch: 45, Loss: 179.49054, Residuals: -2.16633, Convergence: 0.010147\n",
      "Epoch: 46, Loss: 177.72938, Residuals: -2.13370, Convergence: 0.009909\n",
      "Epoch: 47, Loss: 176.02952, Residuals: -2.10145, Convergence: 0.009657\n",
      "Epoch: 48, Loss: 174.39184, Residuals: -2.06964, Convergence: 0.009391\n",
      "Epoch: 49, Loss: 172.81684, Residuals: -2.03830, Convergence: 0.009114\n",
      "Epoch: 50, Loss: 171.30452, Residuals: -2.00747, Convergence: 0.008828\n",
      "Epoch: 51, Loss: 169.85435, Residuals: -1.97719, Convergence: 0.008538\n",
      "Epoch: 52, Loss: 168.46530, Residuals: -1.94750, Convergence: 0.008245\n",
      "Epoch: 53, Loss: 167.13601, Residuals: -1.91840, Convergence: 0.007953\n",
      "Epoch: 54, Loss: 165.86486, Residuals: -1.88994, Convergence: 0.007664\n",
      "Epoch: 55, Loss: 164.65008, Residuals: -1.86212, Convergence: 0.007378\n",
      "Epoch: 56, Loss: 163.48985, Residuals: -1.83497, Convergence: 0.007097\n",
      "Epoch: 57, Loss: 162.38229, Residuals: -1.80851, Convergence: 0.006821\n",
      "Epoch: 58, Loss: 161.32554, Residuals: -1.78274, Convergence: 0.006550\n",
      "Epoch: 59, Loss: 160.31766, Residuals: -1.75770, Convergence: 0.006287\n",
      "Epoch: 60, Loss: 159.35672, Residuals: -1.73338, Convergence: 0.006030\n",
      "Epoch: 61, Loss: 158.44072, Residuals: -1.70980, Convergence: 0.005781\n",
      "Epoch: 62, Loss: 157.56756, Residuals: -1.68696, Convergence: 0.005541\n",
      "Epoch: 63, Loss: 156.73506, Residuals: -1.66486, Convergence: 0.005312\n",
      "Epoch: 64, Loss: 155.94095, Residuals: -1.64349, Convergence: 0.005092\n",
      "Epoch: 65, Loss: 155.18292, Residuals: -1.62284, Convergence: 0.004885\n",
      "Epoch: 66, Loss: 154.45870, Residuals: -1.60289, Convergence: 0.004689\n",
      "Epoch: 67, Loss: 153.76611, Residuals: -1.58361, Convergence: 0.004504\n",
      "Epoch: 68, Loss: 153.10319, Residuals: -1.56499, Convergence: 0.004330\n",
      "Epoch: 69, Loss: 152.46823, Residuals: -1.54699, Convergence: 0.004165\n",
      "Epoch: 70, Loss: 151.85977, Residuals: -1.52961, Convergence: 0.004007\n",
      "Epoch: 71, Loss: 151.27660, Residuals: -1.51281, Convergence: 0.003855\n",
      "Epoch: 72, Loss: 150.71776, Residuals: -1.49658, Convergence: 0.003708\n",
      "Epoch: 73, Loss: 150.18239, Residuals: -1.48091, Convergence: 0.003565\n",
      "Epoch: 74, Loss: 149.66979, Residuals: -1.46579, Convergence: 0.003425\n",
      "Epoch: 75, Loss: 149.17927, Residuals: -1.45121, Convergence: 0.003288\n",
      "Epoch: 76, Loss: 148.71022, Residuals: -1.43716, Convergence: 0.003154\n",
      "Epoch: 77, Loss: 148.26199, Residuals: -1.42362, Convergence: 0.003023\n",
      "Epoch: 78, Loss: 147.83393, Residuals: -1.41059, Convergence: 0.002896\n",
      "Epoch: 79, Loss: 147.42536, Residuals: -1.39805, Convergence: 0.002771\n",
      "Epoch: 80, Loss: 147.03562, Residuals: -1.38600, Convergence: 0.002651\n",
      "Epoch: 81, Loss: 146.66399, Residuals: -1.37442, Convergence: 0.002534\n",
      "Epoch: 82, Loss: 146.30976, Residuals: -1.36329, Convergence: 0.002421\n",
      "Epoch: 83, Loss: 145.97222, Residuals: -1.35261, Convergence: 0.002312\n",
      "Epoch: 84, Loss: 145.65066, Residuals: -1.34236, Convergence: 0.002208\n",
      "Epoch: 85, Loss: 145.34438, Residuals: -1.33252, Convergence: 0.002107\n",
      "Epoch: 86, Loss: 145.05269, Residuals: -1.32308, Convergence: 0.002011\n",
      "Epoch: 87, Loss: 144.77495, Residuals: -1.31403, Convergence: 0.001918\n",
      "Epoch: 88, Loss: 144.51051, Residuals: -1.30536, Convergence: 0.001830\n",
      "Epoch: 89, Loss: 144.25880, Residuals: -1.29704, Convergence: 0.001745\n",
      "Epoch: 90, Loss: 144.01924, Residuals: -1.28907, Convergence: 0.001663\n",
      "Epoch: 91, Loss: 143.79131, Residuals: -1.28142, Convergence: 0.001585\n",
      "Epoch: 92, Loss: 143.57452, Residuals: -1.27410, Convergence: 0.001510\n",
      "Epoch: 93, Loss: 143.36841, Residuals: -1.26708, Convergence: 0.001438\n",
      "Epoch: 94, Loss: 143.17257, Residuals: -1.26035, Convergence: 0.001368\n",
      "Epoch: 95, Loss: 142.98660, Residuals: -1.25391, Convergence: 0.001301\n",
      "Epoch: 96, Loss: 142.81016, Residuals: -1.24774, Convergence: 0.001235\n",
      "Epoch: 97, Loss: 142.64292, Residuals: -1.24182, Convergence: 0.001172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 98, Loss: 142.48460, Residuals: -1.23616, Convergence: 0.001111\n",
      "Epoch: 99, Loss: 142.33490, Residuals: -1.23074, Convergence: 0.001052\n",
      "Epoch: 100, Loss: 142.19360, Residuals: -1.22556, Convergence: 0.000994\n",
      "Evidence -184.027\n",
      "\n",
      "Epoch: 100, Evidence: -184.02710, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.25e-01\n",
      "Epoch: 100, Loss: 1381.73144, Residuals: -1.22556, Convergence:   inf\n",
      "Epoch: 101, Loss: 1318.54377, Residuals: -1.25650, Convergence: 0.047922\n",
      "Epoch: 102, Loss: 1270.93837, Residuals: -1.28068, Convergence: 0.037457\n",
      "Epoch: 103, Loss: 1235.23980, Residuals: -1.29791, Convergence: 0.028900\n",
      "Epoch: 104, Loss: 1207.54594, Residuals: -1.31001, Convergence: 0.022934\n",
      "Epoch: 105, Loss: 1185.16999, Residuals: -1.31898, Convergence: 0.018880\n",
      "Epoch: 106, Loss: 1166.60621, Residuals: -1.32589, Convergence: 0.015913\n",
      "Epoch: 107, Loss: 1150.95477, Residuals: -1.33119, Convergence: 0.013599\n",
      "Epoch: 108, Loss: 1137.60037, Residuals: -1.33511, Convergence: 0.011739\n",
      "Epoch: 109, Loss: 1126.08482, Residuals: -1.33781, Convergence: 0.010226\n",
      "Epoch: 110, Loss: 1116.04836, Residuals: -1.33942, Convergence: 0.008993\n",
      "Epoch: 111, Loss: 1107.20157, Residuals: -1.34005, Convergence: 0.007990\n",
      "Epoch: 112, Loss: 1099.30640, Residuals: -1.33978, Convergence: 0.007182\n",
      "Epoch: 113, Loss: 1092.16485, Residuals: -1.33870, Convergence: 0.006539\n",
      "Epoch: 114, Loss: 1085.60892, Residuals: -1.33686, Convergence: 0.006039\n",
      "Epoch: 115, Loss: 1079.49827, Residuals: -1.33431, Convergence: 0.005661\n",
      "Epoch: 116, Loss: 1073.71248, Residuals: -1.33109, Convergence: 0.005389\n",
      "Epoch: 117, Loss: 1068.14987, Residuals: -1.32720, Convergence: 0.005208\n",
      "Epoch: 118, Loss: 1062.72388, Residuals: -1.32268, Convergence: 0.005106\n",
      "Epoch: 119, Loss: 1057.36301, Residuals: -1.31754, Convergence: 0.005070\n",
      "Epoch: 120, Loss: 1052.00972, Residuals: -1.31181, Convergence: 0.005089\n",
      "Epoch: 121, Loss: 1046.62978, Residuals: -1.30553, Convergence: 0.005140\n",
      "Epoch: 122, Loss: 1041.22417, Residuals: -1.29877, Convergence: 0.005192\n",
      "Epoch: 123, Loss: 1035.83714, Residuals: -1.29163, Convergence: 0.005201\n",
      "Epoch: 124, Loss: 1030.55091, Residuals: -1.28420, Convergence: 0.005130\n",
      "Epoch: 125, Loss: 1025.45644, Residuals: -1.27658, Convergence: 0.004968\n",
      "Epoch: 126, Loss: 1020.62462, Residuals: -1.26883, Convergence: 0.004734\n",
      "Epoch: 127, Loss: 1016.08916, Residuals: -1.26104, Convergence: 0.004464\n",
      "Epoch: 128, Loss: 1011.85374, Residuals: -1.25325, Convergence: 0.004186\n",
      "Epoch: 129, Loss: 1007.90127, Residuals: -1.24552, Convergence: 0.003921\n",
      "Epoch: 130, Loss: 1004.20739, Residuals: -1.23790, Convergence: 0.003678\n",
      "Epoch: 131, Loss: 1000.74512, Residuals: -1.23041, Convergence: 0.003460\n",
      "Epoch: 132, Loss: 997.49072, Residuals: -1.22308, Convergence: 0.003263\n",
      "Epoch: 133, Loss: 994.42331, Residuals: -1.21594, Convergence: 0.003085\n",
      "Epoch: 134, Loss: 991.52504, Residuals: -1.20901, Convergence: 0.002923\n",
      "Epoch: 135, Loss: 988.78184, Residuals: -1.20231, Convergence: 0.002774\n",
      "Epoch: 136, Loss: 986.18133, Residuals: -1.19584, Convergence: 0.002637\n",
      "Epoch: 137, Loss: 983.71288, Residuals: -1.18961, Convergence: 0.002509\n",
      "Epoch: 138, Loss: 981.36768, Residuals: -1.18363, Convergence: 0.002390\n",
      "Epoch: 139, Loss: 979.13713, Residuals: -1.17790, Convergence: 0.002278\n",
      "Epoch: 140, Loss: 977.01401, Residuals: -1.17242, Convergence: 0.002173\n",
      "Epoch: 141, Loss: 974.99092, Residuals: -1.16718, Convergence: 0.002075\n",
      "Epoch: 142, Loss: 973.06133, Residuals: -1.16218, Convergence: 0.001983\n",
      "Epoch: 143, Loss: 971.21864, Residuals: -1.15741, Convergence: 0.001897\n",
      "Epoch: 144, Loss: 969.45713, Residuals: -1.15286, Convergence: 0.001817\n",
      "Epoch: 145, Loss: 967.77141, Residuals: -1.14853, Convergence: 0.001742\n",
      "Epoch: 146, Loss: 966.15625, Residuals: -1.14440, Convergence: 0.001672\n",
      "Epoch: 147, Loss: 964.60703, Residuals: -1.14046, Convergence: 0.001606\n",
      "Epoch: 148, Loss: 963.11931, Residuals: -1.13670, Convergence: 0.001545\n",
      "Epoch: 149, Loss: 961.68944, Residuals: -1.13312, Convergence: 0.001487\n",
      "Epoch: 150, Loss: 960.31344, Residuals: -1.12970, Convergence: 0.001433\n",
      "Epoch: 151, Loss: 958.98841, Residuals: -1.12642, Convergence: 0.001382\n",
      "Epoch: 152, Loss: 957.71089, Residuals: -1.12329, Convergence: 0.001334\n",
      "Epoch: 153, Loss: 956.47748, Residuals: -1.12030, Convergence: 0.001290\n",
      "Epoch: 154, Loss: 955.28555, Residuals: -1.11742, Convergence: 0.001248\n",
      "Epoch: 155, Loss: 954.13140, Residuals: -1.11466, Convergence: 0.001210\n",
      "Epoch: 156, Loss: 953.01227, Residuals: -1.11200, Convergence: 0.001174\n",
      "Epoch: 157, Loss: 951.92449, Residuals: -1.10944, Convergence: 0.001143\n",
      "Epoch: 158, Loss: 950.86487, Residuals: -1.10697, Convergence: 0.001114\n",
      "Epoch: 159, Loss: 949.82910, Residuals: -1.10457, Convergence: 0.001090\n",
      "Epoch: 160, Loss: 948.81377, Residuals: -1.10224, Convergence: 0.001070\n",
      "Epoch: 161, Loss: 947.81511, Residuals: -1.09997, Convergence: 0.001054\n",
      "Epoch: 162, Loss: 946.82889, Residuals: -1.09774, Convergence: 0.001042\n",
      "Epoch: 163, Loss: 945.85206, Residuals: -1.09555, Convergence: 0.001033\n",
      "Epoch: 164, Loss: 944.88110, Residuals: -1.09339, Convergence: 0.001028\n",
      "Epoch: 165, Loss: 943.91398, Residuals: -1.09125, Convergence: 0.001025\n",
      "Epoch: 166, Loss: 942.95032, Residuals: -1.08913, Convergence: 0.001022\n",
      "Epoch: 167, Loss: 941.98952, Residuals: -1.08702, Convergence: 0.001020\n",
      "Epoch: 168, Loss: 941.03436, Residuals: -1.08493, Convergence: 0.001015\n",
      "Epoch: 169, Loss: 940.08790, Residuals: -1.08286, Convergence: 0.001007\n",
      "Epoch: 170, Loss: 939.15446, Residuals: -1.08083, Convergence: 0.000994\n",
      "Evidence 11254.476\n",
      "\n",
      "Epoch: 170, Evidence: 11254.47559, Convergence: 1.016351\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 5.75e-01\n",
      "Epoch: 170, Loss: 2367.20129, Residuals: -1.08083, Convergence:   inf\n",
      "Epoch: 171, Loss: 2328.79603, Residuals: -1.08831, Convergence: 0.016491\n",
      "Epoch: 172, Loss: 2301.91377, Residuals: -1.08661, Convergence: 0.011678\n",
      "Epoch: 173, Loss: 2279.54277, Residuals: -1.08410, Convergence: 0.009814\n",
      "Epoch: 174, Loss: 2260.67691, Residuals: -1.08141, Convergence: 0.008345\n",
      "Epoch: 175, Loss: 2244.64212, Residuals: -1.07866, Convergence: 0.007144\n",
      "Epoch: 176, Loss: 2230.91306, Residuals: -1.07590, Convergence: 0.006154\n",
      "Epoch: 177, Loss: 2219.05899, Residuals: -1.07312, Convergence: 0.005342\n",
      "Epoch: 178, Loss: 2208.71818, Residuals: -1.07034, Convergence: 0.004682\n",
      "Epoch: 179, Loss: 2199.58515, Residuals: -1.06754, Convergence: 0.004152\n",
      "Epoch: 180, Loss: 2191.40472, Residuals: -1.06470, Convergence: 0.003733\n",
      "Epoch: 181, Loss: 2183.97368, Residuals: -1.06179, Convergence: 0.003403\n",
      "Epoch: 182, Loss: 2177.14113, Residuals: -1.05879, Convergence: 0.003138\n",
      "Epoch: 183, Loss: 2170.80818, Residuals: -1.05570, Convergence: 0.002917\n",
      "Epoch: 184, Loss: 2164.91828, Residuals: -1.05256, Convergence: 0.002721\n",
      "Epoch: 185, Loss: 2159.43819, Residuals: -1.04939, Convergence: 0.002538\n",
      "Epoch: 186, Loss: 2154.34386, Residuals: -1.04623, Convergence: 0.002365\n",
      "Epoch: 187, Loss: 2149.61129, Residuals: -1.04313, Convergence: 0.002202\n",
      "Epoch: 188, Loss: 2145.21497, Residuals: -1.04009, Convergence: 0.002049\n",
      "Epoch: 189, Loss: 2141.12769, Residuals: -1.03716, Convergence: 0.001909\n",
      "Epoch: 190, Loss: 2137.32395, Residuals: -1.03434, Convergence: 0.001780\n",
      "Epoch: 191, Loss: 2133.77790, Residuals: -1.03163, Convergence: 0.001662\n",
      "Epoch: 192, Loss: 2130.46802, Residuals: -1.02906, Convergence: 0.001554\n",
      "Epoch: 193, Loss: 2127.37365, Residuals: -1.02660, Convergence: 0.001455\n",
      "Epoch: 194, Loss: 2124.47649, Residuals: -1.02428, Convergence: 0.001364\n",
      "Epoch: 195, Loss: 2121.76078, Residuals: -1.02207, Convergence: 0.001280\n",
      "Epoch: 196, Loss: 2119.21307, Residuals: -1.01999, Convergence: 0.001202\n",
      "Epoch: 197, Loss: 2116.81968, Residuals: -1.01802, Convergence: 0.001131\n",
      "Epoch: 198, Loss: 2114.57050, Residuals: -1.01615, Convergence: 0.001064\n",
      "Epoch: 199, Loss: 2112.45426, Residuals: -1.01439, Convergence: 0.001002\n",
      "Epoch: 200, Loss: 2110.46251, Residuals: -1.01272, Convergence: 0.000944\n",
      "Evidence 14417.355\n",
      "\n",
      "Epoch: 200, Evidence: 14417.35547, Convergence: 0.219380\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 4.36e-01\n",
      "Epoch: 200, Loss: 2490.31984, Residuals: -1.01272, Convergence:   inf\n",
      "Epoch: 201, Loss: 2476.77432, Residuals: -1.00922, Convergence: 0.005469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 202, Loss: 2465.70790, Residuals: -1.00538, Convergence: 0.004488\n",
      "Epoch: 203, Loss: 2456.21058, Residuals: -1.00176, Convergence: 0.003867\n",
      "Epoch: 204, Loss: 2448.00895, Residuals: -0.99841, Convergence: 0.003350\n",
      "Epoch: 205, Loss: 2440.88922, Residuals: -0.99536, Convergence: 0.002917\n",
      "Epoch: 206, Loss: 2434.67815, Residuals: -0.99259, Convergence: 0.002551\n",
      "Epoch: 207, Loss: 2429.22870, Residuals: -0.99008, Convergence: 0.002243\n",
      "Epoch: 208, Loss: 2424.41920, Residuals: -0.98781, Convergence: 0.001984\n",
      "Epoch: 209, Loss: 2420.14839, Residuals: -0.98574, Convergence: 0.001765\n",
      "Epoch: 210, Loss: 2416.33065, Residuals: -0.98386, Convergence: 0.001580\n",
      "Epoch: 211, Loss: 2412.89647, Residuals: -0.98214, Convergence: 0.001423\n",
      "Epoch: 212, Loss: 2409.78992, Residuals: -0.98057, Convergence: 0.001289\n",
      "Epoch: 213, Loss: 2406.96131, Residuals: -0.97913, Convergence: 0.001175\n",
      "Epoch: 214, Loss: 2404.37376, Residuals: -0.97781, Convergence: 0.001076\n",
      "Epoch: 215, Loss: 2401.99408, Residuals: -0.97659, Convergence: 0.000991\n",
      "Evidence 14794.330\n",
      "\n",
      "Epoch: 215, Evidence: 14794.33008, Convergence: 0.025481\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 3.33e-01\n",
      "Epoch: 215, Loss: 2495.92229, Residuals: -0.97659, Convergence:   inf\n",
      "Epoch: 216, Loss: 2489.32660, Residuals: -0.97316, Convergence: 0.002650\n",
      "Epoch: 217, Loss: 2483.88913, Residuals: -0.97022, Convergence: 0.002189\n",
      "Epoch: 218, Loss: 2479.29686, Residuals: -0.96774, Convergence: 0.001852\n",
      "Epoch: 219, Loss: 2475.36257, Residuals: -0.96566, Convergence: 0.001589\n",
      "Epoch: 220, Loss: 2471.94333, Residuals: -0.96390, Convergence: 0.001383\n",
      "Epoch: 221, Loss: 2468.93149, Residuals: -0.96242, Convergence: 0.001220\n",
      "Epoch: 222, Loss: 2466.24605, Residuals: -0.96115, Convergence: 0.001089\n",
      "Epoch: 223, Loss: 2463.82488, Residuals: -0.96007, Convergence: 0.000983\n",
      "Evidence 14874.296\n",
      "\n",
      "Epoch: 223, Evidence: 14874.29590, Convergence: 0.005376\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.60e-01\n",
      "Epoch: 223, Loss: 2497.64653, Residuals: -0.96007, Convergence:   inf\n",
      "Epoch: 224, Loss: 2493.65969, Residuals: -0.95762, Convergence: 0.001599\n",
      "Epoch: 225, Loss: 2490.36670, Residuals: -0.95569, Convergence: 0.001322\n",
      "Epoch: 226, Loss: 2487.55901, Residuals: -0.95415, Convergence: 0.001129\n",
      "Epoch: 227, Loss: 2485.11235, Residuals: -0.95291, Convergence: 0.000985\n",
      "Evidence 14902.478\n",
      "\n",
      "Epoch: 227, Evidence: 14902.47754, Convergence: 0.001891\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.08e-01\n",
      "Epoch: 227, Loss: 2498.69374, Residuals: -0.95291, Convergence:   inf\n",
      "Epoch: 228, Loss: 2495.74222, Residuals: -0.95102, Convergence: 0.001183\n",
      "Epoch: 229, Loss: 2493.27985, Residuals: -0.94955, Convergence: 0.000988\n",
      "Evidence 14914.744\n",
      "\n",
      "Epoch: 229, Evidence: 14914.74414, Convergence: 0.000822\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.72e-01\n",
      "Epoch: 229, Loss: 2499.43924, Residuals: -0.94955, Convergence:   inf\n",
      "Epoch: 230, Loss: 2494.80648, Residuals: -0.94707, Convergence: 0.001857\n",
      "Epoch: 231, Loss: 2491.28980, Residuals: -0.94526, Convergence: 0.001412\n",
      "Epoch: 232, Loss: 2488.43627, Residuals: -0.94402, Convergence: 0.001147\n",
      "Epoch: 233, Loss: 2486.00978, Residuals: -0.94325, Convergence: 0.000976\n",
      "Evidence 14932.688\n",
      "\n",
      "Epoch: 233, Evidence: 14932.68750, Convergence: 0.002023\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.42e-01\n",
      "Epoch: 233, Loss: 2499.60163, Residuals: -0.94325, Convergence:   inf\n",
      "Epoch: 234, Loss: 2496.53765, Residuals: -0.94079, Convergence: 0.001227\n",
      "Epoch: 235, Loss: 2494.11521, Residuals: -0.93948, Convergence: 0.000971\n",
      "Evidence 14943.686\n",
      "\n",
      "Epoch: 235, Evidence: 14943.68555, Convergence: 0.000736\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.20e-01\n",
      "Epoch: 235, Loss: 2499.81091, Residuals: -0.93948, Convergence:   inf\n",
      "Epoch: 236, Loss: 2495.34146, Residuals: -0.93584, Convergence: 0.001791\n",
      "Epoch: 237, Loss: 2492.13649, Residuals: -0.93657, Convergence: 0.001286\n",
      "Epoch: 238, Loss: 2489.45045, Residuals: -0.93680, Convergence: 0.001079\n",
      "Epoch: 239, Loss: 2487.08939, Residuals: -0.93932, Convergence: 0.000949\n",
      "Evidence 14959.561\n",
      "\n",
      "Epoch: 239, Evidence: 14959.56055, Convergence: 0.001796\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.07e-01\n",
      "Epoch: 239, Loss: 2499.11348, Residuals: -0.93932, Convergence:   inf\n",
      "Epoch: 240, Loss: 2497.27029, Residuals: -0.93636, Convergence: 0.000738\n",
      "Evidence 14966.245\n",
      "\n",
      "Epoch: 240, Evidence: 14966.24512, Convergence: 0.000447\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 8.75e-02\n",
      "Epoch: 240, Loss: 2500.11364, Residuals: -0.93636, Convergence:   inf\n",
      "Epoch: 241, Loss: 2539.13517, Residuals: -0.97595, Convergence: -0.015368\n",
      "Epoch: 241, Loss: 2497.85841, Residuals: -0.93457, Convergence: 0.000903\n",
      "Evidence 14970.437\n",
      "\n",
      "Epoch: 241, Evidence: 14970.43652, Convergence: 0.000726\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 8.09e-02\n",
      "Epoch: 241, Loss: 2499.50495, Residuals: -0.93457, Convergence:   inf\n",
      "Epoch: 242, Loss: 2504.30992, Residuals: -0.93744, Convergence: -0.001919\n",
      "Epoch: 242, Loss: 2499.41095, Residuals: -0.93336, Convergence: 0.000038\n",
      "Evidence 14972.173\n",
      "\n",
      "Epoch: 242, Evidence: 14972.17285, Convergence: 0.000842\n",
      "Total samples: 185, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 383.52370, Residuals: -4.48917, Convergence:   inf\n",
      "Epoch: 1, Loss: 357.56557, Residuals: -4.36905, Convergence: 0.072597\n",
      "Epoch: 2, Loss: 336.37343, Residuals: -4.20449, Convergence: 0.063002\n",
      "Epoch: 3, Loss: 320.20627, Residuals: -4.04063, Convergence: 0.050490\n",
      "Epoch: 4, Loss: 307.88003, Residuals: -3.89672, Convergence: 0.040036\n",
      "Epoch: 5, Loss: 298.10769, Residuals: -3.76987, Convergence: 0.032781\n",
      "Epoch: 6, Loss: 290.17681, Residuals: -3.65960, Convergence: 0.027331\n",
      "Epoch: 7, Loss: 283.59322, Residuals: -3.56519, Convergence: 0.023215\n",
      "Epoch: 8, Loss: 277.99104, Residuals: -3.48441, Convergence: 0.020152\n",
      "Epoch: 9, Loss: 273.11162, Residuals: -3.41488, Convergence: 0.017866\n",
      "Epoch: 10, Loss: 268.77060, Residuals: -3.35451, Convergence: 0.016151\n",
      "Epoch: 11, Loss: 264.83429, Residuals: -3.30158, Convergence: 0.014863\n",
      "Epoch: 12, Loss: 261.20539, Residuals: -3.25463, Convergence: 0.013893\n",
      "Epoch: 13, Loss: 257.81391, Residuals: -3.21245, Convergence: 0.013155\n",
      "Epoch: 14, Loss: 254.61086, Residuals: -3.17396, Convergence: 0.012580\n",
      "Epoch: 15, Loss: 251.56396, Residuals: -3.13826, Convergence: 0.012112\n",
      "Epoch: 16, Loss: 248.65458, Residuals: -3.10469, Convergence: 0.011701\n",
      "Epoch: 17, Loss: 245.87026, Residuals: -3.07282, Convergence: 0.011324\n",
      "Epoch: 18, Loss: 243.19250, Residuals: -3.04228, Convergence: 0.011011\n",
      "Epoch: 19, Loss: 240.59179, Residuals: -3.01262, Convergence: 0.010810\n",
      "Epoch: 20, Loss: 238.03367, Residuals: -2.98328, Convergence: 0.010747\n",
      "Epoch: 21, Loss: 235.48800, Residuals: -2.95377, Convergence: 0.010810\n",
      "Epoch: 22, Loss: 232.93289, Residuals: -2.92377, Convergence: 0.010969\n",
      "Epoch: 23, Loss: 230.34470, Residuals: -2.89299, Convergence: 0.011236\n",
      "Epoch: 24, Loss: 227.68105, Residuals: -2.86095, Convergence: 0.011699\n",
      "Epoch: 25, Loss: 224.88452, Residuals: -2.82696, Convergence: 0.012435\n",
      "Epoch: 26, Loss: 221.94587, Residuals: -2.79080, Convergence: 0.013240\n",
      "Epoch: 27, Loss: 218.97745, Residuals: -2.75361, Convergence: 0.013556\n",
      "Epoch: 28, Loss: 216.09258, Residuals: -2.71671, Convergence: 0.013350\n",
      "Epoch: 29, Loss: 213.31165, Residuals: -2.68043, Convergence: 0.013037\n",
      "Epoch: 30, Loss: 210.61818, Residuals: -2.64469, Convergence: 0.012788\n",
      "Epoch: 31, Loss: 207.99396, Residuals: -2.60932, Convergence: 0.012617\n",
      "Epoch: 32, Loss: 205.42633, Residuals: -2.57417, Convergence: 0.012499\n",
      "Epoch: 33, Loss: 202.90820, Residuals: -2.53916, Convergence: 0.012410\n",
      "Epoch: 34, Loss: 200.43676, Residuals: -2.50424, Convergence: 0.012330\n",
      "Epoch: 35, Loss: 198.01208, Residuals: -2.46938, Convergence: 0.012245\n",
      "Epoch: 36, Loss: 195.63595, Residuals: -2.43458, Convergence: 0.012146\n",
      "Epoch: 37, Loss: 193.31098, Residuals: -2.39988, Convergence: 0.012027\n",
      "Epoch: 38, Loss: 191.04000, Residuals: -2.36529, Convergence: 0.011887\n",
      "Epoch: 39, Loss: 188.82561, Residuals: -2.33085, Convergence: 0.011727\n",
      "Epoch: 40, Loss: 186.67007, Residuals: -2.29659, Convergence: 0.011547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41, Loss: 184.57515, Residuals: -2.26255, Convergence: 0.011350\n",
      "Epoch: 42, Loss: 182.54219, Residuals: -2.22876, Convergence: 0.011137\n",
      "Epoch: 43, Loss: 180.57210, Residuals: -2.19525, Convergence: 0.010910\n",
      "Epoch: 44, Loss: 178.66547, Residuals: -2.16205, Convergence: 0.010672\n",
      "Epoch: 45, Loss: 176.82255, Residuals: -2.12919, Convergence: 0.010422\n",
      "Epoch: 46, Loss: 175.04335, Residuals: -2.09671, Convergence: 0.010164\n",
      "Epoch: 47, Loss: 173.32766, Residuals: -2.06464, Convergence: 0.009899\n",
      "Epoch: 48, Loss: 171.67507, Residuals: -2.03299, Convergence: 0.009626\n",
      "Epoch: 49, Loss: 170.08499, Residuals: -2.00181, Convergence: 0.009349\n",
      "Epoch: 50, Loss: 168.55668, Residuals: -1.97110, Convergence: 0.009067\n",
      "Epoch: 51, Loss: 167.08928, Residuals: -1.94090, Convergence: 0.008782\n",
      "Epoch: 52, Loss: 165.68189, Residuals: -1.91122, Convergence: 0.008495\n",
      "Epoch: 53, Loss: 164.33363, Residuals: -1.88209, Convergence: 0.008204\n",
      "Epoch: 54, Loss: 163.04372, Residuals: -1.85353, Convergence: 0.007911\n",
      "Epoch: 55, Loss: 161.81152, Residuals: -1.82554, Convergence: 0.007615\n",
      "Epoch: 56, Loss: 160.63658, Residuals: -1.79817, Convergence: 0.007314\n",
      "Epoch: 57, Loss: 159.51849, Residuals: -1.77143, Convergence: 0.007009\n",
      "Epoch: 58, Loss: 158.45692, Residuals: -1.74534, Convergence: 0.006699\n",
      "Epoch: 59, Loss: 157.45135, Residuals: -1.71995, Convergence: 0.006387\n",
      "Epoch: 60, Loss: 156.50109, Residuals: -1.69526, Convergence: 0.006072\n",
      "Epoch: 61, Loss: 155.60508, Residuals: -1.67131, Convergence: 0.005758\n",
      "Epoch: 62, Loss: 154.76202, Residuals: -1.64812, Convergence: 0.005448\n",
      "Epoch: 63, Loss: 153.97021, Residuals: -1.62572, Convergence: 0.005143\n",
      "Epoch: 64, Loss: 153.22768, Residuals: -1.60411, Convergence: 0.004846\n",
      "Epoch: 65, Loss: 152.53213, Residuals: -1.58331, Convergence: 0.004560\n",
      "Epoch: 66, Loss: 151.88098, Residuals: -1.56331, Convergence: 0.004287\n",
      "Epoch: 67, Loss: 151.27137, Residuals: -1.54412, Convergence: 0.004030\n",
      "Epoch: 68, Loss: 150.70022, Residuals: -1.52571, Convergence: 0.003790\n",
      "Epoch: 69, Loss: 150.16428, Residuals: -1.50807, Convergence: 0.003569\n",
      "Epoch: 70, Loss: 149.66026, Residuals: -1.49116, Convergence: 0.003368\n",
      "Epoch: 71, Loss: 149.18494, Residuals: -1.47496, Convergence: 0.003186\n",
      "Epoch: 72, Loss: 148.73531, Residuals: -1.45940, Convergence: 0.003023\n",
      "Epoch: 73, Loss: 148.30866, Residuals: -1.44447, Convergence: 0.002877\n",
      "Epoch: 74, Loss: 147.90268, Residuals: -1.43012, Convergence: 0.002745\n",
      "Epoch: 75, Loss: 147.51546, Residuals: -1.41632, Convergence: 0.002625\n",
      "Epoch: 76, Loss: 147.14547, Residuals: -1.40305, Convergence: 0.002514\n",
      "Epoch: 77, Loss: 146.79148, Residuals: -1.39029, Convergence: 0.002411\n",
      "Epoch: 78, Loss: 146.45250, Residuals: -1.37803, Convergence: 0.002315\n",
      "Epoch: 79, Loss: 146.12760, Residuals: -1.36625, Convergence: 0.002223\n",
      "Epoch: 80, Loss: 145.81590, Residuals: -1.35494, Convergence: 0.002138\n",
      "Epoch: 81, Loss: 145.51648, Residuals: -1.34410, Convergence: 0.002058\n",
      "Epoch: 82, Loss: 145.22829, Residuals: -1.33370, Convergence: 0.001984\n",
      "Epoch: 83, Loss: 144.95025, Residuals: -1.32372, Convergence: 0.001918\n",
      "Epoch: 84, Loss: 144.68124, Residuals: -1.31415, Convergence: 0.001859\n",
      "Epoch: 85, Loss: 144.42017, Residuals: -1.30495, Convergence: 0.001808\n",
      "Epoch: 86, Loss: 144.16608, Residuals: -1.29609, Convergence: 0.001762\n",
      "Epoch: 87, Loss: 143.91817, Residuals: -1.28756, Convergence: 0.001723\n",
      "Epoch: 88, Loss: 143.67585, Residuals: -1.27932, Convergence: 0.001687\n",
      "Epoch: 89, Loss: 143.43871, Residuals: -1.27136, Convergence: 0.001653\n",
      "Epoch: 90, Loss: 143.20653, Residuals: -1.26365, Convergence: 0.001621\n",
      "Epoch: 91, Loss: 142.97920, Residuals: -1.25620, Convergence: 0.001590\n",
      "Epoch: 92, Loss: 142.75672, Residuals: -1.24898, Convergence: 0.001558\n",
      "Epoch: 93, Loss: 142.53916, Residuals: -1.24199, Convergence: 0.001526\n",
      "Epoch: 94, Loss: 142.32659, Residuals: -1.23522, Convergence: 0.001494\n",
      "Epoch: 95, Loss: 142.11909, Residuals: -1.22867, Convergence: 0.001460\n",
      "Epoch: 96, Loss: 141.91673, Residuals: -1.22233, Convergence: 0.001426\n",
      "Epoch: 97, Loss: 141.71955, Residuals: -1.21620, Convergence: 0.001391\n",
      "Epoch: 98, Loss: 141.52760, Residuals: -1.21027, Convergence: 0.001356\n",
      "Epoch: 99, Loss: 141.34086, Residuals: -1.20454, Convergence: 0.001321\n",
      "Epoch: 100, Loss: 141.15932, Residuals: -1.19899, Convergence: 0.001286\n",
      "Epoch: 101, Loss: 140.98293, Residuals: -1.19363, Convergence: 0.001251\n",
      "Epoch: 102, Loss: 140.81162, Residuals: -1.18846, Convergence: 0.001217\n",
      "Epoch: 103, Loss: 140.64529, Residuals: -1.18345, Convergence: 0.001183\n",
      "Epoch: 104, Loss: 140.48387, Residuals: -1.17862, Convergence: 0.001149\n",
      "Epoch: 105, Loss: 140.32723, Residuals: -1.17395, Convergence: 0.001116\n",
      "Epoch: 106, Loss: 140.17525, Residuals: -1.16944, Convergence: 0.001084\n",
      "Epoch: 107, Loss: 140.02782, Residuals: -1.16508, Convergence: 0.001053\n",
      "Epoch: 108, Loss: 139.88479, Residuals: -1.16086, Convergence: 0.001022\n",
      "Epoch: 109, Loss: 139.74604, Residuals: -1.15679, Convergence: 0.000993\n",
      "Evidence -180.707\n",
      "\n",
      "Epoch: 109, Evidence: -180.70657, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 7.24e-01\n",
      "Epoch: 109, Loss: 1392.25654, Residuals: -1.15679, Convergence:   inf\n",
      "Epoch: 110, Loss: 1335.79950, Residuals: -1.18195, Convergence: 0.042265\n",
      "Epoch: 111, Loss: 1292.02702, Residuals: -1.20317, Convergence: 0.033879\n",
      "Epoch: 112, Loss: 1258.32797, Residuals: -1.22000, Convergence: 0.026781\n",
      "Epoch: 113, Loss: 1231.69199, Residuals: -1.23323, Convergence: 0.021626\n",
      "Epoch: 114, Loss: 1209.96301, Residuals: -1.24397, Convergence: 0.017958\n",
      "Epoch: 115, Loss: 1191.88636, Residuals: -1.25280, Convergence: 0.015166\n",
      "Epoch: 116, Loss: 1176.66343, Residuals: -1.25998, Convergence: 0.012937\n",
      "Epoch: 117, Loss: 1163.71713, Residuals: -1.26570, Convergence: 0.011125\n",
      "Epoch: 118, Loss: 1152.60158, Residuals: -1.27010, Convergence: 0.009644\n",
      "Epoch: 119, Loss: 1142.95994, Residuals: -1.27332, Convergence: 0.008436\n",
      "Epoch: 120, Loss: 1134.50123, Residuals: -1.27547, Convergence: 0.007456\n",
      "Epoch: 121, Loss: 1126.98377, Residuals: -1.27664, Convergence: 0.006670\n",
      "Epoch: 122, Loss: 1120.20358, Residuals: -1.27692, Convergence: 0.006053\n",
      "Epoch: 123, Loss: 1113.98468, Residuals: -1.27637, Convergence: 0.005583\n",
      "Epoch: 124, Loss: 1108.17150, Residuals: -1.27503, Convergence: 0.005246\n",
      "Epoch: 125, Loss: 1102.62306, Residuals: -1.27293, Convergence: 0.005032\n",
      "Epoch: 126, Loss: 1097.21015, Residuals: -1.27007, Convergence: 0.004933\n",
      "Epoch: 127, Loss: 1091.81420, Residuals: -1.26646, Convergence: 0.004942\n",
      "Epoch: 128, Loss: 1086.33452, Residuals: -1.26210, Convergence: 0.005044\n",
      "Epoch: 129, Loss: 1080.70290, Residuals: -1.25703, Convergence: 0.005211\n",
      "Epoch: 130, Loss: 1074.90901, Residuals: -1.25131, Convergence: 0.005390\n",
      "Epoch: 131, Loss: 1069.02013, Residuals: -1.24503, Convergence: 0.005509\n",
      "Epoch: 132, Loss: 1063.16997, Residuals: -1.23831, Convergence: 0.005503\n",
      "Epoch: 133, Loss: 1057.50629, Residuals: -1.23124, Convergence: 0.005356\n",
      "Epoch: 134, Loss: 1052.13534, Residuals: -1.22393, Convergence: 0.005105\n",
      "Epoch: 135, Loss: 1047.10410, Residuals: -1.21647, Convergence: 0.004805\n",
      "Epoch: 136, Loss: 1042.41312, Residuals: -1.20894, Convergence: 0.004500\n",
      "Epoch: 137, Loss: 1038.03712, Residuals: -1.20141, Convergence: 0.004216\n",
      "Epoch: 138, Loss: 1033.94445, Residuals: -1.19393, Convergence: 0.003958\n",
      "Epoch: 139, Loss: 1030.10137, Residuals: -1.18655, Convergence: 0.003731\n",
      "Epoch: 140, Loss: 1026.48000, Residuals: -1.17930, Convergence: 0.003528\n",
      "Epoch: 141, Loss: 1023.05577, Residuals: -1.17223, Convergence: 0.003347\n",
      "Epoch: 142, Loss: 1019.81043, Residuals: -1.16535, Convergence: 0.003182\n",
      "Epoch: 143, Loss: 1016.72832, Residuals: -1.15868, Convergence: 0.003031\n",
      "Epoch: 144, Loss: 1013.79791, Residuals: -1.15223, Convergence: 0.002891\n",
      "Epoch: 145, Loss: 1011.01006, Residuals: -1.14602, Convergence: 0.002757\n",
      "Epoch: 146, Loss: 1008.35693, Residuals: -1.14006, Convergence: 0.002631\n",
      "Epoch: 147, Loss: 1005.83184, Residuals: -1.13435, Convergence: 0.002510\n",
      "Epoch: 148, Loss: 1003.42945, Residuals: -1.12889, Convergence: 0.002394\n",
      "Epoch: 149, Loss: 1001.14406, Residuals: -1.12368, Convergence: 0.002283\n",
      "Epoch: 150, Loss: 998.97082, Residuals: -1.11871, Convergence: 0.002175\n",
      "Epoch: 151, Loss: 996.90491, Residuals: -1.11398, Convergence: 0.002072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 152, Loss: 994.94100, Residuals: -1.10948, Convergence: 0.001974\n",
      "Epoch: 153, Loss: 993.07389, Residuals: -1.10521, Convergence: 0.001880\n",
      "Epoch: 154, Loss: 991.29880, Residuals: -1.10115, Convergence: 0.001791\n",
      "Epoch: 155, Loss: 989.60978, Residuals: -1.09729, Convergence: 0.001707\n",
      "Epoch: 156, Loss: 988.00160, Residuals: -1.09363, Convergence: 0.001628\n",
      "Epoch: 157, Loss: 986.46826, Residuals: -1.09015, Convergence: 0.001554\n",
      "Epoch: 158, Loss: 985.00423, Residuals: -1.08685, Convergence: 0.001486\n",
      "Epoch: 159, Loss: 983.60337, Residuals: -1.08371, Convergence: 0.001424\n",
      "Epoch: 160, Loss: 982.25937, Residuals: -1.08071, Convergence: 0.001368\n",
      "Epoch: 161, Loss: 980.96636, Residuals: -1.07785, Convergence: 0.001318\n",
      "Epoch: 162, Loss: 979.71783, Residuals: -1.07512, Convergence: 0.001274\n",
      "Epoch: 163, Loss: 978.50709, Residuals: -1.07250, Convergence: 0.001237\n",
      "Epoch: 164, Loss: 977.32761, Residuals: -1.06997, Convergence: 0.001207\n",
      "Epoch: 165, Loss: 976.17241, Residuals: -1.06753, Convergence: 0.001183\n",
      "Epoch: 166, Loss: 975.03482, Residuals: -1.06517, Convergence: 0.001167\n",
      "Epoch: 167, Loss: 973.90791, Residuals: -1.06285, Convergence: 0.001157\n",
      "Epoch: 168, Loss: 972.78600, Residuals: -1.06058, Convergence: 0.001153\n",
      "Epoch: 169, Loss: 971.66421, Residuals: -1.05834, Convergence: 0.001155\n",
      "Epoch: 170, Loss: 970.53982, Residuals: -1.05612, Convergence: 0.001159\n",
      "Epoch: 171, Loss: 969.41272, Residuals: -1.05392, Convergence: 0.001163\n",
      "Epoch: 172, Loss: 968.28534, Residuals: -1.05173, Convergence: 0.001164\n",
      "Epoch: 173, Loss: 967.16363, Residuals: -1.04956, Convergence: 0.001160\n",
      "Epoch: 174, Loss: 966.05461, Residuals: -1.04743, Convergence: 0.001148\n",
      "Epoch: 175, Loss: 964.96620, Residuals: -1.04534, Convergence: 0.001128\n",
      "Epoch: 176, Loss: 963.90619, Residuals: -1.04331, Convergence: 0.001100\n",
      "Epoch: 177, Loss: 962.88085, Residuals: -1.04134, Convergence: 0.001065\n",
      "Epoch: 178, Loss: 961.89475, Residuals: -1.03945, Convergence: 0.001025\n",
      "Epoch: 179, Loss: 960.95082, Residuals: -1.03763, Convergence: 0.000982\n",
      "Evidence 11588.579\n",
      "\n",
      "Epoch: 179, Evidence: 11588.57910, Convergence: 1.015594\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 5.68e-01\n",
      "Epoch: 179, Loss: 2392.93647, Residuals: -1.03763, Convergence:   inf\n",
      "Epoch: 180, Loss: 2355.48471, Residuals: -1.04470, Convergence: 0.015900\n",
      "Epoch: 181, Loss: 2330.63185, Residuals: -1.04289, Convergence: 0.010664\n",
      "Epoch: 182, Loss: 2309.83799, Residuals: -1.04089, Convergence: 0.009002\n",
      "Epoch: 183, Loss: 2292.32335, Residuals: -1.03887, Convergence: 0.007641\n",
      "Epoch: 184, Loss: 2277.44977, Residuals: -1.03684, Convergence: 0.006531\n",
      "Epoch: 185, Loss: 2264.67844, Residuals: -1.03479, Convergence: 0.005639\n",
      "Epoch: 186, Loss: 2253.56604, Residuals: -1.03269, Convergence: 0.004931\n",
      "Epoch: 187, Loss: 2243.75236, Residuals: -1.03053, Convergence: 0.004374\n",
      "Epoch: 188, Loss: 2234.96431, Residuals: -1.02828, Convergence: 0.003932\n",
      "Epoch: 189, Loss: 2227.01432, Residuals: -1.02593, Convergence: 0.003570\n",
      "Epoch: 190, Loss: 2219.78253, Residuals: -1.02350, Convergence: 0.003258\n",
      "Epoch: 191, Loss: 2213.19282, Residuals: -1.02103, Convergence: 0.002977\n",
      "Epoch: 192, Loss: 2207.18821, Residuals: -1.01855, Convergence: 0.002720\n",
      "Epoch: 193, Loss: 2201.71426, Residuals: -1.01611, Convergence: 0.002486\n",
      "Epoch: 194, Loss: 2196.71790, Residuals: -1.01372, Convergence: 0.002274\n",
      "Epoch: 195, Loss: 2192.14515, Residuals: -1.01139, Convergence: 0.002086\n",
      "Epoch: 196, Loss: 2187.94539, Residuals: -1.00914, Convergence: 0.001919\n",
      "Epoch: 197, Loss: 2184.07206, Residuals: -1.00696, Convergence: 0.001773\n",
      "Epoch: 198, Loss: 2180.48327, Residuals: -1.00485, Convergence: 0.001646\n",
      "Epoch: 199, Loss: 2177.14384, Residuals: -1.00282, Convergence: 0.001534\n",
      "Epoch: 200, Loss: 2174.02326, Residuals: -1.00086, Convergence: 0.001435\n",
      "Epoch: 201, Loss: 2171.09539, Residuals: -0.99897, Convergence: 0.001349\n",
      "Epoch: 202, Loss: 2168.34065, Residuals: -0.99715, Convergence: 0.001270\n",
      "Epoch: 203, Loss: 2165.73997, Residuals: -0.99538, Convergence: 0.001201\n",
      "Epoch: 204, Loss: 2163.27915, Residuals: -0.99369, Convergence: 0.001138\n",
      "Epoch: 205, Loss: 2160.94561, Residuals: -0.99205, Convergence: 0.001080\n",
      "Epoch: 206, Loss: 2158.72980, Residuals: -0.99047, Convergence: 0.001026\n",
      "Epoch: 207, Loss: 2156.62252, Residuals: -0.98896, Convergence: 0.000977\n",
      "Evidence 14631.338\n",
      "\n",
      "Epoch: 207, Evidence: 14631.33789, Convergence: 0.207962\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 4.31e-01\n",
      "Epoch: 207, Loss: 2506.93933, Residuals: -0.98896, Convergence:   inf\n",
      "Epoch: 208, Loss: 2493.73388, Residuals: -0.98538, Convergence: 0.005295\n",
      "Epoch: 209, Loss: 2482.95851, Residuals: -0.98177, Convergence: 0.004340\n",
      "Epoch: 210, Loss: 2473.68358, Residuals: -0.97848, Convergence: 0.003749\n",
      "Epoch: 211, Loss: 2465.64631, Residuals: -0.97553, Convergence: 0.003260\n",
      "Epoch: 212, Loss: 2458.63966, Residuals: -0.97290, Convergence: 0.002850\n",
      "Epoch: 213, Loss: 2452.49751, Residuals: -0.97056, Convergence: 0.002504\n",
      "Epoch: 214, Loss: 2447.08503, Residuals: -0.96847, Convergence: 0.002212\n",
      "Epoch: 215, Loss: 2442.29009, Residuals: -0.96659, Convergence: 0.001963\n",
      "Epoch: 216, Loss: 2438.01986, Residuals: -0.96490, Convergence: 0.001752\n",
      "Epoch: 217, Loss: 2434.19578, Residuals: -0.96336, Convergence: 0.001571\n",
      "Epoch: 218, Loss: 2430.75434, Residuals: -0.96197, Convergence: 0.001416\n",
      "Epoch: 219, Loss: 2427.63970, Residuals: -0.96070, Convergence: 0.001283\n",
      "Epoch: 220, Loss: 2424.80698, Residuals: -0.95953, Convergence: 0.001168\n",
      "Epoch: 221, Loss: 2422.21901, Residuals: -0.95846, Convergence: 0.001068\n",
      "Epoch: 222, Loss: 2419.84438, Residuals: -0.95748, Convergence: 0.000981\n",
      "Evidence 14970.644\n",
      "\n",
      "Epoch: 222, Evidence: 14970.64355, Convergence: 0.022665\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 3.29e-01\n",
      "Epoch: 222, Loss: 2511.16372, Residuals: -0.95748, Convergence:   inf\n",
      "Epoch: 223, Loss: 2504.50528, Residuals: -0.95446, Convergence: 0.002659\n",
      "Epoch: 224, Loss: 2499.03744, Residuals: -0.95196, Convergence: 0.002188\n",
      "Epoch: 225, Loss: 2494.41152, Residuals: -0.94992, Convergence: 0.001855\n",
      "Epoch: 226, Loss: 2490.43545, Residuals: -0.94821, Convergence: 0.001597\n",
      "Epoch: 227, Loss: 2486.97463, Residuals: -0.94677, Convergence: 0.001392\n",
      "Epoch: 228, Loss: 2483.92764, Residuals: -0.94554, Convergence: 0.001227\n",
      "Epoch: 229, Loss: 2481.21587, Residuals: -0.94448, Convergence: 0.001093\n",
      "Epoch: 230, Loss: 2478.77898, Residuals: -0.94357, Convergence: 0.000983\n",
      "Evidence 15051.227\n",
      "\n",
      "Epoch: 230, Evidence: 15051.22656, Convergence: 0.005354\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 2.57e-01\n",
      "Epoch: 230, Loss: 2512.60942, Residuals: -0.94357, Convergence:   inf\n",
      "Epoch: 231, Loss: 2508.61489, Residuals: -0.94148, Convergence: 0.001592\n",
      "Epoch: 232, Loss: 2505.31663, Residuals: -0.93985, Convergence: 0.001317\n",
      "Epoch: 233, Loss: 2502.49976, Residuals: -0.93853, Convergence: 0.001126\n",
      "Epoch: 234, Loss: 2500.04881, Residuals: -0.93745, Convergence: 0.000980\n",
      "Evidence 15079.609\n",
      "\n",
      "Epoch: 234, Evidence: 15079.60938, Convergence: 0.001882\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 2.07e-01\n",
      "Epoch: 234, Loss: 2513.55698, Residuals: -0.93745, Convergence:   inf\n",
      "Epoch: 235, Loss: 2510.63384, Residuals: -0.93587, Convergence: 0.001164\n",
      "Epoch: 236, Loss: 2508.19436, Residuals: -0.93461, Convergence: 0.000973\n",
      "Evidence 15091.853\n",
      "\n",
      "Epoch: 236, Evidence: 15091.85254, Convergence: 0.000811\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 1.71e-01\n",
      "Epoch: 236, Loss: 2514.25831, Residuals: -0.93461, Convergence:   inf\n",
      "Epoch: 237, Loss: 2509.80076, Residuals: -0.93267, Convergence: 0.001776\n",
      "Epoch: 238, Loss: 2506.32101, Residuals: -0.93091, Convergence: 0.001388\n",
      "Epoch: 239, Loss: 2503.51208, Residuals: -0.92960, Convergence: 0.001122\n",
      "Epoch: 240, Loss: 2501.14088, Residuals: -0.92879, Convergence: 0.000948\n",
      "Evidence 15109.588\n",
      "\n",
      "Epoch: 240, Evidence: 15109.58789, Convergence: 0.001984\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 1.42e-01\n",
      "Epoch: 240, Loss: 2514.35872, Residuals: -0.92879, Convergence:   inf\n",
      "Epoch: 241, Loss: 2511.39524, Residuals: -0.92652, Convergence: 0.001180\n",
      "Epoch: 242, Loss: 2509.04520, Residuals: -0.92518, Convergence: 0.000937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence 15120.520\n",
      "\n",
      "Epoch: 242, Evidence: 15120.51953, Convergence: 0.000723\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 1.20e-01\n",
      "Epoch: 242, Loss: 2514.51203, Residuals: -0.92518, Convergence:   inf\n",
      "Epoch: 243, Loss: 2510.20301, Residuals: -0.92180, Convergence: 0.001717\n",
      "Epoch: 244, Loss: 2507.14752, Residuals: -0.92233, Convergence: 0.001219\n",
      "Epoch: 245, Loss: 2504.62287, Residuals: -0.92306, Convergence: 0.001008\n",
      "Epoch: 246, Loss: 2502.45553, Residuals: -0.92555, Convergence: 0.000866\n",
      "Evidence 15135.924\n",
      "\n",
      "Epoch: 246, Evidence: 15135.92383, Convergence: 0.001740\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 1.07e-01\n",
      "Epoch: 246, Loss: 2513.81927, Residuals: -0.92555, Convergence:   inf\n",
      "Epoch: 247, Loss: 2512.35865, Residuals: -0.92343, Convergence: 0.000581\n",
      "Evidence 15142.315\n",
      "\n",
      "Epoch: 247, Evidence: 15142.31543, Convergence: 0.000422\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 8.78e-02\n",
      "Epoch: 247, Loss: 2514.75222, Residuals: -0.92343, Convergence:   inf\n",
      "Epoch: 248, Loss: 2561.76458, Residuals: -0.97006, Convergence: -0.018352\n",
      "Epoch: 248, Loss: 2512.57498, Residuals: -0.92144, Convergence: 0.000867\n",
      "Evidence 15146.671\n",
      "\n",
      "Epoch: 248, Evidence: 15146.67090, Convergence: 0.000710\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 8.12e-02\n",
      "Epoch: 248, Loss: 2514.18031, Residuals: -0.92144, Convergence:   inf\n",
      "Epoch: 249, Loss: 2518.87121, Residuals: -0.92342, Convergence: -0.001862\n",
      "Epoch: 249, Loss: 2514.11453, Residuals: -0.91982, Convergence: 0.000026\n",
      "Evidence 15148.401\n",
      "\n",
      "Epoch: 249, Evidence: 15148.40137, Convergence: 0.000824\n",
      "Total samples: 185, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 384.27294, Residuals: -4.50604, Convergence:   inf\n",
      "Epoch: 1, Loss: 358.44034, Residuals: -4.38532, Convergence: 0.072069\n",
      "Epoch: 2, Loss: 337.31061, Residuals: -4.22168, Convergence: 0.062642\n",
      "Epoch: 3, Loss: 321.20108, Residuals: -4.05821, Convergence: 0.050154\n",
      "Epoch: 4, Loss: 308.89495, Residuals: -3.91420, Convergence: 0.039839\n",
      "Epoch: 5, Loss: 299.12451, Residuals: -3.78701, Convergence: 0.032663\n",
      "Epoch: 6, Loss: 291.18209, Residuals: -3.67630, Convergence: 0.027276\n",
      "Epoch: 7, Loss: 284.57834, Residuals: -3.58150, Convergence: 0.023205\n",
      "Epoch: 8, Loss: 278.95131, Residuals: -3.50056, Convergence: 0.020172\n",
      "Epoch: 9, Loss: 274.04474, Residuals: -3.43112, Convergence: 0.017904\n",
      "Epoch: 10, Loss: 269.67569, Residuals: -3.37108, Convergence: 0.016201\n",
      "Epoch: 11, Loss: 265.71090, Residuals: -3.31863, Convergence: 0.014921\n",
      "Epoch: 12, Loss: 262.05253, Residuals: -3.27225, Convergence: 0.013960\n",
      "Epoch: 13, Loss: 258.62937, Residuals: -3.23060, Convergence: 0.013236\n",
      "Epoch: 14, Loss: 255.39130, Residuals: -3.19255, Convergence: 0.012679\n",
      "Epoch: 15, Loss: 252.30676, Residuals: -3.15717, Convergence: 0.012225\n",
      "Epoch: 16, Loss: 249.35974, Residuals: -3.12387, Convergence: 0.011818\n",
      "Epoch: 17, Loss: 246.53899, Residuals: -3.09229, Convergence: 0.011441\n",
      "Epoch: 18, Loss: 243.82360, Residuals: -3.06205, Convergence: 0.011137\n",
      "Epoch: 19, Loss: 241.18056, Residuals: -3.03264, Convergence: 0.010959\n",
      "Epoch: 20, Loss: 238.57284, Residuals: -3.00346, Convergence: 0.010930\n",
      "Epoch: 21, Loss: 235.96886, Residuals: -2.97399, Convergence: 0.011035\n",
      "Epoch: 22, Loss: 233.34504, Residuals: -2.94387, Convergence: 0.011244\n",
      "Epoch: 23, Loss: 230.67372, Residuals: -2.91273, Convergence: 0.011581\n",
      "Epoch: 24, Loss: 227.90670, Residuals: -2.88001, Convergence: 0.012141\n",
      "Epoch: 25, Loss: 224.98799, Residuals: -2.84497, Convergence: 0.012973\n",
      "Epoch: 26, Loss: 221.93714, Residuals: -2.80768, Convergence: 0.013746\n",
      "Epoch: 27, Loss: 218.89371, Residuals: -2.76955, Convergence: 0.013904\n",
      "Epoch: 28, Loss: 215.95601, Residuals: -2.73174, Convergence: 0.013603\n",
      "Epoch: 29, Loss: 213.13359, Residuals: -2.69452, Convergence: 0.013243\n",
      "Epoch: 30, Loss: 210.41077, Residuals: -2.65781, Convergence: 0.012940\n",
      "Epoch: 31, Loss: 207.77231, Residuals: -2.62153, Convergence: 0.012699\n",
      "Epoch: 32, Loss: 205.20720, Residuals: -2.58562, Convergence: 0.012500\n",
      "Epoch: 33, Loss: 202.70816, Residuals: -2.55003, Convergence: 0.012328\n",
      "Epoch: 34, Loss: 200.27053, Residuals: -2.51476, Convergence: 0.012172\n",
      "Epoch: 35, Loss: 197.89141, Residuals: -2.47978, Convergence: 0.012022\n",
      "Epoch: 36, Loss: 195.56905, Residuals: -2.44508, Convergence: 0.011875\n",
      "Epoch: 37, Loss: 193.30241, Residuals: -2.41065, Convergence: 0.011726\n",
      "Epoch: 38, Loss: 191.09091, Residuals: -2.37648, Convergence: 0.011573\n",
      "Epoch: 39, Loss: 188.93416, Residuals: -2.34255, Convergence: 0.011415\n",
      "Epoch: 40, Loss: 186.83187, Residuals: -2.30887, Convergence: 0.011252\n",
      "Epoch: 41, Loss: 184.78372, Residuals: -2.27542, Convergence: 0.011084\n",
      "Epoch: 42, Loss: 182.78937, Residuals: -2.24220, Convergence: 0.010911\n",
      "Epoch: 43, Loss: 180.84834, Residuals: -2.20920, Convergence: 0.010733\n",
      "Epoch: 44, Loss: 178.96014, Residuals: -2.17643, Convergence: 0.010551\n",
      "Epoch: 45, Loss: 177.12441, Residuals: -2.14387, Convergence: 0.010364\n",
      "Epoch: 46, Loss: 175.34106, Residuals: -2.11155, Convergence: 0.010171\n",
      "Epoch: 47, Loss: 173.61053, Residuals: -2.07946, Convergence: 0.009968\n",
      "Epoch: 48, Loss: 171.93386, Residuals: -2.04765, Convergence: 0.009752\n",
      "Epoch: 49, Loss: 170.31261, Residuals: -2.01614, Convergence: 0.009519\n",
      "Epoch: 50, Loss: 168.74851, Residuals: -1.98498, Convergence: 0.009269\n",
      "Epoch: 51, Loss: 167.24317, Residuals: -1.95423, Convergence: 0.009001\n",
      "Epoch: 52, Loss: 165.79765, Residuals: -1.92394, Convergence: 0.008719\n",
      "Epoch: 53, Loss: 164.41237, Residuals: -1.89416, Convergence: 0.008426\n",
      "Epoch: 54, Loss: 163.08705, Residuals: -1.86492, Convergence: 0.008126\n",
      "Epoch: 55, Loss: 161.82088, Residuals: -1.83628, Convergence: 0.007825\n",
      "Epoch: 56, Loss: 160.61264, Residuals: -1.80826, Convergence: 0.007523\n",
      "Epoch: 57, Loss: 159.46084, Residuals: -1.78089, Convergence: 0.007223\n",
      "Epoch: 58, Loss: 158.36389, Residuals: -1.75420, Convergence: 0.006927\n",
      "Epoch: 59, Loss: 157.32011, Residuals: -1.72821, Convergence: 0.006635\n",
      "Epoch: 60, Loss: 156.32785, Residuals: -1.70293, Convergence: 0.006347\n",
      "Epoch: 61, Loss: 155.38547, Residuals: -1.67838, Convergence: 0.006065\n",
      "Epoch: 62, Loss: 154.49135, Residuals: -1.65457, Convergence: 0.005787\n",
      "Epoch: 63, Loss: 153.64394, Residuals: -1.63153, Convergence: 0.005515\n",
      "Epoch: 64, Loss: 152.84164, Residuals: -1.60925, Convergence: 0.005249\n",
      "Epoch: 65, Loss: 152.08284, Residuals: -1.58775, Convergence: 0.004989\n",
      "Epoch: 66, Loss: 151.36584, Residuals: -1.56704, Convergence: 0.004737\n",
      "Epoch: 67, Loss: 150.68883, Residuals: -1.54711, Convergence: 0.004493\n",
      "Epoch: 68, Loss: 150.04986, Residuals: -1.52796, Convergence: 0.004258\n",
      "Epoch: 69, Loss: 149.44684, Residuals: -1.50958, Convergence: 0.004035\n",
      "Epoch: 70, Loss: 148.87753, Residuals: -1.49195, Convergence: 0.003824\n",
      "Epoch: 71, Loss: 148.33964, Residuals: -1.47506, Convergence: 0.003626\n",
      "Epoch: 72, Loss: 147.83090, Residuals: -1.45888, Convergence: 0.003441\n",
      "Epoch: 73, Loss: 147.34909, Residuals: -1.44338, Convergence: 0.003270\n",
      "Epoch: 74, Loss: 146.89222, Residuals: -1.42853, Convergence: 0.003110\n",
      "Epoch: 75, Loss: 146.45848, Residuals: -1.41429, Convergence: 0.002962\n",
      "Epoch: 76, Loss: 146.04633, Residuals: -1.40064, Convergence: 0.002822\n",
      "Epoch: 77, Loss: 145.65445, Residuals: -1.38756, Convergence: 0.002691\n",
      "Epoch: 78, Loss: 145.28170, Residuals: -1.37501, Convergence: 0.002566\n",
      "Epoch: 79, Loss: 144.92708, Residuals: -1.36299, Convergence: 0.002447\n",
      "Epoch: 80, Loss: 144.58974, Residuals: -1.35147, Convergence: 0.002333\n",
      "Epoch: 81, Loss: 144.26887, Residuals: -1.34042, Convergence: 0.002224\n",
      "Epoch: 82, Loss: 143.96373, Residuals: -1.32984, Convergence: 0.002120\n",
      "Epoch: 83, Loss: 143.67360, Residuals: -1.31971, Convergence: 0.002019\n",
      "Epoch: 84, Loss: 143.39782, Residuals: -1.31000, Convergence: 0.001923\n",
      "Epoch: 85, Loss: 143.13570, Residuals: -1.30071, Convergence: 0.001831\n",
      "Epoch: 86, Loss: 142.88663, Residuals: -1.29181, Convergence: 0.001743\n",
      "Epoch: 87, Loss: 142.64998, Residuals: -1.28329, Convergence: 0.001659\n",
      "Epoch: 88, Loss: 142.42517, Residuals: -1.27514, Convergence: 0.001578\n",
      "Epoch: 89, Loss: 142.21162, Residuals: -1.26733, Convergence: 0.001502\n",
      "Epoch: 90, Loss: 142.00880, Residuals: -1.25985, Convergence: 0.001428\n",
      "Epoch: 91, Loss: 141.81619, Residuals: -1.25269, Convergence: 0.001358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 92, Loss: 141.63331, Residuals: -1.24583, Convergence: 0.001291\n",
      "Epoch: 93, Loss: 141.45973, Residuals: -1.23926, Convergence: 0.001227\n",
      "Epoch: 94, Loss: 141.29502, Residuals: -1.23297, Convergence: 0.001166\n",
      "Epoch: 95, Loss: 141.13880, Residuals: -1.22694, Convergence: 0.001107\n",
      "Epoch: 96, Loss: 140.99071, Residuals: -1.22116, Convergence: 0.001050\n",
      "Epoch: 97, Loss: 140.85044, Residuals: -1.21562, Convergence: 0.000996\n",
      "Evidence -182.651\n",
      "\n",
      "Epoch: 97, Evidence: -182.65117, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 7.24e-01\n",
      "Epoch: 97, Loss: 1384.18372, Residuals: -1.21562, Convergence:   inf\n",
      "Epoch: 98, Loss: 1320.46106, Residuals: -1.24664, Convergence: 0.048258\n",
      "Epoch: 99, Loss: 1272.60467, Residuals: -1.27059, Convergence: 0.037605\n",
      "Epoch: 100, Loss: 1236.84948, Residuals: -1.28717, Convergence: 0.028908\n",
      "Epoch: 101, Loss: 1209.16867, Residuals: -1.29834, Convergence: 0.022892\n",
      "Epoch: 102, Loss: 1186.81655, Residuals: -1.30629, Convergence: 0.018834\n",
      "Epoch: 103, Loss: 1168.27816, Residuals: -1.31215, Convergence: 0.015868\n",
      "Epoch: 104, Loss: 1152.65532, Residuals: -1.31638, Convergence: 0.013554\n",
      "Epoch: 105, Loss: 1139.33742, Residuals: -1.31924, Convergence: 0.011689\n",
      "Epoch: 106, Loss: 1127.86967, Residuals: -1.32089, Convergence: 0.010168\n",
      "Epoch: 107, Loss: 1117.89664, Residuals: -1.32148, Convergence: 0.008921\n",
      "Epoch: 108, Loss: 1109.13142, Residuals: -1.32111, Convergence: 0.007903\n",
      "Epoch: 109, Loss: 1101.33639, Residuals: -1.31990, Convergence: 0.007078\n",
      "Epoch: 110, Loss: 1094.31078, Residuals: -1.31791, Convergence: 0.006420\n",
      "Epoch: 111, Loss: 1087.87885, Residuals: -1.31523, Convergence: 0.005912\n",
      "Epoch: 112, Loss: 1081.88297, Residuals: -1.31188, Convergence: 0.005542\n",
      "Epoch: 113, Loss: 1076.17677, Residuals: -1.30789, Convergence: 0.005302\n",
      "Epoch: 114, Loss: 1070.62427, Residuals: -1.30326, Convergence: 0.005186\n",
      "Epoch: 115, Loss: 1065.10255, Residuals: -1.29799, Convergence: 0.005184\n",
      "Epoch: 116, Loss: 1059.51258, Residuals: -1.29207, Convergence: 0.005276\n",
      "Epoch: 117, Loss: 1053.79933, Residuals: -1.28555, Convergence: 0.005422\n",
      "Epoch: 118, Loss: 1047.96938, Residuals: -1.27847, Convergence: 0.005563\n",
      "Epoch: 119, Loss: 1042.09655, Residuals: -1.27092, Convergence: 0.005636\n",
      "Epoch: 120, Loss: 1036.30141, Residuals: -1.26302, Convergence: 0.005592\n",
      "Epoch: 121, Loss: 1030.70742, Residuals: -1.25484, Convergence: 0.005427\n",
      "Epoch: 122, Loss: 1025.40528, Residuals: -1.24650, Convergence: 0.005171\n",
      "Epoch: 123, Loss: 1020.43865, Residuals: -1.23807, Convergence: 0.004867\n",
      "Epoch: 124, Loss: 1015.81266, Residuals: -1.22962, Convergence: 0.004554\n",
      "Epoch: 125, Loss: 1011.50974, Residuals: -1.22122, Convergence: 0.004254\n",
      "Epoch: 126, Loss: 1007.50226, Residuals: -1.21293, Convergence: 0.003978\n",
      "Epoch: 127, Loss: 1003.76090, Residuals: -1.20477, Convergence: 0.003727\n",
      "Epoch: 128, Loss: 1000.25910, Residuals: -1.19679, Convergence: 0.003501\n",
      "Epoch: 129, Loss: 996.97359, Residuals: -1.18902, Convergence: 0.003295\n",
      "Epoch: 130, Loss: 993.88492, Residuals: -1.18147, Convergence: 0.003108\n",
      "Epoch: 131, Loss: 990.97719, Residuals: -1.17417, Convergence: 0.002934\n",
      "Epoch: 132, Loss: 988.23733, Residuals: -1.16714, Convergence: 0.002772\n",
      "Epoch: 133, Loss: 985.65354, Residuals: -1.16037, Convergence: 0.002621\n",
      "Epoch: 134, Loss: 983.21577, Residuals: -1.15389, Convergence: 0.002479\n",
      "Epoch: 135, Loss: 980.91471, Residuals: -1.14768, Convergence: 0.002346\n",
      "Epoch: 136, Loss: 978.74182, Residuals: -1.14176, Convergence: 0.002220\n",
      "Epoch: 137, Loss: 976.68857, Residuals: -1.13612, Convergence: 0.002102\n",
      "Epoch: 138, Loss: 974.74716, Residuals: -1.13075, Convergence: 0.001992\n",
      "Epoch: 139, Loss: 972.90936, Residuals: -1.12565, Convergence: 0.001889\n",
      "Epoch: 140, Loss: 971.16799, Residuals: -1.12080, Convergence: 0.001793\n",
      "Epoch: 141, Loss: 969.51573, Residuals: -1.11621, Convergence: 0.001704\n",
      "Epoch: 142, Loss: 967.94547, Residuals: -1.11185, Convergence: 0.001622\n",
      "Epoch: 143, Loss: 966.45074, Residuals: -1.10771, Convergence: 0.001547\n",
      "Epoch: 144, Loss: 965.02513, Residuals: -1.10379, Convergence: 0.001477\n",
      "Epoch: 145, Loss: 963.66296, Residuals: -1.10007, Convergence: 0.001414\n",
      "Epoch: 146, Loss: 962.35863, Residuals: -1.09654, Convergence: 0.001355\n",
      "Epoch: 147, Loss: 961.10740, Residuals: -1.09319, Convergence: 0.001302\n",
      "Epoch: 148, Loss: 959.90443, Residuals: -1.09001, Convergence: 0.001253\n",
      "Epoch: 149, Loss: 958.74588, Residuals: -1.08698, Convergence: 0.001208\n",
      "Epoch: 150, Loss: 957.62744, Residuals: -1.08410, Convergence: 0.001168\n",
      "Epoch: 151, Loss: 956.54577, Residuals: -1.08136, Convergence: 0.001131\n",
      "Epoch: 152, Loss: 955.49752, Residuals: -1.07874, Convergence: 0.001097\n",
      "Epoch: 153, Loss: 954.48014, Residuals: -1.07624, Convergence: 0.001066\n",
      "Epoch: 154, Loss: 953.49022, Residuals: -1.07385, Convergence: 0.001038\n",
      "Epoch: 155, Loss: 952.52596, Residuals: -1.07156, Convergence: 0.001012\n",
      "Epoch: 156, Loss: 951.58470, Residuals: -1.06937, Convergence: 0.000989\n",
      "Evidence 11243.874\n",
      "\n",
      "Epoch: 156, Evidence: 11243.87402, Convergence: 1.016245\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 5.77e-01\n",
      "Epoch: 156, Loss: 2377.00760, Residuals: -1.06937, Convergence:   inf\n",
      "Epoch: 157, Loss: 2335.28235, Residuals: -1.07964, Convergence: 0.017867\n",
      "Epoch: 158, Loss: 2306.30809, Residuals: -1.07944, Convergence: 0.012563\n",
      "Epoch: 159, Loss: 2281.81315, Residuals: -1.07815, Convergence: 0.010735\n",
      "Epoch: 160, Loss: 2260.80709, Residuals: -1.07639, Convergence: 0.009291\n",
      "Epoch: 161, Loss: 2242.65987, Residuals: -1.07427, Convergence: 0.008092\n",
      "Epoch: 162, Loss: 2226.87946, Residuals: -1.07186, Convergence: 0.007086\n",
      "Epoch: 163, Loss: 2213.06441, Residuals: -1.06920, Convergence: 0.006242\n",
      "Epoch: 164, Loss: 2200.88176, Residuals: -1.06633, Convergence: 0.005535\n",
      "Epoch: 165, Loss: 2190.05630, Residuals: -1.06330, Convergence: 0.004943\n",
      "Epoch: 166, Loss: 2180.35552, Residuals: -1.06014, Convergence: 0.004449\n",
      "Epoch: 167, Loss: 2171.58050, Residuals: -1.05686, Convergence: 0.004041\n",
      "Epoch: 168, Loss: 2163.56032, Residuals: -1.05347, Convergence: 0.003707\n",
      "Epoch: 169, Loss: 2156.15328, Residuals: -1.05000, Convergence: 0.003435\n",
      "Epoch: 170, Loss: 2149.25283, Residuals: -1.04642, Convergence: 0.003211\n",
      "Epoch: 171, Loss: 2142.79237, Residuals: -1.04276, Convergence: 0.003015\n",
      "Epoch: 172, Loss: 2136.74255, Residuals: -1.03904, Convergence: 0.002831\n",
      "Epoch: 173, Loss: 2131.09620, Residuals: -1.03532, Convergence: 0.002650\n",
      "Epoch: 174, Loss: 2125.84963, Residuals: -1.03163, Convergence: 0.002468\n",
      "Epoch: 175, Loss: 2120.99282, Residuals: -1.02803, Convergence: 0.002290\n",
      "Epoch: 176, Loss: 2116.50856, Residuals: -1.02453, Convergence: 0.002119\n",
      "Epoch: 177, Loss: 2112.37155, Residuals: -1.02117, Convergence: 0.001958\n",
      "Epoch: 178, Loss: 2108.55448, Residuals: -1.01796, Convergence: 0.001810\n",
      "Epoch: 179, Loss: 2105.02785, Residuals: -1.01490, Convergence: 0.001675\n",
      "Epoch: 180, Loss: 2101.76198, Residuals: -1.01199, Convergence: 0.001554\n",
      "Epoch: 181, Loss: 2098.73093, Residuals: -1.00924, Convergence: 0.001444\n",
      "Epoch: 182, Loss: 2095.90754, Residuals: -1.00663, Convergence: 0.001347\n",
      "Epoch: 183, Loss: 2093.27026, Residuals: -1.00417, Convergence: 0.001260\n",
      "Epoch: 184, Loss: 2090.79889, Residuals: -1.00185, Convergence: 0.001182\n",
      "Epoch: 185, Loss: 2088.47451, Residuals: -0.99965, Convergence: 0.001113\n",
      "Epoch: 186, Loss: 2086.28204, Residuals: -0.99758, Convergence: 0.001051\n",
      "Epoch: 187, Loss: 2084.20724, Residuals: -0.99563, Convergence: 0.000995\n",
      "Evidence 14469.359\n",
      "\n",
      "Epoch: 187, Evidence: 14469.35938, Convergence: 0.222918\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 4.39e-01\n",
      "Epoch: 187, Loss: 2506.03032, Residuals: -0.99563, Convergence:   inf\n",
      "Epoch: 188, Loss: 2490.59644, Residuals: -0.99287, Convergence: 0.006197\n",
      "Epoch: 189, Loss: 2478.03897, Residuals: -0.98945, Convergence: 0.005068\n",
      "Epoch: 190, Loss: 2467.23907, Residuals: -0.98605, Convergence: 0.004377\n",
      "Epoch: 191, Loss: 2457.87949, Residuals: -0.98277, Convergence: 0.003808\n",
      "Epoch: 192, Loss: 2449.71596, Residuals: -0.97965, Convergence: 0.003332\n",
      "Epoch: 193, Loss: 2442.55326, Residuals: -0.97672, Convergence: 0.002932\n",
      "Epoch: 194, Loss: 2436.23166, Residuals: -0.97398, Convergence: 0.002595\n",
      "Epoch: 195, Loss: 2430.62033, Residuals: -0.97144, Convergence: 0.002309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 196, Loss: 2425.61181, Residuals: -0.96908, Convergence: 0.002065\n",
      "Epoch: 197, Loss: 2421.11582, Residuals: -0.96688, Convergence: 0.001857\n",
      "Epoch: 198, Loss: 2417.05793, Residuals: -0.96485, Convergence: 0.001679\n",
      "Epoch: 199, Loss: 2413.37623, Residuals: -0.96296, Convergence: 0.001526\n",
      "Epoch: 200, Loss: 2410.01862, Residuals: -0.96120, Convergence: 0.001393\n",
      "Epoch: 201, Loss: 2406.94272, Residuals: -0.95956, Convergence: 0.001278\n",
      "Epoch: 202, Loss: 2404.11281, Residuals: -0.95803, Convergence: 0.001177\n",
      "Epoch: 203, Loss: 2401.49864, Residuals: -0.95660, Convergence: 0.001089\n",
      "Epoch: 204, Loss: 2399.07434, Residuals: -0.95526, Convergence: 0.001011\n",
      "Epoch: 205, Loss: 2396.81915, Residuals: -0.95401, Convergence: 0.000941\n",
      "Evidence 14919.522\n",
      "\n",
      "Epoch: 205, Evidence: 14919.52246, Convergence: 0.030173\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 3.36e-01\n",
      "Epoch: 205, Loss: 2511.19425, Residuals: -0.95401, Convergence:   inf\n",
      "Epoch: 206, Loss: 2504.20819, Residuals: -0.95101, Convergence: 0.002790\n",
      "Epoch: 207, Loss: 2498.39912, Residuals: -0.94818, Convergence: 0.002325\n",
      "Epoch: 208, Loss: 2493.44525, Residuals: -0.94562, Convergence: 0.001987\n",
      "Epoch: 209, Loss: 2489.16348, Residuals: -0.94332, Convergence: 0.001720\n",
      "Epoch: 210, Loss: 2485.41407, Residuals: -0.94128, Convergence: 0.001509\n",
      "Epoch: 211, Loss: 2482.09415, Residuals: -0.93945, Convergence: 0.001338\n",
      "Epoch: 212, Loss: 2479.12386, Residuals: -0.93783, Convergence: 0.001198\n",
      "Epoch: 213, Loss: 2476.44134, Residuals: -0.93639, Convergence: 0.001083\n",
      "Epoch: 214, Loss: 2473.99881, Residuals: -0.93510, Convergence: 0.000987\n",
      "Evidence 15014.993\n",
      "\n",
      "Epoch: 214, Evidence: 15014.99316, Convergence: 0.006358\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 2.65e-01\n",
      "Epoch: 214, Loss: 2513.02435, Residuals: -0.93510, Convergence:   inf\n",
      "Epoch: 215, Loss: 2508.95462, Residuals: -0.93272, Convergence: 0.001622\n",
      "Epoch: 216, Loss: 2505.53258, Residuals: -0.93064, Convergence: 0.001366\n",
      "Epoch: 217, Loss: 2502.57417, Residuals: -0.92885, Convergence: 0.001182\n",
      "Epoch: 218, Loss: 2499.96980, Residuals: -0.92730, Convergence: 0.001042\n",
      "Epoch: 219, Loss: 2497.64395, Residuals: -0.92597, Convergence: 0.000931\n",
      "Evidence 15047.283\n",
      "\n",
      "Epoch: 219, Evidence: 15047.28320, Convergence: 0.002146\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 2.15e-01\n",
      "Epoch: 219, Loss: 2514.08279, Residuals: -0.92597, Convergence:   inf\n",
      "Epoch: 220, Loss: 2511.19051, Residuals: -0.92408, Convergence: 0.001152\n",
      "Epoch: 221, Loss: 2508.72810, Residuals: -0.92247, Convergence: 0.000982\n",
      "Evidence 15059.930\n",
      "\n",
      "Epoch: 221, Evidence: 15059.92969, Convergence: 0.000840\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 1.80e-01\n",
      "Epoch: 221, Loss: 2514.88706, Residuals: -0.92247, Convergence:   inf\n",
      "Epoch: 222, Loss: 2510.26689, Residuals: -0.91980, Convergence: 0.001841\n",
      "Epoch: 223, Loss: 2506.68114, Residuals: -0.91766, Convergence: 0.001430\n",
      "Epoch: 224, Loss: 2503.74117, Residuals: -0.91617, Convergence: 0.001174\n",
      "Epoch: 225, Loss: 2501.23525, Residuals: -0.91526, Convergence: 0.001002\n",
      "Epoch: 226, Loss: 2499.03397, Residuals: -0.91478, Convergence: 0.000881\n",
      "Evidence 15080.796\n",
      "\n",
      "Epoch: 226, Evidence: 15080.79590, Convergence: 0.002222\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 1.49e-01\n",
      "Epoch: 226, Loss: 2515.12397, Residuals: -0.91478, Convergence:   inf\n",
      "Epoch: 227, Loss: 2512.27768, Residuals: -0.91278, Convergence: 0.001133\n",
      "Epoch: 228, Loss: 2509.98309, Residuals: -0.91185, Convergence: 0.000914\n",
      "Evidence 15092.011\n",
      "\n",
      "Epoch: 228, Evidence: 15092.01074, Convergence: 0.000743\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 1.27e-01\n",
      "Epoch: 228, Loss: 2515.43205, Residuals: -0.91185, Convergence:   inf\n",
      "Epoch: 229, Loss: 2511.24632, Residuals: -0.90953, Convergence: 0.001667\n",
      "Epoch: 230, Loss: 2508.16724, Residuals: -0.91124, Convergence: 0.001228\n",
      "Epoch: 231, Loss: 2505.60704, Residuals: -0.91270, Convergence: 0.001022\n",
      "Epoch: 232, Loss: 2503.40061, Residuals: -0.91577, Convergence: 0.000881\n",
      "Evidence 15107.447\n",
      "\n",
      "Epoch: 232, Evidence: 15107.44727, Convergence: 0.001764\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 1.13e-01\n",
      "Epoch: 232, Loss: 2515.06211, Residuals: -0.91577, Convergence:   inf\n",
      "Epoch: 233, Loss: 2513.82500, Residuals: -0.91542, Convergence: 0.000492\n",
      "Evidence 15113.104\n",
      "\n",
      "Epoch: 233, Evidence: 15113.10449, Convergence: 0.000374\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 9.22e-02\n",
      "Epoch: 233, Loss: 2515.99416, Residuals: -0.91542, Convergence:   inf\n",
      "Epoch: 234, Loss: 2567.36088, Residuals: -0.95268, Convergence: -0.020008\n",
      "Epoch: 234, Loss: 2513.74565, Residuals: -0.91358, Convergence: 0.000894\n",
      "Evidence 15117.562\n",
      "\n",
      "Epoch: 234, Evidence: 15117.56250, Convergence: 0.000669\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 8.43e-02\n",
      "Epoch: 234, Loss: 2515.55932, Residuals: -0.91358, Convergence:   inf\n",
      "Epoch: 235, Loss: 2520.39094, Residuals: -0.91446, Convergence: -0.001917\n",
      "Epoch: 235, Loss: 2515.80686, Residuals: -0.91257, Convergence: -0.000098\n",
      "Evidence 15118.836\n",
      "\n",
      "Epoch: 235, Evidence: 15118.83594, Convergence: 0.000753\n",
      "Total samples: 184, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 383.52813, Residuals: -4.52429, Convergence:   inf\n",
      "Epoch: 1, Loss: 357.79534, Residuals: -4.40399, Convergence: 0.071920\n",
      "Epoch: 2, Loss: 336.70457, Residuals: -4.23989, Convergence: 0.062639\n",
      "Epoch: 3, Loss: 320.56471, Residuals: -4.07480, Convergence: 0.050348\n",
      "Epoch: 4, Loss: 308.26863, Residuals: -3.92953, Convergence: 0.039888\n",
      "Epoch: 5, Loss: 298.52267, Residuals: -3.80115, Convergence: 0.032647\n",
      "Epoch: 6, Loss: 290.61371, Residuals: -3.68923, Convergence: 0.027215\n",
      "Epoch: 7, Loss: 284.04975, Residuals: -3.59328, Convergence: 0.023108\n",
      "Epoch: 8, Loss: 278.46634, Residuals: -3.51123, Convergence: 0.020051\n",
      "Epoch: 9, Loss: 273.60602, Residuals: -3.44079, Convergence: 0.017764\n",
      "Epoch: 10, Loss: 269.28586, Residuals: -3.37987, Convergence: 0.016043\n",
      "Epoch: 11, Loss: 265.37357, Residuals: -3.32669, Convergence: 0.014743\n",
      "Epoch: 12, Loss: 261.77287, Residuals: -3.27976, Convergence: 0.013755\n",
      "Epoch: 13, Loss: 258.41440, Residuals: -3.23778, Convergence: 0.012996\n",
      "Epoch: 14, Loss: 255.24966, Residuals: -3.19963, Convergence: 0.012399\n",
      "Epoch: 15, Loss: 252.24751, Residuals: -3.16436, Convergence: 0.011902\n",
      "Epoch: 16, Loss: 249.39081, Residuals: -3.13131, Convergence: 0.011455\n",
      "Epoch: 17, Loss: 246.66714, Residuals: -3.10004, Convergence: 0.011042\n",
      "Epoch: 18, Loss: 244.05626, Residuals: -3.07013, Convergence: 0.010698\n",
      "Epoch: 19, Loss: 241.52723, Residuals: -3.04105, Convergence: 0.010471\n",
      "Epoch: 20, Loss: 239.04545, Residuals: -3.01222, Convergence: 0.010382\n",
      "Epoch: 21, Loss: 236.58138, Residuals: -2.98317, Convergence: 0.010415\n",
      "Epoch: 22, Loss: 234.11455, Residuals: -2.95359, Convergence: 0.010537\n",
      "Epoch: 23, Loss: 231.62430, Residuals: -2.92325, Convergence: 0.010751\n",
      "Epoch: 24, Loss: 229.07284, Residuals: -2.89175, Convergence: 0.011138\n",
      "Epoch: 25, Loss: 226.40525, Residuals: -2.85845, Convergence: 0.011782\n",
      "Epoch: 26, Loss: 223.59954, Residuals: -2.82302, Convergence: 0.012548\n",
      "Epoch: 27, Loss: 220.74610, Residuals: -2.78636, Convergence: 0.012926\n",
      "Epoch: 28, Loss: 217.96311, Residuals: -2.74982, Convergence: 0.012768\n",
      "Epoch: 29, Loss: 215.28187, Residuals: -2.71387, Convergence: 0.012455\n",
      "Epoch: 30, Loss: 212.68867, Residuals: -2.67848, Convergence: 0.012192\n",
      "Epoch: 31, Loss: 210.16479, Residuals: -2.64348, Convergence: 0.012009\n",
      "Epoch: 32, Loss: 207.69594, Residuals: -2.60875, Convergence: 0.011887\n",
      "Epoch: 33, Loss: 205.27307, Residuals: -2.57419, Convergence: 0.011803\n",
      "Epoch: 34, Loss: 202.89143, Residuals: -2.53972, Convergence: 0.011739\n",
      "Epoch: 35, Loss: 200.54942, Residuals: -2.50532, Convergence: 0.011678\n",
      "Epoch: 36, Loss: 198.24767, Residuals: -2.47098, Convergence: 0.011611\n",
      "Epoch: 37, Loss: 195.98810, Residuals: -2.43669, Convergence: 0.011529\n",
      "Epoch: 38, Loss: 193.77333, Residuals: -2.40248, Convergence: 0.011430\n",
      "Epoch: 39, Loss: 191.60615, Residuals: -2.36836, Convergence: 0.011311\n",
      "Epoch: 40, Loss: 189.48915, Residuals: -2.33436, Convergence: 0.011172\n",
      "Epoch: 41, Loss: 187.42451, Residuals: -2.30052, Convergence: 0.011016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42, Loss: 185.41392, Residuals: -2.26685, Convergence: 0.010844\n",
      "Epoch: 43, Loss: 183.45854, Residuals: -2.23340, Convergence: 0.010658\n",
      "Epoch: 44, Loss: 181.55913, Residuals: -2.20019, Convergence: 0.010462\n",
      "Epoch: 45, Loss: 179.71611, Residuals: -2.16725, Convergence: 0.010255\n",
      "Epoch: 46, Loss: 177.92978, Residuals: -2.13461, Convergence: 0.010040\n",
      "Epoch: 47, Loss: 176.20039, Residuals: -2.10231, Convergence: 0.009815\n",
      "Epoch: 48, Loss: 174.52828, Residuals: -2.07037, Convergence: 0.009581\n",
      "Epoch: 49, Loss: 172.91382, Residuals: -2.03883, Convergence: 0.009337\n",
      "Epoch: 50, Loss: 171.35737, Residuals: -2.00772, Convergence: 0.009083\n",
      "Epoch: 51, Loss: 169.85912, Residuals: -1.97709, Convergence: 0.008821\n",
      "Epoch: 52, Loss: 168.41905, Residuals: -1.94697, Convergence: 0.008550\n",
      "Epoch: 53, Loss: 167.03686, Residuals: -1.91740, Convergence: 0.008275\n",
      "Epoch: 54, Loss: 165.71194, Residuals: -1.88840, Convergence: 0.007995\n",
      "Epoch: 55, Loss: 164.44346, Residuals: -1.86001, Convergence: 0.007714\n",
      "Epoch: 56, Loss: 163.23033, Residuals: -1.83226, Convergence: 0.007432\n",
      "Epoch: 57, Loss: 162.07135, Residuals: -1.80518, Convergence: 0.007151\n",
      "Epoch: 58, Loss: 160.96519, Residuals: -1.77878, Convergence: 0.006872\n",
      "Epoch: 59, Loss: 159.91043, Residuals: -1.75309, Convergence: 0.006596\n",
      "Epoch: 60, Loss: 158.90562, Residuals: -1.72812, Convergence: 0.006323\n",
      "Epoch: 61, Loss: 157.94926, Residuals: -1.70390, Convergence: 0.006055\n",
      "Epoch: 62, Loss: 157.03984, Residuals: -1.68042, Convergence: 0.005791\n",
      "Epoch: 63, Loss: 156.17577, Residuals: -1.65770, Convergence: 0.005533\n",
      "Epoch: 64, Loss: 155.35542, Residuals: -1.63574, Convergence: 0.005280\n",
      "Epoch: 65, Loss: 154.57709, Residuals: -1.61455, Convergence: 0.005035\n",
      "Epoch: 66, Loss: 153.83893, Residuals: -1.59413, Convergence: 0.004798\n",
      "Epoch: 67, Loss: 153.13901, Residuals: -1.57445, Convergence: 0.004571\n",
      "Epoch: 68, Loss: 152.47525, Residuals: -1.55551, Convergence: 0.004353\n",
      "Epoch: 69, Loss: 151.84553, Residuals: -1.53729, Convergence: 0.004147\n",
      "Epoch: 70, Loss: 151.24767, Residuals: -1.51976, Convergence: 0.003953\n",
      "Epoch: 71, Loss: 150.67957, Residuals: -1.50289, Convergence: 0.003770\n",
      "Epoch: 72, Loss: 150.13926, Residuals: -1.48665, Convergence: 0.003599\n",
      "Epoch: 73, Loss: 149.62498, Residuals: -1.47102, Convergence: 0.003437\n",
      "Epoch: 74, Loss: 149.13517, Residuals: -1.45596, Convergence: 0.003284\n",
      "Epoch: 75, Loss: 148.66850, Residuals: -1.44146, Convergence: 0.003139\n",
      "Epoch: 76, Loss: 148.22384, Residuals: -1.42749, Convergence: 0.003000\n",
      "Epoch: 77, Loss: 147.80021, Residuals: -1.41404, Convergence: 0.002866\n",
      "Epoch: 78, Loss: 147.39674, Residuals: -1.40109, Convergence: 0.002737\n",
      "Epoch: 79, Loss: 147.01264, Residuals: -1.38863, Convergence: 0.002613\n",
      "Epoch: 80, Loss: 146.64720, Residuals: -1.37666, Convergence: 0.002492\n",
      "Epoch: 81, Loss: 146.29973, Residuals: -1.36515, Convergence: 0.002375\n",
      "Epoch: 82, Loss: 145.96955, Residuals: -1.35410, Convergence: 0.002262\n",
      "Epoch: 83, Loss: 145.65607, Residuals: -1.34350, Convergence: 0.002152\n",
      "Epoch: 84, Loss: 145.35865, Residuals: -1.33334, Convergence: 0.002046\n",
      "Epoch: 85, Loss: 145.07670, Residuals: -1.32360, Convergence: 0.001943\n",
      "Epoch: 86, Loss: 144.80965, Residuals: -1.31428, Convergence: 0.001844\n",
      "Epoch: 87, Loss: 144.55696, Residuals: -1.30536, Convergence: 0.001748\n",
      "Epoch: 88, Loss: 144.31807, Residuals: -1.29685, Convergence: 0.001655\n",
      "Epoch: 89, Loss: 144.09244, Residuals: -1.28873, Convergence: 0.001566\n",
      "Epoch: 90, Loss: 143.87953, Residuals: -1.28099, Convergence: 0.001480\n",
      "Epoch: 91, Loss: 143.67877, Residuals: -1.27362, Convergence: 0.001397\n",
      "Epoch: 92, Loss: 143.48953, Residuals: -1.26663, Convergence: 0.001319\n",
      "Epoch: 93, Loss: 143.31114, Residuals: -1.26000, Convergence: 0.001245\n",
      "Epoch: 94, Loss: 143.14285, Residuals: -1.25372, Convergence: 0.001176\n",
      "Epoch: 95, Loss: 142.98377, Residuals: -1.24780, Convergence: 0.001113\n",
      "Epoch: 96, Loss: 142.83293, Residuals: -1.24220, Convergence: 0.001056\n",
      "Epoch: 97, Loss: 142.68923, Residuals: -1.23692, Convergence: 0.001007\n",
      "Epoch: 98, Loss: 142.55152, Residuals: -1.23194, Convergence: 0.000966\n",
      "Evidence -183.952\n",
      "\n",
      "Epoch: 98, Evidence: -183.95157, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 7.24e-01\n",
      "Epoch: 98, Loss: 1382.50214, Residuals: -1.23194, Convergence:   inf\n",
      "Epoch: 99, Loss: 1319.97782, Residuals: -1.26139, Convergence: 0.047368\n",
      "Epoch: 100, Loss: 1272.50807, Residuals: -1.28504, Convergence: 0.037304\n",
      "Epoch: 101, Loss: 1236.75827, Residuals: -1.30205, Convergence: 0.028906\n",
      "Epoch: 102, Loss: 1208.93486, Residuals: -1.31378, Convergence: 0.023015\n",
      "Epoch: 103, Loss: 1186.38981, Residuals: -1.32220, Convergence: 0.019003\n",
      "Epoch: 104, Loss: 1167.64666, Residuals: -1.32841, Convergence: 0.016052\n",
      "Epoch: 105, Loss: 1151.81608, Residuals: -1.33294, Convergence: 0.013744\n",
      "Epoch: 106, Loss: 1138.28683, Residuals: -1.33608, Convergence: 0.011886\n",
      "Epoch: 107, Loss: 1126.60210, Residuals: -1.33802, Convergence: 0.010372\n",
      "Epoch: 108, Loss: 1116.40412, Residuals: -1.33890, Convergence: 0.009135\n",
      "Epoch: 109, Loss: 1107.40298, Residuals: -1.33885, Convergence: 0.008128\n",
      "Epoch: 110, Loss: 1099.35873, Residuals: -1.33795, Convergence: 0.007317\n",
      "Epoch: 111, Loss: 1092.06697, Residuals: -1.33628, Convergence: 0.006677\n",
      "Epoch: 112, Loss: 1085.35094, Residuals: -1.33388, Convergence: 0.006188\n",
      "Epoch: 113, Loss: 1079.05345, Residuals: -1.33078, Convergence: 0.005836\n",
      "Epoch: 114, Loss: 1073.03241, Residuals: -1.32698, Convergence: 0.005611\n",
      "Epoch: 115, Loss: 1067.15992, Residuals: -1.32251, Convergence: 0.005503\n",
      "Epoch: 116, Loss: 1061.32595, Residuals: -1.31735, Convergence: 0.005497\n",
      "Epoch: 117, Loss: 1055.45205, Residuals: -1.31155, Convergence: 0.005565\n",
      "Epoch: 118, Loss: 1049.51087, Residuals: -1.30516, Convergence: 0.005661\n",
      "Epoch: 119, Loss: 1043.54479, Residuals: -1.29826, Convergence: 0.005717\n",
      "Epoch: 120, Loss: 1037.66017, Residuals: -1.29096, Convergence: 0.005671\n",
      "Epoch: 121, Loss: 1031.98435, Residuals: -1.28335, Convergence: 0.005500\n",
      "Epoch: 122, Loss: 1026.61654, Residuals: -1.27552, Convergence: 0.005229\n",
      "Epoch: 123, Loss: 1021.60457, Residuals: -1.26755, Convergence: 0.004906\n",
      "Epoch: 124, Loss: 1016.95124, Residuals: -1.25954, Convergence: 0.004576\n",
      "Epoch: 125, Loss: 1012.63355, Residuals: -1.25154, Convergence: 0.004264\n",
      "Epoch: 126, Loss: 1008.61830, Residuals: -1.24360, Convergence: 0.003981\n",
      "Epoch: 127, Loss: 1004.87167, Residuals: -1.23578, Convergence: 0.003728\n",
      "Epoch: 128, Loss: 1001.36310, Residuals: -1.22811, Convergence: 0.003504\n",
      "Epoch: 129, Loss: 998.06575, Residuals: -1.22062, Convergence: 0.003304\n",
      "Epoch: 130, Loss: 994.95841, Residuals: -1.21333, Convergence: 0.003123\n",
      "Epoch: 131, Loss: 992.02313, Residuals: -1.20626, Convergence: 0.002959\n",
      "Epoch: 132, Loss: 989.24566, Residuals: -1.19943, Convergence: 0.002808\n",
      "Epoch: 133, Loss: 986.61380, Residuals: -1.19284, Convergence: 0.002668\n",
      "Epoch: 134, Loss: 984.11731, Residuals: -1.18651, Convergence: 0.002537\n",
      "Epoch: 135, Loss: 981.74768, Residuals: -1.18044, Convergence: 0.002414\n",
      "Epoch: 136, Loss: 979.49708, Residuals: -1.17463, Convergence: 0.002298\n",
      "Epoch: 137, Loss: 977.35774, Residuals: -1.16908, Convergence: 0.002189\n",
      "Epoch: 138, Loss: 975.32333, Residuals: -1.16378, Convergence: 0.002086\n",
      "Epoch: 139, Loss: 973.38692, Residuals: -1.15873, Convergence: 0.001989\n",
      "Epoch: 140, Loss: 971.54272, Residuals: -1.15393, Convergence: 0.001898\n",
      "Epoch: 141, Loss: 969.78428, Residuals: -1.14935, Convergence: 0.001813\n",
      "Epoch: 142, Loss: 968.10618, Residuals: -1.14500, Convergence: 0.001733\n",
      "Epoch: 143, Loss: 966.50282, Residuals: -1.14086, Convergence: 0.001659\n",
      "Epoch: 144, Loss: 964.96949, Residuals: -1.13692, Convergence: 0.001589\n",
      "Epoch: 145, Loss: 963.50129, Residuals: -1.13318, Convergence: 0.001524\n",
      "Epoch: 146, Loss: 962.09370, Residuals: -1.12961, Convergence: 0.001463\n",
      "Epoch: 147, Loss: 960.74257, Residuals: -1.12622, Convergence: 0.001406\n",
      "Epoch: 148, Loss: 959.44418, Residuals: -1.12298, Convergence: 0.001353\n",
      "Epoch: 149, Loss: 958.19499, Residuals: -1.11990, Convergence: 0.001304\n",
      "Epoch: 150, Loss: 956.99202, Residuals: -1.11695, Convergence: 0.001257\n",
      "Epoch: 151, Loss: 955.83200, Residuals: -1.11413, Convergence: 0.001214\n",
      "Epoch: 152, Loss: 954.71224, Residuals: -1.11144, Convergence: 0.001173\n",
      "Epoch: 153, Loss: 953.63019, Residuals: -1.10886, Convergence: 0.001135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 154, Loss: 952.58334, Residuals: -1.10638, Convergence: 0.001099\n",
      "Epoch: 155, Loss: 951.56941, Residuals: -1.10400, Convergence: 0.001066\n",
      "Epoch: 156, Loss: 950.58629, Residuals: -1.10172, Convergence: 0.001034\n",
      "Epoch: 157, Loss: 949.63157, Residuals: -1.09951, Convergence: 0.001005\n",
      "Epoch: 158, Loss: 948.70302, Residuals: -1.09739, Convergence: 0.000979\n",
      "Evidence 11269.224\n",
      "\n",
      "Epoch: 158, Evidence: 11269.22363, Convergence: 1.016323\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 5.77e-01\n",
      "Epoch: 158, Loss: 2373.87621, Residuals: -1.09739, Convergence:   inf\n",
      "Epoch: 159, Loss: 2332.85542, Residuals: -1.10654, Convergence: 0.017584\n",
      "Epoch: 160, Loss: 2304.37196, Residuals: -1.10501, Convergence: 0.012361\n",
      "Epoch: 161, Loss: 2280.49168, Residuals: -1.10263, Convergence: 0.010472\n",
      "Epoch: 162, Loss: 2260.24811, Residuals: -1.09992, Convergence: 0.008956\n",
      "Epoch: 163, Loss: 2242.93294, Residuals: -1.09697, Convergence: 0.007720\n",
      "Epoch: 164, Loss: 2227.99217, Residuals: -1.09385, Convergence: 0.006706\n",
      "Epoch: 165, Loss: 2214.97951, Residuals: -1.09059, Convergence: 0.005875\n",
      "Epoch: 166, Loss: 2203.53253, Residuals: -1.08721, Convergence: 0.005195\n",
      "Epoch: 167, Loss: 2193.35205, Residuals: -1.08372, Convergence: 0.004642\n",
      "Epoch: 168, Loss: 2184.19211, Residuals: -1.08013, Convergence: 0.004194\n",
      "Epoch: 169, Loss: 2175.84883, Residuals: -1.07645, Convergence: 0.003834\n",
      "Epoch: 170, Loss: 2168.15668, Residuals: -1.07266, Convergence: 0.003548\n",
      "Epoch: 171, Loss: 2160.99221, Residuals: -1.06878, Convergence: 0.003315\n",
      "Epoch: 172, Loss: 2154.27609, Residuals: -1.06480, Convergence: 0.003118\n",
      "Epoch: 173, Loss: 2147.96610, Residuals: -1.06077, Convergence: 0.002938\n",
      "Epoch: 174, Loss: 2142.04640, Residuals: -1.05673, Convergence: 0.002764\n",
      "Epoch: 175, Loss: 2136.51108, Residuals: -1.05272, Convergence: 0.002591\n",
      "Epoch: 176, Loss: 2131.35052, Residuals: -1.04880, Convergence: 0.002421\n",
      "Epoch: 177, Loss: 2126.55323, Residuals: -1.04501, Convergence: 0.002256\n",
      "Epoch: 178, Loss: 2122.10267, Residuals: -1.04137, Convergence: 0.002097\n",
      "Epoch: 179, Loss: 2117.97842, Residuals: -1.03790, Convergence: 0.001947\n",
      "Epoch: 180, Loss: 2114.16055, Residuals: -1.03461, Convergence: 0.001806\n",
      "Epoch: 181, Loss: 2110.62744, Residuals: -1.03150, Convergence: 0.001674\n",
      "Epoch: 182, Loss: 2107.35879, Residuals: -1.02857, Convergence: 0.001551\n",
      "Epoch: 183, Loss: 2104.33326, Residuals: -1.02581, Convergence: 0.001438\n",
      "Epoch: 184, Loss: 2101.53108, Residuals: -1.02322, Convergence: 0.001333\n",
      "Epoch: 185, Loss: 2098.93370, Residuals: -1.02079, Convergence: 0.001237\n",
      "Epoch: 186, Loss: 2096.52294, Residuals: -1.01850, Convergence: 0.001150\n",
      "Epoch: 187, Loss: 2094.28220, Residuals: -1.01634, Convergence: 0.001070\n",
      "Epoch: 188, Loss: 2092.19534, Residuals: -1.01431, Convergence: 0.000997\n",
      "Evidence 14443.375\n",
      "\n",
      "Epoch: 188, Evidence: 14443.37500, Convergence: 0.219765\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 4.39e-01\n",
      "Epoch: 188, Loss: 2500.58059, Residuals: -1.01431, Convergence:   inf\n",
      "Epoch: 189, Loss: 2485.81637, Residuals: -1.01160, Convergence: 0.005939\n",
      "Epoch: 190, Loss: 2473.90841, Residuals: -1.00803, Convergence: 0.004813\n",
      "Epoch: 191, Loss: 2463.73246, Residuals: -1.00446, Convergence: 0.004130\n",
      "Epoch: 192, Loss: 2454.94894, Residuals: -1.00104, Convergence: 0.003578\n",
      "Epoch: 193, Loss: 2447.31325, Residuals: -0.99783, Convergence: 0.003120\n",
      "Epoch: 194, Loss: 2440.63550, Residuals: -0.99483, Convergence: 0.002736\n",
      "Epoch: 195, Loss: 2434.76105, Residuals: -0.99204, Convergence: 0.002413\n",
      "Epoch: 196, Loss: 2429.56396, Residuals: -0.98944, Convergence: 0.002139\n",
      "Epoch: 197, Loss: 2424.93798, Residuals: -0.98703, Convergence: 0.001908\n",
      "Epoch: 198, Loss: 2420.79423, Residuals: -0.98479, Convergence: 0.001712\n",
      "Epoch: 199, Loss: 2417.06193, Residuals: -0.98270, Convergence: 0.001544\n",
      "Epoch: 200, Loss: 2413.67807, Residuals: -0.98074, Convergence: 0.001402\n",
      "Epoch: 201, Loss: 2410.59345, Residuals: -0.97892, Convergence: 0.001280\n",
      "Epoch: 202, Loss: 2407.76639, Residuals: -0.97721, Convergence: 0.001174\n",
      "Epoch: 203, Loss: 2405.16180, Residuals: -0.97561, Convergence: 0.001083\n",
      "Epoch: 204, Loss: 2402.75087, Residuals: -0.97410, Convergence: 0.001003\n",
      "Epoch: 205, Loss: 2400.50964, Residuals: -0.97267, Convergence: 0.000934\n",
      "Evidence 14866.365\n",
      "\n",
      "Epoch: 205, Evidence: 14866.36523, Convergence: 0.028453\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 3.36e-01\n",
      "Epoch: 205, Loss: 2505.92249, Residuals: -0.97267, Convergence:   inf\n",
      "Epoch: 206, Loss: 2499.18225, Residuals: -0.96946, Convergence: 0.002697\n",
      "Epoch: 207, Loss: 2493.63237, Residuals: -0.96635, Convergence: 0.002226\n",
      "Epoch: 208, Loss: 2488.91266, Residuals: -0.96356, Convergence: 0.001896\n",
      "Epoch: 209, Loss: 2484.83765, Residuals: -0.96108, Convergence: 0.001640\n",
      "Epoch: 210, Loss: 2481.27274, Residuals: -0.95888, Convergence: 0.001437\n",
      "Epoch: 211, Loss: 2478.11564, Residuals: -0.95692, Convergence: 0.001274\n",
      "Epoch: 212, Loss: 2475.28861, Residuals: -0.95516, Convergence: 0.001142\n",
      "Epoch: 213, Loss: 2472.73089, Residuals: -0.95359, Convergence: 0.001034\n",
      "Epoch: 214, Loss: 2470.39684, Residuals: -0.95216, Convergence: 0.000945\n",
      "Evidence 14955.809\n",
      "\n",
      "Epoch: 214, Evidence: 14955.80859, Convergence: 0.005981\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 2.64e-01\n",
      "Epoch: 214, Loss: 2507.45181, Residuals: -0.95216, Convergence:   inf\n",
      "Epoch: 215, Loss: 2503.48012, Residuals: -0.94943, Convergence: 0.001586\n",
      "Epoch: 216, Loss: 2500.16656, Residuals: -0.94706, Convergence: 0.001325\n",
      "Epoch: 217, Loss: 2497.31096, Residuals: -0.94503, Convergence: 0.001143\n",
      "Epoch: 218, Loss: 2494.79998, Residuals: -0.94328, Convergence: 0.001006\n",
      "Epoch: 219, Loss: 2492.55895, Residuals: -0.94177, Convergence: 0.000899\n",
      "Evidence 14987.498\n",
      "\n",
      "Epoch: 219, Evidence: 14987.49805, Convergence: 0.002114\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 2.13e-01\n",
      "Epoch: 219, Loss: 2508.37603, Residuals: -0.94177, Convergence:   inf\n",
      "Epoch: 220, Loss: 2505.52533, Residuals: -0.93952, Convergence: 0.001138\n",
      "Epoch: 221, Loss: 2503.11672, Residuals: -0.93763, Convergence: 0.000962\n",
      "Evidence 15000.180\n",
      "\n",
      "Epoch: 221, Evidence: 15000.17969, Convergence: 0.000845\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.77e-01\n",
      "Epoch: 221, Loss: 2509.11629, Residuals: -0.93763, Convergence:   inf\n",
      "Epoch: 222, Loss: 2504.54141, Residuals: -0.93433, Convergence: 0.001827\n",
      "Epoch: 223, Loss: 2501.04279, Residuals: -0.93175, Convergence: 0.001399\n",
      "Epoch: 224, Loss: 2498.17151, Residuals: -0.92981, Convergence: 0.001149\n",
      "Epoch: 225, Loss: 2495.71503, Residuals: -0.92840, Convergence: 0.000984\n",
      "Evidence 15018.291\n",
      "\n",
      "Epoch: 225, Evidence: 15018.29102, Convergence: 0.002050\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.47e-01\n",
      "Epoch: 225, Loss: 2509.29943, Residuals: -0.92840, Convergence:   inf\n",
      "Epoch: 226, Loss: 2506.26498, Residuals: -0.92550, Convergence: 0.001211\n",
      "Epoch: 227, Loss: 2503.84567, Residuals: -0.92371, Convergence: 0.000966\n",
      "Evidence 15029.246\n",
      "\n",
      "Epoch: 227, Evidence: 15029.24609, Convergence: 0.000729\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.24e-01\n",
      "Epoch: 227, Loss: 2509.55277, Residuals: -0.92371, Convergence:   inf\n",
      "Epoch: 228, Loss: 2505.09691, Residuals: -0.91959, Convergence: 0.001779\n",
      "Epoch: 229, Loss: 2501.87140, Residuals: -0.91982, Convergence: 0.001289\n",
      "Epoch: 230, Loss: 2499.15447, Residuals: -0.91968, Convergence: 0.001087\n",
      "Epoch: 231, Loss: 2496.80313, Residuals: -0.92121, Convergence: 0.000942\n",
      "Evidence 15045.584\n",
      "\n",
      "Epoch: 231, Evidence: 15045.58398, Convergence: 0.001814\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.11e-01\n",
      "Epoch: 231, Loss: 2508.98144, Residuals: -0.92121, Convergence:   inf\n",
      "Epoch: 232, Loss: 2507.33799, Residuals: -0.91907, Convergence: 0.000655\n",
      "Evidence 15052.123\n",
      "\n",
      "Epoch: 232, Evidence: 15052.12305, Convergence: 0.000434\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 9.06e-02\n",
      "Epoch: 232, Loss: 2509.93026, Residuals: -0.91907, Convergence:   inf\n",
      "Epoch: 233, Loss: 2558.54877, Residuals: -0.95919, Convergence: -0.019002\n",
      "Epoch: 233, Loss: 2507.63542, Residuals: -0.91629, Convergence: 0.000915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence 15056.556\n",
      "\n",
      "Epoch: 233, Evidence: 15056.55566, Convergence: 0.000729\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 8.31e-02\n",
      "Epoch: 233, Loss: 2509.46432, Residuals: -0.91629, Convergence:   inf\n",
      "Epoch: 234, Loss: 2513.18447, Residuals: -0.91727, Convergence: -0.001480\n",
      "Epoch: 234, Loss: 2509.21517, Residuals: -0.91487, Convergence: 0.000099\n",
      "Evidence 15058.420\n",
      "\n",
      "Epoch: 234, Evidence: 15058.41992, Convergence: 0.000852\n",
      "Total samples: 184, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 384.10151, Residuals: -4.51168, Convergence:   inf\n",
      "Epoch: 1, Loss: 358.28079, Residuals: -4.39255, Convergence: 0.072068\n",
      "Epoch: 2, Loss: 337.09864, Residuals: -4.22930, Convergence: 0.062837\n",
      "Epoch: 3, Loss: 320.92638, Residuals: -4.06555, Convergence: 0.050392\n",
      "Epoch: 4, Loss: 308.58634, Residuals: -3.92124, Convergence: 0.039989\n",
      "Epoch: 5, Loss: 298.78935, Residuals: -3.79354, Convergence: 0.032789\n",
      "Epoch: 6, Loss: 290.82699, Residuals: -3.68227, Convergence: 0.027378\n",
      "Epoch: 7, Loss: 284.20755, Residuals: -3.58696, Convergence: 0.023291\n",
      "Epoch: 8, Loss: 278.56731, Residuals: -3.50553, Convergence: 0.020247\n",
      "Epoch: 9, Loss: 273.64970, Residuals: -3.43565, Convergence: 0.017970\n",
      "Epoch: 10, Loss: 269.27225, Residuals: -3.37520, Convergence: 0.016257\n",
      "Epoch: 11, Loss: 265.30274, Residuals: -3.32240, Convergence: 0.014962\n",
      "Epoch: 12, Loss: 261.64477, Residuals: -3.27569, Convergence: 0.013981\n",
      "Epoch: 13, Loss: 258.22851, Residuals: -3.23376, Convergence: 0.013230\n",
      "Epoch: 14, Loss: 255.00470, Residuals: -3.19544, Convergence: 0.012642\n",
      "Epoch: 15, Loss: 251.94183, Residuals: -3.15979, Convergence: 0.012157\n",
      "Epoch: 16, Loss: 249.02387, Residuals: -3.12619, Convergence: 0.011718\n",
      "Epoch: 17, Loss: 246.24006, Residuals: -3.09428, Convergence: 0.011305\n",
      "Epoch: 18, Loss: 243.57045, Residuals: -3.06369, Convergence: 0.010960\n",
      "Epoch: 19, Loss: 240.98313, Residuals: -3.03391, Convergence: 0.010736\n",
      "Epoch: 20, Loss: 238.44228, Residuals: -3.00438, Convergence: 0.010656\n",
      "Epoch: 21, Loss: 235.91755, Residuals: -2.97460, Convergence: 0.010702\n",
      "Epoch: 22, Loss: 233.38784, Residuals: -2.94424, Convergence: 0.010839\n",
      "Epoch: 23, Loss: 230.83162, Residuals: -2.91307, Convergence: 0.011074\n",
      "Epoch: 24, Loss: 228.20946, Residuals: -2.88066, Convergence: 0.011490\n",
      "Epoch: 25, Loss: 225.46318, Residuals: -2.84632, Convergence: 0.012181\n",
      "Epoch: 26, Loss: 222.56739, Residuals: -2.80970, Convergence: 0.013011\n",
      "Epoch: 27, Loss: 219.61929, Residuals: -2.77176, Convergence: 0.013424\n",
      "Epoch: 28, Loss: 216.75049, Residuals: -2.73399, Convergence: 0.013236\n",
      "Epoch: 29, Loss: 213.99743, Residuals: -2.69699, Convergence: 0.012865\n",
      "Epoch: 30, Loss: 211.34713, Residuals: -2.66073, Convergence: 0.012540\n",
      "Epoch: 31, Loss: 208.78061, Residuals: -2.62510, Convergence: 0.012293\n",
      "Epoch: 32, Loss: 206.28302, Residuals: -2.58995, Convergence: 0.012108\n",
      "Epoch: 33, Loss: 203.84434, Residuals: -2.55517, Convergence: 0.011963\n",
      "Epoch: 34, Loss: 201.45851, Residuals: -2.52069, Convergence: 0.011843\n",
      "Epoch: 35, Loss: 199.12241, Residuals: -2.48644, Convergence: 0.011732\n",
      "Epoch: 36, Loss: 196.83497, Residuals: -2.45236, Convergence: 0.011621\n",
      "Epoch: 37, Loss: 194.59648, Residuals: -2.41844, Convergence: 0.011503\n",
      "Epoch: 38, Loss: 192.40799, Residuals: -2.38466, Convergence: 0.011374\n",
      "Epoch: 39, Loss: 190.27097, Residuals: -2.35103, Convergence: 0.011231\n",
      "Epoch: 40, Loss: 188.18709, Residuals: -2.31756, Convergence: 0.011073\n",
      "Epoch: 41, Loss: 186.15807, Residuals: -2.28427, Convergence: 0.010899\n",
      "Epoch: 42, Loss: 184.18565, Residuals: -2.25120, Convergence: 0.010709\n",
      "Epoch: 43, Loss: 182.27159, Residuals: -2.21838, Convergence: 0.010501\n",
      "Epoch: 44, Loss: 180.41769, Residuals: -2.18587, Convergence: 0.010276\n",
      "Epoch: 45, Loss: 178.62567, Residuals: -2.15372, Convergence: 0.010032\n",
      "Epoch: 46, Loss: 176.89703, Residuals: -2.12199, Convergence: 0.009772\n",
      "Epoch: 47, Loss: 175.23275, Residuals: -2.09072, Convergence: 0.009498\n",
      "Epoch: 48, Loss: 173.63308, Residuals: -2.05998, Convergence: 0.009213\n",
      "Epoch: 49, Loss: 172.09744, Residuals: -2.02978, Convergence: 0.008923\n",
      "Epoch: 50, Loss: 170.62447, Residuals: -2.00016, Convergence: 0.008633\n",
      "Epoch: 51, Loss: 169.21223, Residuals: -1.97112, Convergence: 0.008346\n",
      "Epoch: 52, Loss: 167.85836, Residuals: -1.94267, Convergence: 0.008066\n",
      "Epoch: 53, Loss: 166.56035, Residuals: -1.91480, Convergence: 0.007793\n",
      "Epoch: 54, Loss: 165.31561, Residuals: -1.88750, Convergence: 0.007529\n",
      "Epoch: 55, Loss: 164.12166, Residuals: -1.86077, Convergence: 0.007275\n",
      "Epoch: 56, Loss: 162.97615, Residuals: -1.83460, Convergence: 0.007029\n",
      "Epoch: 57, Loss: 161.87695, Residuals: -1.80900, Convergence: 0.006790\n",
      "Epoch: 58, Loss: 160.82212, Residuals: -1.78397, Convergence: 0.006559\n",
      "Epoch: 59, Loss: 159.80996, Residuals: -1.75951, Convergence: 0.006334\n",
      "Epoch: 60, Loss: 158.83893, Residuals: -1.73562, Convergence: 0.006113\n",
      "Epoch: 61, Loss: 157.90767, Residuals: -1.71231, Convergence: 0.005898\n",
      "Epoch: 62, Loss: 157.01490, Residuals: -1.68959, Convergence: 0.005686\n",
      "Epoch: 63, Loss: 156.15948, Residuals: -1.66747, Convergence: 0.005478\n",
      "Epoch: 64, Loss: 155.34030, Residuals: -1.64594, Convergence: 0.005273\n",
      "Epoch: 65, Loss: 154.55632, Residuals: -1.62503, Convergence: 0.005072\n",
      "Epoch: 66, Loss: 153.80650, Residuals: -1.60472, Convergence: 0.004875\n",
      "Epoch: 67, Loss: 153.08984, Residuals: -1.58503, Convergence: 0.004681\n",
      "Epoch: 68, Loss: 152.40534, Residuals: -1.56596, Convergence: 0.004491\n",
      "Epoch: 69, Loss: 151.75201, Residuals: -1.54750, Convergence: 0.004305\n",
      "Epoch: 70, Loss: 151.12887, Residuals: -1.52966, Convergence: 0.004123\n",
      "Epoch: 71, Loss: 150.53490, Residuals: -1.51242, Convergence: 0.003946\n",
      "Epoch: 72, Loss: 149.96911, Residuals: -1.49579, Convergence: 0.003773\n",
      "Epoch: 73, Loss: 149.43049, Residuals: -1.47975, Convergence: 0.003604\n",
      "Epoch: 74, Loss: 148.91801, Residuals: -1.46431, Convergence: 0.003441\n",
      "Epoch: 75, Loss: 148.43066, Residuals: -1.44944, Convergence: 0.003283\n",
      "Epoch: 76, Loss: 147.96740, Residuals: -1.43514, Convergence: 0.003131\n",
      "Epoch: 77, Loss: 147.52721, Residuals: -1.42140, Convergence: 0.002984\n",
      "Epoch: 78, Loss: 147.10906, Residuals: -1.40820, Convergence: 0.002842\n",
      "Epoch: 79, Loss: 146.71197, Residuals: -1.39553, Convergence: 0.002707\n",
      "Epoch: 80, Loss: 146.33494, Residuals: -1.38337, Convergence: 0.002577\n",
      "Epoch: 81, Loss: 145.97700, Residuals: -1.37170, Convergence: 0.002452\n",
      "Epoch: 82, Loss: 145.63723, Residuals: -1.36052, Convergence: 0.002333\n",
      "Epoch: 83, Loss: 145.31471, Residuals: -1.34980, Convergence: 0.002219\n",
      "Epoch: 84, Loss: 145.00860, Residuals: -1.33953, Convergence: 0.002111\n",
      "Epoch: 85, Loss: 144.71805, Residuals: -1.32969, Convergence: 0.002008\n",
      "Epoch: 86, Loss: 144.44229, Residuals: -1.32026, Convergence: 0.001909\n",
      "Epoch: 87, Loss: 144.18057, Residuals: -1.31124, Convergence: 0.001815\n",
      "Epoch: 88, Loss: 143.93218, Residuals: -1.30259, Convergence: 0.001726\n",
      "Epoch: 89, Loss: 143.69648, Residuals: -1.29430, Convergence: 0.001640\n",
      "Epoch: 90, Loss: 143.47283, Residuals: -1.28637, Convergence: 0.001559\n",
      "Epoch: 91, Loss: 143.26068, Residuals: -1.27877, Convergence: 0.001481\n",
      "Epoch: 92, Loss: 143.05946, Residuals: -1.27149, Convergence: 0.001407\n",
      "Epoch: 93, Loss: 142.86870, Residuals: -1.26452, Convergence: 0.001335\n",
      "Epoch: 94, Loss: 142.68792, Residuals: -1.25784, Convergence: 0.001267\n",
      "Epoch: 95, Loss: 142.51671, Residuals: -1.25143, Convergence: 0.001201\n",
      "Epoch: 96, Loss: 142.35466, Residuals: -1.24530, Convergence: 0.001138\n",
      "Epoch: 97, Loss: 142.20145, Residuals: -1.23942, Convergence: 0.001077\n",
      "Epoch: 98, Loss: 142.05671, Residuals: -1.23380, Convergence: 0.001019\n",
      "Epoch: 99, Loss: 141.92017, Residuals: -1.22840, Convergence: 0.000962\n",
      "Evidence -183.781\n",
      "\n",
      "Epoch: 99, Evidence: -183.78082, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 7.25e-01\n",
      "Epoch: 99, Loss: 1380.89767, Residuals: -1.22840, Convergence:   inf\n",
      "Epoch: 100, Loss: 1318.68046, Residuals: -1.25958, Convergence: 0.047181\n",
      "Epoch: 101, Loss: 1270.98589, Residuals: -1.28383, Convergence: 0.037526\n",
      "Epoch: 102, Loss: 1234.68948, Residuals: -1.30135, Convergence: 0.029397\n",
      "Epoch: 103, Loss: 1206.44888, Residuals: -1.31382, Convergence: 0.023408\n",
      "Epoch: 104, Loss: 1183.70564, Residuals: -1.32310, Convergence: 0.019214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 105, Loss: 1164.89669, Residuals: -1.33025, Convergence: 0.016146\n",
      "Epoch: 106, Loss: 1149.06390, Residuals: -1.33576, Convergence: 0.013779\n",
      "Epoch: 107, Loss: 1135.55482, Residuals: -1.33986, Convergence: 0.011896\n",
      "Epoch: 108, Loss: 1123.88892, Residuals: -1.34268, Convergence: 0.010380\n",
      "Epoch: 109, Loss: 1113.69014, Residuals: -1.34436, Convergence: 0.009158\n",
      "Epoch: 110, Loss: 1104.65520, Residuals: -1.34498, Convergence: 0.008179\n",
      "Epoch: 111, Loss: 1096.53243, Residuals: -1.34461, Convergence: 0.007408\n",
      "Epoch: 112, Loss: 1089.10824, Residuals: -1.34332, Convergence: 0.006817\n",
      "Epoch: 113, Loss: 1082.19916, Residuals: -1.34114, Convergence: 0.006384\n",
      "Epoch: 114, Loss: 1075.64793, Residuals: -1.33811, Convergence: 0.006090\n",
      "Epoch: 115, Loss: 1069.31945, Residuals: -1.33424, Convergence: 0.005918\n",
      "Epoch: 116, Loss: 1063.10685, Residuals: -1.32957, Convergence: 0.005844\n",
      "Epoch: 117, Loss: 1056.93460, Residuals: -1.32413, Convergence: 0.005840\n",
      "Epoch: 118, Loss: 1050.77146, Residuals: -1.31799, Convergence: 0.005865\n",
      "Epoch: 119, Loss: 1044.63806, Residuals: -1.31125, Convergence: 0.005871\n",
      "Epoch: 120, Loss: 1038.60588, Residuals: -1.30403, Convergence: 0.005808\n",
      "Epoch: 121, Loss: 1032.77018, Residuals: -1.29642, Convergence: 0.005651\n",
      "Epoch: 122, Loss: 1027.21559, Residuals: -1.28855, Convergence: 0.005407\n",
      "Epoch: 123, Loss: 1021.99163, Residuals: -1.28050, Convergence: 0.005112\n",
      "Epoch: 124, Loss: 1017.11113, Residuals: -1.27235, Convergence: 0.004798\n",
      "Epoch: 125, Loss: 1012.56014, Residuals: -1.26417, Convergence: 0.004495\n",
      "Epoch: 126, Loss: 1008.31258, Residuals: -1.25602, Convergence: 0.004213\n",
      "Epoch: 127, Loss: 1004.33741, Residuals: -1.24794, Convergence: 0.003958\n",
      "Epoch: 128, Loss: 1000.60594, Residuals: -1.23998, Convergence: 0.003729\n",
      "Epoch: 129, Loss: 997.09202, Residuals: -1.23217, Convergence: 0.003524\n",
      "Epoch: 130, Loss: 993.77417, Residuals: -1.22452, Convergence: 0.003339\n",
      "Epoch: 131, Loss: 990.63389, Residuals: -1.21707, Convergence: 0.003170\n",
      "Epoch: 132, Loss: 987.65630, Residuals: -1.20983, Convergence: 0.003015\n",
      "Epoch: 133, Loss: 984.82901, Residuals: -1.20281, Convergence: 0.002871\n",
      "Epoch: 134, Loss: 982.14144, Residuals: -1.19602, Convergence: 0.002736\n",
      "Epoch: 135, Loss: 979.58439, Residuals: -1.18948, Convergence: 0.002610\n",
      "Epoch: 136, Loss: 977.14979, Residuals: -1.18318, Convergence: 0.002492\n",
      "Epoch: 137, Loss: 974.83011, Residuals: -1.17713, Convergence: 0.002380\n",
      "Epoch: 138, Loss: 972.61822, Residuals: -1.17133, Convergence: 0.002274\n",
      "Epoch: 139, Loss: 970.50735, Residuals: -1.16577, Convergence: 0.002175\n",
      "Epoch: 140, Loss: 968.49121, Residuals: -1.16045, Convergence: 0.002082\n",
      "Epoch: 141, Loss: 966.56358, Residuals: -1.15536, Convergence: 0.001994\n",
      "Epoch: 142, Loss: 964.71868, Residuals: -1.15050, Convergence: 0.001912\n",
      "Epoch: 143, Loss: 962.95136, Residuals: -1.14586, Convergence: 0.001835\n",
      "Epoch: 144, Loss: 961.25641, Residuals: -1.14142, Convergence: 0.001763\n",
      "Epoch: 145, Loss: 959.62930, Residuals: -1.13718, Convergence: 0.001696\n",
      "Epoch: 146, Loss: 958.06584, Residuals: -1.13314, Convergence: 0.001632\n",
      "Epoch: 147, Loss: 956.56232, Residuals: -1.12927, Convergence: 0.001572\n",
      "Epoch: 148, Loss: 955.11493, Residuals: -1.12557, Convergence: 0.001515\n",
      "Epoch: 149, Loss: 953.72055, Residuals: -1.12203, Convergence: 0.001462\n",
      "Epoch: 150, Loss: 952.37611, Residuals: -1.11864, Convergence: 0.001412\n",
      "Epoch: 151, Loss: 951.07800, Residuals: -1.11539, Convergence: 0.001365\n",
      "Epoch: 152, Loss: 949.82335, Residuals: -1.11228, Convergence: 0.001321\n",
      "Epoch: 153, Loss: 948.60895, Residuals: -1.10929, Convergence: 0.001280\n",
      "Epoch: 154, Loss: 947.43136, Residuals: -1.10642, Convergence: 0.001243\n",
      "Epoch: 155, Loss: 946.28735, Residuals: -1.10365, Convergence: 0.001209\n",
      "Epoch: 156, Loss: 945.17285, Residuals: -1.10098, Convergence: 0.001179\n",
      "Epoch: 157, Loss: 944.08379, Residuals: -1.09840, Convergence: 0.001154\n",
      "Epoch: 158, Loss: 943.01637, Residuals: -1.09589, Convergence: 0.001132\n",
      "Epoch: 159, Loss: 941.96600, Residuals: -1.09345, Convergence: 0.001115\n",
      "Epoch: 160, Loss: 940.92857, Residuals: -1.09106, Convergence: 0.001103\n",
      "Epoch: 161, Loss: 939.89973, Residuals: -1.08873, Convergence: 0.001095\n",
      "Epoch: 162, Loss: 938.87652, Residuals: -1.08642, Convergence: 0.001090\n",
      "Epoch: 163, Loss: 937.85645, Residuals: -1.08415, Convergence: 0.001088\n",
      "Epoch: 164, Loss: 936.83817, Residuals: -1.08190, Convergence: 0.001087\n",
      "Epoch: 165, Loss: 935.82295, Residuals: -1.07967, Convergence: 0.001085\n",
      "Epoch: 166, Loss: 934.81348, Residuals: -1.07746, Convergence: 0.001080\n",
      "Epoch: 167, Loss: 933.81364, Residuals: -1.07528, Convergence: 0.001071\n",
      "Epoch: 168, Loss: 932.82879, Residuals: -1.07314, Convergence: 0.001056\n",
      "Epoch: 169, Loss: 931.86489, Residuals: -1.07105, Convergence: 0.001034\n",
      "Epoch: 170, Loss: 930.92684, Residuals: -1.06901, Convergence: 0.001008\n",
      "Epoch: 171, Loss: 930.01865, Residuals: -1.06703, Convergence: 0.000977\n",
      "Evidence 11291.860\n",
      "\n",
      "Epoch: 171, Evidence: 11291.86035, Convergence: 1.016276\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 5.75e-01\n",
      "Epoch: 171, Loss: 2373.55214, Residuals: -1.06703, Convergence:   inf\n",
      "Epoch: 172, Loss: 2333.96779, Residuals: -1.07445, Convergence: 0.016960\n",
      "Epoch: 173, Loss: 2306.60974, Residuals: -1.07363, Convergence: 0.011861\n",
      "Epoch: 174, Loss: 2283.93167, Residuals: -1.07187, Convergence: 0.009929\n",
      "Epoch: 175, Loss: 2264.85510, Residuals: -1.06989, Convergence: 0.008423\n",
      "Epoch: 176, Loss: 2248.68044, Residuals: -1.06778, Convergence: 0.007193\n",
      "Epoch: 177, Loss: 2234.86933, Residuals: -1.06560, Convergence: 0.006180\n",
      "Epoch: 178, Loss: 2222.97858, Residuals: -1.06336, Convergence: 0.005349\n",
      "Epoch: 179, Loss: 2212.63785, Residuals: -1.06105, Convergence: 0.004673\n",
      "Epoch: 180, Loss: 2203.53631, Residuals: -1.05868, Convergence: 0.004130\n",
      "Epoch: 181, Loss: 2195.41434, Residuals: -1.05624, Convergence: 0.003700\n",
      "Epoch: 182, Loss: 2188.06468, Residuals: -1.05369, Convergence: 0.003359\n",
      "Epoch: 183, Loss: 2181.33750, Residuals: -1.05105, Convergence: 0.003084\n",
      "Epoch: 184, Loss: 2175.13578, Residuals: -1.04830, Convergence: 0.002851\n",
      "Epoch: 185, Loss: 2169.40307, Residuals: -1.04549, Convergence: 0.002643\n",
      "Epoch: 186, Loss: 2164.10721, Residuals: -1.04265, Convergence: 0.002447\n",
      "Epoch: 187, Loss: 2159.22181, Residuals: -1.03983, Convergence: 0.002263\n",
      "Epoch: 188, Loss: 2154.71680, Residuals: -1.03705, Convergence: 0.002091\n",
      "Epoch: 189, Loss: 2150.56076, Residuals: -1.03434, Convergence: 0.001933\n",
      "Epoch: 190, Loss: 2146.71836, Residuals: -1.03172, Convergence: 0.001790\n",
      "Epoch: 191, Loss: 2143.15504, Residuals: -1.02919, Convergence: 0.001663\n",
      "Epoch: 192, Loss: 2139.83709, Residuals: -1.02675, Convergence: 0.001551\n",
      "Epoch: 193, Loss: 2136.73349, Residuals: -1.02441, Convergence: 0.001452\n",
      "Epoch: 194, Loss: 2133.81799, Residuals: -1.02216, Convergence: 0.001366\n",
      "Epoch: 195, Loss: 2131.06691, Residuals: -1.02001, Convergence: 0.001291\n",
      "Epoch: 196, Loss: 2128.46267, Residuals: -1.01795, Convergence: 0.001224\n",
      "Epoch: 197, Loss: 2125.98897, Residuals: -1.01597, Convergence: 0.001164\n",
      "Epoch: 198, Loss: 2123.63448, Residuals: -1.01409, Convergence: 0.001109\n",
      "Epoch: 199, Loss: 2121.38927, Residuals: -1.01229, Convergence: 0.001058\n",
      "Epoch: 200, Loss: 2119.24813, Residuals: -1.01058, Convergence: 0.001010\n",
      "Epoch: 201, Loss: 2117.20470, Residuals: -1.00896, Convergence: 0.000965\n",
      "Evidence 14531.570\n",
      "\n",
      "Epoch: 201, Evidence: 14531.57031, Convergence: 0.222943\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 4.36e-01\n",
      "Epoch: 201, Loss: 2501.23514, Residuals: -1.00896, Convergence:   inf\n",
      "Epoch: 202, Loss: 2487.68662, Residuals: -1.00578, Convergence: 0.005446\n",
      "Epoch: 203, Loss: 2476.46192, Residuals: -1.00241, Convergence: 0.004533\n",
      "Epoch: 204, Loss: 2466.68852, Residuals: -0.99923, Convergence: 0.003962\n",
      "Epoch: 205, Loss: 2458.15440, Residuals: -0.99628, Convergence: 0.003472\n",
      "Epoch: 206, Loss: 2450.68290, Residuals: -0.99360, Convergence: 0.003049\n",
      "Epoch: 207, Loss: 2444.12243, Residuals: -0.99118, Convergence: 0.002684\n",
      "Epoch: 208, Loss: 2438.34175, Residuals: -0.98900, Convergence: 0.002371\n",
      "Epoch: 209, Loss: 2433.22746, Residuals: -0.98706, Convergence: 0.002102\n",
      "Epoch: 210, Loss: 2428.68278, Residuals: -0.98532, Convergence: 0.001871\n",
      "Epoch: 211, Loss: 2424.62460, Residuals: -0.98377, Convergence: 0.001674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 212, Loss: 2420.98370, Residuals: -0.98238, Convergence: 0.001504\n",
      "Epoch: 213, Loss: 2417.70073, Residuals: -0.98114, Convergence: 0.001358\n",
      "Epoch: 214, Loss: 2414.72613, Residuals: -0.98002, Convergence: 0.001232\n",
      "Epoch: 215, Loss: 2412.01757, Residuals: -0.97902, Convergence: 0.001123\n",
      "Epoch: 216, Loss: 2409.53960, Residuals: -0.97811, Convergence: 0.001028\n",
      "Epoch: 217, Loss: 2407.26406, Residuals: -0.97729, Convergence: 0.000945\n",
      "Evidence 14915.010\n",
      "\n",
      "Epoch: 217, Evidence: 14915.00977, Convergence: 0.025708\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 3.33e-01\n",
      "Epoch: 217, Loss: 2506.36053, Residuals: -0.97729, Convergence:   inf\n",
      "Epoch: 218, Loss: 2499.50614, Residuals: -0.97417, Convergence: 0.002742\n",
      "Epoch: 219, Loss: 2493.84080, Residuals: -0.97161, Convergence: 0.002272\n",
      "Epoch: 220, Loss: 2489.05728, Residuals: -0.96952, Convergence: 0.001922\n",
      "Epoch: 221, Loss: 2484.96398, Residuals: -0.96780, Convergence: 0.001647\n",
      "Epoch: 222, Loss: 2481.41647, Residuals: -0.96639, Convergence: 0.001430\n",
      "Epoch: 223, Loss: 2478.30530, Residuals: -0.96522, Convergence: 0.001255\n",
      "Epoch: 224, Loss: 2475.54382, Residuals: -0.96426, Convergence: 0.001116\n",
      "Epoch: 225, Loss: 2473.06855, Residuals: -0.96347, Convergence: 0.001001\n",
      "Epoch: 226, Loss: 2470.82745, Residuals: -0.96280, Convergence: 0.000907\n",
      "Evidence 15003.050\n",
      "\n",
      "Epoch: 226, Evidence: 15003.04980, Convergence: 0.005868\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 2.59e-01\n",
      "Epoch: 226, Loss: 2507.79586, Residuals: -0.96280, Convergence:   inf\n",
      "Epoch: 227, Loss: 2503.81939, Residuals: -0.96056, Convergence: 0.001588\n",
      "Epoch: 228, Loss: 2500.53500, Residuals: -0.95886, Convergence: 0.001313\n",
      "Epoch: 229, Loss: 2497.74184, Residuals: -0.95754, Convergence: 0.001118\n",
      "Epoch: 230, Loss: 2495.31659, Residuals: -0.95651, Convergence: 0.000972\n",
      "Evidence 15032.816\n",
      "\n",
      "Epoch: 230, Evidence: 15032.81641, Convergence: 0.001980\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 2.08e-01\n",
      "Epoch: 230, Loss: 2508.77943, Residuals: -0.95651, Convergence:   inf\n",
      "Epoch: 231, Loss: 2505.84126, Residuals: -0.95480, Convergence: 0.001173\n",
      "Epoch: 232, Loss: 2503.40411, Residuals: -0.95351, Convergence: 0.000974\n",
      "Evidence 15045.107\n",
      "\n",
      "Epoch: 232, Evidence: 15045.10742, Convergence: 0.000817\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.72e-01\n",
      "Epoch: 232, Loss: 2509.47700, Residuals: -0.95351, Convergence:   inf\n",
      "Epoch: 233, Loss: 2505.00503, Residuals: -0.95159, Convergence: 0.001785\n",
      "Epoch: 234, Loss: 2501.59289, Residuals: -0.95002, Convergence: 0.001364\n",
      "Epoch: 235, Loss: 2498.84141, Residuals: -0.94892, Convergence: 0.001101\n",
      "Epoch: 236, Loss: 2496.51090, Residuals: -0.94828, Convergence: 0.000934\n",
      "Evidence 15062.601\n",
      "\n",
      "Epoch: 236, Evidence: 15062.60059, Convergence: 0.001977\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.42e-01\n",
      "Epoch: 236, Loss: 2509.58425, Residuals: -0.94828, Convergence:   inf\n",
      "Epoch: 237, Loss: 2506.68780, Residuals: -0.94593, Convergence: 0.001155\n",
      "Epoch: 238, Loss: 2504.40089, Residuals: -0.94462, Convergence: 0.000913\n",
      "Evidence 15073.338\n",
      "\n",
      "Epoch: 238, Evidence: 15073.33789, Convergence: 0.000712\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.20e-01\n",
      "Epoch: 238, Loss: 2509.75648, Residuals: -0.94462, Convergence:   inf\n",
      "Epoch: 239, Loss: 2505.60378, Residuals: -0.94074, Convergence: 0.001657\n",
      "Epoch: 240, Loss: 2502.63276, Residuals: -0.94151, Convergence: 0.001187\n",
      "Epoch: 241, Loss: 2500.05552, Residuals: -0.94155, Convergence: 0.001031\n",
      "Epoch: 242, Loss: 2497.83274, Residuals: -0.94407, Convergence: 0.000890\n",
      "Evidence 15088.353\n",
      "\n",
      "Epoch: 242, Evidence: 15088.35254, Convergence: 0.001707\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.07e-01\n",
      "Epoch: 242, Loss: 2509.02600, Residuals: -0.94407, Convergence:   inf\n",
      "Epoch: 243, Loss: 2507.31404, Residuals: -0.94038, Convergence: 0.000683\n",
      "Evidence 15094.900\n",
      "\n",
      "Epoch: 243, Evidence: 15094.90039, Convergence: 0.000434\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 8.74e-02\n",
      "Epoch: 243, Loss: 2509.99145, Residuals: -0.94038, Convergence:   inf\n",
      "Epoch: 244, Loss: 2546.07573, Residuals: -0.97218, Convergence: -0.014173\n",
      "Epoch: 244, Loss: 2508.16323, Residuals: -0.93805, Convergence: 0.000729\n",
      "Evidence 15098.577\n",
      "\n",
      "Epoch: 244, Evidence: 15098.57715, Convergence: 0.000677\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 8.09e-02\n",
      "Epoch: 244, Loss: 2509.37323, Residuals: -0.93805, Convergence:   inf\n",
      "Epoch: 245, Loss: 2515.06113, Residuals: -0.93829, Convergence: -0.002262\n",
      "Epoch: 245, Loss: 2509.56720, Residuals: -0.93568, Convergence: -0.000077\n",
      "Evidence 15100.034\n",
      "\n",
      "Epoch: 245, Evidence: 15100.03418, Convergence: 0.000774\n",
      "Total samples: 181, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 383.35808, Residuals: -4.57468, Convergence:   inf\n",
      "Epoch: 1, Loss: 357.51605, Residuals: -4.45283, Convergence: 0.072282\n",
      "Epoch: 2, Loss: 336.34495, Residuals: -4.28657, Convergence: 0.062945\n",
      "Epoch: 3, Loss: 320.16435, Residuals: -4.12020, Convergence: 0.050538\n",
      "Epoch: 4, Loss: 307.81682, Residuals: -3.97374, Convergence: 0.040113\n",
      "Epoch: 5, Loss: 298.01750, Residuals: -3.84438, Convergence: 0.032882\n",
      "Epoch: 6, Loss: 290.05464, Residuals: -3.73182, Convergence: 0.027453\n",
      "Epoch: 7, Loss: 283.43635, Residuals: -3.63550, Convergence: 0.023350\n",
      "Epoch: 8, Loss: 277.80031, Residuals: -3.55334, Convergence: 0.020288\n",
      "Epoch: 9, Loss: 272.89089, Residuals: -3.48298, Convergence: 0.017990\n",
      "Epoch: 10, Loss: 268.52637, Residuals: -3.42230, Convergence: 0.016254\n",
      "Epoch: 11, Loss: 264.57506, Residuals: -3.36946, Convergence: 0.014935\n",
      "Epoch: 12, Loss: 260.94070, Residuals: -3.32293, Convergence: 0.013928\n",
      "Epoch: 13, Loss: 257.55289, Residuals: -3.28136, Convergence: 0.013154\n",
      "Epoch: 14, Loss: 254.36062, Residuals: -3.24357, Convergence: 0.012550\n",
      "Epoch: 15, Loss: 251.32870, Residuals: -3.20858, Convergence: 0.012064\n",
      "Epoch: 16, Loss: 248.43572, Residuals: -3.17570, Convergence: 0.011645\n",
      "Epoch: 17, Loss: 245.66686, Residuals: -3.14449, Convergence: 0.011271\n",
      "Epoch: 18, Loss: 243.00164, Residuals: -3.11452, Convergence: 0.010968\n",
      "Epoch: 19, Loss: 240.40937, Residuals: -3.08530, Convergence: 0.010783\n",
      "Epoch: 20, Loss: 237.85525, Residuals: -3.05627, Convergence: 0.010738\n",
      "Epoch: 21, Loss: 235.30950, Residuals: -3.02694, Convergence: 0.010819\n",
      "Epoch: 22, Loss: 232.75126, Residuals: -2.99701, Convergence: 0.010991\n",
      "Epoch: 23, Loss: 230.15835, Residuals: -2.96626, Convergence: 0.011266\n",
      "Epoch: 24, Loss: 227.49041, Residuals: -2.93422, Convergence: 0.011728\n",
      "Epoch: 25, Loss: 224.69137, Residuals: -2.90018, Convergence: 0.012457\n",
      "Epoch: 26, Loss: 221.74502, Residuals: -2.86383, Convergence: 0.013287\n",
      "Epoch: 27, Loss: 218.75069, Residuals: -2.82613, Convergence: 0.013688\n",
      "Epoch: 28, Loss: 215.82473, Residuals: -2.78839, Convergence: 0.013557\n",
      "Epoch: 29, Loss: 212.99670, Residuals: -2.75105, Convergence: 0.013277\n",
      "Epoch: 30, Loss: 210.25667, Residuals: -2.71408, Convergence: 0.013032\n",
      "Epoch: 31, Loss: 207.59151, Residuals: -2.67739, Convergence: 0.012838\n",
      "Epoch: 32, Loss: 204.99239, Residuals: -2.64093, Convergence: 0.012679\n",
      "Epoch: 33, Loss: 202.45457, Residuals: -2.60468, Convergence: 0.012535\n",
      "Epoch: 34, Loss: 199.97615, Residuals: -2.56866, Convergence: 0.012394\n",
      "Epoch: 35, Loss: 197.55692, Residuals: -2.53288, Convergence: 0.012246\n",
      "Epoch: 36, Loss: 195.19763, Residuals: -2.49738, Convergence: 0.012087\n",
      "Epoch: 37, Loss: 192.89951, Residuals: -2.46218, Convergence: 0.011914\n",
      "Epoch: 38, Loss: 190.66402, Residuals: -2.42731, Convergence: 0.011725\n",
      "Epoch: 39, Loss: 188.49270, Residuals: -2.39279, Convergence: 0.011519\n",
      "Epoch: 40, Loss: 186.38700, Residuals: -2.35865, Convergence: 0.011297\n",
      "Epoch: 41, Loss: 184.34826, Residuals: -2.32491, Convergence: 0.011059\n",
      "Epoch: 42, Loss: 182.37755, Residuals: -2.29159, Convergence: 0.010806\n",
      "Epoch: 43, Loss: 180.47560, Residuals: -2.25873, Convergence: 0.010539\n",
      "Epoch: 44, Loss: 178.64271, Residuals: -2.22634, Convergence: 0.010260\n",
      "Epoch: 45, Loss: 176.87864, Residuals: -2.19446, Convergence: 0.009973\n",
      "Epoch: 46, Loss: 175.18258, Residuals: -2.16310, Convergence: 0.009682\n",
      "Epoch: 47, Loss: 173.55311, Residuals: -2.13228, Convergence: 0.009389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48, Loss: 171.98822, Residuals: -2.10200, Convergence: 0.009099\n",
      "Epoch: 49, Loss: 170.48548, Residuals: -2.07226, Convergence: 0.008815\n",
      "Epoch: 50, Loss: 169.04217, Residuals: -2.04304, Convergence: 0.008538\n",
      "Epoch: 51, Loss: 167.65561, Residuals: -2.01434, Convergence: 0.008270\n",
      "Epoch: 52, Loss: 166.32342, Residuals: -1.98612, Convergence: 0.008010\n",
      "Epoch: 53, Loss: 165.04376, Residuals: -1.95838, Convergence: 0.007753\n",
      "Epoch: 54, Loss: 163.81543, Residuals: -1.93111, Convergence: 0.007498\n",
      "Epoch: 55, Loss: 162.63778, Residuals: -1.90432, Convergence: 0.007241\n",
      "Epoch: 56, Loss: 161.51049, Residuals: -1.87802, Convergence: 0.006980\n",
      "Epoch: 57, Loss: 160.43333, Residuals: -1.85224, Convergence: 0.006714\n",
      "Epoch: 58, Loss: 159.40591, Residuals: -1.82700, Convergence: 0.006445\n",
      "Epoch: 59, Loss: 158.42756, Residuals: -1.80233, Convergence: 0.006175\n",
      "Epoch: 60, Loss: 157.49728, Residuals: -1.77825, Convergence: 0.005907\n",
      "Epoch: 61, Loss: 156.61378, Residuals: -1.75477, Convergence: 0.005641\n",
      "Epoch: 62, Loss: 155.77554, Residuals: -1.73192, Convergence: 0.005381\n",
      "Epoch: 63, Loss: 154.98089, Residuals: -1.70971, Convergence: 0.005127\n",
      "Epoch: 64, Loss: 154.22806, Residuals: -1.68813, Convergence: 0.004881\n",
      "Epoch: 65, Loss: 153.51529, Residuals: -1.66720, Convergence: 0.004643\n",
      "Epoch: 66, Loss: 152.84077, Residuals: -1.64692, Convergence: 0.004413\n",
      "Epoch: 67, Loss: 152.20278, Residuals: -1.62729, Convergence: 0.004192\n",
      "Epoch: 68, Loss: 151.59960, Residuals: -1.60830, Convergence: 0.003979\n",
      "Epoch: 69, Loss: 151.02959, Residuals: -1.58996, Convergence: 0.003774\n",
      "Epoch: 70, Loss: 150.49112, Residuals: -1.57226, Convergence: 0.003578\n",
      "Epoch: 71, Loss: 149.98258, Residuals: -1.55520, Convergence: 0.003391\n",
      "Epoch: 72, Loss: 149.50236, Residuals: -1.53878, Convergence: 0.003212\n",
      "Epoch: 73, Loss: 149.04879, Residuals: -1.52298, Convergence: 0.003043\n",
      "Epoch: 74, Loss: 148.62017, Residuals: -1.50780, Convergence: 0.002884\n",
      "Epoch: 75, Loss: 148.21474, Residuals: -1.49322, Convergence: 0.002735\n",
      "Epoch: 76, Loss: 147.83068, Residuals: -1.47923, Convergence: 0.002598\n",
      "Epoch: 77, Loss: 147.46617, Residuals: -1.46579, Convergence: 0.002472\n",
      "Epoch: 78, Loss: 147.11943, Residuals: -1.45289, Convergence: 0.002357\n",
      "Epoch: 79, Loss: 146.78879, Residuals: -1.44049, Convergence: 0.002252\n",
      "Epoch: 80, Loss: 146.47277, Residuals: -1.42857, Convergence: 0.002158\n",
      "Epoch: 81, Loss: 146.17006, Residuals: -1.41709, Convergence: 0.002071\n",
      "Epoch: 82, Loss: 145.87963, Residuals: -1.40604, Convergence: 0.001991\n",
      "Epoch: 83, Loss: 145.60061, Residuals: -1.39538, Convergence: 0.001916\n",
      "Epoch: 84, Loss: 145.33236, Residuals: -1.38511, Convergence: 0.001846\n",
      "Epoch: 85, Loss: 145.07440, Residuals: -1.37520, Convergence: 0.001778\n",
      "Epoch: 86, Loss: 144.82636, Residuals: -1.36563, Convergence: 0.001713\n",
      "Epoch: 87, Loss: 144.58797, Residuals: -1.35641, Convergence: 0.001649\n",
      "Epoch: 88, Loss: 144.35902, Residuals: -1.34751, Convergence: 0.001586\n",
      "Epoch: 89, Loss: 144.13934, Residuals: -1.33893, Convergence: 0.001524\n",
      "Epoch: 90, Loss: 143.92878, Residuals: -1.33066, Convergence: 0.001463\n",
      "Epoch: 91, Loss: 143.72721, Residuals: -1.32270, Convergence: 0.001402\n",
      "Epoch: 92, Loss: 143.53450, Residuals: -1.31503, Convergence: 0.001343\n",
      "Epoch: 93, Loss: 143.35051, Residuals: -1.30766, Convergence: 0.001283\n",
      "Epoch: 94, Loss: 143.17512, Residuals: -1.30058, Convergence: 0.001225\n",
      "Epoch: 95, Loss: 143.00817, Residuals: -1.29377, Convergence: 0.001167\n",
      "Epoch: 96, Loss: 142.84951, Residuals: -1.28725, Convergence: 0.001111\n",
      "Epoch: 97, Loss: 142.69893, Residuals: -1.28100, Convergence: 0.001055\n",
      "Epoch: 98, Loss: 142.55622, Residuals: -1.27503, Convergence: 0.001001\n",
      "Epoch: 99, Loss: 142.42108, Residuals: -1.26932, Convergence: 0.000949\n",
      "Evidence -183.285\n",
      "\n",
      "Epoch: 99, Evidence: -183.28497, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 7.24e-01\n",
      "Epoch: 99, Loss: 1377.28890, Residuals: -1.26932, Convergence:   inf\n",
      "Epoch: 100, Loss: 1316.59542, Residuals: -1.29900, Convergence: 0.046099\n",
      "Epoch: 101, Loss: 1270.12486, Residuals: -1.32266, Convergence: 0.036587\n",
      "Epoch: 102, Loss: 1234.77770, Residuals: -1.34002, Convergence: 0.028626\n",
      "Epoch: 103, Loss: 1207.18914, Residuals: -1.35226, Convergence: 0.022854\n",
      "Epoch: 104, Loss: 1184.85328, Residuals: -1.36115, Convergence: 0.018851\n",
      "Epoch: 105, Loss: 1166.28342, Residuals: -1.36774, Convergence: 0.015922\n",
      "Epoch: 106, Loss: 1150.57618, Residuals: -1.37256, Convergence: 0.013652\n",
      "Epoch: 107, Loss: 1137.12203, Residuals: -1.37589, Convergence: 0.011832\n",
      "Epoch: 108, Loss: 1125.47268, Residuals: -1.37793, Convergence: 0.010351\n",
      "Epoch: 109, Loss: 1115.28248, Residuals: -1.37883, Convergence: 0.009137\n",
      "Epoch: 110, Loss: 1106.27453, Residuals: -1.37870, Convergence: 0.008143\n",
      "Epoch: 111, Loss: 1098.22322, Residuals: -1.37765, Convergence: 0.007331\n",
      "Epoch: 112, Loss: 1090.94014, Residuals: -1.37578, Convergence: 0.006676\n",
      "Epoch: 113, Loss: 1084.26681, Residuals: -1.37315, Convergence: 0.006155\n",
      "Epoch: 114, Loss: 1078.06839, Residuals: -1.36983, Convergence: 0.005750\n",
      "Epoch: 115, Loss: 1072.22954, Residuals: -1.36587, Convergence: 0.005446\n",
      "Epoch: 116, Loss: 1066.65422, Residuals: -1.36129, Convergence: 0.005227\n",
      "Epoch: 117, Loss: 1061.26255, Residuals: -1.35614, Convergence: 0.005080\n",
      "Epoch: 118, Loss: 1055.98862, Residuals: -1.35043, Convergence: 0.004994\n",
      "Epoch: 119, Loss: 1050.77780, Residuals: -1.34420, Convergence: 0.004959\n",
      "Epoch: 120, Loss: 1045.58574, Residuals: -1.33746, Convergence: 0.004966\n",
      "Epoch: 121, Loss: 1040.38040, Residuals: -1.33026, Convergence: 0.005003\n",
      "Epoch: 122, Loss: 1035.15393, Residuals: -1.32266, Convergence: 0.005049\n",
      "Epoch: 123, Loss: 1029.93477, Residuals: -1.31474, Convergence: 0.005067\n",
      "Epoch: 124, Loss: 1024.79026, Residuals: -1.30658, Convergence: 0.005020\n",
      "Epoch: 125, Loss: 1019.80622, Residuals: -1.29826, Convergence: 0.004887\n",
      "Epoch: 126, Loss: 1015.05713, Residuals: -1.28986, Convergence: 0.004679\n",
      "Epoch: 127, Loss: 1010.58600, Residuals: -1.28143, Convergence: 0.004424\n",
      "Epoch: 128, Loss: 1006.40201, Residuals: -1.27304, Convergence: 0.004157\n",
      "Epoch: 129, Loss: 1002.49350, Residuals: -1.26474, Convergence: 0.003899\n",
      "Epoch: 130, Loss: 998.83789, Residuals: -1.25657, Convergence: 0.003660\n",
      "Epoch: 131, Loss: 995.40966, Residuals: -1.24855, Convergence: 0.003444\n",
      "Epoch: 132, Loss: 992.18492, Residuals: -1.24072, Convergence: 0.003250\n",
      "Epoch: 133, Loss: 989.14264, Residuals: -1.23311, Convergence: 0.003076\n",
      "Epoch: 134, Loss: 986.26472, Residuals: -1.22571, Convergence: 0.002918\n",
      "Epoch: 135, Loss: 983.53683, Residuals: -1.21856, Convergence: 0.002774\n",
      "Epoch: 136, Loss: 980.94630, Residuals: -1.21166, Convergence: 0.002641\n",
      "Epoch: 137, Loss: 978.48242, Residuals: -1.20501, Convergence: 0.002518\n",
      "Epoch: 138, Loss: 976.13658, Residuals: -1.19861, Convergence: 0.002403\n",
      "Epoch: 139, Loss: 973.90089, Residuals: -1.19248, Convergence: 0.002296\n",
      "Epoch: 140, Loss: 971.76814, Residuals: -1.18660, Convergence: 0.002195\n",
      "Epoch: 141, Loss: 969.73210, Residuals: -1.18098, Convergence: 0.002100\n",
      "Epoch: 142, Loss: 967.78719, Residuals: -1.17560, Convergence: 0.002010\n",
      "Epoch: 143, Loss: 965.92770, Residuals: -1.17047, Convergence: 0.001925\n",
      "Epoch: 144, Loss: 964.14886, Residuals: -1.16557, Convergence: 0.001845\n",
      "Epoch: 145, Loss: 962.44617, Residuals: -1.16089, Convergence: 0.001769\n",
      "Epoch: 146, Loss: 960.81515, Residuals: -1.15644, Convergence: 0.001698\n",
      "Epoch: 147, Loss: 959.25195, Residuals: -1.15219, Convergence: 0.001630\n",
      "Epoch: 148, Loss: 957.75292, Residuals: -1.14814, Convergence: 0.001565\n",
      "Epoch: 149, Loss: 956.31453, Residuals: -1.14428, Convergence: 0.001504\n",
      "Epoch: 150, Loss: 954.93397, Residuals: -1.14060, Convergence: 0.001446\n",
      "Epoch: 151, Loss: 953.60786, Residuals: -1.13710, Convergence: 0.001391\n",
      "Epoch: 152, Loss: 952.33337, Residuals: -1.13375, Convergence: 0.001338\n",
      "Epoch: 153, Loss: 951.10825, Residuals: -1.13056, Convergence: 0.001288\n",
      "Epoch: 154, Loss: 949.92944, Residuals: -1.12751, Convergence: 0.001241\n",
      "Epoch: 155, Loss: 948.79496, Residuals: -1.12459, Convergence: 0.001196\n",
      "Epoch: 156, Loss: 947.70171, Residuals: -1.12181, Convergence: 0.001154\n",
      "Epoch: 157, Loss: 946.64769, Residuals: -1.11914, Convergence: 0.001113\n",
      "Epoch: 158, Loss: 945.63013, Residuals: -1.11659, Convergence: 0.001076\n",
      "Epoch: 159, Loss: 944.64655, Residuals: -1.11414, Convergence: 0.001041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 160, Loss: 943.69458, Residuals: -1.11179, Convergence: 0.001009\n",
      "Epoch: 161, Loss: 942.77146, Residuals: -1.10953, Convergence: 0.000979\n",
      "Evidence 11080.351\n",
      "\n",
      "Epoch: 161, Evidence: 11080.35059, Convergence: 1.016541\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 5.76e-01\n",
      "Epoch: 161, Loss: 2336.21168, Residuals: -1.10953, Convergence:   inf\n",
      "Epoch: 162, Loss: 2297.76691, Residuals: -1.11951, Convergence: 0.016731\n",
      "Epoch: 163, Loss: 2270.08565, Residuals: -1.11835, Convergence: 0.012194\n",
      "Epoch: 164, Loss: 2246.88060, Residuals: -1.11622, Convergence: 0.010328\n",
      "Epoch: 165, Loss: 2227.23763, Residuals: -1.11372, Convergence: 0.008819\n",
      "Epoch: 166, Loss: 2210.44944, Residuals: -1.11097, Convergence: 0.007595\n",
      "Epoch: 167, Loss: 2195.97470, Residuals: -1.10802, Convergence: 0.006591\n",
      "Epoch: 168, Loss: 2183.38337, Residuals: -1.10491, Convergence: 0.005767\n",
      "Epoch: 169, Loss: 2172.32865, Residuals: -1.10168, Convergence: 0.005089\n",
      "Epoch: 170, Loss: 2162.52710, Residuals: -1.09834, Convergence: 0.004532\n",
      "Epoch: 171, Loss: 2153.74589, Residuals: -1.09491, Convergence: 0.004077\n",
      "Epoch: 172, Loss: 2145.78987, Residuals: -1.09141, Convergence: 0.003708\n",
      "Epoch: 173, Loss: 2138.50128, Residuals: -1.08782, Convergence: 0.003408\n",
      "Epoch: 174, Loss: 2131.75831, Residuals: -1.08415, Convergence: 0.003163\n",
      "Epoch: 175, Loss: 2125.47942, Residuals: -1.08041, Convergence: 0.002954\n",
      "Epoch: 176, Loss: 2119.61910, Residuals: -1.07661, Convergence: 0.002765\n",
      "Epoch: 177, Loss: 2114.15561, Residuals: -1.07281, Convergence: 0.002584\n",
      "Epoch: 178, Loss: 2109.07698, Residuals: -1.06903, Convergence: 0.002408\n",
      "Epoch: 179, Loss: 2104.36737, Residuals: -1.06532, Convergence: 0.002238\n",
      "Epoch: 180, Loss: 2100.00487, Residuals: -1.06171, Convergence: 0.002077\n",
      "Epoch: 181, Loss: 2095.96476, Residuals: -1.05822, Convergence: 0.001928\n",
      "Epoch: 182, Loss: 2092.21737, Residuals: -1.05487, Convergence: 0.001791\n",
      "Epoch: 183, Loss: 2088.73491, Residuals: -1.05165, Convergence: 0.001667\n",
      "Epoch: 184, Loss: 2085.48934, Residuals: -1.04858, Convergence: 0.001556\n",
      "Epoch: 185, Loss: 2082.45463, Residuals: -1.04565, Convergence: 0.001457\n",
      "Epoch: 186, Loss: 2079.60859, Residuals: -1.04285, Convergence: 0.001369\n",
      "Epoch: 187, Loss: 2076.92999, Residuals: -1.04020, Convergence: 0.001290\n",
      "Epoch: 188, Loss: 2074.40252, Residuals: -1.03768, Convergence: 0.001218\n",
      "Epoch: 189, Loss: 2072.01043, Residuals: -1.03528, Convergence: 0.001154\n",
      "Epoch: 190, Loss: 2069.74127, Residuals: -1.03300, Convergence: 0.001096\n",
      "Epoch: 191, Loss: 2067.58400, Residuals: -1.03084, Convergence: 0.001043\n",
      "Epoch: 192, Loss: 2065.52982, Residuals: -1.02879, Convergence: 0.000995\n",
      "Evidence 14195.598\n",
      "\n",
      "Epoch: 192, Evidence: 14195.59766, Convergence: 0.219452\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 4.38e-01\n",
      "Epoch: 192, Loss: 2459.61943, Residuals: -1.02879, Convergence:   inf\n",
      "Epoch: 193, Loss: 2445.11911, Residuals: -1.02519, Convergence: 0.005930\n",
      "Epoch: 194, Loss: 2433.29786, Residuals: -1.02100, Convergence: 0.004858\n",
      "Epoch: 195, Loss: 2423.11235, Residuals: -1.01699, Convergence: 0.004203\n",
      "Epoch: 196, Loss: 2414.27341, Residuals: -1.01321, Convergence: 0.003661\n",
      "Epoch: 197, Loss: 2406.55613, Residuals: -1.00971, Convergence: 0.003207\n",
      "Epoch: 198, Loss: 2399.78058, Residuals: -1.00647, Convergence: 0.002823\n",
      "Epoch: 199, Loss: 2393.80049, Residuals: -1.00347, Convergence: 0.002498\n",
      "Epoch: 200, Loss: 2388.49429, Residuals: -1.00071, Convergence: 0.002222\n",
      "Epoch: 201, Loss: 2383.75913, Residuals: -0.99815, Convergence: 0.001986\n",
      "Epoch: 202, Loss: 2379.50896, Residuals: -0.99579, Convergence: 0.001786\n",
      "Epoch: 203, Loss: 2375.67379, Residuals: -0.99361, Convergence: 0.001614\n",
      "Epoch: 204, Loss: 2372.19224, Residuals: -0.99159, Convergence: 0.001468\n",
      "Epoch: 205, Loss: 2369.01637, Residuals: -0.98971, Convergence: 0.001341\n",
      "Epoch: 206, Loss: 2366.10390, Residuals: -0.98798, Convergence: 0.001231\n",
      "Epoch: 207, Loss: 2363.42072, Residuals: -0.98637, Convergence: 0.001135\n",
      "Epoch: 208, Loss: 2360.93755, Residuals: -0.98488, Convergence: 0.001052\n",
      "Epoch: 209, Loss: 2358.63182, Residuals: -0.98349, Convergence: 0.000978\n",
      "Evidence 14604.114\n",
      "\n",
      "Epoch: 209, Evidence: 14604.11426, Convergence: 0.027973\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 3.35e-01\n",
      "Epoch: 209, Loss: 2464.41937, Residuals: -0.98349, Convergence:   inf\n",
      "Epoch: 210, Loss: 2457.56933, Residuals: -0.97972, Convergence: 0.002787\n",
      "Epoch: 211, Loss: 2451.88413, Residuals: -0.97642, Convergence: 0.002319\n",
      "Epoch: 212, Loss: 2447.04078, Residuals: -0.97361, Convergence: 0.001979\n",
      "Epoch: 213, Loss: 2442.85852, Residuals: -0.97118, Convergence: 0.001712\n",
      "Epoch: 214, Loss: 2439.20099, Residuals: -0.96909, Convergence: 0.001499\n",
      "Epoch: 215, Loss: 2435.96567, Residuals: -0.96727, Convergence: 0.001328\n",
      "Epoch: 216, Loss: 2433.07215, Residuals: -0.96569, Convergence: 0.001189\n",
      "Epoch: 217, Loss: 2430.45770, Residuals: -0.96431, Convergence: 0.001076\n",
      "Epoch: 218, Loss: 2428.07514, Residuals: -0.96312, Convergence: 0.000981\n",
      "Evidence 14694.608\n",
      "\n",
      "Epoch: 218, Evidence: 14694.60840, Convergence: 0.006158\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 2.62e-01\n",
      "Epoch: 218, Loss: 2466.08993, Residuals: -0.96312, Convergence:   inf\n",
      "Epoch: 219, Loss: 2462.07865, Residuals: -0.96031, Convergence: 0.001629\n",
      "Epoch: 220, Loss: 2458.71820, Residuals: -0.95808, Convergence: 0.001367\n",
      "Epoch: 221, Loss: 2455.82403, Residuals: -0.95627, Convergence: 0.001178\n",
      "Epoch: 222, Loss: 2453.28353, Residuals: -0.95477, Convergence: 0.001036\n",
      "Epoch: 223, Loss: 2451.01888, Residuals: -0.95354, Convergence: 0.000924\n",
      "Evidence 14726.534\n",
      "\n",
      "Epoch: 223, Evidence: 14726.53418, Convergence: 0.002168\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 2.11e-01\n",
      "Epoch: 223, Loss: 2467.14678, Residuals: -0.95354, Convergence:   inf\n",
      "Epoch: 224, Loss: 2464.28575, Residuals: -0.95141, Convergence: 0.001161\n",
      "Epoch: 225, Loss: 2461.86709, Residuals: -0.94977, Convergence: 0.000982\n",
      "Evidence 14739.250\n",
      "\n",
      "Epoch: 225, Evidence: 14739.25000, Convergence: 0.000863\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.75e-01\n",
      "Epoch: 225, Loss: 2467.94795, Residuals: -0.94977, Convergence:   inf\n",
      "Epoch: 226, Loss: 2463.40093, Residuals: -0.94683, Convergence: 0.001846\n",
      "Epoch: 227, Loss: 2459.90394, Residuals: -0.94480, Convergence: 0.001422\n",
      "Epoch: 228, Loss: 2457.03224, Residuals: -0.94339, Convergence: 0.001169\n",
      "Epoch: 229, Loss: 2454.56974, Residuals: -0.94253, Convergence: 0.001003\n",
      "Epoch: 230, Loss: 2452.39138, Residuals: -0.94200, Convergence: 0.000888\n",
      "Evidence 14759.435\n",
      "\n",
      "Epoch: 230, Evidence: 14759.43457, Convergence: 0.002229\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.44e-01\n",
      "Epoch: 230, Loss: 2468.14134, Residuals: -0.94200, Convergence:   inf\n",
      "Epoch: 231, Loss: 2465.29169, Residuals: -0.93937, Convergence: 0.001156\n",
      "Epoch: 232, Loss: 2462.99810, Residuals: -0.93799, Convergence: 0.000931\n",
      "Evidence 14770.553\n",
      "\n",
      "Epoch: 232, Evidence: 14770.55273, Convergence: 0.000753\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.22e-01\n",
      "Epoch: 232, Loss: 2468.45886, Residuals: -0.93799, Convergence:   inf\n",
      "Epoch: 233, Loss: 2464.24116, Residuals: -0.93414, Convergence: 0.001712\n",
      "Epoch: 234, Loss: 2461.13157, Residuals: -0.93500, Convergence: 0.001263\n",
      "Epoch: 235, Loss: 2458.45441, Residuals: -0.93543, Convergence: 0.001089\n",
      "Epoch: 236, Loss: 2456.17076, Residuals: -0.93797, Convergence: 0.000930\n",
      "Evidence 14785.878\n",
      "\n",
      "Epoch: 236, Evidence: 14785.87793, Convergence: 0.001788\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.09e-01\n",
      "Epoch: 236, Loss: 2467.97918, Residuals: -0.93797, Convergence:   inf\n",
      "Epoch: 237, Loss: 2466.50627, Residuals: -0.93566, Convergence: 0.000597\n",
      "Evidence 14791.891\n",
      "\n",
      "Epoch: 237, Evidence: 14791.89062, Convergence: 0.000406\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 8.85e-02\n",
      "Epoch: 237, Loss: 2468.99856, Residuals: -0.93566, Convergence:   inf\n",
      "Epoch: 238, Loss: 2521.24487, Residuals: -0.97483, Convergence: -0.020722\n",
      "Epoch: 238, Loss: 2466.77035, Residuals: -0.93319, Convergence: 0.000903\n",
      "Evidence 14796.163\n",
      "\n",
      "Epoch: 238, Evidence: 14796.16309, Convergence: 0.000695\n",
      "Updating hyper-parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 181, Updated regularization: 8.11e-02\n",
      "Epoch: 238, Loss: 2468.51035, Residuals: -0.93319, Convergence:   inf\n",
      "Epoch: 239, Loss: 2473.58892, Residuals: -0.93315, Convergence: -0.002053\n",
      "Epoch: 239, Loss: 2468.74907, Residuals: -0.93103, Convergence: -0.000097\n",
      "Evidence 14797.423\n",
      "\n",
      "Epoch: 239, Evidence: 14797.42285, Convergence: 0.000780\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 381.41401, Residuals: -4.52249, Convergence:   inf\n",
      "Epoch: 1, Loss: 355.76182, Residuals: -4.40328, Convergence: 0.072105\n",
      "Epoch: 2, Loss: 334.66463, Residuals: -4.23829, Convergence: 0.063040\n",
      "Epoch: 3, Loss: 318.57482, Residuals: -4.07283, Convergence: 0.050506\n",
      "Epoch: 4, Loss: 306.30595, Residuals: -3.92692, Convergence: 0.040054\n",
      "Epoch: 5, Loss: 296.57452, Residuals: -3.79766, Convergence: 0.032813\n",
      "Epoch: 6, Loss: 288.67449, Residuals: -3.68492, Convergence: 0.027367\n",
      "Epoch: 7, Loss: 282.11465, Residuals: -3.58831, Convergence: 0.023252\n",
      "Epoch: 8, Loss: 276.53115, Residuals: -3.50579, Convergence: 0.020191\n",
      "Epoch: 9, Loss: 271.66787, Residuals: -3.43503, Convergence: 0.017902\n",
      "Epoch: 10, Loss: 267.34345, Residuals: -3.37396, Convergence: 0.016176\n",
      "Epoch: 11, Loss: 263.42743, Residuals: -3.32078, Convergence: 0.014866\n",
      "Epoch: 12, Loss: 259.82568, Residuals: -3.27397, Convergence: 0.013862\n",
      "Epoch: 13, Loss: 256.47069, Residuals: -3.23222, Convergence: 0.013081\n",
      "Epoch: 14, Loss: 253.31435, Residuals: -3.19434, Convergence: 0.012460\n",
      "Epoch: 15, Loss: 250.32344, Residuals: -3.15935, Convergence: 0.011948\n",
      "Epoch: 16, Loss: 247.47748, Residuals: -3.12650, Convergence: 0.011500\n",
      "Epoch: 17, Loss: 244.76354, Residuals: -3.09536, Convergence: 0.011088\n",
      "Epoch: 18, Loss: 242.16470, Residuals: -3.06558, Convergence: 0.010732\n",
      "Epoch: 19, Loss: 239.65354, Residuals: -3.03673, Convergence: 0.010478\n",
      "Epoch: 20, Loss: 237.19672, Residuals: -3.00831, Convergence: 0.010358\n",
      "Epoch: 21, Loss: 234.76358, Residuals: -2.97983, Convergence: 0.010364\n",
      "Epoch: 22, Loss: 232.33283, Residuals: -2.95095, Convergence: 0.010462\n",
      "Epoch: 23, Loss: 229.88957, Residuals: -2.92150, Convergence: 0.010628\n",
      "Epoch: 24, Loss: 227.41058, Residuals: -2.89122, Convergence: 0.010901\n",
      "Epoch: 25, Loss: 224.85286, Residuals: -2.85963, Convergence: 0.011375\n",
      "Epoch: 26, Loss: 222.16385, Residuals: -2.82608, Convergence: 0.012104\n",
      "Epoch: 27, Loss: 219.34050, Residuals: -2.79043, Convergence: 0.012872\n",
      "Epoch: 28, Loss: 216.48803, Residuals: -2.75376, Convergence: 0.013176\n",
      "Epoch: 29, Loss: 213.70984, Residuals: -2.71730, Convergence: 0.013000\n",
      "Epoch: 30, Loss: 211.02719, Residuals: -2.68140, Convergence: 0.012712\n",
      "Epoch: 31, Loss: 208.42752, Residuals: -2.64602, Convergence: 0.012473\n",
      "Epoch: 32, Loss: 205.89597, Residuals: -2.61102, Convergence: 0.012295\n",
      "Epoch: 33, Loss: 203.42203, Residuals: -2.57629, Convergence: 0.012162\n",
      "Epoch: 34, Loss: 200.99964, Residuals: -2.54176, Convergence: 0.012052\n",
      "Epoch: 35, Loss: 198.62601, Residuals: -2.50738, Convergence: 0.011950\n",
      "Epoch: 36, Loss: 196.30050, Residuals: -2.47312, Convergence: 0.011847\n",
      "Epoch: 37, Loss: 194.02367, Residuals: -2.43895, Convergence: 0.011735\n",
      "Epoch: 38, Loss: 191.79658, Residuals: -2.40488, Convergence: 0.011612\n",
      "Epoch: 39, Loss: 189.62049, Residuals: -2.37090, Convergence: 0.011476\n",
      "Epoch: 40, Loss: 187.49655, Residuals: -2.33703, Convergence: 0.011328\n",
      "Epoch: 41, Loss: 185.42585, Residuals: -2.30327, Convergence: 0.011167\n",
      "Epoch: 42, Loss: 183.40936, Residuals: -2.26964, Convergence: 0.010994\n",
      "Epoch: 43, Loss: 181.44812, Residuals: -2.23617, Convergence: 0.010809\n",
      "Epoch: 44, Loss: 179.54333, Residuals: -2.20288, Convergence: 0.010609\n",
      "Epoch: 45, Loss: 177.69646, Residuals: -2.16981, Convergence: 0.010393\n",
      "Epoch: 46, Loss: 175.90915, Residuals: -2.13701, Convergence: 0.010160\n",
      "Epoch: 47, Loss: 174.18322, Residuals: -2.10454, Convergence: 0.009909\n",
      "Epoch: 48, Loss: 172.52039, Residuals: -2.07244, Convergence: 0.009638\n",
      "Epoch: 49, Loss: 170.92209, Residuals: -2.04078, Convergence: 0.009351\n",
      "Epoch: 50, Loss: 169.38930, Residuals: -2.00961, Convergence: 0.009049\n",
      "Epoch: 51, Loss: 167.92240, Residuals: -1.97899, Convergence: 0.008736\n",
      "Epoch: 52, Loss: 166.52113, Residuals: -1.94898, Convergence: 0.008415\n",
      "Epoch: 53, Loss: 165.18460, Residuals: -1.91961, Convergence: 0.008091\n",
      "Epoch: 54, Loss: 163.91127, Residuals: -1.89092, Convergence: 0.007768\n",
      "Epoch: 55, Loss: 162.69905, Residuals: -1.86294, Convergence: 0.007451\n",
      "Epoch: 56, Loss: 161.54534, Residuals: -1.83569, Convergence: 0.007142\n",
      "Epoch: 57, Loss: 160.44708, Residuals: -1.80918, Convergence: 0.006845\n",
      "Epoch: 58, Loss: 159.40096, Residuals: -1.78340, Convergence: 0.006563\n",
      "Epoch: 59, Loss: 158.40345, Residuals: -1.75833, Convergence: 0.006297\n",
      "Epoch: 60, Loss: 157.45109, Residuals: -1.73397, Convergence: 0.006049\n",
      "Epoch: 61, Loss: 156.54063, Residuals: -1.71028, Convergence: 0.005816\n",
      "Epoch: 62, Loss: 155.66917, Residuals: -1.68724, Convergence: 0.005598\n",
      "Epoch: 63, Loss: 154.83426, Residuals: -1.66483, Convergence: 0.005392\n",
      "Epoch: 64, Loss: 154.03394, Residuals: -1.64303, Convergence: 0.005196\n",
      "Epoch: 65, Loss: 153.26664, Residuals: -1.62183, Convergence: 0.005006\n",
      "Epoch: 66, Loss: 152.53116, Residuals: -1.60123, Convergence: 0.004822\n",
      "Epoch: 67, Loss: 151.82657, Residuals: -1.58122, Convergence: 0.004641\n",
      "Epoch: 68, Loss: 151.15210, Residuals: -1.56180, Convergence: 0.004462\n",
      "Epoch: 69, Loss: 150.50705, Residuals: -1.54299, Convergence: 0.004286\n",
      "Epoch: 70, Loss: 149.89077, Residuals: -1.52477, Convergence: 0.004112\n",
      "Epoch: 71, Loss: 149.30258, Residuals: -1.50715, Convergence: 0.003940\n",
      "Epoch: 72, Loss: 148.74180, Residuals: -1.49014, Convergence: 0.003770\n",
      "Epoch: 73, Loss: 148.20765, Residuals: -1.47372, Convergence: 0.003604\n",
      "Epoch: 74, Loss: 147.69934, Residuals: -1.45789, Convergence: 0.003442\n",
      "Epoch: 75, Loss: 147.21602, Residuals: -1.44265, Convergence: 0.003283\n",
      "Epoch: 76, Loss: 146.75679, Residuals: -1.42799, Convergence: 0.003129\n",
      "Epoch: 77, Loss: 146.32074, Residuals: -1.41390, Convergence: 0.002980\n",
      "Epoch: 78, Loss: 145.90695, Residuals: -1.40037, Convergence: 0.002836\n",
      "Epoch: 79, Loss: 145.51449, Residuals: -1.38737, Convergence: 0.002697\n",
      "Epoch: 80, Loss: 145.14243, Residuals: -1.37491, Convergence: 0.002563\n",
      "Epoch: 81, Loss: 144.78988, Residuals: -1.36296, Convergence: 0.002435\n",
      "Epoch: 82, Loss: 144.45596, Residuals: -1.35151, Convergence: 0.002312\n",
      "Epoch: 83, Loss: 144.13985, Residuals: -1.34055, Convergence: 0.002193\n",
      "Epoch: 84, Loss: 143.84074, Residuals: -1.33005, Convergence: 0.002079\n",
      "Epoch: 85, Loss: 143.55789, Residuals: -1.32001, Convergence: 0.001970\n",
      "Epoch: 86, Loss: 143.29059, Residuals: -1.31041, Convergence: 0.001865\n",
      "Epoch: 87, Loss: 143.03817, Residuals: -1.30124, Convergence: 0.001765\n",
      "Epoch: 88, Loss: 142.80000, Residuals: -1.29248, Convergence: 0.001668\n",
      "Epoch: 89, Loss: 142.57548, Residuals: -1.28412, Convergence: 0.001575\n",
      "Epoch: 90, Loss: 142.36404, Residuals: -1.27615, Convergence: 0.001485\n",
      "Epoch: 91, Loss: 142.16511, Residuals: -1.26856, Convergence: 0.001399\n",
      "Epoch: 92, Loss: 141.97812, Residuals: -1.26133, Convergence: 0.001317\n",
      "Epoch: 93, Loss: 141.80248, Residuals: -1.25447, Convergence: 0.001239\n",
      "Epoch: 94, Loss: 141.63755, Residuals: -1.24796, Convergence: 0.001164\n",
      "Epoch: 95, Loss: 141.48260, Residuals: -1.24179, Convergence: 0.001095\n",
      "Epoch: 96, Loss: 141.33686, Residuals: -1.23596, Convergence: 0.001031\n",
      "Epoch: 97, Loss: 141.19941, Residuals: -1.23045, Convergence: 0.000973\n",
      "Evidence -182.724\n",
      "\n",
      "Epoch: 97, Evidence: -182.72365, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.24e-01\n",
      "Epoch: 97, Loss: 1359.60304, Residuals: -1.23045, Convergence:   inf\n",
      "Epoch: 98, Loss: 1297.82960, Residuals: -1.26025, Convergence: 0.047597\n",
      "Epoch: 99, Loss: 1250.94615, Residuals: -1.28388, Convergence: 0.037478\n",
      "Epoch: 100, Loss: 1215.63556, Residuals: -1.30101, Convergence: 0.029047\n",
      "Epoch: 101, Loss: 1188.25896, Residuals: -1.31302, Convergence: 0.023039\n",
      "Epoch: 102, Loss: 1166.18913, Residuals: -1.32173, Convergence: 0.018925\n",
      "Epoch: 103, Loss: 1147.91116, Residuals: -1.32823, Convergence: 0.015923\n",
      "Epoch: 104, Loss: 1132.51230, Residuals: -1.33302, Convergence: 0.013597\n",
      "Epoch: 105, Loss: 1119.37255, Residuals: -1.33637, Convergence: 0.011738\n",
      "Epoch: 106, Loss: 1108.03340, Residuals: -1.33846, Convergence: 0.010234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 107, Loss: 1098.13714, Residuals: -1.33942, Convergence: 0.009012\n",
      "Epoch: 108, Loss: 1089.39586, Residuals: -1.33937, Convergence: 0.008024\n",
      "Epoch: 109, Loss: 1081.57150, Residuals: -1.33837, Convergence: 0.007234\n",
      "Epoch: 110, Loss: 1074.46305, Residuals: -1.33651, Convergence: 0.006616\n",
      "Epoch: 111, Loss: 1067.89757, Residuals: -1.33383, Convergence: 0.006148\n",
      "Epoch: 112, Loss: 1061.72228, Residuals: -1.33037, Convergence: 0.005816\n",
      "Epoch: 113, Loss: 1055.80244, Residuals: -1.32613, Convergence: 0.005607\n",
      "Epoch: 114, Loss: 1050.02117, Residuals: -1.32114, Convergence: 0.005506\n",
      "Epoch: 115, Loss: 1044.28496, Residuals: -1.31542, Convergence: 0.005493\n",
      "Epoch: 116, Loss: 1038.53682, Residuals: -1.30902, Convergence: 0.005535\n",
      "Epoch: 117, Loss: 1032.77036, Residuals: -1.30202, Convergence: 0.005583\n",
      "Epoch: 118, Loss: 1027.03614, Residuals: -1.29452, Convergence: 0.005583\n",
      "Epoch: 119, Loss: 1021.42621, Residuals: -1.28665, Convergence: 0.005492\n",
      "Epoch: 120, Loss: 1016.03923, Residuals: -1.27851, Convergence: 0.005302\n",
      "Epoch: 121, Loss: 1010.94755, Residuals: -1.27021, Convergence: 0.005037\n",
      "Epoch: 122, Loss: 1006.18446, Residuals: -1.26181, Convergence: 0.004734\n",
      "Epoch: 123, Loss: 1001.75002, Residuals: -1.25341, Convergence: 0.004427\n",
      "Epoch: 124, Loss: 997.62531, Residuals: -1.24505, Convergence: 0.004135\n",
      "Epoch: 125, Loss: 993.78191, Residuals: -1.23678, Convergence: 0.003867\n",
      "Epoch: 126, Loss: 990.18955, Residuals: -1.22865, Convergence: 0.003628\n",
      "Epoch: 127, Loss: 986.82080, Residuals: -1.22069, Convergence: 0.003414\n",
      "Epoch: 128, Loss: 983.65074, Residuals: -1.21291, Convergence: 0.003223\n",
      "Epoch: 129, Loss: 980.65803, Residuals: -1.20535, Convergence: 0.003052\n",
      "Epoch: 130, Loss: 977.82497, Residuals: -1.19801, Convergence: 0.002897\n",
      "Epoch: 131, Loss: 975.13696, Residuals: -1.19091, Convergence: 0.002757\n",
      "Epoch: 132, Loss: 972.58160, Residuals: -1.18406, Convergence: 0.002627\n",
      "Epoch: 133, Loss: 970.14831, Residuals: -1.17746, Convergence: 0.002508\n",
      "Epoch: 134, Loss: 967.82929, Residuals: -1.17113, Convergence: 0.002396\n",
      "Epoch: 135, Loss: 965.61682, Residuals: -1.16504, Convergence: 0.002291\n",
      "Epoch: 136, Loss: 963.50421, Residuals: -1.15922, Convergence: 0.002193\n",
      "Epoch: 137, Loss: 961.48618, Residuals: -1.15365, Convergence: 0.002099\n",
      "Epoch: 138, Loss: 959.55706, Residuals: -1.14833, Convergence: 0.002010\n",
      "Epoch: 139, Loss: 957.71226, Residuals: -1.14326, Convergence: 0.001926\n",
      "Epoch: 140, Loss: 955.94714, Residuals: -1.13842, Convergence: 0.001846\n",
      "Epoch: 141, Loss: 954.25708, Residuals: -1.13380, Convergence: 0.001771\n",
      "Epoch: 142, Loss: 952.63797, Residuals: -1.12941, Convergence: 0.001700\n",
      "Epoch: 143, Loss: 951.08567, Residuals: -1.12523, Convergence: 0.001632\n",
      "Epoch: 144, Loss: 949.59614, Residuals: -1.12125, Convergence: 0.001569\n",
      "Epoch: 145, Loss: 948.16554, Residuals: -1.11745, Convergence: 0.001509\n",
      "Epoch: 146, Loss: 946.79013, Residuals: -1.11384, Convergence: 0.001453\n",
      "Epoch: 147, Loss: 945.46624, Residuals: -1.11040, Convergence: 0.001400\n",
      "Epoch: 148, Loss: 944.19071, Residuals: -1.10712, Convergence: 0.001351\n",
      "Epoch: 149, Loss: 942.96016, Residuals: -1.10399, Convergence: 0.001305\n",
      "Epoch: 150, Loss: 941.77166, Residuals: -1.10100, Convergence: 0.001262\n",
      "Epoch: 151, Loss: 940.62236, Residuals: -1.09814, Convergence: 0.001222\n",
      "Epoch: 152, Loss: 939.50940, Residuals: -1.09540, Convergence: 0.001185\n",
      "Epoch: 153, Loss: 938.43035, Residuals: -1.09278, Convergence: 0.001150\n",
      "Epoch: 154, Loss: 937.38288, Residuals: -1.09027, Convergence: 0.001117\n",
      "Epoch: 155, Loss: 936.36424, Residuals: -1.08785, Convergence: 0.001088\n",
      "Epoch: 156, Loss: 935.37244, Residuals: -1.08553, Convergence: 0.001060\n",
      "Epoch: 157, Loss: 934.40446, Residuals: -1.08328, Convergence: 0.001036\n",
      "Epoch: 158, Loss: 933.45823, Residuals: -1.08112, Convergence: 0.001014\n",
      "Epoch: 159, Loss: 932.53105, Residuals: -1.07902, Convergence: 0.000994\n",
      "Evidence 11077.836\n",
      "\n",
      "Epoch: 159, Evidence: 11077.83594, Convergence: 1.016495\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.78e-01\n",
      "Epoch: 159, Loss: 2337.06564, Residuals: -1.07902, Convergence:   inf\n",
      "Epoch: 160, Loss: 2298.21193, Residuals: -1.08760, Convergence: 0.016906\n",
      "Epoch: 161, Loss: 2270.75568, Residuals: -1.08629, Convergence: 0.012091\n",
      "Epoch: 162, Loss: 2247.69728, Residuals: -1.08391, Convergence: 0.010259\n",
      "Epoch: 163, Loss: 2228.09515, Residuals: -1.08110, Convergence: 0.008798\n",
      "Epoch: 164, Loss: 2211.26925, Residuals: -1.07800, Convergence: 0.007609\n",
      "Epoch: 165, Loss: 2196.69956, Residuals: -1.07468, Convergence: 0.006633\n",
      "Epoch: 166, Loss: 2183.97258, Residuals: -1.07120, Convergence: 0.005827\n",
      "Epoch: 167, Loss: 2172.75075, Residuals: -1.06760, Convergence: 0.005165\n",
      "Epoch: 168, Loss: 2162.75593, Residuals: -1.06392, Convergence: 0.004621\n",
      "Epoch: 169, Loss: 2153.75641, Residuals: -1.06015, Convergence: 0.004179\n",
      "Epoch: 170, Loss: 2145.56266, Residuals: -1.05630, Convergence: 0.003819\n",
      "Epoch: 171, Loss: 2138.03391, Residuals: -1.05236, Convergence: 0.003521\n",
      "Epoch: 172, Loss: 2131.07455, Residuals: -1.04836, Convergence: 0.003266\n",
      "Epoch: 173, Loss: 2124.63023, Residuals: -1.04433, Convergence: 0.003033\n",
      "Epoch: 174, Loss: 2118.67110, Residuals: -1.04032, Convergence: 0.002813\n",
      "Epoch: 175, Loss: 2113.17708, Residuals: -1.03637, Convergence: 0.002600\n",
      "Epoch: 176, Loss: 2108.12563, Residuals: -1.03253, Convergence: 0.002396\n",
      "Epoch: 177, Loss: 2103.49157, Residuals: -1.02882, Convergence: 0.002203\n",
      "Epoch: 178, Loss: 2099.24370, Residuals: -1.02528, Convergence: 0.002024\n",
      "Epoch: 179, Loss: 2095.35044, Residuals: -1.02191, Convergence: 0.001858\n",
      "Epoch: 180, Loss: 2091.77868, Residuals: -1.01872, Convergence: 0.001708\n",
      "Epoch: 181, Loss: 2088.49568, Residuals: -1.01570, Convergence: 0.001572\n",
      "Epoch: 182, Loss: 2085.47139, Residuals: -1.01286, Convergence: 0.001450\n",
      "Epoch: 183, Loss: 2082.67711, Residuals: -1.01019, Convergence: 0.001342\n",
      "Epoch: 184, Loss: 2080.08684, Residuals: -1.00767, Convergence: 0.001245\n",
      "Epoch: 185, Loss: 2077.67670, Residuals: -1.00531, Convergence: 0.001160\n",
      "Epoch: 186, Loss: 2075.42666, Residuals: -1.00309, Convergence: 0.001084\n",
      "Epoch: 187, Loss: 2073.31658, Residuals: -1.00100, Convergence: 0.001018\n",
      "Epoch: 188, Loss: 2071.33164, Residuals: -0.99903, Convergence: 0.000958\n",
      "Evidence 14248.693\n",
      "\n",
      "Epoch: 188, Evidence: 14248.69336, Convergence: 0.222537\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.42e-01\n",
      "Epoch: 188, Loss: 2463.72379, Residuals: -0.99903, Convergence:   inf\n",
      "Epoch: 189, Loss: 2449.42857, Residuals: -0.99591, Convergence: 0.005836\n",
      "Epoch: 190, Loss: 2437.88576, Residuals: -0.99199, Convergence: 0.004735\n",
      "Epoch: 191, Loss: 2428.00482, Residuals: -0.98812, Convergence: 0.004070\n",
      "Epoch: 192, Loss: 2419.46061, Residuals: -0.98443, Convergence: 0.003531\n",
      "Epoch: 193, Loss: 2412.01824, Residuals: -0.98100, Convergence: 0.003086\n",
      "Epoch: 194, Loss: 2405.49333, Residuals: -0.97782, Convergence: 0.002713\n",
      "Epoch: 195, Loss: 2399.73848, Residuals: -0.97489, Convergence: 0.002398\n",
      "Epoch: 196, Loss: 2394.62930, Residuals: -0.97220, Convergence: 0.002134\n",
      "Epoch: 197, Loss: 2390.06565, Residuals: -0.96972, Convergence: 0.001909\n",
      "Epoch: 198, Loss: 2385.96407, Residuals: -0.96745, Convergence: 0.001719\n",
      "Epoch: 199, Loss: 2382.25384, Residuals: -0.96535, Convergence: 0.001557\n",
      "Epoch: 200, Loss: 2378.87925, Residuals: -0.96342, Convergence: 0.001419\n",
      "Epoch: 201, Loss: 2375.79273, Residuals: -0.96165, Convergence: 0.001299\n",
      "Epoch: 202, Loss: 2372.95660, Residuals: -0.96000, Convergence: 0.001195\n",
      "Epoch: 203, Loss: 2370.33763, Residuals: -0.95848, Convergence: 0.001105\n",
      "Epoch: 204, Loss: 2367.91097, Residuals: -0.95708, Convergence: 0.001025\n",
      "Epoch: 205, Loss: 2365.65292, Residuals: -0.95577, Convergence: 0.000955\n",
      "Evidence 14659.051\n",
      "\n",
      "Epoch: 205, Evidence: 14659.05078, Convergence: 0.027993\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.39e-01\n",
      "Epoch: 205, Loss: 2468.50427, Residuals: -0.95577, Convergence:   inf\n",
      "Epoch: 206, Loss: 2461.92893, Residuals: -0.95242, Convergence: 0.002671\n",
      "Epoch: 207, Loss: 2456.48974, Residuals: -0.94931, Convergence: 0.002214\n",
      "Epoch: 208, Loss: 2451.85514, Residuals: -0.94659, Convergence: 0.001890\n",
      "Epoch: 209, Loss: 2447.84593, Residuals: -0.94423, Convergence: 0.001638\n",
      "Epoch: 210, Loss: 2444.33244, Residuals: -0.94218, Convergence: 0.001437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 211, Loss: 2441.21466, Residuals: -0.94039, Convergence: 0.001277\n",
      "Epoch: 212, Loss: 2438.41736, Residuals: -0.93882, Convergence: 0.001147\n",
      "Epoch: 213, Loss: 2435.88394, Residuals: -0.93744, Convergence: 0.001040\n",
      "Epoch: 214, Loss: 2433.57001, Residuals: -0.93622, Convergence: 0.000951\n",
      "Evidence 14746.863\n",
      "\n",
      "Epoch: 214, Evidence: 14746.86328, Convergence: 0.005955\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.66e-01\n",
      "Epoch: 214, Loss: 2470.05851, Residuals: -0.93622, Convergence:   inf\n",
      "Epoch: 215, Loss: 2466.18129, Residuals: -0.93357, Convergence: 0.001572\n",
      "Epoch: 216, Loss: 2462.93509, Residuals: -0.93138, Convergence: 0.001318\n",
      "Epoch: 217, Loss: 2460.13169, Residuals: -0.92957, Convergence: 0.001140\n",
      "Epoch: 218, Loss: 2457.66237, Residuals: -0.92807, Convergence: 0.001005\n",
      "Epoch: 219, Loss: 2455.45360, Residuals: -0.92680, Convergence: 0.000900\n",
      "Evidence 14778.115\n",
      "\n",
      "Epoch: 219, Evidence: 14778.11523, Convergence: 0.002115\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.14e-01\n",
      "Epoch: 219, Loss: 2471.04632, Residuals: -0.92680, Convergence:   inf\n",
      "Epoch: 220, Loss: 2468.25801, Residuals: -0.92472, Convergence: 0.001130\n",
      "Epoch: 221, Loss: 2465.89564, Residuals: -0.92308, Convergence: 0.000958\n",
      "Evidence 14790.646\n",
      "\n",
      "Epoch: 221, Evidence: 14790.64551, Convergence: 0.000847\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.78e-01\n",
      "Epoch: 221, Loss: 2471.83222, Residuals: -0.92308, Convergence:   inf\n",
      "Epoch: 222, Loss: 2467.35126, Residuals: -0.92012, Convergence: 0.001816\n",
      "Epoch: 223, Loss: 2463.91696, Residuals: -0.91818, Convergence: 0.001394\n",
      "Epoch: 224, Loss: 2461.09578, Residuals: -0.91676, Convergence: 0.001146\n",
      "Epoch: 225, Loss: 2458.68315, Residuals: -0.91580, Convergence: 0.000981\n",
      "Evidence 14808.590\n",
      "\n",
      "Epoch: 225, Evidence: 14808.58984, Convergence: 0.002058\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.48e-01\n",
      "Epoch: 225, Loss: 2472.10224, Residuals: -0.91580, Convergence:   inf\n",
      "Epoch: 226, Loss: 2469.13889, Residuals: -0.91349, Convergence: 0.001200\n",
      "Epoch: 227, Loss: 2466.75862, Residuals: -0.91229, Convergence: 0.000965\n",
      "Evidence 14819.421\n",
      "\n",
      "Epoch: 227, Evidence: 14819.42090, Convergence: 0.000731\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.26e-01\n",
      "Epoch: 227, Loss: 2472.38611, Residuals: -0.91229, Convergence:   inf\n",
      "Epoch: 228, Loss: 2468.00694, Residuals: -0.90962, Convergence: 0.001774\n",
      "Epoch: 229, Loss: 2464.79238, Residuals: -0.91042, Convergence: 0.001304\n",
      "Epoch: 230, Loss: 2462.13540, Residuals: -0.91088, Convergence: 0.001079\n",
      "Epoch: 231, Loss: 2459.78610, Residuals: -0.91338, Convergence: 0.000955\n",
      "Evidence 14835.360\n",
      "\n",
      "Epoch: 231, Evidence: 14835.36035, Convergence: 0.001805\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.12e-01\n",
      "Epoch: 231, Loss: 2471.91023, Residuals: -0.91338, Convergence:   inf\n",
      "Epoch: 232, Loss: 2470.03781, Residuals: -0.91221, Convergence: 0.000758\n",
      "Evidence 14841.878\n",
      "\n",
      "Epoch: 232, Evidence: 14841.87793, Convergence: 0.000439\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 9.14e-02\n",
      "Epoch: 232, Loss: 2472.85510, Residuals: -0.91221, Convergence:   inf\n",
      "Epoch: 233, Loss: 2517.01240, Residuals: -0.95108, Convergence: -0.017544\n",
      "Epoch: 233, Loss: 2470.48169, Residuals: -0.91100, Convergence: 0.000961\n",
      "Evidence 14846.288\n",
      "\n",
      "Epoch: 233, Evidence: 14846.28809, Convergence: 0.000736\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.39e-02\n",
      "Epoch: 233, Loss: 2472.38500, Residuals: -0.91100, Convergence:   inf\n",
      "Epoch: 234, Loss: 2474.14081, Residuals: -0.91276, Convergence: -0.000710\n",
      "Evidence 14846.146\n",
      "\n",
      "Epoch: 234, Evidence: 14846.14648, Convergence: 0.000727\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 378.62317, Residuals: -4.49581, Convergence:   inf\n",
      "Epoch: 1, Loss: 353.00746, Residuals: -4.37631, Convergence: 0.072564\n",
      "Epoch: 2, Loss: 332.09022, Residuals: -4.21402, Convergence: 0.062987\n",
      "Epoch: 3, Loss: 316.05842, Residuals: -4.05059, Convergence: 0.050724\n",
      "Epoch: 4, Loss: 303.83193, Residuals: -3.90619, Convergence: 0.040241\n",
      "Epoch: 5, Loss: 294.13816, Residuals: -3.77834, Convergence: 0.032957\n",
      "Epoch: 6, Loss: 286.27280, Residuals: -3.66683, Convergence: 0.027475\n",
      "Epoch: 7, Loss: 279.74862, Residuals: -3.57125, Convergence: 0.023322\n",
      "Epoch: 8, Loss: 274.20402, Residuals: -3.48960, Convergence: 0.020221\n",
      "Epoch: 9, Loss: 269.38342, Residuals: -3.41957, Convergence: 0.017895\n",
      "Epoch: 10, Loss: 265.10504, Residuals: -3.35907, Convergence: 0.016138\n",
      "Epoch: 11, Loss: 261.23706, Residuals: -3.30630, Convergence: 0.014806\n",
      "Epoch: 12, Loss: 257.68311, Residuals: -3.25975, Convergence: 0.013792\n",
      "Epoch: 13, Loss: 254.37311, Residuals: -3.21809, Convergence: 0.013012\n",
      "Epoch: 14, Loss: 251.25758, Residuals: -3.18017, Convergence: 0.012400\n",
      "Epoch: 15, Loss: 248.30534, Residuals: -3.14509, Convergence: 0.011890\n",
      "Epoch: 16, Loss: 245.50115, Residuals: -3.11223, Convergence: 0.011422\n",
      "Epoch: 17, Loss: 242.83500, Residuals: -3.08124, Convergence: 0.010979\n",
      "Epoch: 18, Loss: 240.28724, Residuals: -3.05176, Convergence: 0.010603\n",
      "Epoch: 19, Loss: 237.82570, Residuals: -3.02327, Convergence: 0.010350\n",
      "Epoch: 20, Loss: 235.41276, Residuals: -2.99518, Convergence: 0.010250\n",
      "Epoch: 21, Loss: 233.01330, Residuals: -2.96692, Convergence: 0.010298\n",
      "Epoch: 22, Loss: 230.59937, Residuals: -2.93805, Convergence: 0.010468\n",
      "Epoch: 23, Loss: 228.14566, Residuals: -2.90825, Convergence: 0.010755\n",
      "Epoch: 24, Loss: 225.61520, Residuals: -2.87705, Convergence: 0.011216\n",
      "Epoch: 25, Loss: 222.95512, Residuals: -2.84384, Convergence: 0.011931\n",
      "Epoch: 26, Loss: 220.14386, Residuals: -2.80825, Convergence: 0.012770\n",
      "Epoch: 27, Loss: 217.27517, Residuals: -2.77123, Convergence: 0.013203\n",
      "Epoch: 28, Loss: 214.47492, Residuals: -2.73421, Convergence: 0.013056\n",
      "Epoch: 29, Loss: 211.77917, Residuals: -2.69777, Convergence: 0.012729\n",
      "Epoch: 30, Loss: 209.17619, Residuals: -2.66190, Convergence: 0.012444\n",
      "Epoch: 31, Loss: 206.64823, Residuals: -2.62649, Convergence: 0.012233\n",
      "Epoch: 32, Loss: 204.18149, Residuals: -2.59143, Convergence: 0.012081\n",
      "Epoch: 33, Loss: 201.76708, Residuals: -2.55663, Convergence: 0.011966\n",
      "Epoch: 34, Loss: 199.40008, Residuals: -2.52202, Convergence: 0.011871\n",
      "Epoch: 35, Loss: 197.07851, Residuals: -2.48756, Convergence: 0.011780\n",
      "Epoch: 36, Loss: 194.80234, Residuals: -2.45323, Convergence: 0.011685\n",
      "Epoch: 37, Loss: 192.57265, Residuals: -2.41901, Convergence: 0.011578\n",
      "Epoch: 38, Loss: 190.39112, Residuals: -2.38489, Convergence: 0.011458\n",
      "Epoch: 39, Loss: 188.25965, Residuals: -2.35088, Convergence: 0.011322\n",
      "Epoch: 40, Loss: 186.18027, Residuals: -2.31699, Convergence: 0.011169\n",
      "Epoch: 41, Loss: 184.15511, Residuals: -2.28323, Convergence: 0.010997\n",
      "Epoch: 42, Loss: 182.18655, Residuals: -2.24965, Convergence: 0.010805\n",
      "Epoch: 43, Loss: 180.27723, Residuals: -2.21627, Convergence: 0.010591\n",
      "Epoch: 44, Loss: 178.42991, Residuals: -2.18316, Convergence: 0.010353\n",
      "Epoch: 45, Loss: 176.64733, Residuals: -2.15036, Convergence: 0.010091\n",
      "Epoch: 46, Loss: 174.93185, Residuals: -2.11795, Convergence: 0.009807\n",
      "Epoch: 47, Loss: 173.28525, Residuals: -2.08597, Convergence: 0.009502\n",
      "Epoch: 48, Loss: 171.70856, Residuals: -2.05449, Convergence: 0.009182\n",
      "Epoch: 49, Loss: 170.20207, Residuals: -2.02356, Convergence: 0.008851\n",
      "Epoch: 50, Loss: 168.76539, Residuals: -1.99324, Convergence: 0.008513\n",
      "Epoch: 51, Loss: 167.39753, Residuals: -1.96356, Convergence: 0.008171\n",
      "Epoch: 52, Loss: 166.09703, Residuals: -1.93456, Convergence: 0.007830\n",
      "Epoch: 53, Loss: 164.86197, Residuals: -1.90627, Convergence: 0.007491\n",
      "Epoch: 54, Loss: 163.69011, Residuals: -1.87873, Convergence: 0.007159\n",
      "Epoch: 55, Loss: 162.57882, Residuals: -1.85195, Convergence: 0.006835\n",
      "Epoch: 56, Loss: 161.52519, Residuals: -1.82595, Convergence: 0.006523\n",
      "Epoch: 57, Loss: 160.52596, Residuals: -1.80074, Convergence: 0.006225\n",
      "Epoch: 58, Loss: 159.57759, Residuals: -1.77632, Convergence: 0.005943\n",
      "Epoch: 59, Loss: 158.67631, Residuals: -1.75268, Convergence: 0.005680\n",
      "Epoch: 60, Loss: 157.81822, Residuals: -1.72980, Convergence: 0.005437\n",
      "Epoch: 61, Loss: 156.99942, Residuals: -1.70765, Convergence: 0.005215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62, Loss: 156.21619, Residuals: -1.68619, Convergence: 0.005014\n",
      "Epoch: 63, Loss: 155.46515, Residuals: -1.66540, Convergence: 0.004831\n",
      "Epoch: 64, Loss: 154.74335, Residuals: -1.64522, Convergence: 0.004665\n",
      "Epoch: 65, Loss: 154.04831, Residuals: -1.62564, Convergence: 0.004512\n",
      "Epoch: 66, Loss: 153.37806, Residuals: -1.60662, Convergence: 0.004370\n",
      "Epoch: 67, Loss: 152.73108, Residuals: -1.58814, Convergence: 0.004236\n",
      "Epoch: 68, Loss: 152.10623, Residuals: -1.57017, Convergence: 0.004108\n",
      "Epoch: 69, Loss: 151.50269, Residuals: -1.55272, Convergence: 0.003984\n",
      "Epoch: 70, Loss: 150.91985, Residuals: -1.53576, Convergence: 0.003862\n",
      "Epoch: 71, Loss: 150.35729, Residuals: -1.51929, Convergence: 0.003742\n",
      "Epoch: 72, Loss: 149.81464, Residuals: -1.50330, Convergence: 0.003622\n",
      "Epoch: 73, Loss: 149.29164, Residuals: -1.48779, Convergence: 0.003503\n",
      "Epoch: 74, Loss: 148.78804, Residuals: -1.47276, Convergence: 0.003385\n",
      "Epoch: 75, Loss: 148.30359, Residuals: -1.45819, Convergence: 0.003267\n",
      "Epoch: 76, Loss: 147.83802, Residuals: -1.44408, Convergence: 0.003149\n",
      "Epoch: 77, Loss: 147.39109, Residuals: -1.43043, Convergence: 0.003032\n",
      "Epoch: 78, Loss: 146.96248, Residuals: -1.41723, Convergence: 0.002916\n",
      "Epoch: 79, Loss: 146.55187, Residuals: -1.40447, Convergence: 0.002802\n",
      "Epoch: 80, Loss: 146.15894, Residuals: -1.39215, Convergence: 0.002688\n",
      "Epoch: 81, Loss: 145.78333, Residuals: -1.38025, Convergence: 0.002577\n",
      "Epoch: 82, Loss: 145.42464, Residuals: -1.36878, Convergence: 0.002466\n",
      "Epoch: 83, Loss: 145.08250, Residuals: -1.35772, Convergence: 0.002358\n",
      "Epoch: 84, Loss: 144.75649, Residuals: -1.34706, Convergence: 0.002252\n",
      "Epoch: 85, Loss: 144.44618, Residuals: -1.33681, Convergence: 0.002148\n",
      "Epoch: 86, Loss: 144.15115, Residuals: -1.32694, Convergence: 0.002047\n",
      "Epoch: 87, Loss: 143.87097, Residuals: -1.31746, Convergence: 0.001947\n",
      "Epoch: 88, Loss: 143.60519, Residuals: -1.30835, Convergence: 0.001851\n",
      "Epoch: 89, Loss: 143.35339, Residuals: -1.29961, Convergence: 0.001757\n",
      "Epoch: 90, Loss: 143.11510, Residuals: -1.29124, Convergence: 0.001665\n",
      "Epoch: 91, Loss: 142.88989, Residuals: -1.28322, Convergence: 0.001576\n",
      "Epoch: 92, Loss: 142.67731, Residuals: -1.27555, Convergence: 0.001490\n",
      "Epoch: 93, Loss: 142.47688, Residuals: -1.26823, Convergence: 0.001407\n",
      "Epoch: 94, Loss: 142.28811, Residuals: -1.26125, Convergence: 0.001327\n",
      "Epoch: 95, Loss: 142.11048, Residuals: -1.25461, Convergence: 0.001250\n",
      "Epoch: 96, Loss: 141.94341, Residuals: -1.24830, Convergence: 0.001177\n",
      "Epoch: 97, Loss: 141.78625, Residuals: -1.24232, Convergence: 0.001108\n",
      "Epoch: 98, Loss: 141.63824, Residuals: -1.23666, Convergence: 0.001045\n",
      "Epoch: 99, Loss: 141.49853, Residuals: -1.23132, Convergence: 0.000987\n",
      "Evidence -182.951\n",
      "\n",
      "Epoch: 99, Evidence: -182.95082, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.24e-01\n",
      "Epoch: 99, Loss: 1373.61207, Residuals: -1.23132, Convergence:   inf\n",
      "Epoch: 100, Loss: 1309.98093, Residuals: -1.25913, Convergence: 0.048574\n",
      "Epoch: 101, Loss: 1261.30964, Residuals: -1.28101, Convergence: 0.038588\n",
      "Epoch: 102, Loss: 1224.45287, Residuals: -1.29647, Convergence: 0.030101\n",
      "Epoch: 103, Loss: 1195.72707, Residuals: -1.30694, Convergence: 0.024024\n",
      "Epoch: 104, Loss: 1172.45228, Residuals: -1.31434, Convergence: 0.019851\n",
      "Epoch: 105, Loss: 1153.09478, Residuals: -1.31971, Convergence: 0.016787\n",
      "Epoch: 106, Loss: 1136.72372, Residuals: -1.32355, Convergence: 0.014402\n",
      "Epoch: 107, Loss: 1122.69992, Residuals: -1.32611, Convergence: 0.012491\n",
      "Epoch: 108, Loss: 1110.54699, Residuals: -1.32754, Convergence: 0.010943\n",
      "Epoch: 109, Loss: 1099.89042, Residuals: -1.32798, Convergence: 0.009689\n",
      "Epoch: 110, Loss: 1090.42682, Residuals: -1.32752, Convergence: 0.008679\n",
      "Epoch: 111, Loss: 1081.90196, Residuals: -1.32623, Convergence: 0.007880\n",
      "Epoch: 112, Loss: 1074.09779, Residuals: -1.32415, Convergence: 0.007266\n",
      "Epoch: 113, Loss: 1066.82305, Residuals: -1.32130, Convergence: 0.006819\n",
      "Epoch: 114, Loss: 1059.90915, Residuals: -1.31770, Convergence: 0.006523\n",
      "Epoch: 115, Loss: 1053.21416, Residuals: -1.31336, Convergence: 0.006357\n",
      "Epoch: 116, Loss: 1046.63391, Residuals: -1.30832, Convergence: 0.006287\n",
      "Epoch: 117, Loss: 1040.12047, Residuals: -1.30263, Convergence: 0.006262\n",
      "Epoch: 118, Loss: 1033.69305, Residuals: -1.29638, Convergence: 0.006218\n",
      "Epoch: 119, Loss: 1027.42450, Residuals: -1.28967, Convergence: 0.006101\n",
      "Epoch: 120, Loss: 1021.40718, Residuals: -1.28259, Convergence: 0.005891\n",
      "Epoch: 121, Loss: 1015.71526, Residuals: -1.27523, Convergence: 0.005604\n",
      "Epoch: 122, Loss: 1010.38662, Residuals: -1.26767, Convergence: 0.005274\n",
      "Epoch: 123, Loss: 1005.42517, Residuals: -1.25999, Convergence: 0.004935\n",
      "Epoch: 124, Loss: 1000.81287, Residuals: -1.25226, Convergence: 0.004609\n",
      "Epoch: 125, Loss: 996.52170, Residuals: -1.24453, Convergence: 0.004306\n",
      "Epoch: 126, Loss: 992.51966, Residuals: -1.23685, Convergence: 0.004032\n",
      "Epoch: 127, Loss: 988.77684, Residuals: -1.22926, Convergence: 0.003785\n",
      "Epoch: 128, Loss: 985.26488, Residuals: -1.22179, Convergence: 0.003564\n",
      "Epoch: 129, Loss: 981.95887, Residuals: -1.21446, Convergence: 0.003367\n",
      "Epoch: 130, Loss: 978.83805, Residuals: -1.20730, Convergence: 0.003188\n",
      "Epoch: 131, Loss: 975.88381, Residuals: -1.20032, Convergence: 0.003027\n",
      "Epoch: 132, Loss: 973.08077, Residuals: -1.19352, Convergence: 0.002881\n",
      "Epoch: 133, Loss: 970.41623, Residuals: -1.18693, Convergence: 0.002746\n",
      "Epoch: 134, Loss: 967.87906, Residuals: -1.18056, Convergence: 0.002621\n",
      "Epoch: 135, Loss: 965.46001, Residuals: -1.17439, Convergence: 0.002506\n",
      "Epoch: 136, Loss: 963.15137, Residuals: -1.16845, Convergence: 0.002397\n",
      "Epoch: 137, Loss: 960.94623, Residuals: -1.16272, Convergence: 0.002295\n",
      "Epoch: 138, Loss: 958.83856, Residuals: -1.15722, Convergence: 0.002198\n",
      "Epoch: 139, Loss: 956.82298, Residuals: -1.15193, Convergence: 0.002107\n",
      "Epoch: 140, Loss: 954.89492, Residuals: -1.14686, Convergence: 0.002019\n",
      "Epoch: 141, Loss: 953.04979, Residuals: -1.14201, Convergence: 0.001936\n",
      "Epoch: 142, Loss: 951.28315, Residuals: -1.13737, Convergence: 0.001857\n",
      "Epoch: 143, Loss: 949.59162, Residuals: -1.13292, Convergence: 0.001781\n",
      "Epoch: 144, Loss: 947.97126, Residuals: -1.12868, Convergence: 0.001709\n",
      "Epoch: 145, Loss: 946.41830, Residuals: -1.12462, Convergence: 0.001641\n",
      "Epoch: 146, Loss: 944.92995, Residuals: -1.12075, Convergence: 0.001575\n",
      "Epoch: 147, Loss: 943.50263, Residuals: -1.11705, Convergence: 0.001513\n",
      "Epoch: 148, Loss: 942.13370, Residuals: -1.11352, Convergence: 0.001453\n",
      "Epoch: 149, Loss: 940.81999, Residuals: -1.11015, Convergence: 0.001396\n",
      "Epoch: 150, Loss: 939.55878, Residuals: -1.10693, Convergence: 0.001342\n",
      "Epoch: 151, Loss: 938.34676, Residuals: -1.10385, Convergence: 0.001292\n",
      "Epoch: 152, Loss: 937.18205, Residuals: -1.10091, Convergence: 0.001243\n",
      "Epoch: 153, Loss: 936.06111, Residuals: -1.09810, Convergence: 0.001197\n",
      "Epoch: 154, Loss: 934.98139, Residuals: -1.09541, Convergence: 0.001155\n",
      "Epoch: 155, Loss: 933.93974, Residuals: -1.09283, Convergence: 0.001115\n",
      "Epoch: 156, Loss: 932.93355, Residuals: -1.09036, Convergence: 0.001079\n",
      "Epoch: 157, Loss: 931.95896, Residuals: -1.08799, Convergence: 0.001046\n",
      "Epoch: 158, Loss: 931.01334, Residuals: -1.08570, Convergence: 0.001016\n",
      "Epoch: 159, Loss: 930.09311, Residuals: -1.08350, Convergence: 0.000989\n",
      "Evidence 11172.637\n",
      "\n",
      "Epoch: 159, Evidence: 11172.63672, Convergence: 1.016375\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.79e-01\n",
      "Epoch: 159, Loss: 2352.06529, Residuals: -1.08350, Convergence:   inf\n",
      "Epoch: 160, Loss: 2311.14807, Residuals: -1.09359, Convergence: 0.017704\n",
      "Epoch: 161, Loss: 2283.41729, Residuals: -1.09297, Convergence: 0.012144\n",
      "Epoch: 162, Loss: 2260.21803, Residuals: -1.09138, Convergence: 0.010264\n",
      "Epoch: 163, Loss: 2240.54709, Residuals: -1.08933, Convergence: 0.008780\n",
      "Epoch: 164, Loss: 2223.69928, Residuals: -1.08695, Convergence: 0.007576\n",
      "Epoch: 165, Loss: 2209.13495, Residuals: -1.08432, Convergence: 0.006593\n",
      "Epoch: 166, Loss: 2196.42912, Residuals: -1.08147, Convergence: 0.005785\n",
      "Epoch: 167, Loss: 2185.24240, Residuals: -1.07845, Convergence: 0.005119\n",
      "Epoch: 168, Loss: 2175.29755, Residuals: -1.07530, Convergence: 0.004572\n",
      "Epoch: 169, Loss: 2166.36908, Residuals: -1.07203, Convergence: 0.004121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 170, Loss: 2158.27345, Residuals: -1.06865, Convergence: 0.003751\n",
      "Epoch: 171, Loss: 2150.87017, Residuals: -1.06519, Convergence: 0.003442\n",
      "Epoch: 172, Loss: 2144.05780, Residuals: -1.06165, Convergence: 0.003177\n",
      "Epoch: 173, Loss: 2137.77180, Residuals: -1.05807, Convergence: 0.002940\n",
      "Epoch: 174, Loss: 2131.97058, Residuals: -1.05450, Convergence: 0.002721\n",
      "Epoch: 175, Loss: 2126.62474, Residuals: -1.05097, Convergence: 0.002514\n",
      "Epoch: 176, Loss: 2121.70515, Residuals: -1.04753, Convergence: 0.002319\n",
      "Epoch: 177, Loss: 2117.18053, Residuals: -1.04421, Convergence: 0.002137\n",
      "Epoch: 178, Loss: 2113.01765, Residuals: -1.04103, Convergence: 0.001970\n",
      "Epoch: 179, Loss: 2109.18162, Residuals: -1.03799, Convergence: 0.001819\n",
      "Epoch: 180, Loss: 2105.63857, Residuals: -1.03510, Convergence: 0.001683\n",
      "Epoch: 181, Loss: 2102.35607, Residuals: -1.03237, Convergence: 0.001561\n",
      "Epoch: 182, Loss: 2099.30491, Residuals: -1.02977, Convergence: 0.001453\n",
      "Epoch: 183, Loss: 2096.45838, Residuals: -1.02730, Convergence: 0.001358\n",
      "Epoch: 184, Loss: 2093.79396, Residuals: -1.02496, Convergence: 0.001273\n",
      "Epoch: 185, Loss: 2091.29252, Residuals: -1.02274, Convergence: 0.001196\n",
      "Epoch: 186, Loss: 2088.93835, Residuals: -1.02063, Convergence: 0.001127\n",
      "Epoch: 187, Loss: 2086.71721, Residuals: -1.01861, Convergence: 0.001064\n",
      "Epoch: 188, Loss: 2084.61886, Residuals: -1.01669, Convergence: 0.001007\n",
      "Epoch: 189, Loss: 2082.63400, Residuals: -1.01486, Convergence: 0.000953\n",
      "Evidence 14291.781\n",
      "\n",
      "Epoch: 189, Evidence: 14291.78125, Convergence: 0.218247\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.42e-01\n",
      "Epoch: 189, Loss: 2475.66115, Residuals: -1.01486, Convergence:   inf\n",
      "Epoch: 190, Loss: 2461.90125, Residuals: -1.01259, Convergence: 0.005589\n",
      "Epoch: 191, Loss: 2450.53608, Residuals: -1.00965, Convergence: 0.004638\n",
      "Epoch: 192, Loss: 2440.63140, Residuals: -1.00668, Convergence: 0.004058\n",
      "Epoch: 193, Loss: 2431.96045, Residuals: -1.00377, Convergence: 0.003565\n",
      "Epoch: 194, Loss: 2424.34159, Residuals: -1.00101, Convergence: 0.003143\n",
      "Epoch: 195, Loss: 2417.62454, Residuals: -0.99840, Convergence: 0.002778\n",
      "Epoch: 196, Loss: 2411.68125, Residuals: -0.99595, Convergence: 0.002464\n",
      "Epoch: 197, Loss: 2406.40158, Residuals: -0.99366, Convergence: 0.002194\n",
      "Epoch: 198, Loss: 2401.69180, Residuals: -0.99153, Convergence: 0.001961\n",
      "Epoch: 199, Loss: 2397.46950, Residuals: -0.98954, Convergence: 0.001761\n",
      "Epoch: 200, Loss: 2393.66707, Residuals: -0.98769, Convergence: 0.001589\n",
      "Epoch: 201, Loss: 2390.22527, Residuals: -0.98597, Convergence: 0.001440\n",
      "Epoch: 202, Loss: 2387.09447, Residuals: -0.98436, Convergence: 0.001312\n",
      "Epoch: 203, Loss: 2384.23304, Residuals: -0.98286, Convergence: 0.001200\n",
      "Epoch: 204, Loss: 2381.60663, Residuals: -0.98146, Convergence: 0.001103\n",
      "Epoch: 205, Loss: 2379.18448, Residuals: -0.98016, Convergence: 0.001018\n",
      "Epoch: 206, Loss: 2376.94117, Residuals: -0.97894, Convergence: 0.000944\n",
      "Evidence 14698.812\n",
      "\n",
      "Epoch: 206, Evidence: 14698.81250, Convergence: 0.027691\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.39e-01\n",
      "Epoch: 206, Loss: 2480.74733, Residuals: -0.97894, Convergence:   inf\n",
      "Epoch: 207, Loss: 2473.71938, Residuals: -0.97557, Convergence: 0.002841\n",
      "Epoch: 208, Loss: 2467.86937, Residuals: -0.97263, Convergence: 0.002370\n",
      "Epoch: 209, Loss: 2462.88894, Residuals: -0.97010, Convergence: 0.002022\n",
      "Epoch: 210, Loss: 2458.59442, Residuals: -0.96792, Convergence: 0.001747\n",
      "Epoch: 211, Loss: 2454.84673, Residuals: -0.96602, Convergence: 0.001527\n",
      "Epoch: 212, Loss: 2451.53896, Residuals: -0.96437, Convergence: 0.001349\n",
      "Epoch: 213, Loss: 2448.58829, Residuals: -0.96293, Convergence: 0.001205\n",
      "Epoch: 214, Loss: 2445.93091, Residuals: -0.96167, Convergence: 0.001086\n",
      "Epoch: 215, Loss: 2443.51744, Residuals: -0.96058, Convergence: 0.000988\n",
      "Evidence 14791.123\n",
      "\n",
      "Epoch: 215, Evidence: 14791.12305, Convergence: 0.006241\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.66e-01\n",
      "Epoch: 215, Loss: 2482.20604, Residuals: -0.96058, Convergence:   inf\n",
      "Epoch: 216, Loss: 2477.94698, Residuals: -0.95795, Convergence: 0.001719\n",
      "Epoch: 217, Loss: 2474.39037, Residuals: -0.95587, Convergence: 0.001437\n",
      "Epoch: 218, Loss: 2471.33865, Residuals: -0.95417, Convergence: 0.001235\n",
      "Epoch: 219, Loss: 2468.67172, Residuals: -0.95278, Convergence: 0.001080\n",
      "Epoch: 220, Loss: 2466.30534, Residuals: -0.95163, Convergence: 0.000959\n",
      "Evidence 14824.683\n",
      "\n",
      "Epoch: 220, Evidence: 14824.68262, Convergence: 0.002264\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.14e-01\n",
      "Epoch: 220, Loss: 2483.09296, Residuals: -0.95163, Convergence:   inf\n",
      "Epoch: 221, Loss: 2480.02161, Residuals: -0.94974, Convergence: 0.001238\n",
      "Epoch: 222, Loss: 2477.44377, Residuals: -0.94828, Convergence: 0.001041\n",
      "Epoch: 223, Loss: 2475.21022, Residuals: -0.94712, Convergence: 0.000902\n",
      "Evidence 14840.515\n",
      "\n",
      "Epoch: 223, Evidence: 14840.51465, Convergence: 0.001067\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.76e-01\n",
      "Epoch: 223, Loss: 2483.73563, Residuals: -0.94712, Convergence:   inf\n",
      "Epoch: 224, Loss: 2481.28080, Residuals: -0.94563, Convergence: 0.000989\n",
      "Evidence 14847.217\n",
      "\n",
      "Epoch: 224, Evidence: 14847.21680, Convergence: 0.000451\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.50e-01\n",
      "Epoch: 224, Loss: 2484.26015, Residuals: -0.94563, Convergence:   inf\n",
      "Epoch: 225, Loss: 2480.11208, Residuals: -0.94377, Convergence: 0.001673\n",
      "Epoch: 226, Loss: 2476.89888, Residuals: -0.94228, Convergence: 0.001297\n",
      "Epoch: 227, Loss: 2474.26491, Residuals: -0.94119, Convergence: 0.001065\n",
      "Epoch: 228, Loss: 2472.01035, Residuals: -0.94053, Convergence: 0.000912\n",
      "Evidence 14862.781\n",
      "\n",
      "Epoch: 228, Evidence: 14862.78125, Convergence: 0.001498\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.26e-01\n",
      "Epoch: 228, Loss: 2484.40324, Residuals: -0.94053, Convergence:   inf\n",
      "Epoch: 229, Loss: 2481.62466, Residuals: -0.93861, Convergence: 0.001120\n",
      "Epoch: 230, Loss: 2479.38246, Residuals: -0.93759, Convergence: 0.000904\n",
      "Evidence 14872.398\n",
      "\n",
      "Epoch: 230, Evidence: 14872.39844, Convergence: 0.000647\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.08e-01\n",
      "Epoch: 230, Loss: 2484.53576, Residuals: -0.93759, Convergence:   inf\n",
      "Epoch: 231, Loss: 2480.38029, Residuals: -0.93472, Convergence: 0.001675\n",
      "Epoch: 232, Loss: 2477.33207, Residuals: -0.93555, Convergence: 0.001230\n",
      "Epoch: 233, Loss: 2474.75982, Residuals: -0.93567, Convergence: 0.001039\n",
      "Epoch: 234, Loss: 2472.54534, Residuals: -0.93805, Convergence: 0.000896\n",
      "Evidence 14887.023\n",
      "\n",
      "Epoch: 234, Evidence: 14887.02344, Convergence: 0.001628\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 9.77e-02\n",
      "Epoch: 234, Loss: 2483.81810, Residuals: -0.93805, Convergence:   inf\n",
      "Epoch: 235, Loss: 2482.18877, Residuals: -0.93569, Convergence: 0.000656\n",
      "Evidence 14892.800\n",
      "\n",
      "Epoch: 235, Evidence: 14892.79980, Convergence: 0.000388\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.03e-02\n",
      "Epoch: 235, Loss: 2484.82918, Residuals: -0.93569, Convergence:   inf\n",
      "Epoch: 236, Loss: 2534.31914, Residuals: -0.97114, Convergence: -0.019528\n",
      "Epoch: 236, Loss: 2482.56465, Residuals: -0.93341, Convergence: 0.000912\n",
      "Evidence 14896.811\n",
      "\n",
      "Epoch: 236, Evidence: 14896.81055, Convergence: 0.000657\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.44e-02\n",
      "Epoch: 236, Loss: 2484.20183, Residuals: -0.93341, Convergence:   inf\n",
      "Epoch: 237, Loss: 2488.05368, Residuals: -0.93121, Convergence: -0.001548\n",
      "Epoch: 237, Loss: 2484.04810, Residuals: -0.93063, Convergence: 0.000062\n",
      "Evidence 14898.329\n",
      "\n",
      "Epoch: 237, Evidence: 14898.32910, Convergence: 0.000759\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 381.34507, Residuals: -4.49517, Convergence:   inf\n",
      "Epoch: 1, Loss: 355.51461, Residuals: -4.37445, Convergence: 0.072657\n",
      "Epoch: 2, Loss: 334.35686, Residuals: -4.20950, Convergence: 0.063279\n",
      "Epoch: 3, Loss: 318.17978, Residuals: -4.04388, Convergence: 0.050843\n",
      "Epoch: 4, Loss: 305.83071, Residuals: -3.89780, Convergence: 0.040379\n",
      "Epoch: 5, Loss: 296.02404, Residuals: -3.76853, Convergence: 0.033128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Loss: 288.05423, Residuals: -3.65594, Convergence: 0.027668\n",
      "Epoch: 7, Loss: 281.42999, Residuals: -3.55960, Convergence: 0.023538\n",
      "Epoch: 8, Loss: 275.78802, Residuals: -3.47740, Convergence: 0.020458\n",
      "Epoch: 9, Loss: 270.87270, Residuals: -3.40699, Convergence: 0.018146\n",
      "Epoch: 10, Loss: 266.50262, Residuals: -3.34623, Convergence: 0.016398\n",
      "Epoch: 11, Loss: 262.54681, Residuals: -3.29333, Convergence: 0.015067\n",
      "Epoch: 12, Loss: 258.91028, Residuals: -3.24674, Convergence: 0.014046\n",
      "Epoch: 13, Loss: 255.52461, Residuals: -3.20515, Convergence: 0.013250\n",
      "Epoch: 14, Loss: 252.34095, Residuals: -3.16740, Convergence: 0.012617\n",
      "Epoch: 15, Loss: 249.32523, Residuals: -3.13250, Convergence: 0.012096\n",
      "Epoch: 16, Loss: 246.45551, Residuals: -3.09974, Convergence: 0.011644\n",
      "Epoch: 17, Loss: 243.71660, Residuals: -3.06865, Convergence: 0.011238\n",
      "Epoch: 18, Loss: 241.08968, Residuals: -3.03888, Convergence: 0.010896\n",
      "Epoch: 19, Loss: 238.54649, Residuals: -3.00998, Convergence: 0.010661\n",
      "Epoch: 20, Loss: 236.05305, Residuals: -2.98145, Convergence: 0.010563\n",
      "Epoch: 21, Loss: 233.57692, Residuals: -2.95280, Convergence: 0.010601\n",
      "Epoch: 22, Loss: 231.09185, Residuals: -2.92365, Convergence: 0.010754\n",
      "Epoch: 23, Loss: 228.57329, Residuals: -2.89369, Convergence: 0.011019\n",
      "Epoch: 24, Loss: 225.98447, Residuals: -2.86251, Convergence: 0.011456\n",
      "Epoch: 25, Loss: 223.27111, Residuals: -2.82946, Convergence: 0.012153\n",
      "Epoch: 26, Loss: 220.40213, Residuals: -2.79410, Convergence: 0.013017\n",
      "Epoch: 27, Loss: 217.45729, Residuals: -2.75717, Convergence: 0.013542\n",
      "Epoch: 28, Loss: 214.56740, Residuals: -2.72014, Convergence: 0.013468\n",
      "Epoch: 29, Loss: 211.77808, Residuals: -2.68364, Convergence: 0.013171\n",
      "Epoch: 30, Loss: 209.08127, Residuals: -2.64768, Convergence: 0.012898\n",
      "Epoch: 31, Loss: 206.46098, Residuals: -2.61216, Convergence: 0.012691\n",
      "Epoch: 32, Loss: 203.90491, Residuals: -2.57694, Convergence: 0.012536\n",
      "Epoch: 33, Loss: 201.40543, Residuals: -2.54197, Convergence: 0.012410\n",
      "Epoch: 34, Loss: 198.95855, Residuals: -2.50719, Convergence: 0.012298\n",
      "Epoch: 35, Loss: 196.56283, Residuals: -2.47257, Convergence: 0.012188\n",
      "Epoch: 36, Loss: 194.21857, Residuals: -2.43810, Convergence: 0.012070\n",
      "Epoch: 37, Loss: 191.92714, Residuals: -2.40377, Convergence: 0.011939\n",
      "Epoch: 38, Loss: 189.69058, Residuals: -2.36961, Convergence: 0.011791\n",
      "Epoch: 39, Loss: 187.51127, Residuals: -2.33564, Convergence: 0.011622\n",
      "Epoch: 40, Loss: 185.39173, Residuals: -2.30188, Convergence: 0.011433\n",
      "Epoch: 41, Loss: 183.33437, Residuals: -2.26837, Convergence: 0.011222\n",
      "Epoch: 42, Loss: 181.34144, Residuals: -2.23517, Convergence: 0.010990\n",
      "Epoch: 43, Loss: 179.41490, Residuals: -2.20230, Convergence: 0.010738\n",
      "Epoch: 44, Loss: 177.55633, Residuals: -2.16983, Convergence: 0.010468\n",
      "Epoch: 45, Loss: 175.76679, Residuals: -2.13780, Convergence: 0.010181\n",
      "Epoch: 46, Loss: 174.04677, Residuals: -2.10626, Convergence: 0.009882\n",
      "Epoch: 47, Loss: 172.39619, Residuals: -2.07525, Convergence: 0.009574\n",
      "Epoch: 48, Loss: 170.81419, Residuals: -2.04480, Convergence: 0.009261\n",
      "Epoch: 49, Loss: 169.29927, Residuals: -2.01494, Convergence: 0.008948\n",
      "Epoch: 50, Loss: 167.84919, Residuals: -1.98570, Convergence: 0.008639\n",
      "Epoch: 51, Loss: 166.46113, Residuals: -1.95707, Convergence: 0.008339\n",
      "Epoch: 52, Loss: 165.13175, Residuals: -1.92904, Convergence: 0.008050\n",
      "Epoch: 53, Loss: 163.85745, Residuals: -1.90161, Convergence: 0.007777\n",
      "Epoch: 54, Loss: 162.63456, Residuals: -1.87475, Convergence: 0.007519\n",
      "Epoch: 55, Loss: 161.45965, Residuals: -1.84843, Convergence: 0.007277\n",
      "Epoch: 56, Loss: 160.32971, Residuals: -1.82263, Convergence: 0.007048\n",
      "Epoch: 57, Loss: 159.24234, Residuals: -1.79733, Convergence: 0.006828\n",
      "Epoch: 58, Loss: 158.19573, Residuals: -1.77253, Convergence: 0.006616\n",
      "Epoch: 59, Loss: 157.18869, Residuals: -1.74821, Convergence: 0.006407\n",
      "Epoch: 60, Loss: 156.22047, Residuals: -1.72439, Convergence: 0.006198\n",
      "Epoch: 61, Loss: 155.29054, Residuals: -1.70107, Convergence: 0.005988\n",
      "Epoch: 62, Loss: 154.39850, Residuals: -1.67828, Convergence: 0.005778\n",
      "Epoch: 63, Loss: 153.54390, Residuals: -1.65602, Convergence: 0.005566\n",
      "Epoch: 64, Loss: 152.72619, Residuals: -1.63432, Convergence: 0.005354\n",
      "Epoch: 65, Loss: 151.94468, Residuals: -1.61320, Convergence: 0.005143\n",
      "Epoch: 66, Loss: 151.19849, Residuals: -1.59265, Convergence: 0.004935\n",
      "Epoch: 67, Loss: 150.48670, Residuals: -1.57270, Convergence: 0.004730\n",
      "Epoch: 68, Loss: 149.80824, Residuals: -1.55335, Convergence: 0.004529\n",
      "Epoch: 69, Loss: 149.16201, Residuals: -1.53460, Convergence: 0.004332\n",
      "Epoch: 70, Loss: 148.54686, Residuals: -1.51646, Convergence: 0.004141\n",
      "Epoch: 71, Loss: 147.96160, Residuals: -1.49893, Convergence: 0.003955\n",
      "Epoch: 72, Loss: 147.40506, Residuals: -1.48200, Convergence: 0.003776\n",
      "Epoch: 73, Loss: 146.87607, Residuals: -1.46566, Convergence: 0.003602\n",
      "Epoch: 74, Loss: 146.37348, Residuals: -1.44991, Convergence: 0.003434\n",
      "Epoch: 75, Loss: 145.89612, Residuals: -1.43475, Convergence: 0.003272\n",
      "Epoch: 76, Loss: 145.44289, Residuals: -1.42015, Convergence: 0.003116\n",
      "Epoch: 77, Loss: 145.01270, Residuals: -1.40611, Convergence: 0.002967\n",
      "Epoch: 78, Loss: 144.60447, Residuals: -1.39262, Convergence: 0.002823\n",
      "Epoch: 79, Loss: 144.21717, Residuals: -1.37966, Convergence: 0.002686\n",
      "Epoch: 80, Loss: 143.84979, Residuals: -1.36721, Convergence: 0.002554\n",
      "Epoch: 81, Loss: 143.50136, Residuals: -1.35527, Convergence: 0.002428\n",
      "Epoch: 82, Loss: 143.17093, Residuals: -1.34381, Convergence: 0.002308\n",
      "Epoch: 83, Loss: 142.85761, Residuals: -1.33282, Convergence: 0.002193\n",
      "Epoch: 84, Loss: 142.56052, Residuals: -1.32228, Convergence: 0.002084\n",
      "Epoch: 85, Loss: 142.27882, Residuals: -1.31218, Convergence: 0.001980\n",
      "Epoch: 86, Loss: 142.01172, Residuals: -1.30250, Convergence: 0.001881\n",
      "Epoch: 87, Loss: 141.75846, Residuals: -1.29322, Convergence: 0.001787\n",
      "Epoch: 88, Loss: 141.51834, Residuals: -1.28432, Convergence: 0.001697\n",
      "Epoch: 89, Loss: 141.29067, Residuals: -1.27579, Convergence: 0.001611\n",
      "Epoch: 90, Loss: 141.07483, Residuals: -1.26762, Convergence: 0.001530\n",
      "Epoch: 91, Loss: 140.87022, Residuals: -1.25979, Convergence: 0.001452\n",
      "Epoch: 92, Loss: 140.67630, Residuals: -1.25228, Convergence: 0.001379\n",
      "Epoch: 93, Loss: 140.49255, Residuals: -1.24508, Convergence: 0.001308\n",
      "Epoch: 94, Loss: 140.31852, Residuals: -1.23818, Convergence: 0.001240\n",
      "Epoch: 95, Loss: 140.15378, Residuals: -1.23157, Convergence: 0.001175\n",
      "Epoch: 96, Loss: 139.99795, Residuals: -1.22522, Convergence: 0.001113\n",
      "Epoch: 97, Loss: 139.85066, Residuals: -1.21915, Convergence: 0.001053\n",
      "Epoch: 98, Loss: 139.71160, Residuals: -1.21332, Convergence: 0.000995\n",
      "Evidence -181.796\n",
      "\n",
      "Epoch: 98, Evidence: -181.79616, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.25e-01\n",
      "Epoch: 98, Loss: 1358.64831, Residuals: -1.21332, Convergence:   inf\n",
      "Epoch: 99, Loss: 1295.92485, Residuals: -1.24457, Convergence: 0.048401\n",
      "Epoch: 100, Loss: 1248.21502, Residuals: -1.26876, Convergence: 0.038222\n",
      "Epoch: 101, Loss: 1212.23658, Residuals: -1.28595, Convergence: 0.029679\n",
      "Epoch: 102, Loss: 1184.34856, Residuals: -1.29800, Convergence: 0.023547\n",
      "Epoch: 103, Loss: 1161.89881, Residuals: -1.30696, Convergence: 0.019322\n",
      "Epoch: 104, Loss: 1143.34822, Residuals: -1.31390, Convergence: 0.016225\n",
      "Epoch: 105, Loss: 1127.76448, Residuals: -1.31928, Convergence: 0.013818\n",
      "Epoch: 106, Loss: 1114.51125, Residuals: -1.32331, Convergence: 0.011892\n",
      "Epoch: 107, Loss: 1103.11607, Residuals: -1.32613, Convergence: 0.010330\n",
      "Epoch: 108, Loss: 1093.20855, Residuals: -1.32785, Convergence: 0.009063\n",
      "Epoch: 109, Loss: 1084.49025, Residuals: -1.32855, Convergence: 0.008039\n",
      "Epoch: 110, Loss: 1076.71329, Residuals: -1.32830, Convergence: 0.007223\n",
      "Epoch: 111, Loss: 1069.66862, Residuals: -1.32716, Convergence: 0.006586\n",
      "Epoch: 112, Loss: 1063.17563, Residuals: -1.32517, Convergence: 0.006107\n",
      "Epoch: 113, Loss: 1057.07583, Residuals: -1.32235, Convergence: 0.005770\n",
      "Epoch: 114, Loss: 1051.22861, Residuals: -1.31872, Convergence: 0.005562\n",
      "Epoch: 115, Loss: 1045.51209, Residuals: -1.31427, Convergence: 0.005468\n",
      "Epoch: 116, Loss: 1039.82730, Residuals: -1.30904, Convergence: 0.005467\n",
      "Epoch: 117, Loss: 1034.11410, Residuals: -1.30306, Convergence: 0.005525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 118, Loss: 1028.36197, Residuals: -1.29642, Convergence: 0.005593\n",
      "Epoch: 119, Loss: 1022.62078, Residuals: -1.28923, Convergence: 0.005614\n",
      "Epoch: 120, Loss: 1016.98497, Residuals: -1.28159, Convergence: 0.005542\n",
      "Epoch: 121, Loss: 1011.55578, Residuals: -1.27364, Convergence: 0.005367\n",
      "Epoch: 122, Loss: 1006.40880, Residuals: -1.26545, Convergence: 0.005114\n",
      "Epoch: 123, Loss: 1001.57943, Residuals: -1.25713, Convergence: 0.004822\n",
      "Epoch: 124, Loss: 997.06960, Residuals: -1.24875, Convergence: 0.004523\n",
      "Epoch: 125, Loss: 992.86084, Residuals: -1.24037, Convergence: 0.004239\n",
      "Epoch: 126, Loss: 988.92634, Residuals: -1.23205, Convergence: 0.003979\n",
      "Epoch: 127, Loss: 985.23702, Residuals: -1.22383, Convergence: 0.003745\n",
      "Epoch: 128, Loss: 981.76636, Residuals: -1.21575, Convergence: 0.003535\n",
      "Epoch: 129, Loss: 978.49087, Residuals: -1.20783, Convergence: 0.003347\n",
      "Epoch: 130, Loss: 975.39074, Residuals: -1.20009, Convergence: 0.003178\n",
      "Epoch: 131, Loss: 972.44957, Residuals: -1.19257, Convergence: 0.003024\n",
      "Epoch: 132, Loss: 969.65387, Residuals: -1.18527, Convergence: 0.002883\n",
      "Epoch: 133, Loss: 966.99284, Residuals: -1.17821, Convergence: 0.002752\n",
      "Epoch: 134, Loss: 964.45727, Residuals: -1.17139, Convergence: 0.002629\n",
      "Epoch: 135, Loss: 962.03980, Residuals: -1.16483, Convergence: 0.002513\n",
      "Epoch: 136, Loss: 959.73366, Residuals: -1.15853, Convergence: 0.002403\n",
      "Epoch: 137, Loss: 957.53276, Residuals: -1.15248, Convergence: 0.002299\n",
      "Epoch: 138, Loss: 955.43157, Residuals: -1.14669, Convergence: 0.002199\n",
      "Epoch: 139, Loss: 953.42489, Residuals: -1.14116, Convergence: 0.002105\n",
      "Epoch: 140, Loss: 951.50695, Residuals: -1.13587, Convergence: 0.002016\n",
      "Epoch: 141, Loss: 949.67303, Residuals: -1.13083, Convergence: 0.001931\n",
      "Epoch: 142, Loss: 947.91775, Residuals: -1.12602, Convergence: 0.001852\n",
      "Epoch: 143, Loss: 946.23616, Residuals: -1.12144, Convergence: 0.001777\n",
      "Epoch: 144, Loss: 944.62382, Residuals: -1.11706, Convergence: 0.001707\n",
      "Epoch: 145, Loss: 943.07574, Residuals: -1.11289, Convergence: 0.001642\n",
      "Epoch: 146, Loss: 941.58795, Residuals: -1.10892, Convergence: 0.001580\n",
      "Epoch: 147, Loss: 940.15614, Residuals: -1.10512, Convergence: 0.001523\n",
      "Epoch: 148, Loss: 938.77704, Residuals: -1.10150, Convergence: 0.001469\n",
      "Epoch: 149, Loss: 937.44709, Residuals: -1.09804, Convergence: 0.001419\n",
      "Epoch: 150, Loss: 936.16316, Residuals: -1.09473, Convergence: 0.001371\n",
      "Epoch: 151, Loss: 934.92235, Residuals: -1.09156, Convergence: 0.001327\n",
      "Epoch: 152, Loss: 933.72180, Residuals: -1.08852, Convergence: 0.001286\n",
      "Epoch: 153, Loss: 932.55935, Residuals: -1.08561, Convergence: 0.001247\n",
      "Epoch: 154, Loss: 931.43235, Residuals: -1.08282, Convergence: 0.001210\n",
      "Epoch: 155, Loss: 930.33826, Residuals: -1.08014, Convergence: 0.001176\n",
      "Epoch: 156, Loss: 929.27448, Residuals: -1.07756, Convergence: 0.001145\n",
      "Epoch: 157, Loss: 928.23860, Residuals: -1.07507, Convergence: 0.001116\n",
      "Epoch: 158, Loss: 927.22761, Residuals: -1.07267, Convergence: 0.001090\n",
      "Epoch: 159, Loss: 926.23887, Residuals: -1.07035, Convergence: 0.001067\n",
      "Epoch: 160, Loss: 925.26908, Residuals: -1.06810, Convergence: 0.001048\n",
      "Epoch: 161, Loss: 924.31521, Residuals: -1.06591, Convergence: 0.001032\n",
      "Epoch: 162, Loss: 923.37342, Residuals: -1.06378, Convergence: 0.001020\n",
      "Epoch: 163, Loss: 922.44095, Residuals: -1.06169, Convergence: 0.001011\n",
      "Epoch: 164, Loss: 921.51471, Residuals: -1.05964, Convergence: 0.001005\n",
      "Epoch: 165, Loss: 920.59217, Residuals: -1.05762, Convergence: 0.001002\n",
      "Epoch: 166, Loss: 919.67191, Residuals: -1.05562, Convergence: 0.001001\n",
      "Epoch: 167, Loss: 918.75334, Residuals: -1.05365, Convergence: 0.001000\n",
      "Evidence 11128.905\n",
      "\n",
      "Epoch: 167, Evidence: 11128.90527, Convergence: 1.016335\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.77e-01\n",
      "Epoch: 167, Loss: 2345.01427, Residuals: -1.05365, Convergence:   inf\n",
      "Epoch: 168, Loss: 2304.30679, Residuals: -1.06439, Convergence: 0.017666\n",
      "Epoch: 169, Loss: 2276.94436, Residuals: -1.06371, Convergence: 0.012017\n",
      "Epoch: 170, Loss: 2254.09989, Residuals: -1.06199, Convergence: 0.010135\n",
      "Epoch: 171, Loss: 2234.74786, Residuals: -1.05992, Convergence: 0.008660\n",
      "Epoch: 172, Loss: 2218.22988, Residuals: -1.05764, Convergence: 0.007446\n",
      "Epoch: 173, Loss: 2204.04107, Residuals: -1.05523, Convergence: 0.006438\n",
      "Epoch: 174, Loss: 2191.76629, Residuals: -1.05272, Convergence: 0.005600\n",
      "Epoch: 175, Loss: 2181.05521, Residuals: -1.05012, Convergence: 0.004911\n",
      "Epoch: 176, Loss: 2171.60725, Residuals: -1.04744, Convergence: 0.004351\n",
      "Epoch: 177, Loss: 2163.17086, Residuals: -1.04467, Convergence: 0.003900\n",
      "Epoch: 178, Loss: 2155.54182, Residuals: -1.04179, Convergence: 0.003539\n",
      "Epoch: 179, Loss: 2148.56419, Residuals: -1.03880, Convergence: 0.003248\n",
      "Epoch: 180, Loss: 2142.13789, Residuals: -1.03569, Convergence: 0.003000\n",
      "Epoch: 181, Loss: 2136.20197, Residuals: -1.03251, Convergence: 0.002779\n",
      "Epoch: 182, Loss: 2130.71811, Residuals: -1.02930, Convergence: 0.002574\n",
      "Epoch: 183, Loss: 2125.65648, Residuals: -1.02609, Convergence: 0.002381\n",
      "Epoch: 184, Loss: 2120.98809, Residuals: -1.02292, Convergence: 0.002201\n",
      "Epoch: 185, Loss: 2116.68155, Residuals: -1.01983, Convergence: 0.002035\n",
      "Epoch: 186, Loss: 2112.70279, Residuals: -1.01684, Convergence: 0.001883\n",
      "Epoch: 187, Loss: 2109.02037, Residuals: -1.01395, Convergence: 0.001746\n",
      "Epoch: 188, Loss: 2105.60236, Residuals: -1.01117, Convergence: 0.001623\n",
      "Epoch: 189, Loss: 2102.42018, Residuals: -1.00851, Convergence: 0.001514\n",
      "Epoch: 190, Loss: 2099.44822, Residuals: -1.00596, Convergence: 0.001416\n",
      "Epoch: 191, Loss: 2096.66231, Residuals: -1.00353, Convergence: 0.001329\n",
      "Epoch: 192, Loss: 2094.04262, Residuals: -1.00122, Convergence: 0.001251\n",
      "Epoch: 193, Loss: 2091.56994, Residuals: -0.99901, Convergence: 0.001182\n",
      "Epoch: 194, Loss: 2089.22867, Residuals: -0.99690, Convergence: 0.001121\n",
      "Epoch: 195, Loss: 2087.00456, Residuals: -0.99490, Convergence: 0.001066\n",
      "Epoch: 196, Loss: 2084.88621, Residuals: -0.99298, Convergence: 0.001016\n",
      "Epoch: 197, Loss: 2082.86410, Residuals: -0.99116, Convergence: 0.000971\n",
      "Evidence 14368.472\n",
      "\n",
      "Epoch: 197, Evidence: 14368.47168, Convergence: 0.225464\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.40e-01\n",
      "Epoch: 197, Loss: 2472.51675, Residuals: -0.99116, Convergence:   inf\n",
      "Epoch: 198, Loss: 2458.42367, Residuals: -0.98793, Convergence: 0.005733\n",
      "Epoch: 199, Loss: 2446.89293, Residuals: -0.98412, Convergence: 0.004712\n",
      "Epoch: 200, Loss: 2436.91012, Residuals: -0.98049, Convergence: 0.004097\n",
      "Epoch: 201, Loss: 2428.21740, Residuals: -0.97712, Convergence: 0.003580\n",
      "Epoch: 202, Loss: 2420.61116, Residuals: -0.97401, Convergence: 0.003142\n",
      "Epoch: 203, Loss: 2413.92521, Residuals: -0.97116, Convergence: 0.002770\n",
      "Epoch: 204, Loss: 2408.01906, Residuals: -0.96853, Convergence: 0.002453\n",
      "Epoch: 205, Loss: 2402.77681, Residuals: -0.96612, Convergence: 0.002182\n",
      "Epoch: 206, Loss: 2398.10148, Residuals: -0.96389, Convergence: 0.001950\n",
      "Epoch: 207, Loss: 2393.91121, Residuals: -0.96183, Convergence: 0.001750\n",
      "Epoch: 208, Loss: 2390.13892, Residuals: -0.95992, Convergence: 0.001578\n",
      "Epoch: 209, Loss: 2386.72565, Residuals: -0.95816, Convergence: 0.001430\n",
      "Epoch: 210, Loss: 2383.62402, Residuals: -0.95653, Convergence: 0.001301\n",
      "Epoch: 211, Loss: 2380.79368, Residuals: -0.95501, Convergence: 0.001189\n",
      "Epoch: 212, Loss: 2378.20088, Residuals: -0.95360, Convergence: 0.001090\n",
      "Epoch: 213, Loss: 2375.81611, Residuals: -0.95229, Convergence: 0.001004\n",
      "Epoch: 214, Loss: 2373.61546, Residuals: -0.95107, Convergence: 0.000927\n",
      "Evidence 14776.885\n",
      "\n",
      "Epoch: 214, Evidence: 14776.88477, Convergence: 0.027639\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.36e-01\n",
      "Epoch: 214, Loss: 2476.70484, Residuals: -0.95107, Convergence:   inf\n",
      "Epoch: 215, Loss: 2470.06063, Residuals: -0.94777, Convergence: 0.002690\n",
      "Epoch: 216, Loss: 2464.56496, Residuals: -0.94492, Convergence: 0.002230\n",
      "Epoch: 217, Loss: 2459.89505, Residuals: -0.94248, Convergence: 0.001898\n",
      "Epoch: 218, Loss: 2455.86631, Residuals: -0.94038, Convergence: 0.001640\n",
      "Epoch: 219, Loss: 2452.34768, Residuals: -0.93855, Convergence: 0.001435\n",
      "Epoch: 220, Loss: 2449.24137, Residuals: -0.93695, Convergence: 0.001268\n",
      "Epoch: 221, Loss: 2446.46960, Residuals: -0.93555, Convergence: 0.001133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 222, Loss: 2443.97469, Residuals: -0.93433, Convergence: 0.001021\n",
      "Epoch: 223, Loss: 2441.71024, Residuals: -0.93326, Convergence: 0.000927\n",
      "Evidence 14867.015\n",
      "\n",
      "Epoch: 223, Evidence: 14867.01465, Convergence: 0.006062\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.63e-01\n",
      "Epoch: 223, Loss: 2478.37455, Residuals: -0.93326, Convergence:   inf\n",
      "Epoch: 224, Loss: 2474.55846, Residuals: -0.93077, Convergence: 0.001542\n",
      "Epoch: 225, Loss: 2471.37067, Residuals: -0.92877, Convergence: 0.001290\n",
      "Epoch: 226, Loss: 2468.62318, Residuals: -0.92711, Convergence: 0.001113\n",
      "Epoch: 227, Loss: 2466.21132, Residuals: -0.92572, Convergence: 0.000978\n",
      "Evidence 14895.793\n",
      "\n",
      "Epoch: 227, Evidence: 14895.79297, Convergence: 0.001932\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.13e-01\n",
      "Epoch: 227, Loss: 2479.50410, Residuals: -0.92572, Convergence:   inf\n",
      "Epoch: 228, Loss: 2476.67376, Residuals: -0.92373, Convergence: 0.001143\n",
      "Epoch: 229, Loss: 2474.28420, Residuals: -0.92210, Convergence: 0.000966\n",
      "Evidence 14907.624\n",
      "\n",
      "Epoch: 229, Evidence: 14907.62402, Convergence: 0.000794\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.77e-01\n",
      "Epoch: 229, Loss: 2480.29118, Residuals: -0.92210, Convergence:   inf\n",
      "Epoch: 230, Loss: 2475.85680, Residuals: -0.91934, Convergence: 0.001791\n",
      "Epoch: 231, Loss: 2472.41200, Residuals: -0.91709, Convergence: 0.001393\n",
      "Epoch: 232, Loss: 2469.60863, Residuals: -0.91549, Convergence: 0.001135\n",
      "Epoch: 233, Loss: 2467.23179, Residuals: -0.91454, Convergence: 0.000963\n",
      "Evidence 14925.229\n",
      "\n",
      "Epoch: 233, Evidence: 14925.22949, Convergence: 0.001972\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.47e-01\n",
      "Epoch: 233, Loss: 2480.51509, Residuals: -0.91454, Convergence:   inf\n",
      "Epoch: 234, Loss: 2477.56945, Residuals: -0.91178, Convergence: 0.001189\n",
      "Epoch: 235, Loss: 2475.21920, Residuals: -0.91017, Convergence: 0.000950\n",
      "Evidence 14935.959\n",
      "\n",
      "Epoch: 235, Evidence: 14935.95898, Convergence: 0.000718\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.25e-01\n",
      "Epoch: 235, Loss: 2480.70811, Residuals: -0.91017, Convergence:   inf\n",
      "Epoch: 236, Loss: 2476.38848, Residuals: -0.90588, Convergence: 0.001744\n",
      "Epoch: 237, Loss: 2473.33044, Residuals: -0.90645, Convergence: 0.001236\n",
      "Epoch: 238, Loss: 2470.80378, Residuals: -0.90742, Convergence: 0.001023\n",
      "Epoch: 239, Loss: 2468.60541, Residuals: -0.91002, Convergence: 0.000891\n",
      "Evidence 14951.383\n",
      "\n",
      "Epoch: 239, Evidence: 14951.38281, Convergence: 0.001749\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.10e-01\n",
      "Epoch: 239, Loss: 2480.13763, Residuals: -0.91002, Convergence:   inf\n",
      "Epoch: 240, Loss: 2478.61504, Residuals: -0.90620, Convergence: 0.000614\n",
      "Evidence 14957.855\n",
      "\n",
      "Epoch: 240, Evidence: 14957.85547, Convergence: 0.000433\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 9.09e-02\n",
      "Epoch: 240, Loss: 2480.94468, Residuals: -0.90620, Convergence:   inf\n",
      "Epoch: 241, Loss: 2519.07558, Residuals: -0.94223, Convergence: -0.015137\n",
      "Epoch: 241, Loss: 2478.90046, Residuals: -0.90327, Convergence: 0.000825\n",
      "Evidence 14961.945\n",
      "\n",
      "Epoch: 241, Evidence: 14961.94531, Convergence: 0.000706\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.42e-02\n",
      "Epoch: 241, Loss: 2480.41215, Residuals: -0.90327, Convergence:   inf\n",
      "Epoch: 242, Loss: 2486.66849, Residuals: -0.90525, Convergence: -0.002516\n",
      "Epoch: 242, Loss: 2480.77831, Residuals: -0.90156, Convergence: -0.000148\n",
      "Evidence 14963.202\n",
      "\n",
      "Epoch: 242, Evidence: 14963.20215, Convergence: 0.000790\n",
      "Total samples: 183, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 381.62284, Residuals: -4.51695, Convergence:   inf\n",
      "Epoch: 1, Loss: 355.93938, Residuals: -4.39620, Convergence: 0.072157\n",
      "Epoch: 2, Loss: 334.92396, Residuals: -4.23201, Convergence: 0.062747\n",
      "Epoch: 3, Loss: 318.86656, Residuals: -4.06721, Convergence: 0.050358\n",
      "Epoch: 4, Loss: 306.65480, Residuals: -3.92304, Convergence: 0.039823\n",
      "Epoch: 5, Loss: 296.98235, Residuals: -3.79614, Convergence: 0.032569\n",
      "Epoch: 6, Loss: 289.13525, Residuals: -3.68580, Convergence: 0.027140\n",
      "Epoch: 7, Loss: 282.62492, Residuals: -3.59136, Convergence: 0.023035\n",
      "Epoch: 8, Loss: 277.09218, Residuals: -3.51077, Convergence: 0.019967\n",
      "Epoch: 9, Loss: 272.28206, Residuals: -3.44164, Convergence: 0.017666\n",
      "Epoch: 10, Loss: 268.01198, Residuals: -3.38184, Convergence: 0.015932\n",
      "Epoch: 11, Loss: 264.14849, Residuals: -3.32953, Convergence: 0.014626\n",
      "Epoch: 12, Loss: 260.59323, Residuals: -3.28318, Convergence: 0.013643\n",
      "Epoch: 13, Loss: 257.27401, Residuals: -3.24149, Convergence: 0.012902\n",
      "Epoch: 14, Loss: 254.13913, Residuals: -3.20335, Convergence: 0.012335\n",
      "Epoch: 15, Loss: 251.15426, Residuals: -3.16786, Convergence: 0.011885\n",
      "Epoch: 16, Loss: 248.30017, Residuals: -3.13440, Convergence: 0.011494\n",
      "Epoch: 17, Loss: 245.56497, Residuals: -3.10259, Convergence: 0.011138\n",
      "Epoch: 18, Loss: 242.93078, Residuals: -3.07207, Convergence: 0.010843\n",
      "Epoch: 19, Loss: 240.36879, Residuals: -3.04233, Convergence: 0.010659\n",
      "Epoch: 20, Loss: 237.84610, Residuals: -3.01281, Convergence: 0.010606\n",
      "Epoch: 21, Loss: 235.33586, Residuals: -2.98300, Convergence: 0.010667\n",
      "Epoch: 22, Loss: 232.82095, Residuals: -2.95261, Convergence: 0.010802\n",
      "Epoch: 23, Loss: 230.28186, Residuals: -2.92140, Convergence: 0.011026\n",
      "Epoch: 24, Loss: 227.67856, Residuals: -2.88889, Convergence: 0.011434\n",
      "Epoch: 25, Loss: 224.95267, Residuals: -2.85438, Convergence: 0.012118\n",
      "Epoch: 26, Loss: 222.08465, Residuals: -2.81754, Convergence: 0.012914\n",
      "Epoch: 27, Loss: 219.18035, Residuals: -2.77944, Convergence: 0.013251\n",
      "Epoch: 28, Loss: 216.36791, Residuals: -2.74155, Convergence: 0.012998\n",
      "Epoch: 29, Loss: 213.67850, Residuals: -2.70441, Convergence: 0.012586\n",
      "Epoch: 30, Loss: 211.09813, Residuals: -2.66802, Convergence: 0.012224\n",
      "Epoch: 31, Loss: 208.60811, Residuals: -2.63229, Convergence: 0.011936\n",
      "Epoch: 32, Loss: 206.19358, Residuals: -2.59715, Convergence: 0.011710\n",
      "Epoch: 33, Loss: 203.84377, Residuals: -2.56252, Convergence: 0.011528\n",
      "Epoch: 34, Loss: 201.55108, Residuals: -2.52835, Convergence: 0.011375\n",
      "Epoch: 35, Loss: 199.31021, Residuals: -2.49460, Convergence: 0.011243\n",
      "Epoch: 36, Loss: 197.11749, Residuals: -2.46122, Convergence: 0.011124\n",
      "Epoch: 37, Loss: 194.97039, Residuals: -2.42816, Convergence: 0.011012\n",
      "Epoch: 38, Loss: 192.86718, Residuals: -2.39538, Convergence: 0.010905\n",
      "Epoch: 39, Loss: 190.80663, Residuals: -2.36284, Convergence: 0.010799\n",
      "Epoch: 40, Loss: 188.78783, Residuals: -2.33051, Convergence: 0.010693\n",
      "Epoch: 41, Loss: 186.81013, Residuals: -2.29834, Convergence: 0.010587\n",
      "Epoch: 42, Loss: 184.87306, Residuals: -2.26631, Convergence: 0.010478\n",
      "Epoch: 43, Loss: 182.97645, Residuals: -2.23439, Convergence: 0.010365\n",
      "Epoch: 44, Loss: 181.12059, Residuals: -2.20256, Convergence: 0.010247\n",
      "Epoch: 45, Loss: 179.30634, Residuals: -2.17082, Convergence: 0.010118\n",
      "Epoch: 46, Loss: 177.53537, Residuals: -2.13917, Convergence: 0.009975\n",
      "Epoch: 47, Loss: 175.81012, Residuals: -2.10765, Convergence: 0.009813\n",
      "Epoch: 48, Loss: 174.13372, Residuals: -2.07630, Convergence: 0.009627\n",
      "Epoch: 49, Loss: 172.50952, Residuals: -2.04518, Convergence: 0.009415\n",
      "Epoch: 50, Loss: 170.94069, Residuals: -2.01436, Convergence: 0.009178\n",
      "Epoch: 51, Loss: 169.42975, Residuals: -1.98392, Convergence: 0.008918\n",
      "Epoch: 52, Loss: 167.97830, Residuals: -1.95393, Convergence: 0.008641\n",
      "Epoch: 53, Loss: 166.58698, Residuals: -1.92444, Convergence: 0.008352\n",
      "Epoch: 54, Loss: 165.25557, Residuals: -1.89551, Convergence: 0.008057\n",
      "Epoch: 55, Loss: 163.98313, Residuals: -1.86717, Convergence: 0.007760\n",
      "Epoch: 56, Loss: 162.76819, Residuals: -1.83947, Convergence: 0.007464\n",
      "Epoch: 57, Loss: 161.60894, Residuals: -1.81242, Convergence: 0.007173\n",
      "Epoch: 58, Loss: 160.50330, Residuals: -1.78605, Convergence: 0.006889\n",
      "Epoch: 59, Loss: 159.44910, Residuals: -1.76037, Convergence: 0.006611\n",
      "Epoch: 60, Loss: 158.44410, Residuals: -1.73539, Convergence: 0.006343\n",
      "Epoch: 61, Loss: 157.48606, Residuals: -1.71112, Convergence: 0.006083\n",
      "Epoch: 62, Loss: 156.57281, Residuals: -1.68756, Convergence: 0.005833\n",
      "Epoch: 63, Loss: 155.70222, Residuals: -1.66471, Convergence: 0.005591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 64, Loss: 154.87224, Residuals: -1.64257, Convergence: 0.005359\n",
      "Epoch: 65, Loss: 154.08093, Residuals: -1.62114, Convergence: 0.005136\n",
      "Epoch: 66, Loss: 153.32647, Residuals: -1.60041, Convergence: 0.004921\n",
      "Epoch: 67, Loss: 152.60709, Residuals: -1.58038, Convergence: 0.004714\n",
      "Epoch: 68, Loss: 151.92118, Residuals: -1.56103, Convergence: 0.004515\n",
      "Epoch: 69, Loss: 151.26717, Residuals: -1.54236, Convergence: 0.004324\n",
      "Epoch: 70, Loss: 150.64364, Residuals: -1.52435, Convergence: 0.004139\n",
      "Epoch: 71, Loss: 150.04917, Residuals: -1.50701, Convergence: 0.003962\n",
      "Epoch: 72, Loss: 149.48250, Residuals: -1.49031, Convergence: 0.003791\n",
      "Epoch: 73, Loss: 148.94237, Residuals: -1.47425, Convergence: 0.003626\n",
      "Epoch: 74, Loss: 148.42763, Residuals: -1.45880, Convergence: 0.003468\n",
      "Epoch: 75, Loss: 147.93714, Residuals: -1.44397, Convergence: 0.003315\n",
      "Epoch: 76, Loss: 147.46984, Residuals: -1.42972, Convergence: 0.003169\n",
      "Epoch: 77, Loss: 147.02469, Residuals: -1.41605, Convergence: 0.003028\n",
      "Epoch: 78, Loss: 146.60070, Residuals: -1.40295, Convergence: 0.002892\n",
      "Epoch: 79, Loss: 146.19689, Residuals: -1.39039, Convergence: 0.002762\n",
      "Epoch: 80, Loss: 145.81233, Residuals: -1.37836, Convergence: 0.002637\n",
      "Epoch: 81, Loss: 145.44614, Residuals: -1.36684, Convergence: 0.002518\n",
      "Epoch: 82, Loss: 145.09743, Residuals: -1.35582, Convergence: 0.002403\n",
      "Epoch: 83, Loss: 144.76536, Residuals: -1.34528, Convergence: 0.002294\n",
      "Epoch: 84, Loss: 144.44913, Residuals: -1.33519, Convergence: 0.002189\n",
      "Epoch: 85, Loss: 144.14796, Residuals: -1.32556, Convergence: 0.002089\n",
      "Epoch: 86, Loss: 143.86111, Residuals: -1.31635, Convergence: 0.001994\n",
      "Epoch: 87, Loss: 143.58788, Residuals: -1.30755, Convergence: 0.001903\n",
      "Epoch: 88, Loss: 143.32758, Residuals: -1.29914, Convergence: 0.001816\n",
      "Epoch: 89, Loss: 143.07959, Residuals: -1.29111, Convergence: 0.001733\n",
      "Epoch: 90, Loss: 142.84331, Residuals: -1.28344, Convergence: 0.001654\n",
      "Epoch: 91, Loss: 142.61816, Residuals: -1.27612, Convergence: 0.001579\n",
      "Epoch: 92, Loss: 142.40361, Residuals: -1.26912, Convergence: 0.001507\n",
      "Epoch: 93, Loss: 142.19917, Residuals: -1.26244, Convergence: 0.001438\n",
      "Epoch: 94, Loss: 142.00434, Residuals: -1.25607, Convergence: 0.001372\n",
      "Epoch: 95, Loss: 141.81870, Residuals: -1.24998, Convergence: 0.001309\n",
      "Epoch: 96, Loss: 141.64183, Residuals: -1.24416, Convergence: 0.001249\n",
      "Epoch: 97, Loss: 141.47335, Residuals: -1.23860, Convergence: 0.001191\n",
      "Epoch: 98, Loss: 141.31287, Residuals: -1.23329, Convergence: 0.001136\n",
      "Epoch: 99, Loss: 141.16007, Residuals: -1.22821, Convergence: 0.001082\n",
      "Epoch: 100, Loss: 141.01463, Residuals: -1.22336, Convergence: 0.001031\n",
      "Epoch: 101, Loss: 140.87626, Residuals: -1.21872, Convergence: 0.000982\n",
      "Evidence -182.249\n",
      "\n",
      "Epoch: 101, Evidence: -182.24875, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.24e-01\n",
      "Epoch: 101, Loss: 1375.53819, Residuals: -1.21872, Convergence:   inf\n",
      "Epoch: 102, Loss: 1312.64688, Residuals: -1.25030, Convergence: 0.047912\n",
      "Epoch: 103, Loss: 1264.78432, Residuals: -1.27463, Convergence: 0.037842\n",
      "Epoch: 104, Loss: 1228.63161, Residuals: -1.29155, Convergence: 0.029425\n",
      "Epoch: 105, Loss: 1200.54546, Residuals: -1.30298, Convergence: 0.023394\n",
      "Epoch: 106, Loss: 1177.85350, Residuals: -1.31108, Convergence: 0.019266\n",
      "Epoch: 107, Loss: 1158.99970, Residuals: -1.31704, Convergence: 0.016267\n",
      "Epoch: 108, Loss: 1143.05303, Residuals: -1.32136, Convergence: 0.013951\n",
      "Epoch: 109, Loss: 1129.38845, Residuals: -1.32428, Convergence: 0.012099\n",
      "Epoch: 110, Loss: 1117.54733, Residuals: -1.32597, Convergence: 0.010596\n",
      "Epoch: 111, Loss: 1107.17471, Residuals: -1.32657, Convergence: 0.009369\n",
      "Epoch: 112, Loss: 1097.98851, Residuals: -1.32617, Convergence: 0.008366\n",
      "Epoch: 113, Loss: 1089.76038, Residuals: -1.32489, Convergence: 0.007550\n",
      "Epoch: 114, Loss: 1082.30531, Residuals: -1.32280, Convergence: 0.006888\n",
      "Epoch: 115, Loss: 1075.47286, Residuals: -1.31998, Convergence: 0.006353\n",
      "Epoch: 116, Loss: 1069.14422, Residuals: -1.31650, Convergence: 0.005919\n",
      "Epoch: 117, Loss: 1063.22608, Residuals: -1.31243, Convergence: 0.005566\n",
      "Epoch: 118, Loss: 1057.64890, Residuals: -1.30783, Convergence: 0.005273\n",
      "Epoch: 119, Loss: 1052.35907, Residuals: -1.30276, Convergence: 0.005027\n",
      "Epoch: 120, Loss: 1047.31456, Residuals: -1.29726, Convergence: 0.004817\n",
      "Epoch: 121, Loss: 1042.47879, Residuals: -1.29139, Convergence: 0.004639\n",
      "Epoch: 122, Loss: 1037.81608, Residuals: -1.28516, Convergence: 0.004493\n",
      "Epoch: 123, Loss: 1033.28897, Residuals: -1.27862, Convergence: 0.004381\n",
      "Epoch: 124, Loss: 1028.85791, Residuals: -1.27179, Convergence: 0.004307\n",
      "Epoch: 125, Loss: 1024.48438, Residuals: -1.26469, Convergence: 0.004269\n",
      "Epoch: 126, Loss: 1020.13848, Residuals: -1.25735, Convergence: 0.004260\n",
      "Epoch: 127, Loss: 1015.80897, Residuals: -1.24983, Convergence: 0.004262\n",
      "Epoch: 128, Loss: 1011.51140, Residuals: -1.24218, Convergence: 0.004249\n",
      "Epoch: 129, Loss: 1007.28819, Residuals: -1.23448, Convergence: 0.004193\n",
      "Epoch: 130, Loss: 1003.19435, Residuals: -1.22679, Convergence: 0.004081\n",
      "Epoch: 131, Loss: 999.27966, Residuals: -1.21915, Convergence: 0.003918\n",
      "Epoch: 132, Loss: 995.57573, Residuals: -1.21160, Convergence: 0.003720\n",
      "Epoch: 133, Loss: 992.09278, Residuals: -1.20417, Convergence: 0.003511\n",
      "Epoch: 134, Loss: 988.82621, Residuals: -1.19689, Convergence: 0.003303\n",
      "Epoch: 135, Loss: 985.76262, Residuals: -1.18979, Convergence: 0.003108\n",
      "Epoch: 136, Loss: 982.88504, Residuals: -1.18288, Convergence: 0.002928\n",
      "Epoch: 137, Loss: 980.17664, Residuals: -1.17618, Convergence: 0.002763\n",
      "Epoch: 138, Loss: 977.62194, Residuals: -1.16969, Convergence: 0.002613\n",
      "Epoch: 139, Loss: 975.20658, Residuals: -1.16344, Convergence: 0.002477\n",
      "Epoch: 140, Loss: 972.91796, Residuals: -1.15741, Convergence: 0.002352\n",
      "Epoch: 141, Loss: 970.74477, Residuals: -1.15162, Convergence: 0.002239\n",
      "Epoch: 142, Loss: 968.67724, Residuals: -1.14606, Convergence: 0.002134\n",
      "Epoch: 143, Loss: 966.70526, Residuals: -1.14073, Convergence: 0.002040\n",
      "Epoch: 144, Loss: 964.82133, Residuals: -1.13562, Convergence: 0.001953\n",
      "Epoch: 145, Loss: 963.01747, Residuals: -1.13072, Convergence: 0.001873\n",
      "Epoch: 146, Loss: 961.28694, Residuals: -1.12603, Convergence: 0.001800\n",
      "Epoch: 147, Loss: 959.62277, Residuals: -1.12154, Convergence: 0.001734\n",
      "Epoch: 148, Loss: 958.01963, Residuals: -1.11724, Convergence: 0.001673\n",
      "Epoch: 149, Loss: 956.47160, Residuals: -1.11311, Convergence: 0.001618\n",
      "Epoch: 150, Loss: 954.97408, Residuals: -1.10916, Convergence: 0.001568\n",
      "Epoch: 151, Loss: 953.52218, Residuals: -1.10536, Convergence: 0.001523\n",
      "Epoch: 152, Loss: 952.11138, Residuals: -1.10171, Convergence: 0.001482\n",
      "Epoch: 153, Loss: 950.73754, Residuals: -1.09821, Convergence: 0.001445\n",
      "Epoch: 154, Loss: 949.39651, Residuals: -1.09482, Convergence: 0.001413\n",
      "Epoch: 155, Loss: 948.08371, Residuals: -1.09156, Convergence: 0.001385\n",
      "Epoch: 156, Loss: 946.79529, Residuals: -1.08840, Convergence: 0.001361\n",
      "Epoch: 157, Loss: 945.52684, Residuals: -1.08534, Convergence: 0.001342\n",
      "Epoch: 158, Loss: 944.27360, Residuals: -1.08236, Convergence: 0.001327\n",
      "Epoch: 159, Loss: 943.03197, Residuals: -1.07945, Convergence: 0.001317\n",
      "Epoch: 160, Loss: 941.79731, Residuals: -1.07661, Convergence: 0.001311\n",
      "Epoch: 161, Loss: 940.56700, Residuals: -1.07381, Convergence: 0.001308\n",
      "Epoch: 162, Loss: 939.33913, Residuals: -1.07106, Convergence: 0.001307\n",
      "Epoch: 163, Loss: 938.11394, Residuals: -1.06835, Convergence: 0.001306\n",
      "Epoch: 164, Loss: 936.89343, Residuals: -1.06567, Convergence: 0.001303\n",
      "Epoch: 165, Loss: 935.68192, Residuals: -1.06304, Convergence: 0.001295\n",
      "Epoch: 166, Loss: 934.48532, Residuals: -1.06045, Convergence: 0.001280\n",
      "Epoch: 167, Loss: 933.31023, Residuals: -1.05791, Convergence: 0.001259\n",
      "Epoch: 168, Loss: 932.16343, Residuals: -1.05544, Convergence: 0.001230\n",
      "Epoch: 169, Loss: 931.05086, Residuals: -1.05305, Convergence: 0.001195\n",
      "Epoch: 170, Loss: 929.97714, Residuals: -1.05073, Convergence: 0.001155\n",
      "Epoch: 171, Loss: 928.94604, Residuals: -1.04849, Convergence: 0.001110\n",
      "Epoch: 172, Loss: 927.95897, Residuals: -1.04634, Convergence: 0.001064\n",
      "Epoch: 173, Loss: 927.01753, Residuals: -1.04428, Convergence: 0.001016\n",
      "Epoch: 174, Loss: 926.12098, Residuals: -1.04231, Convergence: 0.000968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence 11295.919\n",
      "\n",
      "Epoch: 174, Evidence: 11295.91895, Convergence: 1.016134\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 5.72e-01\n",
      "Epoch: 174, Loss: 2343.95169, Residuals: -1.04231, Convergence:   inf\n",
      "Epoch: 175, Loss: 2301.33043, Residuals: -1.05316, Convergence: 0.018520\n",
      "Epoch: 176, Loss: 2272.36866, Residuals: -1.05218, Convergence: 0.012745\n",
      "Epoch: 177, Loss: 2248.40642, Residuals: -1.05023, Convergence: 0.010657\n",
      "Epoch: 178, Loss: 2228.27621, Residuals: -1.04806, Convergence: 0.009034\n",
      "Epoch: 179, Loss: 2211.23581, Residuals: -1.04576, Convergence: 0.007706\n",
      "Epoch: 180, Loss: 2196.71174, Residuals: -1.04339, Convergence: 0.006612\n",
      "Epoch: 181, Loss: 2184.23506, Residuals: -1.04096, Convergence: 0.005712\n",
      "Epoch: 182, Loss: 2173.41679, Residuals: -1.03849, Convergence: 0.004978\n",
      "Epoch: 183, Loss: 2163.93222, Residuals: -1.03597, Convergence: 0.004383\n",
      "Epoch: 184, Loss: 2155.50885, Residuals: -1.03341, Convergence: 0.003908\n",
      "Epoch: 185, Loss: 2147.92138, Residuals: -1.03078, Convergence: 0.003532\n",
      "Epoch: 186, Loss: 2140.98943, Residuals: -1.02807, Convergence: 0.003238\n",
      "Epoch: 187, Loss: 2134.57638, Residuals: -1.02525, Convergence: 0.003004\n",
      "Epoch: 188, Loss: 2128.59318, Residuals: -1.02234, Convergence: 0.002811\n",
      "Epoch: 189, Loss: 2122.98523, Residuals: -1.01936, Convergence: 0.002642\n",
      "Epoch: 190, Loss: 2117.72495, Residuals: -1.01634, Convergence: 0.002484\n",
      "Epoch: 191, Loss: 2112.79433, Residuals: -1.01332, Convergence: 0.002334\n",
      "Epoch: 192, Loss: 2108.17921, Residuals: -1.01035, Convergence: 0.002189\n",
      "Epoch: 193, Loss: 2103.86512, Residuals: -1.00745, Convergence: 0.002051\n",
      "Epoch: 194, Loss: 2099.83618, Residuals: -1.00466, Convergence: 0.001919\n",
      "Epoch: 195, Loss: 2096.07803, Residuals: -1.00198, Convergence: 0.001793\n",
      "Epoch: 196, Loss: 2092.57291, Residuals: -0.99943, Convergence: 0.001675\n",
      "Epoch: 197, Loss: 2089.30792, Residuals: -0.99701, Convergence: 0.001563\n",
      "Epoch: 198, Loss: 2086.26526, Residuals: -0.99471, Convergence: 0.001458\n",
      "Epoch: 199, Loss: 2083.43142, Residuals: -0.99254, Convergence: 0.001360\n",
      "Epoch: 200, Loss: 2080.79112, Residuals: -0.99049, Convergence: 0.001269\n",
      "Epoch: 201, Loss: 2078.33025, Residuals: -0.98855, Convergence: 0.001184\n",
      "Epoch: 202, Loss: 2076.03474, Residuals: -0.98672, Convergence: 0.001106\n",
      "Epoch: 203, Loss: 2073.89074, Residuals: -0.98499, Convergence: 0.001034\n",
      "Epoch: 204, Loss: 2071.88683, Residuals: -0.98335, Convergence: 0.000967\n",
      "Evidence 14550.359\n",
      "\n",
      "Epoch: 204, Evidence: 14550.35938, Convergence: 0.223667\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 4.33e-01\n",
      "Epoch: 204, Loss: 2459.25713, Residuals: -0.98335, Convergence:   inf\n",
      "Epoch: 205, Loss: 2444.98951, Residuals: -0.98054, Convergence: 0.005835\n",
      "Epoch: 206, Loss: 2433.38377, Residuals: -0.97703, Convergence: 0.004769\n",
      "Epoch: 207, Loss: 2423.38235, Residuals: -0.97366, Convergence: 0.004127\n",
      "Epoch: 208, Loss: 2414.72009, Residuals: -0.97054, Convergence: 0.003587\n",
      "Epoch: 209, Loss: 2407.18887, Residuals: -0.96769, Convergence: 0.003129\n",
      "Epoch: 210, Loss: 2400.61517, Residuals: -0.96508, Convergence: 0.002738\n",
      "Epoch: 211, Loss: 2394.85165, Residuals: -0.96271, Convergence: 0.002407\n",
      "Epoch: 212, Loss: 2389.77296, Residuals: -0.96057, Convergence: 0.002125\n",
      "Epoch: 213, Loss: 2385.27285, Residuals: -0.95862, Convergence: 0.001887\n",
      "Epoch: 214, Loss: 2381.26162, Residuals: -0.95686, Convergence: 0.001684\n",
      "Epoch: 215, Loss: 2377.66502, Residuals: -0.95526, Convergence: 0.001513\n",
      "Epoch: 216, Loss: 2374.42017, Residuals: -0.95381, Convergence: 0.001367\n",
      "Epoch: 217, Loss: 2371.47686, Residuals: -0.95250, Convergence: 0.001241\n",
      "Epoch: 218, Loss: 2368.78910, Residuals: -0.95131, Convergence: 0.001135\n",
      "Epoch: 219, Loss: 2366.32286, Residuals: -0.95023, Convergence: 0.001042\n",
      "Epoch: 220, Loss: 2364.04879, Residuals: -0.94925, Convergence: 0.000962\n",
      "Evidence 14965.082\n",
      "\n",
      "Epoch: 220, Evidence: 14965.08203, Convergence: 0.027713\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 3.31e-01\n",
      "Epoch: 220, Loss: 2463.78986, Residuals: -0.94925, Convergence:   inf\n",
      "Epoch: 221, Loss: 2457.07387, Residuals: -0.94594, Convergence: 0.002733\n",
      "Epoch: 222, Loss: 2451.51251, Residuals: -0.94312, Convergence: 0.002269\n",
      "Epoch: 223, Loss: 2446.78855, Residuals: -0.94080, Convergence: 0.001931\n",
      "Epoch: 224, Loss: 2442.72491, Residuals: -0.93890, Convergence: 0.001664\n",
      "Epoch: 225, Loss: 2439.18442, Residuals: -0.93734, Convergence: 0.001452\n",
      "Epoch: 226, Loss: 2436.06282, Residuals: -0.93607, Convergence: 0.001281\n",
      "Epoch: 227, Loss: 2433.27802, Residuals: -0.93503, Convergence: 0.001144\n",
      "Epoch: 228, Loss: 2430.76580, Residuals: -0.93417, Convergence: 0.001034\n",
      "Epoch: 229, Loss: 2428.47789, Residuals: -0.93348, Convergence: 0.000942\n",
      "Evidence 15052.667\n",
      "\n",
      "Epoch: 229, Evidence: 15052.66699, Convergence: 0.005819\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.59e-01\n",
      "Epoch: 229, Loss: 2465.14224, Residuals: -0.93348, Convergence:   inf\n",
      "Epoch: 230, Loss: 2461.12273, Residuals: -0.93112, Convergence: 0.001633\n",
      "Epoch: 231, Loss: 2457.76365, Residuals: -0.92935, Convergence: 0.001367\n",
      "Epoch: 232, Loss: 2454.87776, Residuals: -0.92802, Convergence: 0.001176\n",
      "Epoch: 233, Loss: 2452.35201, Residuals: -0.92701, Convergence: 0.001030\n",
      "Epoch: 234, Loss: 2450.10499, Residuals: -0.92625, Convergence: 0.000917\n",
      "Evidence 15084.826\n",
      "\n",
      "Epoch: 234, Evidence: 15084.82617, Convergence: 0.002132\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.08e-01\n",
      "Epoch: 234, Loss: 2465.94492, Residuals: -0.92625, Convergence:   inf\n",
      "Epoch: 235, Loss: 2463.01561, Residuals: -0.92451, Convergence: 0.001189\n",
      "Epoch: 236, Loss: 2460.53833, Residuals: -0.92329, Convergence: 0.001007\n",
      "Epoch: 237, Loss: 2458.38046, Residuals: -0.92243, Convergence: 0.000878\n",
      "Evidence 15100.186\n",
      "\n",
      "Epoch: 237, Evidence: 15100.18555, Convergence: 0.001017\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.72e-01\n",
      "Epoch: 237, Loss: 2466.52052, Residuals: -0.92243, Convergence:   inf\n",
      "Epoch: 238, Loss: 2464.13145, Residuals: -0.92098, Convergence: 0.000970\n",
      "Evidence 15106.736\n",
      "\n",
      "Epoch: 238, Evidence: 15106.73633, Convergence: 0.000434\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.47e-01\n",
      "Epoch: 238, Loss: 2466.99958, Residuals: -0.92098, Convergence:   inf\n",
      "Epoch: 239, Loss: 2462.92029, Residuals: -0.91875, Convergence: 0.001656\n",
      "Epoch: 240, Loss: 2459.75130, Residuals: -0.91795, Convergence: 0.001288\n",
      "Epoch: 241, Loss: 2457.14473, Residuals: -0.91768, Convergence: 0.001061\n",
      "Epoch: 242, Loss: 2454.90383, Residuals: -0.91779, Convergence: 0.000913\n",
      "Evidence 15121.969\n",
      "\n",
      "Epoch: 242, Evidence: 15121.96875, Convergence: 0.001441\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.23e-01\n",
      "Epoch: 242, Loss: 2467.06075, Residuals: -0.91779, Convergence:   inf\n",
      "Epoch: 243, Loss: 2464.29700, Residuals: -0.91607, Convergence: 0.001122\n",
      "Epoch: 244, Loss: 2462.07429, Residuals: -0.91557, Convergence: 0.000903\n",
      "Evidence 15131.758\n",
      "\n",
      "Epoch: 244, Evidence: 15131.75781, Convergence: 0.000647\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.06e-01\n",
      "Epoch: 244, Loss: 2467.13882, Residuals: -0.91557, Convergence:   inf\n",
      "Epoch: 245, Loss: 2463.18084, Residuals: -0.91298, Convergence: 0.001607\n",
      "Epoch: 246, Loss: 2460.13817, Residuals: -0.91470, Convergence: 0.001237\n",
      "Epoch: 247, Loss: 2457.40788, Residuals: -0.91604, Convergence: 0.001111\n",
      "Epoch: 248, Loss: 2455.06447, Residuals: -0.91907, Convergence: 0.000955\n",
      "Evidence 15146.467\n",
      "\n",
      "Epoch: 248, Evidence: 15146.46680, Convergence: 0.001617\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 9.45e-02\n",
      "Epoch: 248, Loss: 2466.26450, Residuals: -0.91907, Convergence:   inf\n",
      "Epoch: 249, Loss: 2464.75106, Residuals: -0.91708, Convergence: 0.000614\n",
      "Evidence 15152.821\n",
      "\n",
      "Epoch: 249, Evidence: 15152.82129, Convergence: 0.000419\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.85e-02\n",
      "Epoch: 249, Loss: 2467.06080, Residuals: -0.91708, Convergence:   inf\n",
      "Epoch: 250, Loss: 2516.35840, Residuals: -0.96157, Convergence: -0.019591\n",
      "Epoch: 250, Loss: 2465.53133, Residuals: -0.91469, Convergence: 0.000620\n",
      "Evidence 15156.078\n",
      "\n",
      "Epoch: 250, Evidence: 15156.07812, Convergence: 0.000634\n",
      "Updating hyper-parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 183, Updated regularization: 7.26e-02\n",
      "Epoch: 250, Loss: 2466.42169, Residuals: -0.91469, Convergence:   inf\n",
      "Epoch: 251, Loss: 2471.99493, Residuals: -0.91763, Convergence: -0.002255\n",
      "Epoch: 251, Loss: 2466.73833, Residuals: -0.91357, Convergence: -0.000128\n",
      "Evidence 15157.287\n",
      "\n",
      "Epoch: 251, Evidence: 15157.28711, Convergence: 0.000714\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 383.11659, Residuals: -4.52739, Convergence:   inf\n",
      "Epoch: 1, Loss: 357.46452, Residuals: -4.40723, Convergence: 0.071761\n",
      "Epoch: 2, Loss: 336.39463, Residuals: -4.24254, Convergence: 0.062634\n",
      "Epoch: 3, Loss: 320.26515, Residuals: -4.07641, Convergence: 0.050363\n",
      "Epoch: 4, Loss: 307.98363, Residuals: -3.93012, Convergence: 0.039877\n",
      "Epoch: 5, Loss: 298.25493, Residuals: -3.80061, Convergence: 0.032619\n",
      "Epoch: 6, Loss: 290.36729, Residuals: -3.68756, Convergence: 0.027164\n",
      "Epoch: 7, Loss: 283.82762, Residuals: -3.59058, Convergence: 0.023041\n",
      "Epoch: 8, Loss: 278.27004, Residuals: -3.50770, Convergence: 0.019972\n",
      "Epoch: 9, Loss: 273.43663, Residuals: -3.43662, Convergence: 0.017677\n",
      "Epoch: 10, Loss: 269.14463, Residuals: -3.37525, Convergence: 0.015947\n",
      "Epoch: 11, Loss: 265.26240, Residuals: -3.32184, Convergence: 0.014635\n",
      "Epoch: 12, Loss: 261.69462, Residuals: -3.27487, Convergence: 0.013633\n",
      "Epoch: 13, Loss: 258.37271, Residuals: -3.23304, Convergence: 0.012857\n",
      "Epoch: 14, Loss: 255.24803, Residuals: -3.19517, Convergence: 0.012242\n",
      "Epoch: 15, Loss: 252.28789, Residuals: -3.16028, Convergence: 0.011733\n",
      "Epoch: 16, Loss: 249.47372, Residuals: -3.12764, Convergence: 0.011280\n",
      "Epoch: 17, Loss: 246.79493, Residuals: -3.09681, Convergence: 0.010854\n",
      "Epoch: 18, Loss: 244.23588, Residuals: -3.06743, Convergence: 0.010478\n",
      "Epoch: 19, Loss: 241.76937, Residuals: -3.03906, Convergence: 0.010202\n",
      "Epoch: 20, Loss: 239.36153, Residuals: -3.01118, Convergence: 0.010059\n",
      "Epoch: 21, Loss: 236.97998, Residuals: -2.98327, Convergence: 0.010050\n",
      "Epoch: 22, Loss: 234.59998, Residuals: -2.95496, Convergence: 0.010145\n",
      "Epoch: 23, Loss: 232.20210, Residuals: -2.92599, Convergence: 0.010327\n",
      "Epoch: 24, Loss: 229.75778, Residuals: -2.89608, Convergence: 0.010639\n",
      "Epoch: 25, Loss: 227.21832, Residuals: -2.86466, Convergence: 0.011176\n",
      "Epoch: 26, Loss: 224.53803, Residuals: -2.83119, Convergence: 0.011937\n",
      "Epoch: 27, Loss: 221.75582, Residuals: -2.79599, Convergence: 0.012546\n",
      "Epoch: 28, Loss: 218.99832, Residuals: -2.76043, Convergence: 0.012591\n",
      "Epoch: 29, Loss: 216.33339, Residuals: -2.72534, Convergence: 0.012319\n",
      "Epoch: 30, Loss: 213.75896, Residuals: -2.69082, Convergence: 0.012044\n",
      "Epoch: 31, Loss: 211.25652, Residuals: -2.65672, Convergence: 0.011845\n",
      "Epoch: 32, Loss: 208.81005, Residuals: -2.62287, Convergence: 0.011716\n",
      "Epoch: 33, Loss: 206.40883, Residuals: -2.58917, Convergence: 0.011633\n",
      "Epoch: 34, Loss: 204.04691, Residuals: -2.55552, Convergence: 0.011575\n",
      "Epoch: 35, Loss: 201.72190, Residuals: -2.52189, Convergence: 0.011526\n",
      "Epoch: 36, Loss: 199.43384, Residuals: -2.48825, Convergence: 0.011473\n",
      "Epoch: 37, Loss: 197.18428, Residuals: -2.45459, Convergence: 0.011408\n",
      "Epoch: 38, Loss: 194.97560, Residuals: -2.42093, Convergence: 0.011328\n",
      "Epoch: 39, Loss: 192.81039, Residuals: -2.38729, Convergence: 0.011230\n",
      "Epoch: 40, Loss: 190.69129, Residuals: -2.35369, Convergence: 0.011113\n",
      "Epoch: 41, Loss: 188.62072, Residuals: -2.32017, Convergence: 0.010977\n",
      "Epoch: 42, Loss: 186.60093, Residuals: -2.28677, Convergence: 0.010824\n",
      "Epoch: 43, Loss: 184.63399, Residuals: -2.25351, Convergence: 0.010653\n",
      "Epoch: 44, Loss: 182.72185, Residuals: -2.22045, Convergence: 0.010465\n",
      "Epoch: 45, Loss: 180.86634, Residuals: -2.18764, Convergence: 0.010259\n",
      "Epoch: 46, Loss: 179.06908, Residuals: -2.15513, Convergence: 0.010037\n",
      "Epoch: 47, Loss: 177.33142, Residuals: -2.12296, Convergence: 0.009799\n",
      "Epoch: 48, Loss: 175.65428, Residuals: -2.09118, Convergence: 0.009548\n",
      "Epoch: 49, Loss: 174.03812, Residuals: -2.05984, Convergence: 0.009286\n",
      "Epoch: 50, Loss: 172.48293, Residuals: -2.02897, Convergence: 0.009017\n",
      "Epoch: 51, Loss: 170.98822, Residuals: -1.99862, Convergence: 0.008742\n",
      "Epoch: 52, Loss: 169.55320, Residuals: -1.96880, Convergence: 0.008464\n",
      "Epoch: 53, Loss: 168.17680, Residuals: -1.93956, Convergence: 0.008184\n",
      "Epoch: 54, Loss: 166.85774, Residuals: -1.91093, Convergence: 0.007905\n",
      "Epoch: 55, Loss: 165.59458, Residuals: -1.88292, Convergence: 0.007628\n",
      "Epoch: 56, Loss: 164.38571, Residuals: -1.85556, Convergence: 0.007354\n",
      "Epoch: 57, Loss: 163.22937, Residuals: -1.82887, Convergence: 0.007084\n",
      "Epoch: 58, Loss: 162.12362, Residuals: -1.80287, Convergence: 0.006820\n",
      "Epoch: 59, Loss: 161.06636, Residuals: -1.77757, Convergence: 0.006564\n",
      "Epoch: 60, Loss: 160.05538, Residuals: -1.75296, Convergence: 0.006316\n",
      "Epoch: 61, Loss: 159.08842, Residuals: -1.72904, Convergence: 0.006078\n",
      "Epoch: 62, Loss: 158.16324, Residuals: -1.70580, Convergence: 0.005850\n",
      "Epoch: 63, Loss: 157.27773, Residuals: -1.68323, Convergence: 0.005630\n",
      "Epoch: 64, Loss: 156.43002, Residuals: -1.66131, Convergence: 0.005419\n",
      "Epoch: 65, Loss: 155.61846, Residuals: -1.64003, Convergence: 0.005215\n",
      "Epoch: 66, Loss: 154.84167, Residuals: -1.61938, Convergence: 0.005017\n",
      "Epoch: 67, Loss: 154.09847, Residuals: -1.59936, Convergence: 0.004823\n",
      "Epoch: 68, Loss: 153.38787, Residuals: -1.57995, Convergence: 0.004633\n",
      "Epoch: 69, Loss: 152.70896, Residuals: -1.56115, Convergence: 0.004446\n",
      "Epoch: 70, Loss: 152.06090, Residuals: -1.54296, Convergence: 0.004262\n",
      "Epoch: 71, Loss: 151.44288, Residuals: -1.52538, Convergence: 0.004081\n",
      "Epoch: 72, Loss: 150.85404, Residuals: -1.50840, Convergence: 0.003903\n",
      "Epoch: 73, Loss: 150.29352, Residuals: -1.49202, Convergence: 0.003729\n",
      "Epoch: 74, Loss: 149.76039, Residuals: -1.47623, Convergence: 0.003560\n",
      "Epoch: 75, Loss: 149.25372, Residuals: -1.46102, Convergence: 0.003395\n",
      "Epoch: 76, Loss: 148.77250, Residuals: -1.44639, Convergence: 0.003235\n",
      "Epoch: 77, Loss: 148.31573, Residuals: -1.43232, Convergence: 0.003080\n",
      "Epoch: 78, Loss: 147.88240, Residuals: -1.41881, Convergence: 0.002930\n",
      "Epoch: 79, Loss: 147.47149, Residuals: -1.40584, Convergence: 0.002786\n",
      "Epoch: 80, Loss: 147.08199, Residuals: -1.39339, Convergence: 0.002648\n",
      "Epoch: 81, Loss: 146.71294, Residuals: -1.38146, Convergence: 0.002515\n",
      "Epoch: 82, Loss: 146.36339, Residuals: -1.37002, Convergence: 0.002388\n",
      "Epoch: 83, Loss: 146.03244, Residuals: -1.35907, Convergence: 0.002266\n",
      "Epoch: 84, Loss: 145.71925, Residuals: -1.34858, Convergence: 0.002149\n",
      "Epoch: 85, Loss: 145.42303, Residuals: -1.33856, Convergence: 0.002037\n",
      "Epoch: 86, Loss: 145.14301, Residuals: -1.32897, Convergence: 0.001929\n",
      "Epoch: 87, Loss: 144.87850, Residuals: -1.31981, Convergence: 0.001826\n",
      "Epoch: 88, Loss: 144.62883, Residuals: -1.31107, Convergence: 0.001726\n",
      "Epoch: 89, Loss: 144.39338, Residuals: -1.30274, Convergence: 0.001631\n",
      "Epoch: 90, Loss: 144.17152, Residuals: -1.29480, Convergence: 0.001539\n",
      "Epoch: 91, Loss: 143.96268, Residuals: -1.28724, Convergence: 0.001451\n",
      "Epoch: 92, Loss: 143.76623, Residuals: -1.28006, Convergence: 0.001366\n",
      "Epoch: 93, Loss: 143.58155, Residuals: -1.27325, Convergence: 0.001286\n",
      "Epoch: 94, Loss: 143.40790, Residuals: -1.26681, Convergence: 0.001211\n",
      "Epoch: 95, Loss: 143.24452, Residuals: -1.26071, Convergence: 0.001141\n",
      "Epoch: 96, Loss: 143.09054, Residuals: -1.25495, Convergence: 0.001076\n",
      "Epoch: 97, Loss: 142.94495, Residuals: -1.24952, Convergence: 0.001018\n",
      "Epoch: 98, Loss: 142.80667, Residuals: -1.24440, Convergence: 0.000968\n",
      "Evidence -184.350\n",
      "\n",
      "Epoch: 98, Evidence: -184.35013, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.24e-01\n",
      "Epoch: 98, Loss: 1367.03183, Residuals: -1.24440, Convergence:   inf\n",
      "Epoch: 99, Loss: 1305.26723, Residuals: -1.27420, Convergence: 0.047320\n",
      "Epoch: 100, Loss: 1258.44884, Residuals: -1.29778, Convergence: 0.037203\n",
      "Epoch: 101, Loss: 1223.19933, Residuals: -1.31468, Convergence: 0.028817\n",
      "Epoch: 102, Loss: 1195.79898, Residuals: -1.32636, Convergence: 0.022914\n",
      "Epoch: 103, Loss: 1173.62505, Residuals: -1.33475, Convergence: 0.018894\n",
      "Epoch: 104, Loss: 1155.19333, Residuals: -1.34093, Convergence: 0.015956\n",
      "Epoch: 105, Loss: 1139.61088, Residuals: -1.34543, Convergence: 0.013673\n",
      "Epoch: 106, Loss: 1126.27026, Residuals: -1.34853, Convergence: 0.011845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 107, Loss: 1114.72030, Residuals: -1.35043, Convergence: 0.010361\n",
      "Epoch: 108, Loss: 1104.60886, Residuals: -1.35126, Convergence: 0.009154\n",
      "Epoch: 109, Loss: 1095.65058, Residuals: -1.35113, Convergence: 0.008176\n",
      "Epoch: 110, Loss: 1087.60934, Residuals: -1.35012, Convergence: 0.007393\n",
      "Epoch: 111, Loss: 1080.28426, Residuals: -1.34831, Convergence: 0.006781\n",
      "Epoch: 112, Loss: 1073.49976, Residuals: -1.34573, Convergence: 0.006320\n",
      "Epoch: 113, Loss: 1067.10182, Residuals: -1.34239, Convergence: 0.005996\n",
      "Epoch: 114, Loss: 1060.95319, Residuals: -1.33831, Convergence: 0.005795\n",
      "Epoch: 115, Loss: 1054.93810, Residuals: -1.33351, Convergence: 0.005702\n",
      "Epoch: 116, Loss: 1048.96903, Residuals: -1.32801, Convergence: 0.005690\n",
      "Epoch: 117, Loss: 1043.00230, Residuals: -1.32186, Convergence: 0.005721\n",
      "Epoch: 118, Loss: 1037.05177, Residuals: -1.31515, Convergence: 0.005738\n",
      "Epoch: 119, Loss: 1031.18585, Residuals: -1.30799, Convergence: 0.005689\n",
      "Epoch: 120, Loss: 1025.50251, Residuals: -1.30046, Convergence: 0.005542\n",
      "Epoch: 121, Loss: 1020.09129, Residuals: -1.29267, Convergence: 0.005305\n",
      "Epoch: 122, Loss: 1015.00729, Residuals: -1.28470, Convergence: 0.005009\n",
      "Epoch: 123, Loss: 1010.26637, Residuals: -1.27662, Convergence: 0.004693\n",
      "Epoch: 124, Loss: 1005.85735, Residuals: -1.26852, Convergence: 0.004383\n",
      "Epoch: 125, Loss: 1001.75429, Residuals: -1.26043, Convergence: 0.004096\n",
      "Epoch: 126, Loss: 997.92657, Residuals: -1.25242, Convergence: 0.003836\n",
      "Epoch: 127, Loss: 994.34387, Residuals: -1.24451, Convergence: 0.003603\n",
      "Epoch: 128, Loss: 990.97866, Residuals: -1.23675, Convergence: 0.003396\n",
      "Epoch: 129, Loss: 987.80739, Residuals: -1.22915, Convergence: 0.003210\n",
      "Epoch: 130, Loss: 984.81041, Residuals: -1.22175, Convergence: 0.003043\n",
      "Epoch: 131, Loss: 981.97102, Residuals: -1.21456, Convergence: 0.002892\n",
      "Epoch: 132, Loss: 979.27579, Residuals: -1.20759, Convergence: 0.002752\n",
      "Epoch: 133, Loss: 976.71315, Residuals: -1.20085, Convergence: 0.002624\n",
      "Epoch: 134, Loss: 974.27371, Residuals: -1.19437, Convergence: 0.002504\n",
      "Epoch: 135, Loss: 971.94908, Residuals: -1.18813, Convergence: 0.002392\n",
      "Epoch: 136, Loss: 969.73231, Residuals: -1.18214, Convergence: 0.002286\n",
      "Epoch: 137, Loss: 967.61620, Residuals: -1.17640, Convergence: 0.002187\n",
      "Epoch: 138, Loss: 965.59550, Residuals: -1.17092, Convergence: 0.002093\n",
      "Epoch: 139, Loss: 963.66466, Residuals: -1.16568, Convergence: 0.002004\n",
      "Epoch: 140, Loss: 961.81753, Residuals: -1.16067, Convergence: 0.001920\n",
      "Epoch: 141, Loss: 960.05001, Residuals: -1.15590, Convergence: 0.001841\n",
      "Epoch: 142, Loss: 958.35703, Residuals: -1.15135, Convergence: 0.001767\n",
      "Epoch: 143, Loss: 956.73403, Residuals: -1.14702, Convergence: 0.001696\n",
      "Epoch: 144, Loss: 955.17696, Residuals: -1.14289, Convergence: 0.001630\n",
      "Epoch: 145, Loss: 953.68196, Residuals: -1.13895, Convergence: 0.001568\n",
      "Epoch: 146, Loss: 952.24535, Residuals: -1.13520, Convergence: 0.001509\n",
      "Epoch: 147, Loss: 950.86325, Residuals: -1.13163, Convergence: 0.001454\n",
      "Epoch: 148, Loss: 949.53284, Residuals: -1.12822, Convergence: 0.001401\n",
      "Epoch: 149, Loss: 948.25121, Residuals: -1.12497, Convergence: 0.001352\n",
      "Epoch: 150, Loss: 947.01549, Residuals: -1.12186, Convergence: 0.001305\n",
      "Epoch: 151, Loss: 945.82305, Residuals: -1.11889, Convergence: 0.001261\n",
      "Epoch: 152, Loss: 944.67133, Residuals: -1.11605, Convergence: 0.001219\n",
      "Epoch: 153, Loss: 943.55815, Residuals: -1.11333, Convergence: 0.001180\n",
      "Epoch: 154, Loss: 942.48091, Residuals: -1.11073, Convergence: 0.001143\n",
      "Epoch: 155, Loss: 941.43736, Residuals: -1.10823, Convergence: 0.001108\n",
      "Epoch: 156, Loss: 940.42526, Residuals: -1.10582, Convergence: 0.001076\n",
      "Epoch: 157, Loss: 939.44171, Residuals: -1.10351, Convergence: 0.001047\n",
      "Epoch: 158, Loss: 938.48469, Residuals: -1.10129, Convergence: 0.001020\n",
      "Epoch: 159, Loss: 937.55128, Residuals: -1.09914, Convergence: 0.000996\n",
      "Evidence 11071.090\n",
      "\n",
      "Epoch: 159, Evidence: 11071.08984, Convergence: 1.016652\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.78e-01\n",
      "Epoch: 159, Loss: 2344.82859, Residuals: -1.09914, Convergence:   inf\n",
      "Epoch: 160, Loss: 2304.08311, Residuals: -1.10744, Convergence: 0.017684\n",
      "Epoch: 161, Loss: 2275.72783, Residuals: -1.10646, Convergence: 0.012460\n",
      "Epoch: 162, Loss: 2252.03535, Residuals: -1.10441, Convergence: 0.010520\n",
      "Epoch: 163, Loss: 2231.94251, Residuals: -1.10194, Convergence: 0.009002\n",
      "Epoch: 164, Loss: 2214.73159, Residuals: -1.09918, Convergence: 0.007771\n",
      "Epoch: 165, Loss: 2199.85655, Residuals: -1.09620, Convergence: 0.006762\n",
      "Epoch: 166, Loss: 2186.88666, Residuals: -1.09305, Convergence: 0.005931\n",
      "Epoch: 167, Loss: 2175.47330, Residuals: -1.08978, Convergence: 0.005246\n",
      "Epoch: 168, Loss: 2165.33033, Residuals: -1.08640, Convergence: 0.004684\n",
      "Epoch: 169, Loss: 2156.21831, Residuals: -1.08295, Convergence: 0.004226\n",
      "Epoch: 170, Loss: 2147.93824, Residuals: -1.07941, Convergence: 0.003855\n",
      "Epoch: 171, Loss: 2140.32945, Residuals: -1.07579, Convergence: 0.003555\n",
      "Epoch: 172, Loss: 2133.27259, Residuals: -1.07210, Convergence: 0.003308\n",
      "Epoch: 173, Loss: 2126.69165, Residuals: -1.06833, Convergence: 0.003094\n",
      "Epoch: 174, Loss: 2120.54439, Residuals: -1.06453, Convergence: 0.002899\n",
      "Epoch: 175, Loss: 2114.81013, Residuals: -1.06074, Convergence: 0.002711\n",
      "Epoch: 176, Loss: 2109.47376, Residuals: -1.05699, Convergence: 0.002530\n",
      "Epoch: 177, Loss: 2104.52068, Residuals: -1.05334, Convergence: 0.002354\n",
      "Epoch: 178, Loss: 2099.93183, Residuals: -1.04982, Convergence: 0.002185\n",
      "Epoch: 179, Loss: 2095.68712, Residuals: -1.04644, Convergence: 0.002025\n",
      "Epoch: 180, Loss: 2091.76282, Residuals: -1.04321, Convergence: 0.001876\n",
      "Epoch: 181, Loss: 2088.13664, Residuals: -1.04015, Convergence: 0.001737\n",
      "Epoch: 182, Loss: 2084.78626, Residuals: -1.03725, Convergence: 0.001607\n",
      "Epoch: 183, Loss: 2081.69000, Residuals: -1.03450, Convergence: 0.001487\n",
      "Epoch: 184, Loss: 2078.82595, Residuals: -1.03192, Convergence: 0.001378\n",
      "Epoch: 185, Loss: 2076.17465, Residuals: -1.02948, Convergence: 0.001277\n",
      "Epoch: 186, Loss: 2073.71708, Residuals: -1.02717, Convergence: 0.001185\n",
      "Epoch: 187, Loss: 2071.43488, Residuals: -1.02499, Convergence: 0.001102\n",
      "Epoch: 188, Loss: 2069.31282, Residuals: -1.02293, Convergence: 0.001025\n",
      "Epoch: 189, Loss: 2067.33442, Residuals: -1.02098, Convergence: 0.000957\n",
      "Evidence 14200.023\n",
      "\n",
      "Epoch: 189, Evidence: 14200.02344, Convergence: 0.220347\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.40e-01\n",
      "Epoch: 189, Loss: 2471.44441, Residuals: -1.02098, Convergence:   inf\n",
      "Epoch: 190, Loss: 2456.92462, Residuals: -1.01825, Convergence: 0.005910\n",
      "Epoch: 191, Loss: 2445.16474, Residuals: -1.01480, Convergence: 0.004809\n",
      "Epoch: 192, Loss: 2435.12020, Residuals: -1.01131, Convergence: 0.004125\n",
      "Epoch: 193, Loss: 2426.46200, Residuals: -1.00795, Convergence: 0.003568\n",
      "Epoch: 194, Loss: 2418.94751, Residuals: -1.00477, Convergence: 0.003107\n",
      "Epoch: 195, Loss: 2412.38458, Residuals: -1.00180, Convergence: 0.002721\n",
      "Epoch: 196, Loss: 2406.61759, Residuals: -0.99903, Convergence: 0.002396\n",
      "Epoch: 197, Loss: 2401.51715, Residuals: -0.99645, Convergence: 0.002124\n",
      "Epoch: 198, Loss: 2396.97886, Residuals: -0.99405, Convergence: 0.001893\n",
      "Epoch: 199, Loss: 2392.91394, Residuals: -0.99181, Convergence: 0.001699\n",
      "Epoch: 200, Loss: 2389.25002, Residuals: -0.98973, Convergence: 0.001534\n",
      "Epoch: 201, Loss: 2385.92700, Residuals: -0.98778, Convergence: 0.001393\n",
      "Epoch: 202, Loss: 2382.89600, Residuals: -0.98595, Convergence: 0.001272\n",
      "Epoch: 203, Loss: 2380.11610, Residuals: -0.98424, Convergence: 0.001168\n",
      "Epoch: 204, Loss: 2377.55363, Residuals: -0.98263, Convergence: 0.001078\n",
      "Epoch: 205, Loss: 2375.18132, Residuals: -0.98111, Convergence: 0.000999\n",
      "Evidence 14620.703\n",
      "\n",
      "Epoch: 205, Evidence: 14620.70312, Convergence: 0.028773\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.36e-01\n",
      "Epoch: 205, Loss: 2477.24276, Residuals: -0.98111, Convergence:   inf\n",
      "Epoch: 206, Loss: 2470.51619, Residuals: -0.97773, Convergence: 0.002723\n",
      "Epoch: 207, Loss: 2464.96910, Residuals: -0.97453, Convergence: 0.002250\n",
      "Epoch: 208, Loss: 2460.26140, Residuals: -0.97166, Convergence: 0.001913\n",
      "Epoch: 209, Loss: 2456.20312, Residuals: -0.96912, Convergence: 0.001652\n",
      "Epoch: 210, Loss: 2452.65469, Residuals: -0.96687, Convergence: 0.001447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 211, Loss: 2449.51110, Residuals: -0.96487, Convergence: 0.001283\n",
      "Epoch: 212, Loss: 2446.69175, Residuals: -0.96309, Convergence: 0.001152\n",
      "Epoch: 213, Loss: 2444.13834, Residuals: -0.96150, Convergence: 0.001045\n",
      "Epoch: 214, Loss: 2441.80420, Residuals: -0.96008, Convergence: 0.000956\n",
      "Evidence 14708.131\n",
      "\n",
      "Epoch: 214, Evidence: 14708.13086, Convergence: 0.005944\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.64e-01\n",
      "Epoch: 214, Loss: 2478.72496, Residuals: -0.96008, Convergence:   inf\n",
      "Epoch: 215, Loss: 2474.79476, Residuals: -0.95716, Convergence: 0.001588\n",
      "Epoch: 216, Loss: 2471.50857, Residuals: -0.95469, Convergence: 0.001330\n",
      "Epoch: 217, Loss: 2468.67377, Residuals: -0.95261, Convergence: 0.001148\n",
      "Epoch: 218, Loss: 2466.17707, Residuals: -0.95084, Convergence: 0.001012\n",
      "Epoch: 219, Loss: 2463.94243, Residuals: -0.94934, Convergence: 0.000907\n",
      "Evidence 14739.792\n",
      "\n",
      "Epoch: 219, Evidence: 14739.79199, Convergence: 0.002148\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.12e-01\n",
      "Epoch: 219, Loss: 2479.62922, Residuals: -0.94934, Convergence:   inf\n",
      "Epoch: 220, Loss: 2476.80593, Residuals: -0.94694, Convergence: 0.001140\n",
      "Epoch: 221, Loss: 2474.40870, Residuals: -0.94499, Convergence: 0.000969\n",
      "Evidence 14752.463\n",
      "\n",
      "Epoch: 221, Evidence: 14752.46289, Convergence: 0.000859\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.76e-01\n",
      "Epoch: 221, Loss: 2480.37098, Residuals: -0.94499, Convergence:   inf\n",
      "Epoch: 222, Loss: 2475.81441, Residuals: -0.94135, Convergence: 0.001840\n",
      "Epoch: 223, Loss: 2472.30317, Residuals: -0.93885, Convergence: 0.001420\n",
      "Epoch: 224, Loss: 2469.41385, Residuals: -0.93708, Convergence: 0.001170\n",
      "Epoch: 225, Loss: 2466.93789, Residuals: -0.93591, Convergence: 0.001004\n",
      "Epoch: 226, Loss: 2464.75052, Residuals: -0.93514, Convergence: 0.000887\n",
      "Evidence 14772.799\n",
      "\n",
      "Epoch: 226, Evidence: 14772.79883, Convergence: 0.002234\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.46e-01\n",
      "Epoch: 226, Loss: 2480.32915, Residuals: -0.93514, Convergence:   inf\n",
      "Epoch: 227, Loss: 2477.46583, Residuals: -0.93217, Convergence: 0.001156\n",
      "Epoch: 228, Loss: 2475.17104, Residuals: -0.93053, Convergence: 0.000927\n",
      "Evidence 14784.095\n",
      "\n",
      "Epoch: 228, Evidence: 14784.09473, Convergence: 0.000764\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.23e-01\n",
      "Epoch: 228, Loss: 2480.59552, Residuals: -0.93053, Convergence:   inf\n",
      "Epoch: 229, Loss: 2476.39641, Residuals: -0.92618, Convergence: 0.001696\n",
      "Epoch: 230, Loss: 2473.32791, Residuals: -0.92669, Convergence: 0.001241\n",
      "Epoch: 231, Loss: 2470.68152, Residuals: -0.92693, Convergence: 0.001071\n",
      "Epoch: 232, Loss: 2468.38288, Residuals: -0.92925, Convergence: 0.000931\n",
      "Evidence 14799.432\n",
      "\n",
      "Epoch: 232, Evidence: 14799.43164, Convergence: 0.001800\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.10e-01\n",
      "Epoch: 232, Loss: 2479.97890, Residuals: -0.92925, Convergence:   inf\n",
      "Epoch: 233, Loss: 2478.30539, Residuals: -0.92683, Convergence: 0.000675\n",
      "Evidence 14805.819\n",
      "\n",
      "Epoch: 233, Evidence: 14805.81934, Convergence: 0.000431\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.93e-02\n",
      "Epoch: 233, Loss: 2480.97491, Residuals: -0.92683, Convergence:   inf\n",
      "Epoch: 234, Loss: 2525.80782, Residuals: -0.96309, Convergence: -0.017750\n",
      "Epoch: 234, Loss: 2478.90663, Residuals: -0.92384, Convergence: 0.000834\n",
      "Evidence 14809.812\n",
      "\n",
      "Epoch: 234, Evidence: 14809.81250, Convergence: 0.000701\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.19e-02\n",
      "Epoch: 234, Loss: 2480.45845, Residuals: -0.92384, Convergence:   inf\n",
      "Epoch: 235, Loss: 2482.90013, Residuals: -0.92451, Convergence: -0.000983\n",
      "Evidence 14808.865\n",
      "\n",
      "Epoch: 235, Evidence: 14808.86523, Convergence: 0.000637\n",
      "Total samples: 183, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 381.84220, Residuals: -4.50487, Convergence:   inf\n",
      "Epoch: 1, Loss: 356.01227, Residuals: -4.38458, Convergence: 0.072553\n",
      "Epoch: 2, Loss: 334.90927, Residuals: -4.21986, Convergence: 0.063011\n",
      "Epoch: 3, Loss: 318.83308, Residuals: -4.05588, Convergence: 0.050422\n",
      "Epoch: 4, Loss: 306.57901, Residuals: -3.91199, Convergence: 0.039970\n",
      "Epoch: 5, Loss: 296.86664, Residuals: -3.78523, Convergence: 0.032716\n",
      "Epoch: 6, Loss: 288.98806, Residuals: -3.67501, Convergence: 0.027263\n",
      "Epoch: 7, Loss: 282.45550, Residuals: -3.58067, Convergence: 0.023128\n",
      "Epoch: 8, Loss: 276.90661, Residuals: -3.50002, Convergence: 0.020039\n",
      "Epoch: 9, Loss: 272.08436, Residuals: -3.43069, Convergence: 0.017723\n",
      "Epoch: 10, Loss: 267.80532, Residuals: -3.37058, Convergence: 0.015978\n",
      "Epoch: 11, Loss: 263.93617, Residuals: -3.31795, Convergence: 0.014659\n",
      "Epoch: 12, Loss: 260.37929, Residuals: -3.27137, Convergence: 0.013660\n",
      "Epoch: 13, Loss: 257.06377, Residuals: -3.22962, Convergence: 0.012898\n",
      "Epoch: 14, Loss: 253.93912, Residuals: -3.19164, Convergence: 0.012305\n",
      "Epoch: 15, Loss: 250.97169, Residuals: -3.15652, Convergence: 0.011824\n",
      "Epoch: 16, Loss: 248.14263, Residuals: -3.12361, Convergence: 0.011401\n",
      "Epoch: 17, Loss: 245.44127, Residuals: -3.09251, Convergence: 0.011006\n",
      "Epoch: 18, Loss: 242.85196, Residuals: -3.06286, Convergence: 0.010662\n",
      "Epoch: 19, Loss: 240.34742, Residuals: -3.03420, Convergence: 0.010421\n",
      "Epoch: 20, Loss: 237.89374, Residuals: -3.00595, Convergence: 0.010314\n",
      "Epoch: 21, Loss: 235.45908, Residuals: -2.97755, Convergence: 0.010340\n",
      "Epoch: 22, Loss: 233.02008, Residuals: -2.94861, Convergence: 0.010467\n",
      "Epoch: 23, Loss: 230.55911, Residuals: -2.91885, Convergence: 0.010674\n",
      "Epoch: 24, Loss: 228.04979, Residuals: -2.88796, Convergence: 0.011003\n",
      "Epoch: 25, Loss: 225.44617, Residuals: -2.85543, Convergence: 0.011549\n",
      "Epoch: 26, Loss: 222.70263, Residuals: -2.82066, Convergence: 0.012319\n",
      "Epoch: 27, Loss: 219.85063, Residuals: -2.78393, Convergence: 0.012972\n",
      "Epoch: 28, Loss: 217.01581, Residuals: -2.74661, Convergence: 0.013063\n",
      "Epoch: 29, Loss: 214.27620, Residuals: -2.70969, Convergence: 0.012785\n",
      "Epoch: 30, Loss: 211.63612, Residuals: -2.67339, Convergence: 0.012475\n",
      "Epoch: 31, Loss: 209.07936, Residuals: -2.63762, Convergence: 0.012229\n",
      "Epoch: 32, Loss: 206.59057, Residuals: -2.60227, Convergence: 0.012047\n",
      "Epoch: 33, Loss: 204.15902, Residuals: -2.56725, Convergence: 0.011910\n",
      "Epoch: 34, Loss: 201.77824, Residuals: -2.53249, Convergence: 0.011799\n",
      "Epoch: 35, Loss: 199.44496, Residuals: -2.49795, Convergence: 0.011699\n",
      "Epoch: 36, Loss: 197.15815, Residuals: -2.46359, Convergence: 0.011599\n",
      "Epoch: 37, Loss: 194.91809, Residuals: -2.42939, Convergence: 0.011492\n",
      "Epoch: 38, Loss: 192.72574, Residuals: -2.39535, Convergence: 0.011375\n",
      "Epoch: 39, Loss: 190.58227, Residuals: -2.36146, Convergence: 0.011247\n",
      "Epoch: 40, Loss: 188.48876, Residuals: -2.32771, Convergence: 0.011107\n",
      "Epoch: 41, Loss: 186.44607, Residuals: -2.29410, Convergence: 0.010956\n",
      "Epoch: 42, Loss: 184.45492, Residuals: -2.26065, Convergence: 0.010795\n",
      "Epoch: 43, Loss: 182.51587, Residuals: -2.22735, Convergence: 0.010624\n",
      "Epoch: 44, Loss: 180.62958, Residuals: -2.19423, Convergence: 0.010443\n",
      "Epoch: 45, Loss: 178.79696, Residuals: -2.16130, Convergence: 0.010250\n",
      "Epoch: 46, Loss: 177.01928, Residuals: -2.12859, Convergence: 0.010042\n",
      "Epoch: 47, Loss: 175.29816, Residuals: -2.09615, Convergence: 0.009818\n",
      "Epoch: 48, Loss: 173.63539, Residuals: -2.06402, Convergence: 0.009576\n",
      "Epoch: 49, Loss: 172.03272, Residuals: -2.03226, Convergence: 0.009316\n",
      "Epoch: 50, Loss: 170.49160, Residuals: -2.00091, Convergence: 0.009039\n",
      "Epoch: 51, Loss: 169.01292, Residuals: -1.97004, Convergence: 0.008749\n",
      "Epoch: 52, Loss: 167.59705, Residuals: -1.93968, Convergence: 0.008448\n",
      "Epoch: 53, Loss: 166.24376, Residuals: -1.90988, Convergence: 0.008140\n",
      "Epoch: 54, Loss: 164.95238, Residuals: -1.88069, Convergence: 0.007829\n",
      "Epoch: 55, Loss: 163.72181, Residuals: -1.85213, Convergence: 0.007516\n",
      "Epoch: 56, Loss: 162.55063, Residuals: -1.82425, Convergence: 0.007205\n",
      "Epoch: 57, Loss: 161.43721, Residuals: -1.79706, Convergence: 0.006897\n",
      "Epoch: 58, Loss: 160.37964, Residuals: -1.77060, Convergence: 0.006594\n",
      "Epoch: 59, Loss: 159.37587, Residuals: -1.74487, Convergence: 0.006298\n",
      "Epoch: 60, Loss: 158.42371, Residuals: -1.71989, Convergence: 0.006010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61, Loss: 157.52083, Residuals: -1.69568, Convergence: 0.005732\n",
      "Epoch: 62, Loss: 156.66484, Residuals: -1.67222, Convergence: 0.005464\n",
      "Epoch: 63, Loss: 155.85341, Residuals: -1.64951, Convergence: 0.005206\n",
      "Epoch: 64, Loss: 155.08423, Residuals: -1.62756, Convergence: 0.004960\n",
      "Epoch: 65, Loss: 154.35520, Residuals: -1.60635, Convergence: 0.004723\n",
      "Epoch: 66, Loss: 153.66432, Residuals: -1.58588, Convergence: 0.004496\n",
      "Epoch: 67, Loss: 153.00976, Residuals: -1.56614, Convergence: 0.004278\n",
      "Epoch: 68, Loss: 152.38978, Residuals: -1.54712, Convergence: 0.004068\n",
      "Epoch: 69, Loss: 151.80272, Residuals: -1.52882, Convergence: 0.003867\n",
      "Epoch: 70, Loss: 151.24692, Residuals: -1.51122, Convergence: 0.003675\n",
      "Epoch: 71, Loss: 150.72071, Residuals: -1.49432, Convergence: 0.003491\n",
      "Epoch: 72, Loss: 150.22243, Residuals: -1.47811, Convergence: 0.003317\n",
      "Epoch: 73, Loss: 149.75046, Residuals: -1.46256, Convergence: 0.003152\n",
      "Epoch: 74, Loss: 149.30322, Residuals: -1.44766, Convergence: 0.002996\n",
      "Epoch: 75, Loss: 148.87916, Residuals: -1.43340, Convergence: 0.002848\n",
      "Epoch: 76, Loss: 148.47680, Residuals: -1.41976, Convergence: 0.002710\n",
      "Epoch: 77, Loss: 148.09465, Residuals: -1.40672, Convergence: 0.002580\n",
      "Epoch: 78, Loss: 147.73125, Residuals: -1.39426, Convergence: 0.002460\n",
      "Epoch: 79, Loss: 147.38506, Residuals: -1.38237, Convergence: 0.002349\n",
      "Epoch: 80, Loss: 147.05455, Residuals: -1.37100, Convergence: 0.002248\n",
      "Epoch: 81, Loss: 146.73814, Residuals: -1.36015, Convergence: 0.002156\n",
      "Epoch: 82, Loss: 146.43428, Residuals: -1.34978, Convergence: 0.002075\n",
      "Epoch: 83, Loss: 146.14151, Residuals: -1.33985, Convergence: 0.002003\n",
      "Epoch: 84, Loss: 145.85851, Residuals: -1.33034, Convergence: 0.001940\n",
      "Epoch: 85, Loss: 145.58416, Residuals: -1.32120, Convergence: 0.001885\n",
      "Epoch: 86, Loss: 145.31753, Residuals: -1.31241, Convergence: 0.001835\n",
      "Epoch: 87, Loss: 145.05795, Residuals: -1.30395, Convergence: 0.001789\n",
      "Epoch: 88, Loss: 144.80495, Residuals: -1.29579, Convergence: 0.001747\n",
      "Epoch: 89, Loss: 144.55823, Residuals: -1.28791, Convergence: 0.001707\n",
      "Epoch: 90, Loss: 144.31760, Residuals: -1.28030, Convergence: 0.001667\n",
      "Epoch: 91, Loss: 144.08300, Residuals: -1.27295, Convergence: 0.001628\n",
      "Epoch: 92, Loss: 143.85438, Residuals: -1.26584, Convergence: 0.001589\n",
      "Epoch: 93, Loss: 143.63174, Residuals: -1.25897, Convergence: 0.001550\n",
      "Epoch: 94, Loss: 143.41510, Residuals: -1.25234, Convergence: 0.001511\n",
      "Epoch: 95, Loss: 143.20444, Residuals: -1.24593, Convergence: 0.001471\n",
      "Epoch: 96, Loss: 142.99974, Residuals: -1.23974, Convergence: 0.001431\n",
      "Epoch: 97, Loss: 142.80097, Residuals: -1.23376, Convergence: 0.001392\n",
      "Epoch: 98, Loss: 142.60806, Residuals: -1.22798, Convergence: 0.001353\n",
      "Epoch: 99, Loss: 142.42092, Residuals: -1.22241, Convergence: 0.001314\n",
      "Epoch: 100, Loss: 142.23946, Residuals: -1.21703, Convergence: 0.001276\n",
      "Epoch: 101, Loss: 142.06355, Residuals: -1.21183, Convergence: 0.001238\n",
      "Epoch: 102, Loss: 141.89306, Residuals: -1.20681, Convergence: 0.001202\n",
      "Epoch: 103, Loss: 141.72785, Residuals: -1.20197, Convergence: 0.001166\n",
      "Epoch: 104, Loss: 141.56777, Residuals: -1.19730, Convergence: 0.001131\n",
      "Epoch: 105, Loss: 141.41267, Residuals: -1.19278, Convergence: 0.001097\n",
      "Epoch: 106, Loss: 141.26237, Residuals: -1.18843, Convergence: 0.001064\n",
      "Epoch: 107, Loss: 141.11673, Residuals: -1.18422, Convergence: 0.001032\n",
      "Epoch: 108, Loss: 140.97559, Residuals: -1.18015, Convergence: 0.001001\n",
      "Epoch: 109, Loss: 140.83878, Residuals: -1.17622, Convergence: 0.000971\n",
      "Evidence -181.399\n",
      "\n",
      "Epoch: 109, Evidence: -181.39929, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.24e-01\n",
      "Epoch: 109, Loss: 1379.07952, Residuals: -1.17622, Convergence:   inf\n",
      "Epoch: 110, Loss: 1323.58554, Residuals: -1.20038, Convergence: 0.041927\n",
      "Epoch: 111, Loss: 1280.58880, Residuals: -1.22099, Convergence: 0.033576\n",
      "Epoch: 112, Loss: 1247.52363, Residuals: -1.23747, Convergence: 0.026505\n",
      "Epoch: 113, Loss: 1221.46894, Residuals: -1.25038, Convergence: 0.021331\n",
      "Epoch: 114, Loss: 1200.25917, Residuals: -1.26073, Convergence: 0.017671\n",
      "Epoch: 115, Loss: 1182.61484, Residuals: -1.26914, Convergence: 0.014920\n",
      "Epoch: 116, Loss: 1167.73372, Residuals: -1.27590, Convergence: 0.012744\n",
      "Epoch: 117, Loss: 1155.04717, Residuals: -1.28118, Convergence: 0.010984\n",
      "Epoch: 118, Loss: 1144.12194, Residuals: -1.28514, Convergence: 0.009549\n",
      "Epoch: 119, Loss: 1134.61548, Residuals: -1.28792, Convergence: 0.008379\n",
      "Epoch: 120, Loss: 1126.24997, Residuals: -1.28965, Convergence: 0.007428\n",
      "Epoch: 121, Loss: 1118.79808, Residuals: -1.29042, Convergence: 0.006661\n",
      "Epoch: 122, Loss: 1112.06900, Residuals: -1.29033, Convergence: 0.006051\n",
      "Epoch: 123, Loss: 1105.90014, Residuals: -1.28945, Convergence: 0.005578\n",
      "Epoch: 124, Loss: 1100.14884, Residuals: -1.28784, Convergence: 0.005228\n",
      "Epoch: 125, Loss: 1094.68767, Residuals: -1.28553, Convergence: 0.004989\n",
      "Epoch: 126, Loss: 1089.39903, Residuals: -1.28254, Convergence: 0.004855\n",
      "Epoch: 127, Loss: 1084.17511, Residuals: -1.27887, Convergence: 0.004818\n",
      "Epoch: 128, Loss: 1078.92145, Residuals: -1.27454, Convergence: 0.004869\n",
      "Epoch: 129, Loss: 1073.56524, Residuals: -1.26957, Convergence: 0.004989\n",
      "Epoch: 130, Loss: 1068.07549, Residuals: -1.26399, Convergence: 0.005140\n",
      "Epoch: 131, Loss: 1062.48178, Residuals: -1.25789, Convergence: 0.005265\n",
      "Epoch: 132, Loss: 1056.87887, Residuals: -1.25136, Convergence: 0.005301\n",
      "Epoch: 133, Loss: 1051.39564, Residuals: -1.24448, Convergence: 0.005215\n",
      "Epoch: 134, Loss: 1046.14580, Residuals: -1.23735, Convergence: 0.005018\n",
      "Epoch: 135, Loss: 1041.19509, Residuals: -1.23006, Convergence: 0.004755\n",
      "Epoch: 136, Loss: 1036.56223, Residuals: -1.22267, Convergence: 0.004469\n",
      "Epoch: 137, Loss: 1032.23503, Residuals: -1.21527, Convergence: 0.004192\n",
      "Epoch: 138, Loss: 1028.18740, Residuals: -1.20790, Convergence: 0.003937\n",
      "Epoch: 139, Loss: 1024.39016, Residuals: -1.20060, Convergence: 0.003707\n",
      "Epoch: 140, Loss: 1020.81568, Residuals: -1.19342, Convergence: 0.003502\n",
      "Epoch: 141, Loss: 1017.44059, Residuals: -1.18638, Convergence: 0.003317\n",
      "Epoch: 142, Loss: 1014.24617, Residuals: -1.17952, Convergence: 0.003150\n",
      "Epoch: 143, Loss: 1011.21607, Residuals: -1.17284, Convergence: 0.002996\n",
      "Epoch: 144, Loss: 1008.33827, Residuals: -1.16638, Convergence: 0.002854\n",
      "Epoch: 145, Loss: 1005.60211, Residuals: -1.16013, Convergence: 0.002721\n",
      "Epoch: 146, Loss: 1002.99902, Residuals: -1.15411, Convergence: 0.002595\n",
      "Epoch: 147, Loss: 1000.52138, Residuals: -1.14833, Convergence: 0.002476\n",
      "Epoch: 148, Loss: 998.16194, Residuals: -1.14278, Convergence: 0.002364\n",
      "Epoch: 149, Loss: 995.91434, Residuals: -1.13748, Convergence: 0.002257\n",
      "Epoch: 150, Loss: 993.77248, Residuals: -1.13241, Convergence: 0.002155\n",
      "Epoch: 151, Loss: 991.72998, Residuals: -1.12757, Convergence: 0.002060\n",
      "Epoch: 152, Loss: 989.78098, Residuals: -1.12295, Convergence: 0.001969\n",
      "Epoch: 153, Loss: 987.91915, Residuals: -1.11854, Convergence: 0.001885\n",
      "Epoch: 154, Loss: 986.13876, Residuals: -1.11434, Convergence: 0.001805\n",
      "Epoch: 155, Loss: 984.43341, Residuals: -1.11034, Convergence: 0.001732\n",
      "Epoch: 156, Loss: 982.79620, Residuals: -1.10651, Convergence: 0.001666\n",
      "Epoch: 157, Loss: 981.22062, Residuals: -1.10285, Convergence: 0.001606\n",
      "Epoch: 158, Loss: 979.69992, Residuals: -1.09935, Convergence: 0.001552\n",
      "Epoch: 159, Loss: 978.22586, Residuals: -1.09598, Convergence: 0.001507\n",
      "Epoch: 160, Loss: 976.79119, Residuals: -1.09273, Convergence: 0.001469\n",
      "Epoch: 161, Loss: 975.38723, Residuals: -1.08959, Convergence: 0.001439\n",
      "Epoch: 162, Loss: 974.00676, Residuals: -1.08653, Convergence: 0.001417\n",
      "Epoch: 163, Loss: 972.64196, Residuals: -1.08355, Convergence: 0.001403\n",
      "Epoch: 164, Loss: 971.28668, Residuals: -1.08062, Convergence: 0.001395\n",
      "Epoch: 165, Loss: 969.93647, Residuals: -1.07773, Convergence: 0.001392\n",
      "Epoch: 166, Loss: 968.58988, Residuals: -1.07487, Convergence: 0.001390\n",
      "Epoch: 167, Loss: 967.24832, Residuals: -1.07205, Convergence: 0.001387\n",
      "Epoch: 168, Loss: 965.91617, Residuals: -1.06927, Convergence: 0.001379\n",
      "Epoch: 169, Loss: 964.60010, Residuals: -1.06654, Convergence: 0.001364\n",
      "Epoch: 170, Loss: 963.30767, Residuals: -1.06386, Convergence: 0.001342\n",
      "Epoch: 171, Loss: 962.04575, Residuals: -1.06126, Convergence: 0.001312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 172, Loss: 960.82055, Residuals: -1.05874, Convergence: 0.001275\n",
      "Epoch: 173, Loss: 959.63678, Residuals: -1.05630, Convergence: 0.001234\n",
      "Epoch: 174, Loss: 958.49734, Residuals: -1.05396, Convergence: 0.001189\n",
      "Epoch: 175, Loss: 957.40382, Residuals: -1.05171, Convergence: 0.001142\n",
      "Epoch: 176, Loss: 956.35712, Residuals: -1.04955, Convergence: 0.001094\n",
      "Epoch: 177, Loss: 955.35732, Residuals: -1.04749, Convergence: 0.001047\n",
      "Epoch: 178, Loss: 954.40329, Residuals: -1.04551, Convergence: 0.001000\n",
      "Evidence 11424.603\n",
      "\n",
      "Epoch: 178, Evidence: 11424.60254, Convergence: 1.015878\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 5.69e-01\n",
      "Epoch: 178, Loss: 2364.88861, Residuals: -1.04551, Convergence:   inf\n",
      "Epoch: 179, Loss: 2328.58921, Residuals: -1.05136, Convergence: 0.015589\n",
      "Epoch: 180, Loss: 2303.50037, Residuals: -1.04920, Convergence: 0.010892\n",
      "Epoch: 181, Loss: 2282.48895, Residuals: -1.04675, Convergence: 0.009205\n",
      "Epoch: 182, Loss: 2264.77136, Residuals: -1.04430, Convergence: 0.007823\n",
      "Epoch: 183, Loss: 2249.70775, Residuals: -1.04186, Convergence: 0.006696\n",
      "Epoch: 184, Loss: 2236.76566, Residuals: -1.03943, Convergence: 0.005786\n",
      "Epoch: 185, Loss: 2225.50122, Residuals: -1.03698, Convergence: 0.005062\n",
      "Epoch: 186, Loss: 2215.55591, Residuals: -1.03448, Convergence: 0.004489\n",
      "Epoch: 187, Loss: 2206.65346, Residuals: -1.03190, Convergence: 0.004034\n",
      "Epoch: 188, Loss: 2198.60019, Residuals: -1.02924, Convergence: 0.003663\n",
      "Epoch: 189, Loss: 2191.26887, Residuals: -1.02650, Convergence: 0.003346\n",
      "Epoch: 190, Loss: 2184.57777, Residuals: -1.02373, Convergence: 0.003063\n",
      "Epoch: 191, Loss: 2178.46490, Residuals: -1.02097, Convergence: 0.002806\n",
      "Epoch: 192, Loss: 2172.87452, Residuals: -1.01825, Convergence: 0.002573\n",
      "Epoch: 193, Loss: 2167.75042, Residuals: -1.01561, Convergence: 0.002364\n",
      "Epoch: 194, Loss: 2163.03873, Residuals: -1.01305, Convergence: 0.002178\n",
      "Epoch: 195, Loss: 2158.68831, Residuals: -1.01058, Convergence: 0.002015\n",
      "Epoch: 196, Loss: 2154.65375, Residuals: -1.00821, Convergence: 0.001872\n",
      "Epoch: 197, Loss: 2150.89550, Residuals: -1.00594, Convergence: 0.001747\n",
      "Epoch: 198, Loss: 2147.38074, Residuals: -1.00376, Convergence: 0.001637\n",
      "Epoch: 199, Loss: 2144.08344, Residuals: -1.00169, Convergence: 0.001538\n",
      "Epoch: 200, Loss: 2140.98356, Residuals: -0.99971, Convergence: 0.001448\n",
      "Epoch: 201, Loss: 2138.06469, Residuals: -0.99783, Convergence: 0.001365\n",
      "Epoch: 202, Loss: 2135.31481, Residuals: -0.99605, Convergence: 0.001288\n",
      "Epoch: 203, Loss: 2132.72419, Residuals: -0.99437, Convergence: 0.001215\n",
      "Epoch: 204, Loss: 2130.28374, Residuals: -0.99278, Convergence: 0.001146\n",
      "Epoch: 205, Loss: 2127.98618, Residuals: -0.99128, Convergence: 0.001080\n",
      "Epoch: 206, Loss: 2125.82441, Residuals: -0.98986, Convergence: 0.001017\n",
      "Epoch: 207, Loss: 2123.79151, Residuals: -0.98853, Convergence: 0.000957\n",
      "Evidence 14402.488\n",
      "\n",
      "Epoch: 207, Evidence: 14402.48828, Convergence: 0.206762\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 4.32e-01\n",
      "Epoch: 207, Loss: 2479.58187, Residuals: -0.98853, Convergence:   inf\n",
      "Epoch: 208, Loss: 2466.50422, Residuals: -0.98489, Convergence: 0.005302\n",
      "Epoch: 209, Loss: 2455.75307, Residuals: -0.98120, Convergence: 0.004378\n",
      "Epoch: 210, Loss: 2446.50125, Residuals: -0.97787, Convergence: 0.003782\n",
      "Epoch: 211, Loss: 2438.50235, Residuals: -0.97491, Convergence: 0.003280\n",
      "Epoch: 212, Loss: 2431.55485, Residuals: -0.97232, Convergence: 0.002857\n",
      "Epoch: 213, Loss: 2425.49193, Residuals: -0.97005, Convergence: 0.002500\n",
      "Epoch: 214, Loss: 2420.17419, Residuals: -0.96808, Convergence: 0.002197\n",
      "Epoch: 215, Loss: 2415.48644, Residuals: -0.96635, Convergence: 0.001941\n",
      "Epoch: 216, Loss: 2411.33212, Residuals: -0.96484, Convergence: 0.001723\n",
      "Epoch: 217, Loss: 2407.62897, Residuals: -0.96351, Convergence: 0.001538\n",
      "Epoch: 218, Loss: 2404.30947, Residuals: -0.96234, Convergence: 0.001381\n",
      "Epoch: 219, Loss: 2401.31815, Residuals: -0.96130, Convergence: 0.001246\n",
      "Epoch: 220, Loss: 2398.60736, Residuals: -0.96037, Convergence: 0.001130\n",
      "Epoch: 221, Loss: 2396.13916, Residuals: -0.95954, Convergence: 0.001030\n",
      "Epoch: 222, Loss: 2393.87975, Residuals: -0.95879, Convergence: 0.000944\n",
      "Evidence 14749.030\n",
      "\n",
      "Epoch: 222, Evidence: 14749.03027, Convergence: 0.023496\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 3.30e-01\n",
      "Epoch: 222, Loss: 2484.64559, Residuals: -0.95879, Convergence:   inf\n",
      "Epoch: 223, Loss: 2478.24696, Residuals: -0.95563, Convergence: 0.002582\n",
      "Epoch: 224, Loss: 2472.97946, Residuals: -0.95301, Convergence: 0.002130\n",
      "Epoch: 225, Loss: 2468.54099, Residuals: -0.95091, Convergence: 0.001798\n",
      "Epoch: 226, Loss: 2464.74731, Residuals: -0.94920, Convergence: 0.001539\n",
      "Epoch: 227, Loss: 2461.46145, Residuals: -0.94782, Convergence: 0.001335\n",
      "Epoch: 228, Loss: 2458.57901, Residuals: -0.94671, Convergence: 0.001172\n",
      "Epoch: 229, Loss: 2456.02040, Residuals: -0.94580, Convergence: 0.001042\n",
      "Epoch: 230, Loss: 2453.72324, Residuals: -0.94505, Convergence: 0.000936\n",
      "Evidence 14827.693\n",
      "\n",
      "Epoch: 230, Evidence: 14827.69336, Convergence: 0.005305\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.57e-01\n",
      "Epoch: 230, Loss: 2486.19859, Residuals: -0.94505, Convergence:   inf\n",
      "Epoch: 231, Loss: 2482.39850, Residuals: -0.94278, Convergence: 0.001531\n",
      "Epoch: 232, Loss: 2479.26588, Residuals: -0.94106, Convergence: 0.001264\n",
      "Epoch: 233, Loss: 2476.60507, Residuals: -0.93975, Convergence: 0.001074\n",
      "Epoch: 234, Loss: 2474.29551, Residuals: -0.93873, Convergence: 0.000933\n",
      "Evidence 14855.133\n",
      "\n",
      "Epoch: 234, Evidence: 14855.13281, Convergence: 0.001847\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.07e-01\n",
      "Epoch: 234, Loss: 2487.11414, Residuals: -0.93873, Convergence:   inf\n",
      "Epoch: 235, Loss: 2484.33488, Residuals: -0.93697, Convergence: 0.001119\n",
      "Epoch: 236, Loss: 2482.02350, Residuals: -0.93566, Convergence: 0.000931\n",
      "Evidence 14867.020\n",
      "\n",
      "Epoch: 236, Evidence: 14867.01953, Convergence: 0.000800\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.72e-01\n",
      "Epoch: 236, Loss: 2487.76741, Residuals: -0.93566, Convergence:   inf\n",
      "Epoch: 237, Loss: 2483.47782, Residuals: -0.93340, Convergence: 0.001727\n",
      "Epoch: 238, Loss: 2480.20029, Residuals: -0.93191, Convergence: 0.001321\n",
      "Epoch: 239, Loss: 2477.55169, Residuals: -0.93097, Convergence: 0.001069\n",
      "Epoch: 240, Loss: 2475.31189, Residuals: -0.93054, Convergence: 0.000905\n",
      "Evidence 14884.039\n",
      "\n",
      "Epoch: 240, Evidence: 14884.03906, Convergence: 0.001942\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.42e-01\n",
      "Epoch: 240, Loss: 2487.89242, Residuals: -0.93054, Convergence:   inf\n",
      "Epoch: 241, Loss: 2485.06161, Residuals: -0.92827, Convergence: 0.001139\n",
      "Epoch: 242, Loss: 2482.83483, Residuals: -0.92721, Convergence: 0.000897\n",
      "Evidence 14894.610\n",
      "\n",
      "Epoch: 242, Evidence: 14894.61035, Convergence: 0.000710\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.20e-01\n",
      "Epoch: 242, Loss: 2488.03281, Residuals: -0.92721, Convergence:   inf\n",
      "Epoch: 243, Loss: 2483.95733, Residuals: -0.92421, Convergence: 0.001641\n",
      "Epoch: 244, Loss: 2481.07381, Residuals: -0.92544, Convergence: 0.001162\n",
      "Epoch: 245, Loss: 2478.68567, Residuals: -0.92623, Convergence: 0.000963\n",
      "Evidence 14907.426\n",
      "\n",
      "Epoch: 245, Evidence: 14907.42578, Convergence: 0.001569\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 9.78e-02\n",
      "Epoch: 245, Loss: 2488.05152, Residuals: -0.92623, Convergence:   inf\n",
      "Epoch: 246, Loss: 2485.57081, Residuals: -0.92445, Convergence: 0.000998\n",
      "Evidence 14914.078\n",
      "\n",
      "Epoch: 246, Evidence: 14914.07812, Convergence: 0.000446\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 8.81e-02\n",
      "Epoch: 246, Loss: 2487.70814, Residuals: -0.92445, Convergence:   inf\n",
      "Epoch: 247, Loss: 2488.02752, Residuals: -0.92651, Convergence: -0.000128\n",
      "Evidence 14915.934\n",
      "\n",
      "Epoch: 247, Evidence: 14915.93359, Convergence: 0.000570\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 6.63e-02\n",
      "Epoch: 247, Loss: 2488.79271, Residuals: -0.92651, Convergence:   inf\n",
      "Epoch: 248, Loss: 17389.66758, Residuals: -0.85127, Convergence: -0.856881\n",
      "Epoch: 248, Loss: 2534.75229, Residuals: -0.97327, Convergence: -0.018132\n",
      "Epoch: 248, Loss: 2486.18760, Residuals: -0.92242, Convergence: 0.001048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 249, Loss: 2485.46232, Residuals: -0.92224, Convergence: 0.000292\n",
      "Evidence 14923.244\n",
      "\n",
      "Epoch: 249, Evidence: 14923.24414, Convergence: 0.001060\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 5.93e-02\n",
      "Epoch: 249, Loss: 2487.84815, Residuals: -0.92224, Convergence:   inf\n",
      "Epoch: 250, Loss: 2487.20941, Residuals: -0.92152, Convergence: 0.000257\n",
      "Evidence 14926.018\n",
      "\n",
      "Epoch: 250, Evidence: 14926.01758, Convergence: 0.000186\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 5.23e-02\n",
      "Epoch: 250, Loss: 2487.87742, Residuals: -0.92152, Convergence:   inf\n",
      "Epoch: 251, Loss: 2520.95033, Residuals: -0.95308, Convergence: -0.013119\n",
      "Epoch: 251, Loss: 2488.22658, Residuals: -0.91763, Convergence: -0.000140\n",
      "Evidence 14926.957\n",
      "\n",
      "Epoch: 251, Evidence: 14926.95703, Convergence: 0.000249\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 4.73e-02\n",
      "Epoch: 251, Loss: 2487.98457, Residuals: -0.91763, Convergence:   inf\n",
      "Epoch: 252, Loss: 2538.97365, Residuals: -0.94739, Convergence: -0.020083\n",
      "Epoch: 252, Loss: 2487.25356, Residuals: -0.91623, Convergence: 0.000294\n",
      "Evidence 14928.879\n",
      "\n",
      "Epoch: 252, Evidence: 14928.87891, Convergence: 0.000377\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 382.70389, Residuals: -4.52595, Convergence:   inf\n",
      "Epoch: 1, Loss: 356.95995, Residuals: -4.40539, Convergence: 0.072120\n",
      "Epoch: 2, Loss: 335.89929, Residuals: -4.24120, Convergence: 0.062699\n",
      "Epoch: 3, Loss: 319.79880, Residuals: -4.07620, Convergence: 0.050346\n",
      "Epoch: 4, Loss: 307.52312, Residuals: -3.93087, Convergence: 0.039918\n",
      "Epoch: 5, Loss: 297.78981, Residuals: -3.80246, Convergence: 0.032685\n",
      "Epoch: 6, Loss: 289.88870, Residuals: -3.69064, Convergence: 0.027256\n",
      "Epoch: 7, Loss: 283.32941, Residuals: -3.59490, Convergence: 0.023151\n",
      "Epoch: 8, Loss: 277.74982, Residuals: -3.51317, Convergence: 0.020089\n",
      "Epoch: 9, Loss: 272.89432, Residuals: -3.44312, Convergence: 0.017793\n",
      "Epoch: 10, Loss: 268.58128, Residuals: -3.38264, Convergence: 0.016059\n",
      "Epoch: 11, Loss: 264.67930, Residuals: -3.32993, Convergence: 0.014742\n",
      "Epoch: 12, Loss: 261.09256, Residuals: -3.28343, Convergence: 0.013737\n",
      "Epoch: 13, Loss: 257.75160, Residuals: -3.24183, Convergence: 0.012962\n",
      "Epoch: 14, Loss: 254.60693, Residuals: -3.20395, Convergence: 0.012351\n",
      "Epoch: 15, Loss: 251.62538, Residuals: -3.16887, Convergence: 0.011849\n",
      "Epoch: 16, Loss: 248.78762, Residuals: -3.13590, Convergence: 0.011406\n",
      "Epoch: 17, Loss: 246.08062, Residuals: -3.10466, Convergence: 0.011000\n",
      "Epoch: 18, Loss: 243.48507, Residuals: -3.07478, Convergence: 0.010660\n",
      "Epoch: 19, Loss: 240.97060, Residuals: -3.04581, Convergence: 0.010435\n",
      "Epoch: 20, Loss: 238.50166, Residuals: -3.01720, Convergence: 0.010352\n",
      "Epoch: 21, Loss: 236.04655, Residuals: -2.98845, Convergence: 0.010401\n",
      "Epoch: 22, Loss: 233.58237, Residuals: -2.95920, Convergence: 0.010549\n",
      "Epoch: 23, Loss: 231.08683, Residuals: -2.92919, Convergence: 0.010799\n",
      "Epoch: 24, Loss: 228.52031, Residuals: -2.89798, Convergence: 0.011231\n",
      "Epoch: 25, Loss: 225.82449, Residuals: -2.86487, Convergence: 0.011938\n",
      "Epoch: 26, Loss: 222.97663, Residuals: -2.82949, Convergence: 0.012772\n",
      "Epoch: 27, Loss: 220.07536, Residuals: -2.79282, Convergence: 0.013183\n",
      "Epoch: 28, Loss: 217.24441, Residuals: -2.75621, Convergence: 0.013031\n",
      "Epoch: 29, Loss: 214.51411, Residuals: -2.72015, Convergence: 0.012728\n",
      "Epoch: 30, Loss: 211.87042, Residuals: -2.68455, Convergence: 0.012478\n",
      "Epoch: 31, Loss: 209.29553, Residuals: -2.64927, Convergence: 0.012303\n",
      "Epoch: 32, Loss: 206.77676, Residuals: -2.61420, Convergence: 0.012181\n",
      "Epoch: 33, Loss: 204.30679, Residuals: -2.57926, Convergence: 0.012090\n",
      "Epoch: 34, Loss: 201.88249, Residuals: -2.54439, Convergence: 0.012008\n",
      "Epoch: 35, Loss: 199.50361, Residuals: -2.50959, Convergence: 0.011924\n",
      "Epoch: 36, Loss: 197.17174, Residuals: -2.47485, Convergence: 0.011827\n",
      "Epoch: 37, Loss: 194.88946, Residuals: -2.44019, Convergence: 0.011711\n",
      "Epoch: 38, Loss: 192.65975, Residuals: -2.40565, Convergence: 0.011573\n",
      "Epoch: 39, Loss: 190.48562, Residuals: -2.37124, Convergence: 0.011414\n",
      "Epoch: 40, Loss: 188.36980, Residuals: -2.33701, Convergence: 0.011232\n",
      "Epoch: 41, Loss: 186.31465, Residuals: -2.30300, Convergence: 0.011031\n",
      "Epoch: 42, Loss: 184.32202, Residuals: -2.26924, Convergence: 0.010811\n",
      "Epoch: 43, Loss: 182.39323, Residuals: -2.23577, Convergence: 0.010575\n",
      "Epoch: 44, Loss: 180.52911, Residuals: -2.20263, Convergence: 0.010326\n",
      "Epoch: 45, Loss: 178.72998, Residuals: -2.16986, Convergence: 0.010066\n",
      "Epoch: 46, Loss: 176.99577, Residuals: -2.13749, Convergence: 0.009798\n",
      "Epoch: 47, Loss: 175.32616, Residuals: -2.10556, Convergence: 0.009523\n",
      "Epoch: 48, Loss: 173.72064, Residuals: -2.07411, Convergence: 0.009242\n",
      "Epoch: 49, Loss: 172.17866, Residuals: -2.04316, Convergence: 0.008956\n",
      "Epoch: 50, Loss: 170.69965, Residuals: -2.01275, Convergence: 0.008664\n",
      "Epoch: 51, Loss: 169.28293, Residuals: -1.98293, Convergence: 0.008369\n",
      "Epoch: 52, Loss: 167.92760, Residuals: -1.95372, Convergence: 0.008071\n",
      "Epoch: 53, Loss: 166.63230, Residuals: -1.92515, Convergence: 0.007773\n",
      "Epoch: 54, Loss: 165.39522, Residuals: -1.89726, Convergence: 0.007480\n",
      "Epoch: 55, Loss: 164.21398, Residuals: -1.87005, Convergence: 0.007193\n",
      "Epoch: 56, Loss: 163.08579, Residuals: -1.84352, Convergence: 0.006918\n",
      "Epoch: 57, Loss: 162.00757, Residuals: -1.81768, Convergence: 0.006655\n",
      "Epoch: 58, Loss: 160.97609, Residuals: -1.79252, Convergence: 0.006408\n",
      "Epoch: 59, Loss: 159.98826, Residuals: -1.76801, Convergence: 0.006174\n",
      "Epoch: 60, Loss: 159.04117, Residuals: -1.74414, Convergence: 0.005955\n",
      "Epoch: 61, Loss: 158.13227, Residuals: -1.72090, Convergence: 0.005748\n",
      "Epoch: 62, Loss: 157.25937, Residuals: -1.69826, Convergence: 0.005551\n",
      "Epoch: 63, Loss: 156.42069, Residuals: -1.67623, Convergence: 0.005362\n",
      "Epoch: 64, Loss: 155.61476, Residuals: -1.65479, Convergence: 0.005179\n",
      "Epoch: 65, Loss: 154.84038, Residuals: -1.63394, Convergence: 0.005001\n",
      "Epoch: 66, Loss: 154.09655, Residuals: -1.61368, Convergence: 0.004827\n",
      "Epoch: 67, Loss: 153.38240, Residuals: -1.59400, Convergence: 0.004656\n",
      "Epoch: 68, Loss: 152.69719, Residuals: -1.57491, Convergence: 0.004487\n",
      "Epoch: 69, Loss: 152.04020, Residuals: -1.55641, Convergence: 0.004321\n",
      "Epoch: 70, Loss: 151.41075, Residuals: -1.53849, Convergence: 0.004157\n",
      "Epoch: 71, Loss: 150.80817, Residuals: -1.52115, Convergence: 0.003996\n",
      "Epoch: 72, Loss: 150.23180, Residuals: -1.50439, Convergence: 0.003837\n",
      "Epoch: 73, Loss: 149.68094, Residuals: -1.48821, Convergence: 0.003680\n",
      "Epoch: 74, Loss: 149.15489, Residuals: -1.47260, Convergence: 0.003527\n",
      "Epoch: 75, Loss: 148.65292, Residuals: -1.45755, Convergence: 0.003377\n",
      "Epoch: 76, Loss: 148.17427, Residuals: -1.44305, Convergence: 0.003230\n",
      "Epoch: 77, Loss: 147.71817, Residuals: -1.42910, Convergence: 0.003088\n",
      "Epoch: 78, Loss: 147.28380, Residuals: -1.41569, Convergence: 0.002949\n",
      "Epoch: 79, Loss: 146.87032, Residuals: -1.40279, Convergence: 0.002815\n",
      "Epoch: 80, Loss: 146.47692, Residuals: -1.39041, Convergence: 0.002686\n",
      "Epoch: 81, Loss: 146.10272, Residuals: -1.37852, Convergence: 0.002561\n",
      "Epoch: 82, Loss: 145.74688, Residuals: -1.36712, Convergence: 0.002441\n",
      "Epoch: 83, Loss: 145.40856, Residuals: -1.35619, Convergence: 0.002327\n",
      "Epoch: 84, Loss: 145.08692, Residuals: -1.34571, Convergence: 0.002217\n",
      "Epoch: 85, Loss: 144.78116, Residuals: -1.33567, Convergence: 0.002112\n",
      "Epoch: 86, Loss: 144.49048, Residuals: -1.32605, Convergence: 0.002012\n",
      "Epoch: 87, Loss: 144.21414, Residuals: -1.31684, Convergence: 0.001916\n",
      "Epoch: 88, Loss: 143.95143, Residuals: -1.30803, Convergence: 0.001825\n",
      "Epoch: 89, Loss: 143.70167, Residuals: -1.29959, Convergence: 0.001738\n",
      "Epoch: 90, Loss: 143.46425, Residuals: -1.29152, Convergence: 0.001655\n",
      "Epoch: 91, Loss: 143.23856, Residuals: -1.28379, Convergence: 0.001576\n",
      "Epoch: 92, Loss: 143.02406, Residuals: -1.27641, Convergence: 0.001500\n",
      "Epoch: 93, Loss: 142.82028, Residuals: -1.26934, Convergence: 0.001427\n",
      "Epoch: 94, Loss: 142.62676, Residuals: -1.26259, Convergence: 0.001357\n",
      "Epoch: 95, Loss: 142.44309, Residuals: -1.25614, Convergence: 0.001289\n",
      "Epoch: 96, Loss: 142.26890, Residuals: -1.24997, Convergence: 0.001224\n",
      "Epoch: 97, Loss: 142.10387, Residuals: -1.24409, Convergence: 0.001161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 98, Loss: 141.94769, Residuals: -1.23848, Convergence: 0.001100\n",
      "Epoch: 99, Loss: 141.80010, Residuals: -1.23313, Convergence: 0.001041\n",
      "Epoch: 100, Loss: 141.66081, Residuals: -1.22804, Convergence: 0.000983\n",
      "Evidence -183.439\n",
      "\n",
      "Epoch: 100, Evidence: -183.43852, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.25e-01\n",
      "Epoch: 100, Loss: 1373.03906, Residuals: -1.22804, Convergence:   inf\n",
      "Epoch: 101, Loss: 1309.62980, Residuals: -1.25768, Convergence: 0.048418\n",
      "Epoch: 102, Loss: 1261.50123, Residuals: -1.28110, Convergence: 0.038152\n",
      "Epoch: 103, Loss: 1225.32548, Residuals: -1.29780, Convergence: 0.029523\n",
      "Epoch: 104, Loss: 1197.27272, Residuals: -1.30940, Convergence: 0.023431\n",
      "Epoch: 105, Loss: 1174.61907, Residuals: -1.31791, Convergence: 0.019286\n",
      "Epoch: 106, Loss: 1155.83087, Residuals: -1.32438, Convergence: 0.016255\n",
      "Epoch: 107, Loss: 1139.98739, Residuals: -1.32927, Convergence: 0.013898\n",
      "Epoch: 108, Loss: 1126.46141, Residuals: -1.33280, Convergence: 0.012007\n",
      "Epoch: 109, Loss: 1114.78691, Residuals: -1.33514, Convergence: 0.010472\n",
      "Epoch: 110, Loss: 1104.59989, Residuals: -1.33640, Convergence: 0.009222\n",
      "Epoch: 111, Loss: 1095.60830, Residuals: -1.33670, Convergence: 0.008207\n",
      "Epoch: 112, Loss: 1087.57163, Residuals: -1.33610, Convergence: 0.007390\n",
      "Epoch: 113, Loss: 1080.28879, Residuals: -1.33469, Convergence: 0.006742\n",
      "Epoch: 114, Loss: 1073.58714, Residuals: -1.33250, Convergence: 0.006242\n",
      "Epoch: 115, Loss: 1067.31709, Residuals: -1.32958, Convergence: 0.005875\n",
      "Epoch: 116, Loss: 1061.34547, Residuals: -1.32594, Convergence: 0.005626\n",
      "Epoch: 117, Loss: 1055.55575, Residuals: -1.32160, Convergence: 0.005485\n",
      "Epoch: 118, Loss: 1049.85062, Residuals: -1.31656, Convergence: 0.005434\n",
      "Epoch: 119, Loss: 1044.15990, Residuals: -1.31087, Convergence: 0.005450\n",
      "Epoch: 120, Loss: 1038.45454, Residuals: -1.30458, Convergence: 0.005494\n",
      "Epoch: 121, Loss: 1032.75578, Residuals: -1.29778, Convergence: 0.005518\n",
      "Epoch: 122, Loss: 1027.13348, Residuals: -1.29057, Convergence: 0.005474\n",
      "Epoch: 123, Loss: 1021.68015, Residuals: -1.28306, Convergence: 0.005338\n",
      "Epoch: 124, Loss: 1016.47886, Residuals: -1.27535, Convergence: 0.005117\n",
      "Epoch: 125, Loss: 1011.58002, Residuals: -1.26750, Convergence: 0.004843\n",
      "Epoch: 126, Loss: 1006.99948, Residuals: -1.25960, Convergence: 0.004549\n",
      "Epoch: 127, Loss: 1002.72743, Residuals: -1.25170, Convergence: 0.004260\n",
      "Epoch: 128, Loss: 998.74108, Residuals: -1.24386, Convergence: 0.003991\n",
      "Epoch: 129, Loss: 995.01224, Residuals: -1.23611, Convergence: 0.003748\n",
      "Epoch: 130, Loss: 991.51382, Residuals: -1.22849, Convergence: 0.003528\n",
      "Epoch: 131, Loss: 988.22101, Residuals: -1.22103, Convergence: 0.003332\n",
      "Epoch: 132, Loss: 985.11246, Residuals: -1.21375, Convergence: 0.003156\n",
      "Epoch: 133, Loss: 982.17023, Residuals: -1.20667, Convergence: 0.002996\n",
      "Epoch: 134, Loss: 979.37932, Residuals: -1.19980, Convergence: 0.002850\n",
      "Epoch: 135, Loss: 976.72733, Residuals: -1.19315, Convergence: 0.002715\n",
      "Epoch: 136, Loss: 974.20354, Residuals: -1.18673, Convergence: 0.002591\n",
      "Epoch: 137, Loss: 971.79896, Residuals: -1.18054, Convergence: 0.002474\n",
      "Epoch: 138, Loss: 969.50583, Residuals: -1.17459, Convergence: 0.002365\n",
      "Epoch: 139, Loss: 967.31702, Residuals: -1.16888, Convergence: 0.002263\n",
      "Epoch: 140, Loss: 965.22617, Residuals: -1.16341, Convergence: 0.002166\n",
      "Epoch: 141, Loss: 963.22747, Residuals: -1.15817, Convergence: 0.002075\n",
      "Epoch: 142, Loss: 961.31524, Residuals: -1.15315, Convergence: 0.001989\n",
      "Epoch: 143, Loss: 959.48501, Residuals: -1.14836, Convergence: 0.001908\n",
      "Epoch: 144, Loss: 957.73195, Residuals: -1.14378, Convergence: 0.001830\n",
      "Epoch: 145, Loss: 956.05224, Residuals: -1.13940, Convergence: 0.001757\n",
      "Epoch: 146, Loss: 954.44174, Residuals: -1.13522, Convergence: 0.001687\n",
      "Epoch: 147, Loss: 952.89717, Residuals: -1.13122, Convergence: 0.001621\n",
      "Epoch: 148, Loss: 951.41501, Residuals: -1.12741, Convergence: 0.001558\n",
      "Epoch: 149, Loss: 949.99228, Residuals: -1.12377, Convergence: 0.001498\n",
      "Epoch: 150, Loss: 948.62635, Residuals: -1.12029, Convergence: 0.001440\n",
      "Epoch: 151, Loss: 947.31371, Residuals: -1.11697, Convergence: 0.001386\n",
      "Epoch: 152, Loss: 946.05173, Residuals: -1.11379, Convergence: 0.001334\n",
      "Epoch: 153, Loss: 944.83717, Residuals: -1.11075, Convergence: 0.001285\n",
      "Epoch: 154, Loss: 943.66746, Residuals: -1.10784, Convergence: 0.001240\n",
      "Epoch: 155, Loss: 942.53888, Residuals: -1.10505, Convergence: 0.001197\n",
      "Epoch: 156, Loss: 941.44865, Residuals: -1.10237, Convergence: 0.001158\n",
      "Epoch: 157, Loss: 940.39301, Residuals: -1.09981, Convergence: 0.001123\n",
      "Epoch: 158, Loss: 939.36853, Residuals: -1.09733, Convergence: 0.001091\n",
      "Epoch: 159, Loss: 938.37131, Residuals: -1.09495, Convergence: 0.001063\n",
      "Epoch: 160, Loss: 937.39762, Residuals: -1.09265, Convergence: 0.001039\n",
      "Epoch: 161, Loss: 936.44348, Residuals: -1.09042, Convergence: 0.001019\n",
      "Epoch: 162, Loss: 935.50479, Residuals: -1.08824, Convergence: 0.001003\n",
      "Epoch: 163, Loss: 934.57791, Residuals: -1.08612, Convergence: 0.000992\n",
      "Evidence 11162.616\n",
      "\n",
      "Epoch: 163, Evidence: 11162.61621, Convergence: 1.016433\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.77e-01\n",
      "Epoch: 163, Loss: 2348.79176, Residuals: -1.08612, Convergence:   inf\n",
      "Epoch: 164, Loss: 2309.08038, Residuals: -1.09546, Convergence: 0.017198\n",
      "Epoch: 165, Loss: 2281.93740, Residuals: -1.09437, Convergence: 0.011895\n",
      "Epoch: 166, Loss: 2259.17055, Residuals: -1.09228, Convergence: 0.010078\n",
      "Epoch: 167, Loss: 2239.78606, Residuals: -1.08977, Convergence: 0.008655\n",
      "Epoch: 168, Loss: 2223.12685, Residuals: -1.08699, Convergence: 0.007494\n",
      "Epoch: 169, Loss: 2208.69808, Residuals: -1.08402, Convergence: 0.006533\n",
      "Epoch: 170, Loss: 2196.10399, Residuals: -1.08092, Convergence: 0.005735\n",
      "Epoch: 171, Loss: 2185.01631, Residuals: -1.07773, Convergence: 0.005074\n",
      "Epoch: 172, Loss: 2175.15671, Residuals: -1.07445, Convergence: 0.004533\n",
      "Epoch: 173, Loss: 2166.29317, Residuals: -1.07109, Convergence: 0.004092\n",
      "Epoch: 174, Loss: 2158.23562, Residuals: -1.06765, Convergence: 0.003733\n",
      "Epoch: 175, Loss: 2150.84279, Residuals: -1.06413, Convergence: 0.003437\n",
      "Epoch: 176, Loss: 2144.01744, Residuals: -1.06053, Convergence: 0.003183\n",
      "Epoch: 177, Loss: 2137.69939, Residuals: -1.05690, Convergence: 0.002956\n",
      "Epoch: 178, Loss: 2131.84656, Residuals: -1.05328, Convergence: 0.002745\n",
      "Epoch: 179, Loss: 2126.42574, Residuals: -1.04970, Convergence: 0.002549\n",
      "Epoch: 180, Loss: 2121.40507, Residuals: -1.04620, Convergence: 0.002367\n",
      "Epoch: 181, Loss: 2116.75270, Residuals: -1.04282, Convergence: 0.002198\n",
      "Epoch: 182, Loss: 2112.43655, Residuals: -1.03956, Convergence: 0.002043\n",
      "Epoch: 183, Loss: 2108.42946, Residuals: -1.03645, Convergence: 0.001901\n",
      "Epoch: 184, Loss: 2104.70385, Residuals: -1.03349, Convergence: 0.001770\n",
      "Epoch: 185, Loss: 2101.23765, Residuals: -1.03068, Convergence: 0.001650\n",
      "Epoch: 186, Loss: 2098.01122, Residuals: -1.02803, Convergence: 0.001538\n",
      "Epoch: 187, Loss: 2095.00456, Residuals: -1.02553, Convergence: 0.001435\n",
      "Epoch: 188, Loss: 2092.20303, Residuals: -1.02317, Convergence: 0.001339\n",
      "Epoch: 189, Loss: 2089.59069, Residuals: -1.02095, Convergence: 0.001250\n",
      "Epoch: 190, Loss: 2087.15244, Residuals: -1.01886, Convergence: 0.001168\n",
      "Epoch: 191, Loss: 2084.87586, Residuals: -1.01689, Convergence: 0.001092\n",
      "Epoch: 192, Loss: 2082.74857, Residuals: -1.01503, Convergence: 0.001021\n",
      "Epoch: 193, Loss: 2080.75768, Residuals: -1.01327, Convergence: 0.000957\n",
      "Evidence 14276.141\n",
      "\n",
      "Epoch: 193, Evidence: 14276.14062, Convergence: 0.218093\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.39e-01\n",
      "Epoch: 193, Loss: 2472.33095, Residuals: -1.01327, Convergence:   inf\n",
      "Epoch: 194, Loss: 2458.54223, Residuals: -1.01022, Convergence: 0.005608\n",
      "Epoch: 195, Loss: 2447.28692, Residuals: -1.00663, Convergence: 0.004599\n",
      "Epoch: 196, Loss: 2437.60627, Residuals: -1.00313, Convergence: 0.003971\n",
      "Epoch: 197, Loss: 2429.22985, Residuals: -0.99983, Convergence: 0.003448\n",
      "Epoch: 198, Loss: 2421.94663, Residuals: -0.99678, Convergence: 0.003007\n",
      "Epoch: 199, Loss: 2415.58083, Residuals: -0.99398, Convergence: 0.002635\n",
      "Epoch: 200, Loss: 2409.98609, Residuals: -0.99141, Convergence: 0.002321\n",
      "Epoch: 201, Loss: 2405.04142, Residuals: -0.98906, Convergence: 0.002056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 202, Loss: 2400.64450, Residuals: -0.98690, Convergence: 0.001832\n",
      "Epoch: 203, Loss: 2396.71075, Residuals: -0.98492, Convergence: 0.001641\n",
      "Epoch: 204, Loss: 2393.17057, Residuals: -0.98310, Convergence: 0.001479\n",
      "Epoch: 205, Loss: 2389.96584, Residuals: -0.98142, Convergence: 0.001341\n",
      "Epoch: 206, Loss: 2387.04841, Residuals: -0.97987, Convergence: 0.001222\n",
      "Epoch: 207, Loss: 2384.37874, Residuals: -0.97843, Convergence: 0.001120\n",
      "Epoch: 208, Loss: 2381.92483, Residuals: -0.97711, Convergence: 0.001030\n",
      "Epoch: 209, Loss: 2379.65798, Residuals: -0.97589, Convergence: 0.000953\n",
      "Evidence 14667.844\n",
      "\n",
      "Epoch: 209, Evidence: 14667.84375, Convergence: 0.026705\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.36e-01\n",
      "Epoch: 209, Loss: 2477.97054, Residuals: -0.97589, Convergence:   inf\n",
      "Epoch: 210, Loss: 2471.43797, Residuals: -0.97247, Convergence: 0.002643\n",
      "Epoch: 211, Loss: 2466.01988, Residuals: -0.96946, Convergence: 0.002197\n",
      "Epoch: 212, Loss: 2461.41548, Residuals: -0.96687, Convergence: 0.001871\n",
      "Epoch: 213, Loss: 2457.44809, Residuals: -0.96465, Convergence: 0.001614\n",
      "Epoch: 214, Loss: 2453.98473, Residuals: -0.96276, Convergence: 0.001411\n",
      "Epoch: 215, Loss: 2450.92470, Residuals: -0.96113, Convergence: 0.001249\n",
      "Epoch: 216, Loss: 2448.18996, Residuals: -0.95973, Convergence: 0.001117\n",
      "Epoch: 217, Loss: 2445.72148, Residuals: -0.95853, Convergence: 0.001009\n",
      "Epoch: 218, Loss: 2443.47345, Residuals: -0.95750, Convergence: 0.000920\n",
      "Evidence 14752.772\n",
      "\n",
      "Epoch: 218, Evidence: 14752.77246, Convergence: 0.005757\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.63e-01\n",
      "Epoch: 218, Loss: 2479.63672, Residuals: -0.95750, Convergence:   inf\n",
      "Epoch: 219, Loss: 2475.80228, Residuals: -0.95492, Convergence: 0.001549\n",
      "Epoch: 220, Loss: 2472.59476, Residuals: -0.95285, Convergence: 0.001297\n",
      "Epoch: 221, Loss: 2469.83375, Residuals: -0.95118, Convergence: 0.001118\n",
      "Epoch: 222, Loss: 2467.40953, Residuals: -0.94982, Convergence: 0.000982\n",
      "Evidence 14781.403\n",
      "\n",
      "Epoch: 222, Evidence: 14781.40332, Convergence: 0.001937\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.12e-01\n",
      "Epoch: 222, Loss: 2480.75255, Residuals: -0.94982, Convergence:   inf\n",
      "Epoch: 223, Loss: 2477.86624, Residuals: -0.94778, Convergence: 0.001165\n",
      "Epoch: 224, Loss: 2475.43123, Residuals: -0.94620, Convergence: 0.000984\n",
      "Evidence 14793.377\n",
      "\n",
      "Epoch: 224, Evidence: 14793.37695, Convergence: 0.000809\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.76e-01\n",
      "Epoch: 224, Loss: 2481.53776, Residuals: -0.94620, Convergence:   inf\n",
      "Epoch: 225, Loss: 2476.93702, Residuals: -0.94344, Convergence: 0.001857\n",
      "Epoch: 226, Loss: 2473.42407, Residuals: -0.94157, Convergence: 0.001420\n",
      "Epoch: 227, Loss: 2470.55576, Residuals: -0.94039, Convergence: 0.001161\n",
      "Epoch: 228, Loss: 2468.11207, Residuals: -0.93975, Convergence: 0.000990\n",
      "Evidence 14811.317\n",
      "\n",
      "Epoch: 228, Evidence: 14811.31738, Convergence: 0.002020\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.46e-01\n",
      "Epoch: 228, Loss: 2481.81489, Residuals: -0.93975, Convergence:   inf\n",
      "Epoch: 229, Loss: 2478.75124, Residuals: -0.93740, Convergence: 0.001236\n",
      "Epoch: 230, Loss: 2476.31180, Residuals: -0.93628, Convergence: 0.000985\n",
      "Evidence 14822.224\n",
      "\n",
      "Epoch: 230, Evidence: 14822.22363, Convergence: 0.000736\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.24e-01\n",
      "Epoch: 230, Loss: 2482.06489, Residuals: -0.93628, Convergence:   inf\n",
      "Epoch: 231, Loss: 2477.56757, Residuals: -0.93302, Convergence: 0.001815\n",
      "Epoch: 232, Loss: 2474.31237, Residuals: -0.93442, Convergence: 0.001316\n",
      "Epoch: 233, Loss: 2471.62506, Residuals: -0.93514, Convergence: 0.001087\n",
      "Epoch: 234, Loss: 2469.28836, Residuals: -0.93790, Convergence: 0.000946\n",
      "Evidence 14838.106\n",
      "\n",
      "Epoch: 234, Evidence: 14838.10645, Convergence: 0.001805\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.10e-01\n",
      "Epoch: 234, Loss: 2481.44499, Residuals: -0.93790, Convergence:   inf\n",
      "Epoch: 235, Loss: 2479.57178, Residuals: -0.93596, Convergence: 0.000755\n",
      "Evidence 14844.740\n",
      "\n",
      "Epoch: 235, Evidence: 14844.74023, Convergence: 0.000447\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 9.00e-02\n",
      "Epoch: 235, Loss: 2482.41633, Residuals: -0.93596, Convergence:   inf\n",
      "Epoch: 236, Loss: 2527.31328, Residuals: -0.97713, Convergence: -0.017765\n",
      "Epoch: 236, Loss: 2480.09676, Residuals: -0.93354, Convergence: 0.000935\n",
      "Evidence 14848.946\n",
      "\n",
      "Epoch: 236, Evidence: 14848.94629, Convergence: 0.000730\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.28e-02\n",
      "Epoch: 236, Loss: 2481.89959, Residuals: -0.93354, Convergence:   inf\n",
      "Epoch: 237, Loss: 2485.70235, Residuals: -0.93598, Convergence: -0.001530\n",
      "Epoch: 237, Loss: 2481.63764, Residuals: -0.93257, Convergence: 0.000106\n",
      "Evidence 14850.712\n",
      "\n",
      "Epoch: 237, Evidence: 14850.71191, Convergence: 0.000849\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 384.30518, Residuals: -4.52670, Convergence:   inf\n",
      "Epoch: 1, Loss: 358.63484, Residuals: -4.40792, Convergence: 0.071578\n",
      "Epoch: 2, Loss: 337.64197, Residuals: -4.24563, Convergence: 0.062175\n",
      "Epoch: 3, Loss: 321.59262, Residuals: -4.08257, Convergence: 0.049906\n",
      "Epoch: 4, Loss: 309.34510, Residuals: -3.93879, Convergence: 0.039592\n",
      "Epoch: 5, Loss: 299.62510, Residuals: -3.81157, Convergence: 0.032441\n",
      "Epoch: 6, Loss: 291.72984, Residuals: -3.70072, Convergence: 0.027064\n",
      "Epoch: 7, Loss: 285.17066, Residuals: -3.60578, Convergence: 0.023001\n",
      "Epoch: 8, Loss: 279.58564, Residuals: -3.52468, Convergence: 0.019976\n",
      "Epoch: 9, Loss: 274.71959, Residuals: -3.45507, Convergence: 0.017713\n",
      "Epoch: 10, Loss: 270.39121, Residuals: -3.39485, Convergence: 0.016008\n",
      "Epoch: 11, Loss: 266.46922, Residuals: -3.34224, Convergence: 0.014718\n",
      "Epoch: 12, Loss: 262.85791, Residuals: -3.29572, Convergence: 0.013739\n",
      "Epoch: 13, Loss: 259.48795, Residuals: -3.25397, Convergence: 0.012987\n",
      "Epoch: 14, Loss: 256.31025, Residuals: -3.21584, Convergence: 0.012398\n",
      "Epoch: 15, Loss: 253.29287, Residuals: -3.18042, Convergence: 0.011913\n",
      "Epoch: 16, Loss: 250.41864, Residuals: -3.14707, Convergence: 0.011478\n",
      "Epoch: 17, Loss: 247.67610, Residuals: -3.11543, Convergence: 0.011073\n",
      "Epoch: 18, Loss: 245.04586, Residuals: -3.08515, Convergence: 0.010734\n",
      "Epoch: 19, Loss: 242.49673, Residuals: -3.05576, Convergence: 0.010512\n",
      "Epoch: 20, Loss: 239.99225, Residuals: -3.02669, Convergence: 0.010436\n",
      "Epoch: 21, Loss: 237.49890, Residuals: -2.99743, Convergence: 0.010498\n",
      "Epoch: 22, Loss: 234.98972, Residuals: -2.96758, Convergence: 0.010678\n",
      "Epoch: 23, Loss: 232.43666, Residuals: -2.93679, Convergence: 0.010984\n",
      "Epoch: 24, Loss: 229.79635, Residuals: -2.90458, Convergence: 0.011490\n",
      "Epoch: 25, Loss: 227.01859, Residuals: -2.87034, Convergence: 0.012236\n",
      "Epoch: 26, Loss: 224.11550, Residuals: -2.83408, Convergence: 0.012954\n",
      "Epoch: 27, Loss: 221.20384, Residuals: -2.79704, Convergence: 0.013163\n",
      "Epoch: 28, Loss: 218.37056, Residuals: -2.76030, Convergence: 0.012975\n",
      "Epoch: 29, Loss: 215.62299, Residuals: -2.72406, Convergence: 0.012742\n",
      "Epoch: 30, Loss: 212.94439, Residuals: -2.68820, Convergence: 0.012579\n",
      "Epoch: 31, Loss: 210.31939, Residuals: -2.65257, Convergence: 0.012481\n",
      "Epoch: 32, Loss: 207.73832, Residuals: -2.61706, Convergence: 0.012425\n",
      "Epoch: 33, Loss: 205.19671, Residuals: -2.58158, Convergence: 0.012386\n",
      "Epoch: 34, Loss: 202.69397, Residuals: -2.54609, Convergence: 0.012347\n",
      "Epoch: 35, Loss: 200.23203, Residuals: -2.51058, Convergence: 0.012295\n",
      "Epoch: 36, Loss: 197.81435, Residuals: -2.47506, Convergence: 0.012222\n",
      "Epoch: 37, Loss: 195.44504, Residuals: -2.43955, Convergence: 0.012123\n",
      "Epoch: 38, Loss: 193.12840, Residuals: -2.40410, Convergence: 0.011995\n",
      "Epoch: 39, Loss: 190.86848, Residuals: -2.36873, Convergence: 0.011840\n",
      "Epoch: 40, Loss: 188.66894, Residuals: -2.33351, Convergence: 0.011658\n",
      "Epoch: 41, Loss: 186.53286, Residuals: -2.29849, Convergence: 0.011451\n",
      "Epoch: 42, Loss: 184.46272, Residuals: -2.26371, Convergence: 0.011223\n",
      "Epoch: 43, Loss: 182.46040, Residuals: -2.22922, Convergence: 0.010974\n",
      "Epoch: 44, Loss: 180.52726, Residuals: -2.19508, Convergence: 0.010708\n",
      "Epoch: 45, Loss: 178.66427, Residuals: -2.16134, Convergence: 0.010427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46, Loss: 176.87214, Residuals: -2.12804, Convergence: 0.010132\n",
      "Epoch: 47, Loss: 175.15144, Residuals: -2.09523, Convergence: 0.009824\n",
      "Epoch: 48, Loss: 173.50266, Residuals: -2.06298, Convergence: 0.009503\n",
      "Epoch: 49, Loss: 171.92612, Residuals: -2.03133, Convergence: 0.009170\n",
      "Epoch: 50, Loss: 170.42172, Residuals: -2.00035, Convergence: 0.008828\n",
      "Epoch: 51, Loss: 168.98877, Residuals: -1.97007, Convergence: 0.008480\n",
      "Epoch: 52, Loss: 167.62578, Residuals: -1.94054, Convergence: 0.008131\n",
      "Epoch: 53, Loss: 166.33042, Residuals: -1.91180, Convergence: 0.007788\n",
      "Epoch: 54, Loss: 165.09959, Residuals: -1.88385, Convergence: 0.007455\n",
      "Epoch: 55, Loss: 163.92957, Residuals: -1.85669, Convergence: 0.007137\n",
      "Epoch: 56, Loss: 162.81631, Residuals: -1.83032, Convergence: 0.006838\n",
      "Epoch: 57, Loss: 161.75563, Residuals: -1.80472, Convergence: 0.006557\n",
      "Epoch: 58, Loss: 160.74344, Residuals: -1.77986, Convergence: 0.006297\n",
      "Epoch: 59, Loss: 159.77599, Residuals: -1.75571, Convergence: 0.006055\n",
      "Epoch: 60, Loss: 158.84990, Residuals: -1.73224, Convergence: 0.005830\n",
      "Epoch: 61, Loss: 157.96226, Residuals: -1.70943, Convergence: 0.005619\n",
      "Epoch: 62, Loss: 157.11061, Residuals: -1.68725, Convergence: 0.005421\n",
      "Epoch: 63, Loss: 156.29291, Residuals: -1.66569, Convergence: 0.005232\n",
      "Epoch: 64, Loss: 155.50748, Residuals: -1.64475, Convergence: 0.005051\n",
      "Epoch: 65, Loss: 154.75292, Residuals: -1.62440, Convergence: 0.004876\n",
      "Epoch: 66, Loss: 154.02808, Residuals: -1.60464, Convergence: 0.004706\n",
      "Epoch: 67, Loss: 153.33196, Residuals: -1.58546, Convergence: 0.004540\n",
      "Epoch: 68, Loss: 152.66369, Residuals: -1.56687, Convergence: 0.004377\n",
      "Epoch: 69, Loss: 152.02248, Residuals: -1.54885, Convergence: 0.004218\n",
      "Epoch: 70, Loss: 151.40758, Residuals: -1.53139, Convergence: 0.004061\n",
      "Epoch: 71, Loss: 150.81831, Residuals: -1.51450, Convergence: 0.003907\n",
      "Epoch: 72, Loss: 150.25395, Residuals: -1.49816, Convergence: 0.003756\n",
      "Epoch: 73, Loss: 149.71383, Residuals: -1.48238, Convergence: 0.003608\n",
      "Epoch: 74, Loss: 149.19725, Residuals: -1.46713, Convergence: 0.003462\n",
      "Epoch: 75, Loss: 148.70349, Residuals: -1.45241, Convergence: 0.003320\n",
      "Epoch: 76, Loss: 148.23185, Residuals: -1.43822, Convergence: 0.003182\n",
      "Epoch: 77, Loss: 147.78159, Residuals: -1.42454, Convergence: 0.003047\n",
      "Epoch: 78, Loss: 147.35197, Residuals: -1.41136, Convergence: 0.002916\n",
      "Epoch: 79, Loss: 146.94222, Residuals: -1.39866, Convergence: 0.002789\n",
      "Epoch: 80, Loss: 146.55157, Residuals: -1.38645, Convergence: 0.002666\n",
      "Epoch: 81, Loss: 146.17925, Residuals: -1.37469, Convergence: 0.002547\n",
      "Epoch: 82, Loss: 145.82447, Residuals: -1.36339, Convergence: 0.002433\n",
      "Epoch: 83, Loss: 145.48647, Residuals: -1.35253, Convergence: 0.002323\n",
      "Epoch: 84, Loss: 145.16448, Residuals: -1.34209, Convergence: 0.002218\n",
      "Epoch: 85, Loss: 144.85775, Residuals: -1.33206, Convergence: 0.002117\n",
      "Epoch: 86, Loss: 144.56557, Residuals: -1.32243, Convergence: 0.002021\n",
      "Epoch: 87, Loss: 144.28722, Residuals: -1.31318, Convergence: 0.001929\n",
      "Epoch: 88, Loss: 144.02205, Residuals: -1.30430, Convergence: 0.001841\n",
      "Epoch: 89, Loss: 143.76940, Residuals: -1.29578, Convergence: 0.001757\n",
      "Epoch: 90, Loss: 143.52870, Residuals: -1.28760, Convergence: 0.001677\n",
      "Epoch: 91, Loss: 143.29938, Residuals: -1.27975, Convergence: 0.001600\n",
      "Epoch: 92, Loss: 143.08092, Residuals: -1.27221, Convergence: 0.001527\n",
      "Epoch: 93, Loss: 142.87287, Residuals: -1.26499, Convergence: 0.001456\n",
      "Epoch: 94, Loss: 142.67480, Residuals: -1.25805, Convergence: 0.001388\n",
      "Epoch: 95, Loss: 142.48630, Residuals: -1.25140, Convergence: 0.001323\n",
      "Epoch: 96, Loss: 142.30704, Residuals: -1.24502, Convergence: 0.001260\n",
      "Epoch: 97, Loss: 142.13671, Residuals: -1.23890, Convergence: 0.001198\n",
      "Epoch: 98, Loss: 141.97503, Residuals: -1.23304, Convergence: 0.001139\n",
      "Epoch: 99, Loss: 141.82175, Residuals: -1.22742, Convergence: 0.001081\n",
      "Epoch: 100, Loss: 141.67666, Residuals: -1.22205, Convergence: 0.001024\n",
      "Epoch: 101, Loss: 141.53956, Residuals: -1.21690, Convergence: 0.000969\n",
      "Evidence -183.133\n",
      "\n",
      "Epoch: 101, Evidence: -183.13309, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.25e-01\n",
      "Epoch: 101, Loss: 1364.94224, Residuals: -1.21690, Convergence:   inf\n",
      "Epoch: 102, Loss: 1303.29708, Residuals: -1.24876, Convergence: 0.047299\n",
      "Epoch: 103, Loss: 1256.21866, Residuals: -1.27394, Convergence: 0.037476\n",
      "Epoch: 104, Loss: 1220.60724, Residuals: -1.29234, Convergence: 0.029175\n",
      "Epoch: 105, Loss: 1192.99969, Residuals: -1.30553, Convergence: 0.023141\n",
      "Epoch: 106, Loss: 1170.78092, Residuals: -1.31547, Convergence: 0.018978\n",
      "Epoch: 107, Loss: 1152.40462, Residuals: -1.32323, Convergence: 0.015946\n",
      "Epoch: 108, Loss: 1136.93911, Residuals: -1.32931, Convergence: 0.013603\n",
      "Epoch: 109, Loss: 1123.75401, Residuals: -1.33394, Convergence: 0.011733\n",
      "Epoch: 110, Loss: 1112.38432, Residuals: -1.33729, Convergence: 0.010221\n",
      "Epoch: 111, Loss: 1102.46700, Residuals: -1.33947, Convergence: 0.008996\n",
      "Epoch: 112, Loss: 1093.71092, Residuals: -1.34060, Convergence: 0.008006\n",
      "Epoch: 113, Loss: 1085.87641, Residuals: -1.34074, Convergence: 0.007215\n",
      "Epoch: 114, Loss: 1078.76383, Residuals: -1.33999, Convergence: 0.006593\n",
      "Epoch: 115, Loss: 1072.20612, Residuals: -1.33839, Convergence: 0.006116\n",
      "Epoch: 116, Loss: 1066.06175, Residuals: -1.33598, Convergence: 0.005764\n",
      "Epoch: 117, Loss: 1060.21470, Residuals: -1.33281, Convergence: 0.005515\n",
      "Epoch: 118, Loss: 1054.56938, Residuals: -1.32890, Convergence: 0.005353\n",
      "Epoch: 119, Loss: 1049.04974, Residuals: -1.32427, Convergence: 0.005262\n",
      "Epoch: 120, Loss: 1043.59746, Residuals: -1.31897, Convergence: 0.005225\n",
      "Epoch: 121, Loss: 1038.17437, Residuals: -1.31303, Convergence: 0.005224\n",
      "Epoch: 122, Loss: 1032.76769, Residuals: -1.30652, Convergence: 0.005235\n",
      "Epoch: 123, Loss: 1027.39771, Residuals: -1.29953, Convergence: 0.005227\n",
      "Epoch: 124, Loss: 1022.11682, Residuals: -1.29216, Convergence: 0.005167\n",
      "Epoch: 125, Loss: 1016.99653, Residuals: -1.28451, Convergence: 0.005035\n",
      "Epoch: 126, Loss: 1012.10257, Residuals: -1.27667, Convergence: 0.004835\n",
      "Epoch: 127, Loss: 1007.47758, Residuals: -1.26871, Convergence: 0.004591\n",
      "Epoch: 128, Loss: 1003.13614, Residuals: -1.26071, Convergence: 0.004328\n",
      "Epoch: 129, Loss: 999.07190, Residuals: -1.25271, Convergence: 0.004068\n",
      "Epoch: 130, Loss: 995.26614, Residuals: -1.24477, Convergence: 0.003824\n",
      "Epoch: 131, Loss: 991.69509, Residuals: -1.23692, Convergence: 0.003601\n",
      "Epoch: 132, Loss: 988.33593, Residuals: -1.22920, Convergence: 0.003399\n",
      "Epoch: 133, Loss: 985.16734, Residuals: -1.22163, Convergence: 0.003216\n",
      "Epoch: 134, Loss: 982.17099, Residuals: -1.21424, Convergence: 0.003051\n",
      "Epoch: 135, Loss: 979.33165, Residuals: -1.20703, Convergence: 0.002899\n",
      "Epoch: 136, Loss: 976.63633, Residuals: -1.20003, Convergence: 0.002760\n",
      "Epoch: 137, Loss: 974.07426, Residuals: -1.19325, Convergence: 0.002630\n",
      "Epoch: 138, Loss: 971.63660, Residuals: -1.18670, Convergence: 0.002509\n",
      "Epoch: 139, Loss: 969.31563, Residuals: -1.18038, Convergence: 0.002394\n",
      "Epoch: 140, Loss: 967.10479, Residuals: -1.17431, Convergence: 0.002286\n",
      "Epoch: 141, Loss: 964.99780, Residuals: -1.16847, Convergence: 0.002183\n",
      "Epoch: 142, Loss: 962.98932, Residuals: -1.16288, Convergence: 0.002086\n",
      "Epoch: 143, Loss: 961.07388, Residuals: -1.15752, Convergence: 0.001993\n",
      "Epoch: 144, Loss: 959.24647, Residuals: -1.15241, Convergence: 0.001905\n",
      "Epoch: 145, Loss: 957.50230, Residuals: -1.14752, Convergence: 0.001822\n",
      "Epoch: 146, Loss: 955.83650, Residuals: -1.14287, Convergence: 0.001743\n",
      "Epoch: 147, Loss: 954.24421, Residuals: -1.13843, Convergence: 0.001669\n",
      "Epoch: 148, Loss: 952.72120, Residuals: -1.13420, Convergence: 0.001599\n",
      "Epoch: 149, Loss: 951.26297, Residuals: -1.13017, Convergence: 0.001533\n",
      "Epoch: 150, Loss: 949.86512, Residuals: -1.12633, Convergence: 0.001472\n",
      "Epoch: 151, Loss: 948.52348, Residuals: -1.12268, Convergence: 0.001414\n",
      "Epoch: 152, Loss: 947.23403, Residuals: -1.11920, Convergence: 0.001361\n",
      "Epoch: 153, Loss: 945.99254, Residuals: -1.11588, Convergence: 0.001312\n",
      "Epoch: 154, Loss: 944.79523, Residuals: -1.11271, Convergence: 0.001267\n",
      "Epoch: 155, Loss: 943.63807, Residuals: -1.10968, Convergence: 0.001226\n",
      "Epoch: 156, Loss: 942.51668, Residuals: -1.10678, Convergence: 0.001190\n",
      "Epoch: 157, Loss: 941.42711, Residuals: -1.10401, Convergence: 0.001157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 158, Loss: 940.36507, Residuals: -1.10134, Convergence: 0.001129\n",
      "Epoch: 159, Loss: 939.32636, Residuals: -1.09878, Convergence: 0.001106\n",
      "Epoch: 160, Loss: 938.30658, Residuals: -1.09630, Convergence: 0.001087\n",
      "Epoch: 161, Loss: 937.30109, Residuals: -1.09389, Convergence: 0.001073\n",
      "Epoch: 162, Loss: 936.30568, Residuals: -1.09155, Convergence: 0.001063\n",
      "Epoch: 163, Loss: 935.31618, Residuals: -1.08926, Convergence: 0.001058\n",
      "Epoch: 164, Loss: 934.32923, Residuals: -1.08701, Convergence: 0.001056\n",
      "Epoch: 165, Loss: 933.34201, Residuals: -1.08479, Convergence: 0.001058\n",
      "Epoch: 166, Loss: 932.35270, Residuals: -1.08259, Convergence: 0.001061\n",
      "Epoch: 167, Loss: 931.36169, Residuals: -1.08042, Convergence: 0.001064\n",
      "Epoch: 168, Loss: 930.37027, Residuals: -1.07826, Convergence: 0.001066\n",
      "Epoch: 169, Loss: 929.38162, Residuals: -1.07613, Convergence: 0.001064\n",
      "Epoch: 170, Loss: 928.40003, Residuals: -1.07403, Convergence: 0.001057\n",
      "Epoch: 171, Loss: 927.43049, Residuals: -1.07196, Convergence: 0.001045\n",
      "Epoch: 172, Loss: 926.47798, Residuals: -1.06994, Convergence: 0.001028\n",
      "Epoch: 173, Loss: 925.54745, Residuals: -1.06797, Convergence: 0.001005\n",
      "Epoch: 174, Loss: 924.64264, Residuals: -1.06606, Convergence: 0.000979\n",
      "Evidence 11215.934\n",
      "\n",
      "Epoch: 174, Evidence: 11215.93359, Convergence: 1.016328\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.74e-01\n",
      "Epoch: 174, Loss: 2354.77964, Residuals: -1.06606, Convergence:   inf\n",
      "Epoch: 175, Loss: 2315.35127, Residuals: -1.07336, Convergence: 0.017029\n",
      "Epoch: 176, Loss: 2288.69974, Residuals: -1.07177, Convergence: 0.011645\n",
      "Epoch: 177, Loss: 2266.47956, Residuals: -1.06919, Convergence: 0.009804\n",
      "Epoch: 178, Loss: 2247.63141, Residuals: -1.06641, Convergence: 0.008386\n",
      "Epoch: 179, Loss: 2231.51673, Residuals: -1.06354, Convergence: 0.007221\n",
      "Epoch: 180, Loss: 2217.63335, Residuals: -1.06061, Convergence: 0.006260\n",
      "Epoch: 181, Loss: 2205.56488, Residuals: -1.05764, Convergence: 0.005472\n",
      "Epoch: 182, Loss: 2194.96006, Residuals: -1.05462, Convergence: 0.004831\n",
      "Epoch: 183, Loss: 2185.52837, Residuals: -1.05152, Convergence: 0.004316\n",
      "Epoch: 184, Loss: 2177.04287, Residuals: -1.04835, Convergence: 0.003898\n",
      "Epoch: 185, Loss: 2169.34162, Residuals: -1.04509, Convergence: 0.003550\n",
      "Epoch: 186, Loss: 2162.32069, Residuals: -1.04177, Convergence: 0.003247\n",
      "Epoch: 187, Loss: 2155.91418, Residuals: -1.03844, Convergence: 0.002972\n",
      "Epoch: 188, Loss: 2150.07259, Residuals: -1.03513, Convergence: 0.002717\n",
      "Epoch: 189, Loss: 2144.75016, Residuals: -1.03190, Convergence: 0.002482\n",
      "Epoch: 190, Loss: 2139.89942, Residuals: -1.02877, Convergence: 0.002267\n",
      "Epoch: 191, Loss: 2135.47030, Residuals: -1.02577, Convergence: 0.002074\n",
      "Epoch: 192, Loss: 2131.41361, Residuals: -1.02290, Convergence: 0.001903\n",
      "Epoch: 193, Loss: 2127.68212, Residuals: -1.02018, Convergence: 0.001754\n",
      "Epoch: 194, Loss: 2124.23259, Residuals: -1.01759, Convergence: 0.001624\n",
      "Epoch: 195, Loss: 2121.02613, Residuals: -1.01513, Convergence: 0.001512\n",
      "Epoch: 196, Loss: 2118.02811, Residuals: -1.01280, Convergence: 0.001415\n",
      "Epoch: 197, Loss: 2115.20975, Residuals: -1.01059, Convergence: 0.001332\n",
      "Epoch: 198, Loss: 2112.54666, Residuals: -1.00848, Convergence: 0.001261\n",
      "Epoch: 199, Loss: 2110.01874, Residuals: -1.00648, Convergence: 0.001198\n",
      "Epoch: 200, Loss: 2107.61044, Residuals: -1.00458, Convergence: 0.001143\n",
      "Epoch: 201, Loss: 2105.31052, Residuals: -1.00276, Convergence: 0.001092\n",
      "Epoch: 202, Loss: 2103.10967, Residuals: -1.00102, Convergence: 0.001046\n",
      "Epoch: 203, Loss: 2101.00217, Residuals: -0.99936, Convergence: 0.001003\n",
      "Epoch: 204, Loss: 2098.98420, Residuals: -0.99776, Convergence: 0.000961\n",
      "Evidence 14365.045\n",
      "\n",
      "Epoch: 204, Evidence: 14365.04492, Convergence: 0.219220\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.36e-01\n",
      "Epoch: 204, Loss: 2476.27707, Residuals: -0.99776, Convergence:   inf\n",
      "Epoch: 205, Loss: 2462.73884, Residuals: -0.99497, Convergence: 0.005497\n",
      "Epoch: 206, Loss: 2451.50495, Residuals: -0.99159, Convergence: 0.004582\n",
      "Epoch: 207, Loss: 2441.66608, Residuals: -0.98825, Convergence: 0.004030\n",
      "Epoch: 208, Loss: 2433.00528, Residuals: -0.98508, Convergence: 0.003560\n",
      "Epoch: 209, Loss: 2425.35993, Residuals: -0.98213, Convergence: 0.003152\n",
      "Epoch: 210, Loss: 2418.59619, Residuals: -0.97941, Convergence: 0.002797\n",
      "Epoch: 211, Loss: 2412.59782, Residuals: -0.97692, Convergence: 0.002486\n",
      "Epoch: 212, Loss: 2407.26272, Residuals: -0.97463, Convergence: 0.002216\n",
      "Epoch: 213, Loss: 2402.50209, Residuals: -0.97254, Convergence: 0.001982\n",
      "Epoch: 214, Loss: 2398.23893, Residuals: -0.97063, Convergence: 0.001778\n",
      "Epoch: 215, Loss: 2394.40506, Residuals: -0.96887, Convergence: 0.001601\n",
      "Epoch: 216, Loss: 2390.94285, Residuals: -0.96725, Convergence: 0.001448\n",
      "Epoch: 217, Loss: 2387.80230, Residuals: -0.96576, Convergence: 0.001315\n",
      "Epoch: 218, Loss: 2384.94179, Residuals: -0.96438, Convergence: 0.001199\n",
      "Epoch: 219, Loss: 2382.32410, Residuals: -0.96310, Convergence: 0.001099\n",
      "Epoch: 220, Loss: 2379.91827, Residuals: -0.96192, Convergence: 0.001011\n",
      "Epoch: 221, Loss: 2377.69918, Residuals: -0.96083, Convergence: 0.000933\n",
      "Evidence 14747.760\n",
      "\n",
      "Epoch: 221, Evidence: 14747.75977, Convergence: 0.025951\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.33e-01\n",
      "Epoch: 221, Loss: 2481.20145, Residuals: -0.96083, Convergence:   inf\n",
      "Epoch: 222, Loss: 2474.37018, Residuals: -0.95763, Convergence: 0.002761\n",
      "Epoch: 223, Loss: 2468.70696, Residuals: -0.95489, Convergence: 0.002294\n",
      "Epoch: 224, Loss: 2463.89706, Residuals: -0.95262, Convergence: 0.001952\n",
      "Epoch: 225, Loss: 2459.75641, Residuals: -0.95074, Convergence: 0.001683\n",
      "Epoch: 226, Loss: 2456.14875, Residuals: -0.94918, Convergence: 0.001469\n",
      "Epoch: 227, Loss: 2452.97195, Residuals: -0.94789, Convergence: 0.001295\n",
      "Epoch: 228, Loss: 2450.14347, Residuals: -0.94680, Convergence: 0.001154\n",
      "Epoch: 229, Loss: 2447.60252, Residuals: -0.94588, Convergence: 0.001038\n",
      "Epoch: 230, Loss: 2445.29916, Residuals: -0.94511, Convergence: 0.000942\n",
      "Evidence 14837.518\n",
      "\n",
      "Epoch: 230, Evidence: 14837.51758, Convergence: 0.006049\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.59e-01\n",
      "Epoch: 230, Loss: 2482.79955, Residuals: -0.94511, Convergence:   inf\n",
      "Epoch: 231, Loss: 2478.80859, Residuals: -0.94283, Convergence: 0.001610\n",
      "Epoch: 232, Loss: 2475.49268, Residuals: -0.94111, Convergence: 0.001339\n",
      "Epoch: 233, Loss: 2472.65780, Residuals: -0.93980, Convergence: 0.001146\n",
      "Epoch: 234, Loss: 2470.18825, Residuals: -0.93878, Convergence: 0.001000\n",
      "Evidence 14867.030\n",
      "\n",
      "Epoch: 234, Evidence: 14867.03027, Convergence: 0.001985\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.08e-01\n",
      "Epoch: 234, Loss: 2483.78501, Residuals: -0.93878, Convergence:   inf\n",
      "Epoch: 235, Loss: 2480.83220, Residuals: -0.93717, Convergence: 0.001190\n",
      "Epoch: 236, Loss: 2478.37084, Residuals: -0.93600, Convergence: 0.000993\n",
      "Evidence 14879.289\n",
      "\n",
      "Epoch: 236, Evidence: 14879.28906, Convergence: 0.000824\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.72e-01\n",
      "Epoch: 236, Loss: 2484.45689, Residuals: -0.93600, Convergence:   inf\n",
      "Epoch: 237, Loss: 2480.08443, Residuals: -0.93455, Convergence: 0.001763\n",
      "Epoch: 238, Loss: 2476.58681, Residuals: -0.93327, Convergence: 0.001412\n",
      "Epoch: 239, Loss: 2473.74882, Residuals: -0.93223, Convergence: 0.001147\n",
      "Epoch: 240, Loss: 2471.34317, Residuals: -0.93160, Convergence: 0.000973\n",
      "Evidence 14896.961\n",
      "\n",
      "Epoch: 240, Evidence: 14896.96094, Convergence: 0.002009\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.42e-01\n",
      "Epoch: 240, Loss: 2484.63932, Residuals: -0.93160, Convergence:   inf\n",
      "Epoch: 241, Loss: 2481.71441, Residuals: -0.92982, Convergence: 0.001179\n",
      "Epoch: 242, Loss: 2479.36298, Residuals: -0.92878, Convergence: 0.000948\n",
      "Evidence 14907.697\n",
      "\n",
      "Epoch: 242, Evidence: 14907.69727, Convergence: 0.000720\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.20e-01\n",
      "Epoch: 242, Loss: 2484.79792, Residuals: -0.92878, Convergence:   inf\n",
      "Epoch: 243, Loss: 2480.54835, Residuals: -0.92695, Convergence: 0.001713\n",
      "Epoch: 244, Loss: 2477.38555, Residuals: -0.92731, Convergence: 0.001277\n",
      "Epoch: 245, Loss: 2474.78529, Residuals: -0.92768, Convergence: 0.001051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 246, Loss: 2472.49896, Residuals: -0.93016, Convergence: 0.000925\n",
      "Evidence 14923.124\n",
      "\n",
      "Epoch: 246, Evidence: 14923.12402, Convergence: 0.001753\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.07e-01\n",
      "Epoch: 246, Loss: 2484.12693, Residuals: -0.93016, Convergence:   inf\n",
      "Epoch: 247, Loss: 2482.35532, Residuals: -0.92908, Convergence: 0.000714\n",
      "Evidence 14929.523\n",
      "\n",
      "Epoch: 247, Evidence: 14929.52344, Convergence: 0.000429\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.72e-02\n",
      "Epoch: 247, Loss: 2485.15862, Residuals: -0.92908, Convergence:   inf\n",
      "Epoch: 248, Loss: 2530.26936, Residuals: -0.98001, Convergence: -0.017828\n",
      "Epoch: 248, Loss: 2482.92916, Residuals: -0.92916, Convergence: 0.000898\n",
      "Evidence 14933.832\n",
      "\n",
      "Epoch: 248, Evidence: 14933.83203, Convergence: 0.000717\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.04e-02\n",
      "Epoch: 248, Loss: 2484.56244, Residuals: -0.92916, Convergence:   inf\n",
      "Epoch: 249, Loss: 2487.29927, Residuals: -0.93284, Convergence: -0.001100\n",
      "Epoch: 249, Loss: 2483.93747, Residuals: -0.92823, Convergence: 0.000252\n",
      "Evidence 14936.072\n",
      "\n",
      "Epoch: 249, Evidence: 14936.07227, Convergence: 0.000867\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 382.48242, Residuals: -4.51621, Convergence:   inf\n",
      "Epoch: 1, Loss: 356.68247, Residuals: -4.39552, Convergence: 0.072333\n",
      "Epoch: 2, Loss: 335.53535, Residuals: -4.23032, Convergence: 0.063025\n",
      "Epoch: 3, Loss: 319.38195, Residuals: -4.06496, Convergence: 0.050577\n",
      "Epoch: 4, Loss: 307.04491, Residuals: -3.91906, Convergence: 0.040180\n",
      "Epoch: 5, Loss: 297.24568, Residuals: -3.78998, Convergence: 0.032967\n",
      "Epoch: 6, Loss: 289.27871, Residuals: -3.67759, Convergence: 0.027541\n",
      "Epoch: 7, Loss: 282.65428, Residuals: -3.58144, Convergence: 0.023436\n",
      "Epoch: 8, Loss: 277.00942, Residuals: -3.49943, Convergence: 0.020378\n",
      "Epoch: 9, Loss: 272.08774, Residuals: -3.42918, Convergence: 0.018089\n",
      "Epoch: 10, Loss: 267.70683, Residuals: -3.36857, Convergence: 0.016365\n",
      "Epoch: 11, Loss: 263.73456, Residuals: -3.31577, Convergence: 0.015062\n",
      "Epoch: 12, Loss: 260.07462, Residuals: -3.26925, Convergence: 0.014073\n",
      "Epoch: 13, Loss: 256.65720, Residuals: -3.22769, Convergence: 0.013315\n",
      "Epoch: 14, Loss: 253.43224, Residuals: -3.18992, Convergence: 0.012725\n",
      "Epoch: 15, Loss: 250.36563, Residuals: -3.15496, Convergence: 0.012249\n",
      "Epoch: 16, Loss: 247.43745, Residuals: -3.12206, Convergence: 0.011834\n",
      "Epoch: 17, Loss: 244.63524, Residuals: -3.09080, Convergence: 0.011455\n",
      "Epoch: 18, Loss: 241.94093, Residuals: -3.06075, Convergence: 0.011136\n",
      "Epoch: 19, Loss: 239.32531, Residuals: -3.03141, Convergence: 0.010929\n",
      "Epoch: 20, Loss: 236.75436, Residuals: -3.00223, Convergence: 0.010859\n",
      "Epoch: 21, Loss: 234.19913, Residuals: -2.97273, Convergence: 0.010911\n",
      "Epoch: 22, Loss: 231.64071, Residuals: -2.94265, Convergence: 0.011045\n",
      "Epoch: 23, Loss: 229.06104, Residuals: -2.91181, Convergence: 0.011262\n",
      "Epoch: 24, Loss: 226.42559, Residuals: -2.87985, Convergence: 0.011639\n",
      "Epoch: 25, Loss: 223.67971, Residuals: -2.84615, Convergence: 0.012276\n",
      "Epoch: 26, Loss: 220.78420, Residuals: -2.81021, Convergence: 0.013115\n",
      "Epoch: 27, Loss: 217.80068, Residuals: -2.77259, Convergence: 0.013698\n",
      "Epoch: 28, Loss: 214.86000, Residuals: -2.73474, Convergence: 0.013687\n",
      "Epoch: 29, Loss: 212.01914, Residuals: -2.69743, Convergence: 0.013399\n",
      "Epoch: 30, Loss: 209.27488, Residuals: -2.66071, Convergence: 0.013113\n",
      "Epoch: 31, Loss: 206.61265, Residuals: -2.62450, Convergence: 0.012885\n",
      "Epoch: 32, Loss: 204.02075, Residuals: -2.58867, Convergence: 0.012704\n",
      "Epoch: 33, Loss: 201.49187, Residuals: -2.55314, Convergence: 0.012551\n",
      "Epoch: 34, Loss: 199.02228, Residuals: -2.51786, Convergence: 0.012409\n",
      "Epoch: 35, Loss: 196.61064, Residuals: -2.48279, Convergence: 0.012266\n",
      "Epoch: 36, Loss: 194.25713, Residuals: -2.44791, Convergence: 0.012115\n",
      "Epoch: 37, Loss: 191.96280, Residuals: -2.41322, Convergence: 0.011952\n",
      "Epoch: 38, Loss: 189.72907, Residuals: -2.37872, Convergence: 0.011773\n",
      "Epoch: 39, Loss: 187.55751, Residuals: -2.34444, Convergence: 0.011578\n",
      "Epoch: 40, Loss: 185.44958, Residuals: -2.31039, Convergence: 0.011367\n",
      "Epoch: 41, Loss: 183.40665, Residuals: -2.27662, Convergence: 0.011139\n",
      "Epoch: 42, Loss: 181.42983, Residuals: -2.24314, Convergence: 0.010896\n",
      "Epoch: 43, Loss: 179.52002, Residuals: -2.21000, Convergence: 0.010638\n",
      "Epoch: 44, Loss: 177.67784, Residuals: -2.17723, Convergence: 0.010368\n",
      "Epoch: 45, Loss: 175.90363, Residuals: -2.14487, Convergence: 0.010086\n",
      "Epoch: 46, Loss: 174.19743, Residuals: -2.11296, Convergence: 0.009795\n",
      "Epoch: 47, Loss: 172.55904, Residuals: -2.08153, Convergence: 0.009495\n",
      "Epoch: 48, Loss: 170.98791, Residuals: -2.05061, Convergence: 0.009189\n",
      "Epoch: 49, Loss: 169.48327, Residuals: -2.02023, Convergence: 0.008878\n",
      "Epoch: 50, Loss: 168.04411, Residuals: -1.99042, Convergence: 0.008564\n",
      "Epoch: 51, Loss: 166.66920, Residuals: -1.96120, Convergence: 0.008249\n",
      "Epoch: 52, Loss: 165.35716, Residuals: -1.93259, Convergence: 0.007935\n",
      "Epoch: 53, Loss: 164.10649, Residuals: -1.90461, Convergence: 0.007621\n",
      "Epoch: 54, Loss: 162.91552, Residuals: -1.87728, Convergence: 0.007310\n",
      "Epoch: 55, Loss: 161.78251, Residuals: -1.85062, Convergence: 0.007003\n",
      "Epoch: 56, Loss: 160.70556, Residuals: -1.82463, Convergence: 0.006701\n",
      "Epoch: 57, Loss: 159.68254, Residuals: -1.79934, Convergence: 0.006407\n",
      "Epoch: 58, Loss: 158.71110, Residuals: -1.77474, Convergence: 0.006121\n",
      "Epoch: 59, Loss: 157.78855, Residuals: -1.75083, Convergence: 0.005847\n",
      "Epoch: 60, Loss: 156.91190, Residuals: -1.72762, Convergence: 0.005587\n",
      "Epoch: 61, Loss: 156.07788, Residuals: -1.70508, Convergence: 0.005344\n",
      "Epoch: 62, Loss: 155.28306, Residuals: -1.68320, Convergence: 0.005119\n",
      "Epoch: 63, Loss: 154.52402, Residuals: -1.66194, Convergence: 0.004912\n",
      "Epoch: 64, Loss: 153.79754, Residuals: -1.64128, Convergence: 0.004724\n",
      "Epoch: 65, Loss: 153.10074, Residuals: -1.62118, Convergence: 0.004551\n",
      "Epoch: 66, Loss: 152.43117, Residuals: -1.60163, Convergence: 0.004393\n",
      "Epoch: 67, Loss: 151.78686, Residuals: -1.58259, Convergence: 0.004245\n",
      "Epoch: 68, Loss: 151.16628, Residuals: -1.56406, Convergence: 0.004105\n",
      "Epoch: 69, Loss: 150.56828, Residuals: -1.54602, Convergence: 0.003972\n",
      "Epoch: 70, Loss: 149.99197, Residuals: -1.52848, Convergence: 0.003842\n",
      "Epoch: 71, Loss: 149.43670, Residuals: -1.51142, Convergence: 0.003716\n",
      "Epoch: 72, Loss: 148.90193, Residuals: -1.49486, Convergence: 0.003591\n",
      "Epoch: 73, Loss: 148.38723, Residuals: -1.47879, Convergence: 0.003469\n",
      "Epoch: 74, Loss: 147.89218, Residuals: -1.46320, Convergence: 0.003347\n",
      "Epoch: 75, Loss: 147.41639, Residuals: -1.44811, Convergence: 0.003228\n",
      "Epoch: 76, Loss: 146.95945, Residuals: -1.43350, Convergence: 0.003109\n",
      "Epoch: 77, Loss: 146.52097, Residuals: -1.41938, Convergence: 0.002993\n",
      "Epoch: 78, Loss: 146.10050, Residuals: -1.40575, Convergence: 0.002878\n",
      "Epoch: 79, Loss: 145.69761, Residuals: -1.39258, Convergence: 0.002765\n",
      "Epoch: 80, Loss: 145.31183, Residuals: -1.37989, Convergence: 0.002655\n",
      "Epoch: 81, Loss: 144.94269, Residuals: -1.36766, Convergence: 0.002547\n",
      "Epoch: 82, Loss: 144.58971, Residuals: -1.35589, Convergence: 0.002441\n",
      "Epoch: 83, Loss: 144.25239, Residuals: -1.34456, Convergence: 0.002338\n",
      "Epoch: 84, Loss: 143.93024, Residuals: -1.33366, Convergence: 0.002238\n",
      "Epoch: 85, Loss: 143.62278, Residuals: -1.32319, Convergence: 0.002141\n",
      "Epoch: 86, Loss: 143.32952, Residuals: -1.31313, Convergence: 0.002046\n",
      "Epoch: 87, Loss: 143.04997, Residuals: -1.30347, Convergence: 0.001954\n",
      "Epoch: 88, Loss: 142.78367, Residuals: -1.29419, Convergence: 0.001865\n",
      "Epoch: 89, Loss: 142.53016, Residuals: -1.28529, Convergence: 0.001779\n",
      "Epoch: 90, Loss: 142.28900, Residuals: -1.27675, Convergence: 0.001695\n",
      "Epoch: 91, Loss: 142.05974, Residuals: -1.26856, Convergence: 0.001614\n",
      "Epoch: 92, Loss: 141.84198, Residuals: -1.26071, Convergence: 0.001535\n",
      "Epoch: 93, Loss: 141.63531, Residuals: -1.25318, Convergence: 0.001459\n",
      "Epoch: 94, Loss: 141.43936, Residuals: -1.24597, Convergence: 0.001385\n",
      "Epoch: 95, Loss: 141.25378, Residuals: -1.23905, Convergence: 0.001314\n",
      "Epoch: 96, Loss: 141.07819, Residuals: -1.23243, Convergence: 0.001245\n",
      "Epoch: 97, Loss: 140.91231, Residuals: -1.22609, Convergence: 0.001177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 98, Loss: 140.75580, Residuals: -1.22003, Convergence: 0.001112\n",
      "Epoch: 99, Loss: 140.60835, Residuals: -1.21423, Convergence: 0.001049\n",
      "Epoch: 100, Loss: 140.46968, Residuals: -1.20869, Convergence: 0.000987\n",
      "Evidence -181.837\n",
      "\n",
      "Epoch: 100, Evidence: -181.83655, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.24e-01\n",
      "Epoch: 100, Loss: 1359.44025, Residuals: -1.20869, Convergence:   inf\n",
      "Epoch: 101, Loss: 1297.77248, Residuals: -1.23927, Convergence: 0.047518\n",
      "Epoch: 102, Loss: 1250.90138, Residuals: -1.26355, Convergence: 0.037470\n",
      "Epoch: 103, Loss: 1215.45704, Residuals: -1.28141, Convergence: 0.029161\n",
      "Epoch: 104, Loss: 1187.90037, Residuals: -1.29426, Convergence: 0.023198\n",
      "Epoch: 105, Loss: 1165.65598, Residuals: -1.30389, Convergence: 0.019083\n",
      "Epoch: 106, Loss: 1147.21463, Residuals: -1.31132, Convergence: 0.016075\n",
      "Epoch: 107, Loss: 1131.66390, Residuals: -1.31702, Convergence: 0.013741\n",
      "Epoch: 108, Loss: 1118.38562, Residuals: -1.32125, Convergence: 0.011873\n",
      "Epoch: 109, Loss: 1106.92450, Residuals: -1.32417, Convergence: 0.010354\n",
      "Epoch: 110, Loss: 1096.92379, Residuals: -1.32592, Convergence: 0.009117\n",
      "Epoch: 111, Loss: 1088.09753, Residuals: -1.32659, Convergence: 0.008112\n",
      "Epoch: 112, Loss: 1080.20826, Residuals: -1.32630, Convergence: 0.007303\n",
      "Epoch: 113, Loss: 1073.05711, Residuals: -1.32511, Convergence: 0.006664\n",
      "Epoch: 114, Loss: 1066.47322, Residuals: -1.32310, Convergence: 0.006174\n",
      "Epoch: 115, Loss: 1060.30864, Residuals: -1.32030, Convergence: 0.005814\n",
      "Epoch: 116, Loss: 1054.43494, Residuals: -1.31674, Convergence: 0.005570\n",
      "Epoch: 117, Loss: 1048.73992, Residuals: -1.31246, Convergence: 0.005430\n",
      "Epoch: 118, Loss: 1043.12898, Residuals: -1.30745, Convergence: 0.005379\n",
      "Epoch: 119, Loss: 1037.52721, Residuals: -1.30176, Convergence: 0.005399\n",
      "Epoch: 120, Loss: 1031.88659, Residuals: -1.29541, Convergence: 0.005466\n",
      "Epoch: 121, Loss: 1026.19958, Residuals: -1.28849, Convergence: 0.005542\n",
      "Epoch: 122, Loss: 1020.50962, Residuals: -1.28107, Convergence: 0.005576\n",
      "Epoch: 123, Loss: 1014.90565, Residuals: -1.27328, Convergence: 0.005522\n",
      "Epoch: 124, Loss: 1009.49161, Residuals: -1.26522, Convergence: 0.005363\n",
      "Epoch: 125, Loss: 1004.35053, Residuals: -1.25698, Convergence: 0.005119\n",
      "Epoch: 126, Loss: 999.52573, Residuals: -1.24865, Convergence: 0.004827\n",
      "Epoch: 127, Loss: 995.02315, Residuals: -1.24031, Convergence: 0.004525\n",
      "Epoch: 128, Loss: 990.82551, Residuals: -1.23201, Convergence: 0.004237\n",
      "Epoch: 129, Loss: 986.90501, Residuals: -1.22380, Convergence: 0.003973\n",
      "Epoch: 130, Loss: 983.23244, Residuals: -1.21573, Convergence: 0.003735\n",
      "Epoch: 131, Loss: 979.78026, Residuals: -1.20782, Convergence: 0.003523\n",
      "Epoch: 132, Loss: 976.52488, Residuals: -1.20010, Convergence: 0.003334\n",
      "Epoch: 133, Loss: 973.44676, Residuals: -1.19261, Convergence: 0.003162\n",
      "Epoch: 134, Loss: 970.53034, Residuals: -1.18534, Convergence: 0.003005\n",
      "Epoch: 135, Loss: 967.76258, Residuals: -1.17832, Convergence: 0.002860\n",
      "Epoch: 136, Loss: 965.13357, Residuals: -1.17156, Convergence: 0.002724\n",
      "Epoch: 137, Loss: 962.63430, Residuals: -1.16507, Convergence: 0.002596\n",
      "Epoch: 138, Loss: 960.25699, Residuals: -1.15884, Convergence: 0.002476\n",
      "Epoch: 139, Loss: 957.99512, Residuals: -1.15288, Convergence: 0.002361\n",
      "Epoch: 140, Loss: 955.84289, Residuals: -1.14719, Convergence: 0.002252\n",
      "Epoch: 141, Loss: 953.79336, Residuals: -1.14176, Convergence: 0.002149\n",
      "Epoch: 142, Loss: 951.84124, Residuals: -1.13658, Convergence: 0.002051\n",
      "Epoch: 143, Loss: 949.98029, Residuals: -1.13166, Convergence: 0.001959\n",
      "Epoch: 144, Loss: 948.20543, Residuals: -1.12697, Convergence: 0.001872\n",
      "Epoch: 145, Loss: 946.51071, Residuals: -1.12251, Convergence: 0.001790\n",
      "Epoch: 146, Loss: 944.89075, Residuals: -1.11827, Convergence: 0.001714\n",
      "Epoch: 147, Loss: 943.34040, Residuals: -1.11424, Convergence: 0.001643\n",
      "Epoch: 148, Loss: 941.85488, Residuals: -1.11040, Convergence: 0.001577\n",
      "Epoch: 149, Loss: 940.42946, Residuals: -1.10674, Convergence: 0.001516\n",
      "Epoch: 150, Loss: 939.05969, Residuals: -1.10326, Convergence: 0.001459\n",
      "Epoch: 151, Loss: 937.74143, Residuals: -1.09994, Convergence: 0.001406\n",
      "Epoch: 152, Loss: 936.47043, Residuals: -1.09676, Convergence: 0.001357\n",
      "Epoch: 153, Loss: 935.24343, Residuals: -1.09373, Convergence: 0.001312\n",
      "Epoch: 154, Loss: 934.05674, Residuals: -1.09082, Convergence: 0.001270\n",
      "Epoch: 155, Loss: 932.90674, Residuals: -1.08803, Convergence: 0.001233\n",
      "Epoch: 156, Loss: 931.78972, Residuals: -1.08535, Convergence: 0.001199\n",
      "Epoch: 157, Loss: 930.70215, Residuals: -1.08276, Convergence: 0.001169\n",
      "Epoch: 158, Loss: 929.64093, Residuals: -1.08027, Convergence: 0.001142\n",
      "Epoch: 159, Loss: 928.60164, Residuals: -1.07784, Convergence: 0.001119\n",
      "Epoch: 160, Loss: 927.58129, Residuals: -1.07549, Convergence: 0.001100\n",
      "Epoch: 161, Loss: 926.57554, Residuals: -1.07319, Convergence: 0.001085\n",
      "Epoch: 162, Loss: 925.58106, Residuals: -1.07093, Convergence: 0.001074\n",
      "Epoch: 163, Loss: 924.59459, Residuals: -1.06871, Convergence: 0.001067\n",
      "Epoch: 164, Loss: 923.61376, Residuals: -1.06652, Convergence: 0.001062\n",
      "Epoch: 165, Loss: 922.63648, Residuals: -1.06435, Convergence: 0.001059\n",
      "Epoch: 166, Loss: 921.66233, Residuals: -1.06220, Convergence: 0.001057\n",
      "Epoch: 167, Loss: 920.69239, Residuals: -1.06006, Convergence: 0.001053\n",
      "Epoch: 168, Loss: 919.72906, Residuals: -1.05794, Convergence: 0.001047\n",
      "Epoch: 169, Loss: 918.77579, Residuals: -1.05584, Convergence: 0.001038\n",
      "Epoch: 170, Loss: 917.83716, Residuals: -1.05377, Convergence: 0.001023\n",
      "Epoch: 171, Loss: 916.91758, Residuals: -1.05175, Convergence: 0.001003\n",
      "Epoch: 172, Loss: 916.02189, Residuals: -1.04977, Convergence: 0.000978\n",
      "Evidence 11238.534\n",
      "\n",
      "Epoch: 172, Evidence: 11238.53418, Convergence: 1.016180\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.75e-01\n",
      "Epoch: 172, Loss: 2341.87519, Residuals: -1.04977, Convergence:   inf\n",
      "Epoch: 173, Loss: 2304.68627, Residuals: -1.05929, Convergence: 0.016136\n",
      "Epoch: 174, Loss: 2278.24077, Residuals: -1.05852, Convergence: 0.011608\n",
      "Epoch: 175, Loss: 2256.22719, Residuals: -1.05688, Convergence: 0.009757\n",
      "Epoch: 176, Loss: 2237.73166, Residuals: -1.05501, Convergence: 0.008265\n",
      "Epoch: 177, Loss: 2222.06650, Residuals: -1.05303, Convergence: 0.007050\n",
      "Epoch: 178, Loss: 2208.69506, Residuals: -1.05097, Convergence: 0.006054\n",
      "Epoch: 179, Loss: 2197.17860, Residuals: -1.04886, Convergence: 0.005241\n",
      "Epoch: 180, Loss: 2187.15237, Residuals: -1.04669, Convergence: 0.004584\n",
      "Epoch: 181, Loss: 2178.30989, Residuals: -1.04445, Convergence: 0.004059\n",
      "Epoch: 182, Loss: 2170.39809, Residuals: -1.04212, Convergence: 0.003645\n",
      "Epoch: 183, Loss: 2163.21507, Residuals: -1.03968, Convergence: 0.003321\n",
      "Epoch: 184, Loss: 2156.61329, Residuals: -1.03711, Convergence: 0.003061\n",
      "Epoch: 185, Loss: 2150.49930, Residuals: -1.03442, Convergence: 0.002843\n",
      "Epoch: 186, Loss: 2144.82147, Residuals: -1.03163, Convergence: 0.002647\n",
      "Epoch: 187, Loss: 2139.55282, Residuals: -1.02879, Convergence: 0.002463\n",
      "Epoch: 188, Loss: 2134.67428, Residuals: -1.02595, Convergence: 0.002285\n",
      "Epoch: 189, Loss: 2130.16520, Residuals: -1.02313, Convergence: 0.002117\n",
      "Epoch: 190, Loss: 2126.00134, Residuals: -1.02038, Convergence: 0.001959\n",
      "Epoch: 191, Loss: 2122.15536, Residuals: -1.01772, Convergence: 0.001812\n",
      "Epoch: 192, Loss: 2118.59952, Residuals: -1.01515, Convergence: 0.001678\n",
      "Epoch: 193, Loss: 2115.30459, Residuals: -1.01269, Convergence: 0.001558\n",
      "Epoch: 194, Loss: 2112.24411, Residuals: -1.01035, Convergence: 0.001449\n",
      "Epoch: 195, Loss: 2109.39149, Residuals: -1.00812, Convergence: 0.001352\n",
      "Epoch: 196, Loss: 2106.72506, Residuals: -1.00599, Convergence: 0.001266\n",
      "Epoch: 197, Loss: 2104.22234, Residuals: -1.00398, Convergence: 0.001189\n",
      "Epoch: 198, Loss: 2101.86617, Residuals: -1.00207, Convergence: 0.001121\n",
      "Epoch: 199, Loss: 2099.63993, Residuals: -1.00025, Convergence: 0.001060\n",
      "Epoch: 200, Loss: 2097.53068, Residuals: -0.99854, Convergence: 0.001006\n",
      "Epoch: 201, Loss: 2095.52668, Residuals: -0.99691, Convergence: 0.000956\n",
      "Evidence 14425.802\n",
      "\n",
      "Epoch: 201, Evidence: 14425.80176, Convergence: 0.220942\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.37e-01\n",
      "Epoch: 201, Loss: 2471.18884, Residuals: -0.99691, Convergence:   inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 202, Loss: 2457.65233, Residuals: -0.99400, Convergence: 0.005508\n",
      "Epoch: 203, Loss: 2446.69064, Residuals: -0.99051, Convergence: 0.004480\n",
      "Epoch: 204, Loss: 2437.26755, Residuals: -0.98720, Convergence: 0.003866\n",
      "Epoch: 205, Loss: 2429.08967, Residuals: -0.98415, Convergence: 0.003367\n",
      "Epoch: 206, Loss: 2421.94382, Residuals: -0.98136, Convergence: 0.002950\n",
      "Epoch: 207, Loss: 2415.66398, Residuals: -0.97883, Convergence: 0.002600\n",
      "Epoch: 208, Loss: 2410.11416, Residuals: -0.97653, Convergence: 0.002303\n",
      "Epoch: 209, Loss: 2405.18445, Residuals: -0.97444, Convergence: 0.002050\n",
      "Epoch: 210, Loss: 2400.78387, Residuals: -0.97255, Convergence: 0.001833\n",
      "Epoch: 211, Loss: 2396.83508, Residuals: -0.97082, Convergence: 0.001647\n",
      "Epoch: 212, Loss: 2393.27371, Residuals: -0.96925, Convergence: 0.001488\n",
      "Epoch: 213, Loss: 2390.04685, Residuals: -0.96781, Convergence: 0.001350\n",
      "Epoch: 214, Loss: 2387.10975, Residuals: -0.96649, Convergence: 0.001230\n",
      "Epoch: 215, Loss: 2384.42404, Residuals: -0.96527, Convergence: 0.001126\n",
      "Epoch: 216, Loss: 2381.95852, Residuals: -0.96415, Convergence: 0.001035\n",
      "Epoch: 217, Loss: 2379.68597, Residuals: -0.96312, Convergence: 0.000955\n",
      "Evidence 14792.254\n",
      "\n",
      "Epoch: 217, Evidence: 14792.25391, Convergence: 0.024773\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.33e-01\n",
      "Epoch: 217, Loss: 2476.57547, Residuals: -0.96312, Convergence:   inf\n",
      "Epoch: 218, Loss: 2470.02652, Residuals: -0.95997, Convergence: 0.002651\n",
      "Epoch: 219, Loss: 2464.64211, Residuals: -0.95728, Convergence: 0.002185\n",
      "Epoch: 220, Loss: 2460.06156, Residuals: -0.95500, Convergence: 0.001862\n",
      "Epoch: 221, Loss: 2456.10455, Residuals: -0.95307, Convergence: 0.001611\n",
      "Epoch: 222, Loss: 2452.64376, Residuals: -0.95141, Convergence: 0.001411\n",
      "Epoch: 223, Loss: 2449.58342, Residuals: -0.94997, Convergence: 0.001249\n",
      "Epoch: 224, Loss: 2446.84940, Residuals: -0.94873, Convergence: 0.001117\n",
      "Epoch: 225, Loss: 2444.38331, Residuals: -0.94763, Convergence: 0.001009\n",
      "Epoch: 226, Loss: 2442.14091, Residuals: -0.94667, Convergence: 0.000918\n",
      "Evidence 14876.268\n",
      "\n",
      "Epoch: 226, Evidence: 14876.26758, Convergence: 0.005647\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.60e-01\n",
      "Epoch: 226, Loss: 2478.15559, Residuals: -0.94667, Convergence:   inf\n",
      "Epoch: 227, Loss: 2474.32241, Residuals: -0.94423, Convergence: 0.001549\n",
      "Epoch: 228, Loss: 2471.12351, Residuals: -0.94232, Convergence: 0.001295\n",
      "Epoch: 229, Loss: 2468.36714, Residuals: -0.94078, Convergence: 0.001117\n",
      "Epoch: 230, Loss: 2465.94921, Residuals: -0.93950, Convergence: 0.000981\n",
      "Evidence 14904.871\n",
      "\n",
      "Epoch: 230, Evidence: 14904.87109, Convergence: 0.001919\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.10e-01\n",
      "Epoch: 230, Loss: 2479.21052, Residuals: -0.93950, Convergence:   inf\n",
      "Epoch: 231, Loss: 2476.32685, Residuals: -0.93754, Convergence: 0.001164\n",
      "Epoch: 232, Loss: 2473.89706, Residuals: -0.93603, Convergence: 0.000982\n",
      "Evidence 14916.783\n",
      "\n",
      "Epoch: 232, Evidence: 14916.78320, Convergence: 0.000799\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.74e-01\n",
      "Epoch: 232, Loss: 2479.95684, Residuals: -0.93603, Convergence:   inf\n",
      "Epoch: 233, Loss: 2475.38951, Residuals: -0.93342, Convergence: 0.001845\n",
      "Epoch: 234, Loss: 2471.91725, Residuals: -0.93162, Convergence: 0.001405\n",
      "Epoch: 235, Loss: 2469.10197, Residuals: -0.93034, Convergence: 0.001140\n",
      "Epoch: 236, Loss: 2466.70820, Residuals: -0.92951, Convergence: 0.000970\n",
      "Evidence 14934.646\n",
      "\n",
      "Epoch: 236, Evidence: 14934.64551, Convergence: 0.001994\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.44e-01\n",
      "Epoch: 236, Loss: 2480.17391, Residuals: -0.92951, Convergence:   inf\n",
      "Epoch: 237, Loss: 2477.18476, Residuals: -0.92713, Convergence: 0.001207\n",
      "Epoch: 238, Loss: 2474.80919, Residuals: -0.92589, Convergence: 0.000960\n",
      "Evidence 14945.553\n",
      "\n",
      "Epoch: 238, Evidence: 14945.55273, Convergence: 0.000730\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.22e-01\n",
      "Epoch: 238, Loss: 2480.37002, Residuals: -0.92589, Convergence:   inf\n",
      "Epoch: 239, Loss: 2476.02661, Residuals: -0.92247, Convergence: 0.001754\n",
      "Epoch: 240, Loss: 2472.86384, Residuals: -0.92327, Convergence: 0.001279\n",
      "Epoch: 241, Loss: 2470.28409, Residuals: -0.92357, Convergence: 0.001044\n",
      "Epoch: 242, Loss: 2467.92781, Residuals: -0.92558, Convergence: 0.000955\n",
      "Evidence 14961.129\n",
      "\n",
      "Epoch: 242, Evidence: 14961.12891, Convergence: 0.001770\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.08e-01\n",
      "Epoch: 242, Loss: 2479.69579, Residuals: -0.92558, Convergence:   inf\n",
      "Epoch: 243, Loss: 2478.54487, Residuals: -0.92177, Convergence: 0.000464\n",
      "Evidence 14967.113\n",
      "\n",
      "Epoch: 243, Evidence: 14967.11328, Convergence: 0.000400\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.88e-02\n",
      "Epoch: 243, Loss: 2480.55344, Residuals: -0.92177, Convergence:   inf\n",
      "Epoch: 244, Loss: 2525.38668, Residuals: -0.95069, Convergence: -0.017753\n",
      "Epoch: 244, Loss: 2478.30256, Residuals: -0.91922, Convergence: 0.000908\n",
      "Evidence 14971.301\n",
      "\n",
      "Epoch: 244, Evidence: 14971.30078, Convergence: 0.000679\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.22e-02\n",
      "Epoch: 244, Loss: 2480.01211, Residuals: -0.91922, Convergence:   inf\n",
      "Epoch: 245, Loss: 2488.62785, Residuals: -0.92033, Convergence: -0.003462\n",
      "Epoch: 245, Loss: 2480.92682, Residuals: -0.91775, Convergence: -0.000369\n",
      "Evidence 14971.892\n",
      "\n",
      "Epoch: 245, Evidence: 14971.89160, Convergence: 0.000719\n",
      "Total samples: 181, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 381.56136, Residuals: -4.55087, Convergence:   inf\n",
      "Epoch: 1, Loss: 355.87756, Residuals: -4.43112, Convergence: 0.072170\n",
      "Epoch: 2, Loss: 334.80346, Residuals: -4.26643, Convergence: 0.062945\n",
      "Epoch: 3, Loss: 318.71542, Residuals: -4.10099, Convergence: 0.050478\n",
      "Epoch: 4, Loss: 306.44447, Residuals: -3.95489, Convergence: 0.040043\n",
      "Epoch: 5, Loss: 296.71469, Residuals: -3.82550, Convergence: 0.032792\n",
      "Epoch: 6, Loss: 288.82032, Residuals: -3.71267, Convergence: 0.027333\n",
      "Epoch: 7, Loss: 282.26984, Residuals: -3.61597, Convergence: 0.023206\n",
      "Epoch: 8, Loss: 276.69867, Residuals: -3.53332, Convergence: 0.020134\n",
      "Epoch: 9, Loss: 271.84979, Residuals: -3.46242, Convergence: 0.017837\n",
      "Epoch: 10, Loss: 267.54100, Residuals: -3.40117, Convergence: 0.016105\n",
      "Epoch: 11, Loss: 263.64108, Residuals: -3.34781, Convergence: 0.014793\n",
      "Epoch: 12, Loss: 260.05537, Residuals: -3.30086, Convergence: 0.013788\n",
      "Epoch: 13, Loss: 256.71629, Residuals: -3.25903, Convergence: 0.013007\n",
      "Epoch: 14, Loss: 253.57640, Residuals: -3.22117, Convergence: 0.012382\n",
      "Epoch: 15, Loss: 250.60360, Residuals: -3.18631, Convergence: 0.011863\n",
      "Epoch: 16, Loss: 247.77800, Residuals: -3.15373, Convergence: 0.011404\n",
      "Epoch: 17, Loss: 245.08516, Residuals: -3.12295, Convergence: 0.010987\n",
      "Epoch: 18, Loss: 242.50509, Residuals: -3.09356, Convergence: 0.010639\n",
      "Epoch: 19, Loss: 240.00767, Residuals: -3.06506, Convergence: 0.010406\n",
      "Epoch: 20, Loss: 237.55807, Residuals: -3.03686, Convergence: 0.010312\n",
      "Epoch: 21, Loss: 235.12528, Residuals: -3.00845, Convergence: 0.010347\n",
      "Epoch: 22, Loss: 232.68775, Residuals: -2.97946, Convergence: 0.010476\n",
      "Epoch: 23, Loss: 230.22770, Residuals: -2.94965, Convergence: 0.010685\n",
      "Epoch: 24, Loss: 227.71427, Residuals: -2.91870, Convergence: 0.011038\n",
      "Epoch: 25, Loss: 225.09508, Residuals: -2.88598, Convergence: 0.011636\n",
      "Epoch: 26, Loss: 222.32710, Residuals: -2.85096, Convergence: 0.012450\n",
      "Epoch: 27, Loss: 219.46638, Residuals: -2.81415, Convergence: 0.013035\n",
      "Epoch: 28, Loss: 216.65023, Residuals: -2.77707, Convergence: 0.012999\n",
      "Epoch: 29, Loss: 213.94173, Residuals: -2.74058, Convergence: 0.012660\n",
      "Epoch: 30, Loss: 211.33558, Residuals: -2.70477, Convergence: 0.012332\n",
      "Epoch: 31, Loss: 208.81231, Residuals: -2.66951, Convergence: 0.012084\n",
      "Epoch: 32, Loss: 206.35515, Residuals: -2.63466, Convergence: 0.011907\n",
      "Epoch: 33, Loss: 203.95253, Residuals: -2.60012, Convergence: 0.011780\n",
      "Epoch: 34, Loss: 201.59746, Residuals: -2.56580, Convergence: 0.011682\n",
      "Epoch: 35, Loss: 199.28652, Residuals: -2.53163, Convergence: 0.011596\n",
      "Epoch: 36, Loss: 197.01879, Residuals: -2.49759, Convergence: 0.011510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37, Loss: 194.79499, Residuals: -2.46365, Convergence: 0.011416\n",
      "Epoch: 38, Loss: 192.61667, Residuals: -2.42982, Convergence: 0.011309\n",
      "Epoch: 39, Loss: 190.48576, Residuals: -2.39608, Convergence: 0.011187\n",
      "Epoch: 40, Loss: 188.40415, Residuals: -2.36246, Convergence: 0.011049\n",
      "Epoch: 41, Loss: 186.37350, Residuals: -2.32898, Convergence: 0.010896\n",
      "Epoch: 42, Loss: 184.39518, Residuals: -2.29565, Convergence: 0.010729\n",
      "Epoch: 43, Loss: 182.47030, Residuals: -2.26250, Convergence: 0.010549\n",
      "Epoch: 44, Loss: 180.59978, Residuals: -2.22957, Convergence: 0.010357\n",
      "Epoch: 45, Loss: 178.78450, Residuals: -2.19687, Convergence: 0.010153\n",
      "Epoch: 46, Loss: 177.02535, Residuals: -2.16445, Convergence: 0.009937\n",
      "Epoch: 47, Loss: 175.32325, Residuals: -2.13234, Convergence: 0.009708\n",
      "Epoch: 48, Loss: 173.67904, Residuals: -2.10059, Convergence: 0.009467\n",
      "Epoch: 49, Loss: 172.09339, Residuals: -2.06924, Convergence: 0.009214\n",
      "Epoch: 50, Loss: 170.56661, Residuals: -2.03833, Convergence: 0.008951\n",
      "Epoch: 51, Loss: 169.09861, Residuals: -2.00789, Convergence: 0.008681\n",
      "Epoch: 52, Loss: 167.68888, Residuals: -1.97796, Convergence: 0.008407\n",
      "Epoch: 53, Loss: 166.33656, Residuals: -1.94857, Convergence: 0.008130\n",
      "Epoch: 54, Loss: 165.04046, Residuals: -1.91975, Convergence: 0.007853\n",
      "Epoch: 55, Loss: 163.79915, Residuals: -1.89154, Convergence: 0.007578\n",
      "Epoch: 56, Loss: 162.61096, Residuals: -1.86394, Convergence: 0.007307\n",
      "Epoch: 57, Loss: 161.47405, Residuals: -1.83700, Convergence: 0.007041\n",
      "Epoch: 58, Loss: 160.38639, Residuals: -1.81071, Convergence: 0.006782\n",
      "Epoch: 59, Loss: 159.34584, Residuals: -1.78509, Convergence: 0.006530\n",
      "Epoch: 60, Loss: 158.35020, Residuals: -1.76013, Convergence: 0.006288\n",
      "Epoch: 61, Loss: 157.39729, Residuals: -1.73585, Convergence: 0.006054\n",
      "Epoch: 62, Loss: 156.48508, Residuals: -1.71223, Convergence: 0.005829\n",
      "Epoch: 63, Loss: 155.61175, Residuals: -1.68925, Convergence: 0.005612\n",
      "Epoch: 64, Loss: 154.77571, Residuals: -1.66693, Convergence: 0.005402\n",
      "Epoch: 65, Loss: 153.97565, Residuals: -1.64525, Convergence: 0.005196\n",
      "Epoch: 66, Loss: 153.21043, Residuals: -1.62420, Convergence: 0.004995\n",
      "Epoch: 67, Loss: 152.47909, Residuals: -1.60380, Convergence: 0.004796\n",
      "Epoch: 68, Loss: 151.78076, Residuals: -1.58404, Convergence: 0.004601\n",
      "Epoch: 69, Loss: 151.11460, Residuals: -1.56491, Convergence: 0.004408\n",
      "Epoch: 70, Loss: 150.47978, Residuals: -1.54643, Convergence: 0.004219\n",
      "Epoch: 71, Loss: 149.87542, Residuals: -1.52858, Convergence: 0.004032\n",
      "Epoch: 72, Loss: 149.30063, Residuals: -1.51137, Convergence: 0.003850\n",
      "Epoch: 73, Loss: 148.75444, Residuals: -1.49479, Convergence: 0.003672\n",
      "Epoch: 74, Loss: 148.23583, Residuals: -1.47883, Convergence: 0.003499\n",
      "Epoch: 75, Loss: 147.74379, Residuals: -1.46348, Convergence: 0.003330\n",
      "Epoch: 76, Loss: 147.27722, Residuals: -1.44873, Convergence: 0.003168\n",
      "Epoch: 77, Loss: 146.83508, Residuals: -1.43457, Convergence: 0.003011\n",
      "Epoch: 78, Loss: 146.41627, Residuals: -1.42099, Convergence: 0.002860\n",
      "Epoch: 79, Loss: 146.01974, Residuals: -1.40796, Convergence: 0.002716\n",
      "Epoch: 80, Loss: 145.64445, Residuals: -1.39548, Convergence: 0.002577\n",
      "Epoch: 81, Loss: 145.28941, Residuals: -1.38353, Convergence: 0.002444\n",
      "Epoch: 82, Loss: 144.95362, Residuals: -1.37208, Convergence: 0.002317\n",
      "Epoch: 83, Loss: 144.63619, Residuals: -1.36113, Convergence: 0.002195\n",
      "Epoch: 84, Loss: 144.33624, Residuals: -1.35066, Convergence: 0.002078\n",
      "Epoch: 85, Loss: 144.05294, Residuals: -1.34064, Convergence: 0.001967\n",
      "Epoch: 86, Loss: 143.78553, Residuals: -1.33107, Convergence: 0.001860\n",
      "Epoch: 87, Loss: 143.53327, Residuals: -1.32193, Convergence: 0.001758\n",
      "Epoch: 88, Loss: 143.29547, Residuals: -1.31321, Convergence: 0.001660\n",
      "Epoch: 89, Loss: 143.07150, Residuals: -1.30489, Convergence: 0.001565\n",
      "Epoch: 90, Loss: 142.86070, Residuals: -1.29696, Convergence: 0.001476\n",
      "Epoch: 91, Loss: 142.66249, Residuals: -1.28941, Convergence: 0.001389\n",
      "Epoch: 92, Loss: 142.47624, Residuals: -1.28223, Convergence: 0.001307\n",
      "Epoch: 93, Loss: 142.30132, Residuals: -1.27541, Convergence: 0.001229\n",
      "Epoch: 94, Loss: 142.13706, Residuals: -1.26895, Convergence: 0.001156\n",
      "Epoch: 95, Loss: 141.98269, Residuals: -1.26283, Convergence: 0.001087\n",
      "Epoch: 96, Loss: 141.83740, Residuals: -1.25703, Convergence: 0.001024\n",
      "Epoch: 97, Loss: 141.70025, Residuals: -1.25156, Convergence: 0.000968\n",
      "Evidence -183.401\n",
      "\n",
      "Epoch: 97, Evidence: -183.40125, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 7.25e-01\n",
      "Epoch: 97, Loss: 1353.28769, Residuals: -1.25156, Convergence:   inf\n",
      "Epoch: 98, Loss: 1291.15033, Residuals: -1.28102, Convergence: 0.048126\n",
      "Epoch: 99, Loss: 1243.73287, Residuals: -1.30434, Convergence: 0.038125\n",
      "Epoch: 100, Loss: 1207.84294, Residuals: -1.32122, Convergence: 0.029714\n",
      "Epoch: 101, Loss: 1179.90372, Residuals: -1.33307, Convergence: 0.023679\n",
      "Epoch: 102, Loss: 1157.32246, Residuals: -1.34173, Convergence: 0.019512\n",
      "Epoch: 103, Loss: 1138.59808, Residuals: -1.34824, Convergence: 0.016445\n",
      "Epoch: 104, Loss: 1122.81728, Residuals: -1.35308, Convergence: 0.014055\n",
      "Epoch: 105, Loss: 1109.35473, Residuals: -1.35652, Convergence: 0.012135\n",
      "Epoch: 106, Loss: 1097.74565, Residuals: -1.35873, Convergence: 0.010575\n",
      "Epoch: 107, Loss: 1087.62614, Residuals: -1.35984, Convergence: 0.009304\n",
      "Epoch: 108, Loss: 1078.70392, Residuals: -1.35996, Convergence: 0.008271\n",
      "Epoch: 109, Loss: 1070.73675, Residuals: -1.35916, Convergence: 0.007441\n",
      "Epoch: 110, Loss: 1063.52211, Residuals: -1.35752, Convergence: 0.006784\n",
      "Epoch: 111, Loss: 1056.88541, Residuals: -1.35509, Convergence: 0.006279\n",
      "Epoch: 112, Loss: 1050.67521, Residuals: -1.35190, Convergence: 0.005911\n",
      "Epoch: 113, Loss: 1044.75828, Residuals: -1.34797, Convergence: 0.005663\n",
      "Epoch: 114, Loss: 1039.01747, Residuals: -1.34332, Convergence: 0.005525\n",
      "Epoch: 115, Loss: 1033.35688, Residuals: -1.33795, Convergence: 0.005478\n",
      "Epoch: 116, Loss: 1027.70847, Residuals: -1.33190, Convergence: 0.005496\n",
      "Epoch: 117, Loss: 1022.04660, Residuals: -1.32525, Convergence: 0.005540\n",
      "Epoch: 118, Loss: 1016.39822, Residuals: -1.31806, Convergence: 0.005557\n",
      "Epoch: 119, Loss: 1010.83803, Residuals: -1.31046, Convergence: 0.005501\n",
      "Epoch: 120, Loss: 1005.45895, Residuals: -1.30254, Convergence: 0.005350\n",
      "Epoch: 121, Loss: 1000.34065, Residuals: -1.29439, Convergence: 0.005117\n",
      "Epoch: 122, Loss: 995.52690, Residuals: -1.28611, Convergence: 0.004835\n",
      "Epoch: 123, Loss: 991.02790, Residuals: -1.27776, Convergence: 0.004540\n",
      "Epoch: 124, Loss: 986.83074, Residuals: -1.26942, Convergence: 0.004253\n",
      "Epoch: 125, Loss: 982.91238, Residuals: -1.26112, Convergence: 0.003986\n",
      "Epoch: 126, Loss: 979.24564, Residuals: -1.25293, Convergence: 0.003744\n",
      "Epoch: 127, Loss: 975.80541, Residuals: -1.24487, Convergence: 0.003526\n",
      "Epoch: 128, Loss: 972.56914, Residuals: -1.23698, Convergence: 0.003328\n",
      "Epoch: 129, Loss: 969.51808, Residuals: -1.22928, Convergence: 0.003147\n",
      "Epoch: 130, Loss: 966.63627, Residuals: -1.22179, Convergence: 0.002981\n",
      "Epoch: 131, Loss: 963.91080, Residuals: -1.21454, Convergence: 0.002828\n",
      "Epoch: 132, Loss: 961.33083, Residuals: -1.20754, Convergence: 0.002684\n",
      "Epoch: 133, Loss: 958.88619, Residuals: -1.20079, Convergence: 0.002549\n",
      "Epoch: 134, Loss: 956.56910, Residuals: -1.19431, Convergence: 0.002422\n",
      "Epoch: 135, Loss: 954.37102, Residuals: -1.18810, Convergence: 0.002303\n",
      "Epoch: 136, Loss: 952.28491, Residuals: -1.18217, Convergence: 0.002191\n",
      "Epoch: 137, Loss: 950.30369, Residuals: -1.17650, Convergence: 0.002085\n",
      "Epoch: 138, Loss: 948.42011, Residuals: -1.17110, Convergence: 0.001986\n",
      "Epoch: 139, Loss: 946.62818, Residuals: -1.16596, Convergence: 0.001893\n",
      "Epoch: 140, Loss: 944.92078, Residuals: -1.16107, Convergence: 0.001807\n",
      "Epoch: 141, Loss: 943.29222, Residuals: -1.15642, Convergence: 0.001726\n",
      "Epoch: 142, Loss: 941.73693, Residuals: -1.15201, Convergence: 0.001652\n",
      "Epoch: 143, Loss: 940.24903, Residuals: -1.14781, Convergence: 0.001582\n",
      "Epoch: 144, Loss: 938.82330, Residuals: -1.14383, Convergence: 0.001519\n",
      "Epoch: 145, Loss: 937.45520, Residuals: -1.14004, Convergence: 0.001459\n",
      "Epoch: 146, Loss: 936.13995, Residuals: -1.13644, Convergence: 0.001405\n",
      "Epoch: 147, Loss: 934.87333, Residuals: -1.13301, Convergence: 0.001355\n",
      "Epoch: 148, Loss: 933.65154, Residuals: -1.12975, Convergence: 0.001309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 149, Loss: 932.47076, Residuals: -1.12664, Convergence: 0.001266\n",
      "Epoch: 150, Loss: 931.32770, Residuals: -1.12368, Convergence: 0.001227\n",
      "Epoch: 151, Loss: 930.21892, Residuals: -1.12085, Convergence: 0.001192\n",
      "Epoch: 152, Loss: 929.14161, Residuals: -1.11814, Convergence: 0.001159\n",
      "Epoch: 153, Loss: 928.09264, Residuals: -1.11555, Convergence: 0.001130\n",
      "Epoch: 154, Loss: 927.06944, Residuals: -1.11306, Convergence: 0.001104\n",
      "Epoch: 155, Loss: 926.06916, Residuals: -1.11068, Convergence: 0.001080\n",
      "Epoch: 156, Loss: 925.08963, Residuals: -1.10837, Convergence: 0.001059\n",
      "Epoch: 157, Loss: 924.12793, Residuals: -1.10615, Convergence: 0.001041\n",
      "Epoch: 158, Loss: 923.18153, Residuals: -1.10400, Convergence: 0.001025\n",
      "Epoch: 159, Loss: 922.24771, Residuals: -1.10191, Convergence: 0.001013\n",
      "Epoch: 160, Loss: 921.32432, Residuals: -1.09988, Convergence: 0.001002\n",
      "Epoch: 161, Loss: 920.40846, Residuals: -1.09789, Convergence: 0.000995\n",
      "Evidence 10974.912\n",
      "\n",
      "Epoch: 161, Evidence: 10974.91211, Convergence: 1.016711\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 5.78e-01\n",
      "Epoch: 161, Loss: 2326.10942, Residuals: -1.09789, Convergence:   inf\n",
      "Epoch: 162, Loss: 2287.57313, Residuals: -1.10597, Convergence: 0.016846\n",
      "Epoch: 163, Loss: 2260.09151, Residuals: -1.10477, Convergence: 0.012160\n",
      "Epoch: 164, Loss: 2236.88166, Residuals: -1.10260, Convergence: 0.010376\n",
      "Epoch: 165, Loss: 2217.06732, Residuals: -1.10002, Convergence: 0.008937\n",
      "Epoch: 166, Loss: 2200.02608, Residuals: -1.09718, Convergence: 0.007746\n",
      "Epoch: 167, Loss: 2185.27647, Residuals: -1.09417, Convergence: 0.006750\n",
      "Epoch: 168, Loss: 2172.42818, Residuals: -1.09105, Convergence: 0.005914\n",
      "Epoch: 169, Loss: 2161.15421, Residuals: -1.08786, Convergence: 0.005217\n",
      "Epoch: 170, Loss: 2151.17606, Residuals: -1.08462, Convergence: 0.004638\n",
      "Epoch: 171, Loss: 2142.25193, Residuals: -1.08134, Convergence: 0.004166\n",
      "Epoch: 172, Loss: 2134.17728, Residuals: -1.07801, Convergence: 0.003783\n",
      "Epoch: 173, Loss: 2126.78363, Residuals: -1.07462, Convergence: 0.003476\n",
      "Epoch: 174, Loss: 2119.94540, Residuals: -1.07117, Convergence: 0.003226\n",
      "Epoch: 175, Loss: 2113.57883, Residuals: -1.06766, Convergence: 0.003012\n",
      "Epoch: 176, Loss: 2107.63773, Residuals: -1.06412, Convergence: 0.002819\n",
      "Epoch: 177, Loss: 2102.09698, Residuals: -1.06058, Convergence: 0.002636\n",
      "Epoch: 178, Loss: 2096.94063, Residuals: -1.05709, Convergence: 0.002459\n",
      "Epoch: 179, Loss: 2092.15322, Residuals: -1.05369, Convergence: 0.002288\n",
      "Epoch: 180, Loss: 2087.71572, Residuals: -1.05039, Convergence: 0.002126\n",
      "Epoch: 181, Loss: 2083.60505, Residuals: -1.04723, Convergence: 0.001973\n",
      "Epoch: 182, Loss: 2079.79770, Residuals: -1.04421, Convergence: 0.001831\n",
      "Epoch: 183, Loss: 2076.26881, Residuals: -1.04134, Convergence: 0.001700\n",
      "Epoch: 184, Loss: 2072.99359, Residuals: -1.03862, Convergence: 0.001580\n",
      "Epoch: 185, Loss: 2069.94967, Residuals: -1.03606, Convergence: 0.001471\n",
      "Epoch: 186, Loss: 2067.11688, Residuals: -1.03364, Convergence: 0.001370\n",
      "Epoch: 187, Loss: 2064.47586, Residuals: -1.03137, Convergence: 0.001279\n",
      "Epoch: 188, Loss: 2062.01031, Residuals: -1.02924, Convergence: 0.001196\n",
      "Epoch: 189, Loss: 2059.70569, Residuals: -1.02724, Convergence: 0.001119\n",
      "Epoch: 190, Loss: 2057.54872, Residuals: -1.02536, Convergence: 0.001048\n",
      "Epoch: 191, Loss: 2055.52723, Residuals: -1.02360, Convergence: 0.000983\n",
      "Evidence 14157.835\n",
      "\n",
      "Epoch: 191, Evidence: 14157.83496, Convergence: 0.224817\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 4.41e-01\n",
      "Epoch: 191, Loss: 2455.33712, Residuals: -1.02360, Convergence:   inf\n",
      "Epoch: 192, Loss: 2441.08129, Residuals: -1.02055, Convergence: 0.005840\n",
      "Epoch: 193, Loss: 2429.47104, Residuals: -1.01707, Convergence: 0.004779\n",
      "Epoch: 194, Loss: 2419.54038, Residuals: -1.01367, Convergence: 0.004104\n",
      "Epoch: 195, Loss: 2410.98642, Residuals: -1.01046, Convergence: 0.003548\n",
      "Epoch: 196, Loss: 2403.57523, Residuals: -1.00749, Convergence: 0.003083\n",
      "Epoch: 197, Loss: 2397.11632, Residuals: -1.00476, Convergence: 0.002694\n",
      "Epoch: 198, Loss: 2391.45361, Residuals: -1.00227, Convergence: 0.002368\n",
      "Epoch: 199, Loss: 2386.45682, Residuals: -1.00000, Convergence: 0.002094\n",
      "Epoch: 200, Loss: 2382.01789, Residuals: -0.99792, Convergence: 0.001864\n",
      "Epoch: 201, Loss: 2378.04931, Residuals: -0.99603, Convergence: 0.001669\n",
      "Epoch: 202, Loss: 2374.47777, Residuals: -0.99430, Convergence: 0.001504\n",
      "Epoch: 203, Loss: 2371.24370, Residuals: -0.99271, Convergence: 0.001364\n",
      "Epoch: 204, Loss: 2368.29870, Residuals: -0.99126, Convergence: 0.001244\n",
      "Epoch: 205, Loss: 2365.60211, Residuals: -0.98993, Convergence: 0.001140\n",
      "Epoch: 206, Loss: 2363.12079, Residuals: -0.98871, Convergence: 0.001050\n",
      "Epoch: 207, Loss: 2360.82648, Residuals: -0.98759, Convergence: 0.000972\n",
      "Evidence 14565.713\n",
      "\n",
      "Epoch: 207, Evidence: 14565.71289, Convergence: 0.028003\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 3.37e-01\n",
      "Epoch: 207, Loss: 2461.24615, Residuals: -0.98759, Convergence:   inf\n",
      "Epoch: 208, Loss: 2454.75642, Residuals: -0.98443, Convergence: 0.002644\n",
      "Epoch: 209, Loss: 2449.39751, Residuals: -0.98164, Convergence: 0.002188\n",
      "Epoch: 210, Loss: 2444.85877, Residuals: -0.97927, Convergence: 0.001856\n",
      "Epoch: 211, Loss: 2440.95674, Residuals: -0.97726, Convergence: 0.001599\n",
      "Epoch: 212, Loss: 2437.55287, Residuals: -0.97556, Convergence: 0.001396\n",
      "Epoch: 213, Loss: 2434.54372, Residuals: -0.97413, Convergence: 0.001236\n",
      "Epoch: 214, Loss: 2431.85010, Residuals: -0.97292, Convergence: 0.001108\n",
      "Epoch: 215, Loss: 2429.41363, Residuals: -0.97190, Convergence: 0.001003\n",
      "Epoch: 216, Loss: 2427.18979, Residuals: -0.97104, Convergence: 0.000916\n",
      "Evidence 14649.840\n",
      "\n",
      "Epoch: 216, Evidence: 14649.83984, Convergence: 0.005743\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 2.63e-01\n",
      "Epoch: 216, Loss: 2462.91714, Residuals: -0.97104, Convergence:   inf\n",
      "Epoch: 217, Loss: 2459.12887, Residuals: -0.96869, Convergence: 0.001540\n",
      "Epoch: 218, Loss: 2455.97797, Residuals: -0.96684, Convergence: 0.001283\n",
      "Epoch: 219, Loss: 2453.27104, Residuals: -0.96537, Convergence: 0.001103\n",
      "Epoch: 220, Loss: 2450.89351, Residuals: -0.96421, Convergence: 0.000970\n",
      "Evidence 14678.085\n",
      "\n",
      "Epoch: 220, Evidence: 14678.08496, Convergence: 0.001924\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 2.11e-01\n",
      "Epoch: 220, Loss: 2464.01138, Residuals: -0.96421, Convergence:   inf\n",
      "Epoch: 221, Loss: 2461.16027, Residuals: -0.96233, Convergence: 0.001158\n",
      "Epoch: 222, Loss: 2458.76261, Residuals: -0.96090, Convergence: 0.000975\n",
      "Evidence 14690.005\n",
      "\n",
      "Epoch: 222, Evidence: 14690.00488, Convergence: 0.000811\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.75e-01\n",
      "Epoch: 222, Loss: 2464.79562, Residuals: -0.96090, Convergence:   inf\n",
      "Epoch: 223, Loss: 2460.23124, Residuals: -0.95835, Convergence: 0.001855\n",
      "Epoch: 224, Loss: 2456.75634, Residuals: -0.95676, Convergence: 0.001414\n",
      "Epoch: 225, Loss: 2453.91151, Residuals: -0.95578, Convergence: 0.001159\n",
      "Epoch: 226, Loss: 2451.48222, Residuals: -0.95526, Convergence: 0.000991\n",
      "Evidence 14707.807\n",
      "\n",
      "Epoch: 226, Evidence: 14707.80664, Convergence: 0.002021\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.45e-01\n",
      "Epoch: 226, Loss: 2465.02399, Residuals: -0.95526, Convergence:   inf\n",
      "Epoch: 227, Loss: 2461.97393, Residuals: -0.95303, Convergence: 0.001239\n",
      "Epoch: 228, Loss: 2459.54064, Residuals: -0.95200, Convergence: 0.000989\n",
      "Evidence 14718.664\n",
      "\n",
      "Epoch: 228, Evidence: 14718.66406, Convergence: 0.000738\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.22e-01\n",
      "Epoch: 228, Loss: 2465.27982, Residuals: -0.95200, Convergence:   inf\n",
      "Epoch: 229, Loss: 2460.76911, Residuals: -0.94890, Convergence: 0.001833\n",
      "Epoch: 230, Loss: 2457.49525, Residuals: -0.95012, Convergence: 0.001332\n",
      "Epoch: 231, Loss: 2454.77945, Residuals: -0.95069, Convergence: 0.001106\n",
      "Epoch: 232, Loss: 2452.41591, Residuals: -0.95333, Convergence: 0.000964\n",
      "Evidence 14734.611\n",
      "\n",
      "Epoch: 232, Evidence: 14734.61133, Convergence: 0.001819\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.09e-01\n",
      "Epoch: 232, Loss: 2464.61513, Residuals: -0.95333, Convergence:   inf\n",
      "Epoch: 233, Loss: 2462.80376, Residuals: -0.95157, Convergence: 0.000735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence 14741.216\n",
      "\n",
      "Epoch: 233, Evidence: 14741.21582, Convergence: 0.000448\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 8.88e-02\n",
      "Epoch: 233, Loss: 2465.63418, Residuals: -0.95157, Convergence:   inf\n",
      "Epoch: 234, Loss: 2514.01617, Residuals: -0.99155, Convergence: -0.019245\n",
      "Epoch: 234, Loss: 2463.24034, Residuals: -0.94907, Convergence: 0.000972\n",
      "Evidence 14745.605\n",
      "\n",
      "Epoch: 234, Evidence: 14745.60547, Convergence: 0.000746\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 8.17e-02\n",
      "Epoch: 234, Loss: 2465.08617, Residuals: -0.94907, Convergence:   inf\n",
      "Epoch: 235, Loss: 2469.28111, Residuals: -0.95018, Convergence: -0.001699\n",
      "Epoch: 235, Loss: 2464.93606, Residuals: -0.94754, Convergence: 0.000061\n",
      "Evidence 14747.314\n",
      "\n",
      "Epoch: 235, Evidence: 14747.31445, Convergence: 0.000861\n",
      "Total samples: 181, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 383.78581, Residuals: -4.56487, Convergence:   inf\n",
      "Epoch: 1, Loss: 357.99793, Residuals: -4.44340, Convergence: 0.072034\n",
      "Epoch: 2, Loss: 336.85728, Residuals: -4.27771, Convergence: 0.062758\n",
      "Epoch: 3, Loss: 320.70390, Residuals: -4.11176, Convergence: 0.050369\n",
      "Epoch: 4, Loss: 308.37837, Residuals: -3.96565, Convergence: 0.039969\n",
      "Epoch: 5, Loss: 298.59687, Residuals: -3.83659, Convergence: 0.032758\n",
      "Epoch: 6, Loss: 290.65001, Residuals: -3.72416, Convergence: 0.027342\n",
      "Epoch: 7, Loss: 284.05080, Residuals: -3.62786, Convergence: 0.023232\n",
      "Epoch: 8, Loss: 278.43809, Residuals: -3.54565, Convergence: 0.020158\n",
      "Epoch: 9, Loss: 273.55582, Residuals: -3.47518, Convergence: 0.017847\n",
      "Epoch: 10, Loss: 269.22143, Residuals: -3.41428, Convergence: 0.016100\n",
      "Epoch: 11, Loss: 265.30220, Residuals: -3.36112, Convergence: 0.014773\n",
      "Epoch: 12, Loss: 261.70069, Residuals: -3.31416, Convergence: 0.013762\n",
      "Epoch: 13, Loss: 258.34545, Residuals: -3.27205, Convergence: 0.012987\n",
      "Epoch: 14, Loss: 255.18490, Residuals: -3.23363, Convergence: 0.012385\n",
      "Epoch: 15, Loss: 252.18442, Residuals: -3.19795, Convergence: 0.011898\n",
      "Epoch: 16, Loss: 249.32490, Residuals: -3.16435, Convergence: 0.011469\n",
      "Epoch: 17, Loss: 246.59485, Residuals: -3.13242, Convergence: 0.011071\n",
      "Epoch: 18, Loss: 243.97621, Residuals: -3.10183, Convergence: 0.010733\n",
      "Epoch: 19, Loss: 241.43883, Residuals: -3.07208, Convergence: 0.010509\n",
      "Epoch: 20, Loss: 238.94666, Residuals: -3.04259, Convergence: 0.010430\n",
      "Epoch: 21, Loss: 236.46661, Residuals: -3.01282, Convergence: 0.010488\n",
      "Epoch: 22, Loss: 233.97392, Residuals: -2.98238, Convergence: 0.010654\n",
      "Epoch: 23, Loss: 231.44620, Residuals: -2.95096, Convergence: 0.010921\n",
      "Epoch: 24, Loss: 228.84618, Residuals: -2.91817, Convergence: 0.011361\n",
      "Epoch: 25, Loss: 226.11760, Residuals: -2.88334, Convergence: 0.012067\n",
      "Epoch: 26, Loss: 223.23670, Residuals: -2.84615, Convergence: 0.012905\n",
      "Epoch: 27, Loss: 220.30234, Residuals: -2.80759, Convergence: 0.013320\n",
      "Epoch: 28, Loss: 217.44509, Residuals: -2.76916, Convergence: 0.013140\n",
      "Epoch: 29, Loss: 214.70059, Residuals: -2.73145, Convergence: 0.012783\n",
      "Epoch: 30, Loss: 212.05632, Residuals: -2.69447, Convergence: 0.012470\n",
      "Epoch: 31, Loss: 209.49415, Residuals: -2.65809, Convergence: 0.012230\n",
      "Epoch: 32, Loss: 207.00008, Residuals: -2.62222, Convergence: 0.012049\n",
      "Epoch: 33, Loss: 204.56484, Residuals: -2.58675, Convergence: 0.011904\n",
      "Epoch: 34, Loss: 202.18298, Residuals: -2.55162, Convergence: 0.011781\n",
      "Epoch: 35, Loss: 199.85175, Residuals: -2.51676, Convergence: 0.011665\n",
      "Epoch: 36, Loss: 197.57033, Residuals: -2.48213, Convergence: 0.011547\n",
      "Epoch: 37, Loss: 195.33906, Residuals: -2.44772, Convergence: 0.011423\n",
      "Epoch: 38, Loss: 193.15893, Residuals: -2.41350, Convergence: 0.011287\n",
      "Epoch: 39, Loss: 191.03121, Residuals: -2.37947, Convergence: 0.011138\n",
      "Epoch: 40, Loss: 188.95719, Residuals: -2.34565, Convergence: 0.010976\n",
      "Epoch: 41, Loss: 186.93804, Residuals: -2.31204, Convergence: 0.010801\n",
      "Epoch: 42, Loss: 184.97471, Residuals: -2.27868, Convergence: 0.010614\n",
      "Epoch: 43, Loss: 183.06805, Residuals: -2.24560, Convergence: 0.010415\n",
      "Epoch: 44, Loss: 181.21881, Residuals: -2.21283, Convergence: 0.010204\n",
      "Epoch: 45, Loss: 179.42777, Residuals: -2.18040, Convergence: 0.009982\n",
      "Epoch: 46, Loss: 177.69578, Residuals: -2.14837, Convergence: 0.009747\n",
      "Epoch: 47, Loss: 176.02368, Residuals: -2.11677, Convergence: 0.009499\n",
      "Epoch: 48, Loss: 174.41223, Residuals: -2.08564, Convergence: 0.009239\n",
      "Epoch: 49, Loss: 172.86195, Residuals: -2.05501, Convergence: 0.008968\n",
      "Epoch: 50, Loss: 171.37300, Residuals: -2.02492, Convergence: 0.008688\n",
      "Epoch: 51, Loss: 169.94513, Residuals: -1.99539, Convergence: 0.008402\n",
      "Epoch: 52, Loss: 168.57762, Residuals: -1.96644, Convergence: 0.008112\n",
      "Epoch: 53, Loss: 167.26924, Residuals: -1.93810, Convergence: 0.007822\n",
      "Epoch: 54, Loss: 166.01833, Residuals: -1.91037, Convergence: 0.007535\n",
      "Epoch: 55, Loss: 164.82280, Residuals: -1.88328, Convergence: 0.007253\n",
      "Epoch: 56, Loss: 163.68024, Residuals: -1.85682, Convergence: 0.006980\n",
      "Epoch: 57, Loss: 162.58804, Residuals: -1.83100, Convergence: 0.006718\n",
      "Epoch: 58, Loss: 161.54354, Residuals: -1.80582, Convergence: 0.006466\n",
      "Epoch: 59, Loss: 160.54414, Residuals: -1.78126, Convergence: 0.006225\n",
      "Epoch: 60, Loss: 159.58740, Residuals: -1.75733, Convergence: 0.005995\n",
      "Epoch: 61, Loss: 158.67105, Residuals: -1.73401, Convergence: 0.005775\n",
      "Epoch: 62, Loss: 157.79310, Residuals: -1.71130, Convergence: 0.005564\n",
      "Epoch: 63, Loss: 156.95175, Residuals: -1.68920, Convergence: 0.005361\n",
      "Epoch: 64, Loss: 156.14541, Residuals: -1.66769, Convergence: 0.005164\n",
      "Epoch: 65, Loss: 155.37270, Residuals: -1.64679, Convergence: 0.004973\n",
      "Epoch: 66, Loss: 154.63233, Residuals: -1.62648, Convergence: 0.004788\n",
      "Epoch: 67, Loss: 153.92320, Residuals: -1.60677, Convergence: 0.004607\n",
      "Epoch: 68, Loss: 153.24422, Residuals: -1.58765, Convergence: 0.004431\n",
      "Epoch: 69, Loss: 152.59444, Residuals: -1.56913, Convergence: 0.004258\n",
      "Epoch: 70, Loss: 151.97292, Residuals: -1.55119, Convergence: 0.004090\n",
      "Epoch: 71, Loss: 151.37877, Residuals: -1.53384, Convergence: 0.003925\n",
      "Epoch: 72, Loss: 150.81112, Residuals: -1.51707, Convergence: 0.003764\n",
      "Epoch: 73, Loss: 150.26912, Residuals: -1.50088, Convergence: 0.003607\n",
      "Epoch: 74, Loss: 149.75188, Residuals: -1.48525, Convergence: 0.003454\n",
      "Epoch: 75, Loss: 149.25856, Residuals: -1.47019, Convergence: 0.003305\n",
      "Epoch: 76, Loss: 148.78827, Residuals: -1.45568, Convergence: 0.003161\n",
      "Epoch: 77, Loss: 148.34011, Residuals: -1.44171, Convergence: 0.003021\n",
      "Epoch: 78, Loss: 147.91320, Residuals: -1.42828, Convergence: 0.002886\n",
      "Epoch: 79, Loss: 147.50663, Residuals: -1.41536, Convergence: 0.002756\n",
      "Epoch: 80, Loss: 147.11950, Residuals: -1.40295, Convergence: 0.002631\n",
      "Epoch: 81, Loss: 146.75094, Residuals: -1.39104, Convergence: 0.002511\n",
      "Epoch: 82, Loss: 146.40003, Residuals: -1.37960, Convergence: 0.002397\n",
      "Epoch: 83, Loss: 146.06596, Residuals: -1.36864, Convergence: 0.002287\n",
      "Epoch: 84, Loss: 145.74788, Residuals: -1.35812, Convergence: 0.002182\n",
      "Epoch: 85, Loss: 145.44498, Residuals: -1.34803, Convergence: 0.002083\n",
      "Epoch: 86, Loss: 145.15653, Residuals: -1.33837, Convergence: 0.001987\n",
      "Epoch: 87, Loss: 144.88179, Residuals: -1.32910, Convergence: 0.001896\n",
      "Epoch: 88, Loss: 144.62008, Residuals: -1.32023, Convergence: 0.001810\n",
      "Epoch: 89, Loss: 144.37078, Residuals: -1.31173, Convergence: 0.001727\n",
      "Epoch: 90, Loss: 144.13327, Residuals: -1.30359, Convergence: 0.001648\n",
      "Epoch: 91, Loss: 143.90701, Residuals: -1.29579, Convergence: 0.001572\n",
      "Epoch: 92, Loss: 143.69150, Residuals: -1.28832, Convergence: 0.001500\n",
      "Epoch: 93, Loss: 143.48624, Residuals: -1.28117, Convergence: 0.001430\n",
      "Epoch: 94, Loss: 143.29083, Residuals: -1.27432, Convergence: 0.001364\n",
      "Epoch: 95, Loss: 143.10484, Residuals: -1.26776, Convergence: 0.001300\n",
      "Epoch: 96, Loss: 142.92792, Residuals: -1.26148, Convergence: 0.001238\n",
      "Epoch: 97, Loss: 142.75973, Residuals: -1.25547, Convergence: 0.001178\n",
      "Epoch: 98, Loss: 142.59998, Residuals: -1.24971, Convergence: 0.001120\n",
      "Epoch: 99, Loss: 142.44838, Residuals: -1.24420, Convergence: 0.001064\n",
      "Epoch: 100, Loss: 142.30469, Residuals: -1.23892, Convergence: 0.001010\n",
      "Epoch: 101, Loss: 142.16868, Residuals: -1.23387, Convergence: 0.000957\n",
      "Evidence -184.181\n",
      "\n",
      "Epoch: 101, Evidence: -184.18138, Convergence:   inf\n",
      "Updating hyper-parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 181, Updated regularization: 7.25e-01\n",
      "Epoch: 101, Loss: 1365.88992, Residuals: -1.23387, Convergence:   inf\n",
      "Epoch: 102, Loss: 1304.46795, Residuals: -1.26386, Convergence: 0.047086\n",
      "Epoch: 103, Loss: 1257.40570, Residuals: -1.28720, Convergence: 0.037428\n",
      "Epoch: 104, Loss: 1221.59717, Residuals: -1.30383, Convergence: 0.029313\n",
      "Epoch: 105, Loss: 1193.64124, Residuals: -1.31541, Convergence: 0.023421\n",
      "Epoch: 106, Loss: 1171.01376, Residuals: -1.32393, Convergence: 0.019323\n",
      "Epoch: 107, Loss: 1152.22179, Residuals: -1.33044, Convergence: 0.016309\n",
      "Epoch: 108, Loss: 1136.35515, Residuals: -1.33541, Convergence: 0.013963\n",
      "Epoch: 109, Loss: 1122.79594, Residuals: -1.33908, Convergence: 0.012076\n",
      "Epoch: 110, Loss: 1111.08697, Residuals: -1.34159, Convergence: 0.010538\n",
      "Epoch: 111, Loss: 1100.87343, Residuals: -1.34306, Convergence: 0.009278\n",
      "Epoch: 112, Loss: 1091.87227, Residuals: -1.34360, Convergence: 0.008244\n",
      "Epoch: 113, Loss: 1083.85221, Residuals: -1.34328, Convergence: 0.007400\n",
      "Epoch: 114, Loss: 1076.62080, Residuals: -1.34219, Convergence: 0.006717\n",
      "Epoch: 115, Loss: 1070.01542, Residuals: -1.34038, Convergence: 0.006173\n",
      "Epoch: 116, Loss: 1063.89493, Residuals: -1.33788, Convergence: 0.005753\n",
      "Epoch: 117, Loss: 1058.13608, Residuals: -1.33473, Convergence: 0.005442\n",
      "Epoch: 118, Loss: 1052.62873, Residuals: -1.33094, Convergence: 0.005232\n",
      "Epoch: 119, Loss: 1047.27587, Residuals: -1.32652, Convergence: 0.005111\n",
      "Epoch: 120, Loss: 1041.99524, Residuals: -1.32149, Convergence: 0.005068\n",
      "Epoch: 121, Loss: 1036.72635, Residuals: -1.31586, Convergence: 0.005082\n",
      "Epoch: 122, Loss: 1031.44166, Residuals: -1.30968, Convergence: 0.005124\n",
      "Epoch: 123, Loss: 1026.15750, Residuals: -1.30302, Convergence: 0.005149\n",
      "Epoch: 124, Loss: 1020.93462, Residuals: -1.29598, Convergence: 0.005116\n",
      "Epoch: 125, Loss: 1015.85887, Residuals: -1.28864, Convergence: 0.004997\n",
      "Epoch: 126, Loss: 1011.01018, Residuals: -1.28109, Convergence: 0.004796\n",
      "Epoch: 127, Loss: 1006.43900, Residuals: -1.27340, Convergence: 0.004542\n",
      "Epoch: 128, Loss: 1002.16180, Residuals: -1.26566, Convergence: 0.004268\n",
      "Epoch: 129, Loss: 998.17065, Residuals: -1.25792, Convergence: 0.003998\n",
      "Epoch: 130, Loss: 994.44367, Residuals: -1.25023, Convergence: 0.003748\n",
      "Epoch: 131, Loss: 990.95492, Residuals: -1.24264, Convergence: 0.003521\n",
      "Epoch: 132, Loss: 987.67839, Residuals: -1.23517, Convergence: 0.003317\n",
      "Epoch: 133, Loss: 984.59072, Residuals: -1.22787, Convergence: 0.003136\n",
      "Epoch: 134, Loss: 981.67216, Residuals: -1.22073, Convergence: 0.002973\n",
      "Epoch: 135, Loss: 978.90484, Residuals: -1.21379, Convergence: 0.002827\n",
      "Epoch: 136, Loss: 976.27457, Residuals: -1.20706, Convergence: 0.002694\n",
      "Epoch: 137, Loss: 973.76875, Residuals: -1.20053, Convergence: 0.002573\n",
      "Epoch: 138, Loss: 971.37724, Residuals: -1.19423, Convergence: 0.002462\n",
      "Epoch: 139, Loss: 969.09094, Residuals: -1.18814, Convergence: 0.002359\n",
      "Epoch: 140, Loss: 966.90190, Residuals: -1.18227, Convergence: 0.002264\n",
      "Epoch: 141, Loss: 964.80317, Residuals: -1.17662, Convergence: 0.002175\n",
      "Epoch: 142, Loss: 962.78915, Residuals: -1.17118, Convergence: 0.002092\n",
      "Epoch: 143, Loss: 960.85364, Residuals: -1.16595, Convergence: 0.002014\n",
      "Epoch: 144, Loss: 958.99257, Residuals: -1.16093, Convergence: 0.001941\n",
      "Epoch: 145, Loss: 957.20126, Residuals: -1.15611, Convergence: 0.001871\n",
      "Epoch: 146, Loss: 955.47555, Residuals: -1.15148, Convergence: 0.001806\n",
      "Epoch: 147, Loss: 953.81197, Residuals: -1.14703, Convergence: 0.001744\n",
      "Epoch: 148, Loss: 952.20667, Residuals: -1.14277, Convergence: 0.001686\n",
      "Epoch: 149, Loss: 950.65669, Residuals: -1.13867, Convergence: 0.001630\n",
      "Epoch: 150, Loss: 949.15836, Residuals: -1.13474, Convergence: 0.001579\n",
      "Epoch: 151, Loss: 947.70905, Residuals: -1.13097, Convergence: 0.001529\n",
      "Epoch: 152, Loss: 946.30488, Residuals: -1.12734, Convergence: 0.001484\n",
      "Epoch: 153, Loss: 944.94263, Residuals: -1.12385, Convergence: 0.001442\n",
      "Epoch: 154, Loss: 943.61869, Residuals: -1.12049, Convergence: 0.001403\n",
      "Epoch: 155, Loss: 942.32923, Residuals: -1.11725, Convergence: 0.001368\n",
      "Epoch: 156, Loss: 941.07030, Residuals: -1.11412, Convergence: 0.001338\n",
      "Epoch: 157, Loss: 939.83745, Residuals: -1.11109, Convergence: 0.001312\n",
      "Epoch: 158, Loss: 938.62621, Residuals: -1.10815, Convergence: 0.001290\n",
      "Epoch: 159, Loss: 937.43180, Residuals: -1.10530, Convergence: 0.001274\n",
      "Epoch: 160, Loss: 936.24966, Residuals: -1.10251, Convergence: 0.001263\n",
      "Epoch: 161, Loss: 935.07538, Residuals: -1.09977, Convergence: 0.001256\n",
      "Epoch: 162, Loss: 933.90506, Residuals: -1.09709, Convergence: 0.001253\n",
      "Epoch: 163, Loss: 932.73578, Residuals: -1.09444, Convergence: 0.001254\n",
      "Epoch: 164, Loss: 931.56577, Residuals: -1.09183, Convergence: 0.001256\n",
      "Epoch: 165, Loss: 930.39517, Residuals: -1.08924, Convergence: 0.001258\n",
      "Epoch: 166, Loss: 929.22545, Residuals: -1.08669, Convergence: 0.001259\n",
      "Epoch: 167, Loss: 928.05986, Residuals: -1.08417, Convergence: 0.001256\n",
      "Epoch: 168, Loss: 926.90174, Residuals: -1.08168, Convergence: 0.001249\n",
      "Epoch: 169, Loss: 925.75558, Residuals: -1.07924, Convergence: 0.001238\n",
      "Epoch: 170, Loss: 924.62512, Residuals: -1.07686, Convergence: 0.001223\n",
      "Epoch: 171, Loss: 923.51417, Residuals: -1.07452, Convergence: 0.001203\n",
      "Epoch: 172, Loss: 922.42609, Residuals: -1.07225, Convergence: 0.001180\n",
      "Epoch: 173, Loss: 921.36282, Residuals: -1.07004, Convergence: 0.001154\n",
      "Epoch: 174, Loss: 920.32656, Residuals: -1.06790, Convergence: 0.001126\n",
      "Epoch: 175, Loss: 919.31893, Residuals: -1.06583, Convergence: 0.001096\n",
      "Epoch: 176, Loss: 918.34132, Residuals: -1.06383, Convergence: 0.001065\n",
      "Epoch: 177, Loss: 917.39487, Residuals: -1.06191, Convergence: 0.001032\n",
      "Epoch: 178, Loss: 916.48016, Residuals: -1.06006, Convergence: 0.000998\n",
      "Evidence 11024.656\n",
      "\n",
      "Epoch: 178, Evidence: 11024.65625, Convergence: 1.016706\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 5.75e-01\n",
      "Epoch: 178, Loss: 2328.45386, Residuals: -1.06006, Convergence:   inf\n",
      "Epoch: 179, Loss: 2290.22563, Residuals: -1.06868, Convergence: 0.016692\n",
      "Epoch: 180, Loss: 2264.06198, Residuals: -1.06864, Convergence: 0.011556\n",
      "Epoch: 181, Loss: 2242.43408, Residuals: -1.06762, Convergence: 0.009645\n",
      "Epoch: 182, Loss: 2224.23861, Residuals: -1.06633, Convergence: 0.008181\n",
      "Epoch: 183, Loss: 2208.77557, Residuals: -1.06487, Convergence: 0.007001\n",
      "Epoch: 184, Loss: 2195.50679, Residuals: -1.06328, Convergence: 0.006044\n",
      "Epoch: 185, Loss: 2183.99369, Residuals: -1.06156, Convergence: 0.005272\n",
      "Epoch: 186, Loss: 2173.87460, Residuals: -1.05969, Convergence: 0.004655\n",
      "Epoch: 187, Loss: 2164.85825, Residuals: -1.05768, Convergence: 0.004165\n",
      "Epoch: 188, Loss: 2156.71985, Residuals: -1.05548, Convergence: 0.003774\n",
      "Epoch: 189, Loss: 2149.30164, Residuals: -1.05312, Convergence: 0.003451\n",
      "Epoch: 190, Loss: 2142.50253, Residuals: -1.05060, Convergence: 0.003173\n",
      "Epoch: 191, Loss: 2136.25916, Residuals: -1.04797, Convergence: 0.002923\n",
      "Epoch: 192, Loss: 2130.52725, Residuals: -1.04528, Convergence: 0.002690\n",
      "Epoch: 193, Loss: 2125.26782, Residuals: -1.04258, Convergence: 0.002475\n",
      "Epoch: 194, Loss: 2120.44494, Residuals: -1.03989, Convergence: 0.002274\n",
      "Epoch: 195, Loss: 2116.02339, Residuals: -1.03725, Convergence: 0.002090\n",
      "Epoch: 196, Loss: 2111.97109, Residuals: -1.03468, Convergence: 0.001919\n",
      "Epoch: 197, Loss: 2108.25688, Residuals: -1.03220, Convergence: 0.001762\n",
      "Epoch: 198, Loss: 2104.85242, Residuals: -1.02982, Convergence: 0.001617\n",
      "Epoch: 199, Loss: 2101.73022, Residuals: -1.02753, Convergence: 0.001486\n",
      "Epoch: 200, Loss: 2098.86454, Residuals: -1.02536, Convergence: 0.001365\n",
      "Epoch: 201, Loss: 2096.23057, Residuals: -1.02328, Convergence: 0.001257\n",
      "Epoch: 202, Loss: 2093.80550, Residuals: -1.02131, Convergence: 0.001158\n",
      "Epoch: 203, Loss: 2091.56809, Residuals: -1.01945, Convergence: 0.001070\n",
      "Epoch: 204, Loss: 2089.49837, Residuals: -1.01767, Convergence: 0.000991\n",
      "Evidence 14144.009\n",
      "\n",
      "Epoch: 204, Evidence: 14144.00879, Convergence: 0.220542\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 4.37e-01\n",
      "Epoch: 204, Loss: 2456.22645, Residuals: -1.01767, Convergence:   inf\n",
      "Epoch: 205, Loss: 2443.03478, Residuals: -1.01550, Convergence: 0.005400\n",
      "Epoch: 206, Loss: 2432.27853, Residuals: -1.01260, Convergence: 0.004422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 207, Loss: 2423.00725, Residuals: -1.00972, Convergence: 0.003826\n",
      "Epoch: 208, Loss: 2414.95603, Residuals: -1.00696, Convergence: 0.003334\n",
      "Epoch: 209, Loss: 2407.92602, Residuals: -1.00437, Convergence: 0.002920\n",
      "Epoch: 210, Loss: 2401.75750, Residuals: -1.00197, Convergence: 0.002568\n",
      "Epoch: 211, Loss: 2396.31922, Residuals: -0.99975, Convergence: 0.002269\n",
      "Epoch: 212, Loss: 2391.50179, Residuals: -0.99769, Convergence: 0.002014\n",
      "Epoch: 213, Loss: 2387.21287, Residuals: -0.99577, Convergence: 0.001797\n",
      "Epoch: 214, Loss: 2383.37591, Residuals: -0.99400, Convergence: 0.001610\n",
      "Epoch: 215, Loss: 2379.92462, Residuals: -0.99234, Convergence: 0.001450\n",
      "Epoch: 216, Loss: 2376.80505, Residuals: -0.99081, Convergence: 0.001313\n",
      "Epoch: 217, Loss: 2373.97105, Residuals: -0.98938, Convergence: 0.001194\n",
      "Epoch: 218, Loss: 2371.38340, Residuals: -0.98804, Convergence: 0.001091\n",
      "Epoch: 219, Loss: 2369.00991, Residuals: -0.98680, Convergence: 0.001002\n",
      "Epoch: 220, Loss: 2366.82259, Residuals: -0.98565, Convergence: 0.000924\n",
      "Evidence 14509.617\n",
      "\n",
      "Epoch: 220, Evidence: 14509.61719, Convergence: 0.025198\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 3.35e-01\n",
      "Epoch: 220, Loss: 2461.32299, Residuals: -0.98565, Convergence:   inf\n",
      "Epoch: 221, Loss: 2454.91325, Residuals: -0.98248, Convergence: 0.002611\n",
      "Epoch: 222, Loss: 2449.60671, Residuals: -0.97964, Convergence: 0.002166\n",
      "Epoch: 223, Loss: 2445.10323, Residuals: -0.97718, Convergence: 0.001842\n",
      "Epoch: 224, Loss: 2441.23220, Residuals: -0.97506, Convergence: 0.001586\n",
      "Epoch: 225, Loss: 2437.86538, Residuals: -0.97322, Convergence: 0.001381\n",
      "Epoch: 226, Loss: 2434.90248, Residuals: -0.97162, Convergence: 0.001217\n",
      "Epoch: 227, Loss: 2432.26618, Residuals: -0.97022, Convergence: 0.001084\n",
      "Epoch: 228, Loss: 2429.89706, Residuals: -0.96900, Convergence: 0.000975\n",
      "Evidence 14590.898\n",
      "\n",
      "Epoch: 228, Evidence: 14590.89844, Convergence: 0.005571\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 2.64e-01\n",
      "Epoch: 228, Loss: 2462.88696, Residuals: -0.96900, Convergence:   inf\n",
      "Epoch: 229, Loss: 2458.90533, Residuals: -0.96651, Convergence: 0.001619\n",
      "Epoch: 230, Loss: 2455.59936, Residuals: -0.96449, Convergence: 0.001346\n",
      "Epoch: 231, Loss: 2452.77785, Residuals: -0.96285, Convergence: 0.001150\n",
      "Epoch: 232, Loss: 2450.32517, Residuals: -0.96148, Convergence: 0.001001\n",
      "Epoch: 233, Loss: 2448.15832, Residuals: -0.96035, Convergence: 0.000885\n",
      "Evidence 14621.594\n",
      "\n",
      "Epoch: 233, Evidence: 14621.59375, Convergence: 0.002099\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 2.13e-01\n",
      "Epoch: 233, Loss: 2463.68880, Residuals: -0.96035, Convergence:   inf\n",
      "Epoch: 234, Loss: 2460.83777, Residuals: -0.95844, Convergence: 0.001159\n",
      "Epoch: 235, Loss: 2458.44722, Residuals: -0.95695, Convergence: 0.000972\n",
      "Evidence 14634.498\n",
      "\n",
      "Epoch: 235, Evidence: 14634.49805, Convergence: 0.000882\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.77e-01\n",
      "Epoch: 235, Loss: 2464.32813, Residuals: -0.95695, Convergence:   inf\n",
      "Epoch: 236, Loss: 2459.77819, Residuals: -0.95435, Convergence: 0.001850\n",
      "Epoch: 237, Loss: 2456.34108, Residuals: -0.95262, Convergence: 0.001399\n",
      "Epoch: 238, Loss: 2453.57181, Residuals: -0.95141, Convergence: 0.001129\n",
      "Epoch: 239, Loss: 2451.23557, Residuals: -0.95070, Convergence: 0.000953\n",
      "Evidence 14652.401\n",
      "\n",
      "Epoch: 239, Evidence: 14652.40137, Convergence: 0.002103\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.47e-01\n",
      "Epoch: 239, Loss: 2464.43093, Residuals: -0.95070, Convergence:   inf\n",
      "Epoch: 240, Loss: 2461.38275, Residuals: -0.94835, Convergence: 0.001238\n",
      "Epoch: 241, Loss: 2458.99112, Residuals: -0.94712, Convergence: 0.000973\n",
      "Evidence 14663.673\n",
      "\n",
      "Epoch: 241, Evidence: 14663.67285, Convergence: 0.000769\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.24e-01\n",
      "Epoch: 241, Loss: 2464.58070, Residuals: -0.94712, Convergence:   inf\n",
      "Epoch: 242, Loss: 2460.10016, Residuals: -0.94394, Convergence: 0.001821\n",
      "Epoch: 243, Loss: 2456.94726, Residuals: -0.94440, Convergence: 0.001283\n",
      "Epoch: 244, Loss: 2454.31713, Residuals: -0.94478, Convergence: 0.001072\n",
      "Epoch: 245, Loss: 2452.03935, Residuals: -0.94696, Convergence: 0.000929\n",
      "Evidence 14679.684\n",
      "\n",
      "Epoch: 245, Evidence: 14679.68359, Convergence: 0.001859\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.10e-01\n",
      "Epoch: 245, Loss: 2463.79810, Residuals: -0.94696, Convergence:   inf\n",
      "Epoch: 246, Loss: 2461.73353, Residuals: -0.94451, Convergence: 0.000839\n",
      "Evidence 14686.995\n",
      "\n",
      "Epoch: 246, Evidence: 14686.99512, Convergence: 0.000498\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 9.04e-02\n",
      "Epoch: 246, Loss: 2464.60055, Residuals: -0.94451, Convergence:   inf\n",
      "Epoch: 247, Loss: 2501.90031, Residuals: -0.98416, Convergence: -0.014909\n",
      "Epoch: 247, Loss: 2462.33775, Residuals: -0.94274, Convergence: 0.000919\n",
      "Evidence 14691.391\n",
      "\n",
      "Epoch: 247, Evidence: 14691.39062, Convergence: 0.000797\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 8.34e-02\n",
      "Epoch: 247, Loss: 2464.02859, Residuals: -0.94274, Convergence:   inf\n",
      "Epoch: 248, Loss: 2468.16582, Residuals: -0.94475, Convergence: -0.001676\n",
      "Epoch: 248, Loss: 2463.76960, Residuals: -0.94093, Convergence: 0.000105\n",
      "Evidence 14693.354\n",
      "\n",
      "Epoch: 248, Evidence: 14693.35352, Convergence: 0.000930\n",
      "Total samples: 183, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 381.66102, Residuals: -4.51360, Convergence:   inf\n",
      "Epoch: 1, Loss: 355.91018, Residuals: -4.39268, Convergence: 0.072352\n",
      "Epoch: 2, Loss: 334.78551, Residuals: -4.22730, Convergence: 0.063099\n",
      "Epoch: 3, Loss: 318.64281, Residuals: -4.06150, Convergence: 0.050661\n",
      "Epoch: 4, Loss: 306.33099, Residuals: -3.91572, Convergence: 0.040191\n",
      "Epoch: 5, Loss: 296.55846, Residuals: -3.78700, Convergence: 0.032953\n",
      "Epoch: 6, Loss: 288.61699, Residuals: -3.67497, Convergence: 0.027516\n",
      "Epoch: 7, Loss: 282.01789, Residuals: -3.57909, Convergence: 0.023400\n",
      "Epoch: 8, Loss: 276.39991, Residuals: -3.49727, Convergence: 0.020326\n",
      "Epoch: 9, Loss: 271.50799, Residuals: -3.42716, Convergence: 0.018018\n",
      "Epoch: 10, Loss: 267.16053, Residuals: -3.36663, Convergence: 0.016273\n",
      "Epoch: 11, Loss: 263.22584, Residuals: -3.31388, Convergence: 0.014948\n",
      "Epoch: 12, Loss: 259.60776, Residuals: -3.26737, Convergence: 0.013937\n",
      "Epoch: 13, Loss: 256.23642, Residuals: -3.22578, Convergence: 0.013157\n",
      "Epoch: 14, Loss: 253.06191, Residuals: -3.18795, Convergence: 0.012544\n",
      "Epoch: 15, Loss: 250.05072, Residuals: -3.15288, Convergence: 0.012042\n",
      "Epoch: 16, Loss: 247.18361, Residuals: -3.11987, Convergence: 0.011599\n",
      "Epoch: 17, Loss: 244.44843, Residuals: -3.08849, Convergence: 0.011189\n",
      "Epoch: 18, Loss: 241.82719, Residuals: -3.05835, Convergence: 0.010839\n",
      "Epoch: 19, Loss: 239.29069, Residuals: -3.02899, Convergence: 0.010600\n",
      "Epoch: 20, Loss: 236.80364, Residuals: -2.99990, Convergence: 0.010503\n",
      "Epoch: 21, Loss: 234.33212, Residuals: -2.97057, Convergence: 0.010547\n",
      "Epoch: 22, Loss: 231.84811, Residuals: -2.94064, Convergence: 0.010714\n",
      "Epoch: 23, Loss: 229.32462, Residuals: -2.90982, Convergence: 0.011004\n",
      "Epoch: 24, Loss: 226.72023, Residuals: -2.87767, Convergence: 0.011487\n",
      "Epoch: 25, Loss: 223.97535, Residuals: -2.84355, Convergence: 0.012255\n",
      "Epoch: 26, Loss: 221.06825, Residuals: -2.80713, Convergence: 0.013150\n",
      "Epoch: 27, Loss: 218.10671, Residuals: -2.76948, Convergence: 0.013578\n",
      "Epoch: 28, Loss: 215.22037, Residuals: -2.73208, Convergence: 0.013411\n",
      "Epoch: 29, Loss: 212.43842, Residuals: -2.69534, Convergence: 0.013095\n",
      "Epoch: 30, Loss: 209.74557, Residuals: -2.65919, Convergence: 0.012839\n",
      "Epoch: 31, Loss: 207.12383, Residuals: -2.62343, Convergence: 0.012658\n",
      "Epoch: 32, Loss: 204.56094, Residuals: -2.58792, Convergence: 0.012529\n",
      "Epoch: 33, Loss: 202.05023, Residuals: -2.55257, Convergence: 0.012426\n",
      "Epoch: 34, Loss: 199.58914, Residuals: -2.51731, Convergence: 0.012331\n",
      "Epoch: 35, Loss: 197.17774, Residuals: -2.48215, Convergence: 0.012230\n",
      "Epoch: 36, Loss: 194.81760, Residuals: -2.44707, Convergence: 0.012115\n",
      "Epoch: 37, Loss: 192.51102, Residuals: -2.41209, Convergence: 0.011982\n",
      "Epoch: 38, Loss: 190.26044, Residuals: -2.37725, Convergence: 0.011829\n",
      "Epoch: 39, Loss: 188.06818, Residuals: -2.34258, Convergence: 0.011657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40, Loss: 185.93628, Residuals: -2.30812, Convergence: 0.011466\n",
      "Epoch: 41, Loss: 183.86638, Residuals: -2.27390, Convergence: 0.011258\n",
      "Epoch: 42, Loss: 181.85983, Residuals: -2.23996, Convergence: 0.011033\n",
      "Epoch: 43, Loss: 179.91773, Residuals: -2.20634, Convergence: 0.010794\n",
      "Epoch: 44, Loss: 178.04110, Residuals: -2.17309, Convergence: 0.010540\n",
      "Epoch: 45, Loss: 176.23100, Residuals: -2.14024, Convergence: 0.010271\n",
      "Epoch: 46, Loss: 174.48853, Residuals: -2.10786, Convergence: 0.009986\n",
      "Epoch: 47, Loss: 172.81476, Residuals: -2.07600, Convergence: 0.009685\n",
      "Epoch: 48, Loss: 171.21048, Residuals: -2.04470, Convergence: 0.009370\n",
      "Epoch: 49, Loss: 169.67589, Residuals: -2.01401, Convergence: 0.009044\n",
      "Epoch: 50, Loss: 168.21034, Residuals: -1.98398, Convergence: 0.008713\n",
      "Epoch: 51, Loss: 166.81221, Residuals: -1.95462, Convergence: 0.008381\n",
      "Epoch: 52, Loss: 165.47898, Residuals: -1.92597, Convergence: 0.008057\n",
      "Epoch: 53, Loss: 164.20743, Residuals: -1.89800, Convergence: 0.007744\n",
      "Epoch: 54, Loss: 162.99394, Residuals: -1.87070, Convergence: 0.007445\n",
      "Epoch: 55, Loss: 161.83477, Residuals: -1.84406, Convergence: 0.007163\n",
      "Epoch: 56, Loss: 160.72632, Residuals: -1.81805, Convergence: 0.006897\n",
      "Epoch: 57, Loss: 159.66531, Residuals: -1.79266, Convergence: 0.006645\n",
      "Epoch: 58, Loss: 158.64890, Residuals: -1.76785, Convergence: 0.006407\n",
      "Epoch: 59, Loss: 157.67461, Residuals: -1.74363, Convergence: 0.006179\n",
      "Epoch: 60, Loss: 156.74036, Residuals: -1.71999, Convergence: 0.005960\n",
      "Epoch: 61, Loss: 155.84437, Residuals: -1.69693, Convergence: 0.005749\n",
      "Epoch: 62, Loss: 154.98510, Residuals: -1.67445, Convergence: 0.005544\n",
      "Epoch: 63, Loss: 154.16115, Residuals: -1.65256, Convergence: 0.005345\n",
      "Epoch: 64, Loss: 153.37128, Residuals: -1.63125, Convergence: 0.005150\n",
      "Epoch: 65, Loss: 152.61434, Residuals: -1.61054, Convergence: 0.004960\n",
      "Epoch: 66, Loss: 151.88924, Residuals: -1.59042, Convergence: 0.004774\n",
      "Epoch: 67, Loss: 151.19494, Residuals: -1.57091, Convergence: 0.004592\n",
      "Epoch: 68, Loss: 150.53045, Residuals: -1.55199, Convergence: 0.004414\n",
      "Epoch: 69, Loss: 149.89482, Residuals: -1.53367, Convergence: 0.004241\n",
      "Epoch: 70, Loss: 149.28710, Residuals: -1.51595, Convergence: 0.004071\n",
      "Epoch: 71, Loss: 148.70639, Residuals: -1.49883, Convergence: 0.003905\n",
      "Epoch: 72, Loss: 148.15178, Residuals: -1.48229, Convergence: 0.003743\n",
      "Epoch: 73, Loss: 147.62240, Residuals: -1.46634, Convergence: 0.003586\n",
      "Epoch: 74, Loss: 147.11736, Residuals: -1.45097, Convergence: 0.003433\n",
      "Epoch: 75, Loss: 146.63578, Residuals: -1.43617, Convergence: 0.003284\n",
      "Epoch: 76, Loss: 146.17679, Residuals: -1.42192, Convergence: 0.003140\n",
      "Epoch: 77, Loss: 145.73954, Residuals: -1.40823, Convergence: 0.003000\n",
      "Epoch: 78, Loss: 145.32315, Residuals: -1.39507, Convergence: 0.002865\n",
      "Epoch: 79, Loss: 144.92675, Residuals: -1.38243, Convergence: 0.002735\n",
      "Epoch: 80, Loss: 144.54951, Residuals: -1.37031, Convergence: 0.002610\n",
      "Epoch: 81, Loss: 144.19058, Residuals: -1.35868, Convergence: 0.002489\n",
      "Epoch: 82, Loss: 143.84914, Residuals: -1.34752, Convergence: 0.002374\n",
      "Epoch: 83, Loss: 143.52438, Residuals: -1.33684, Convergence: 0.002263\n",
      "Epoch: 84, Loss: 143.21554, Residuals: -1.32659, Convergence: 0.002157\n",
      "Epoch: 85, Loss: 142.92185, Residuals: -1.31679, Convergence: 0.002055\n",
      "Epoch: 86, Loss: 142.64260, Residuals: -1.30739, Convergence: 0.001958\n",
      "Epoch: 87, Loss: 142.37713, Residuals: -1.29840, Convergence: 0.001865\n",
      "Epoch: 88, Loss: 142.12477, Residuals: -1.28979, Convergence: 0.001776\n",
      "Epoch: 89, Loss: 141.88493, Residuals: -1.28155, Convergence: 0.001690\n",
      "Epoch: 90, Loss: 141.65705, Residuals: -1.27367, Convergence: 0.001609\n",
      "Epoch: 91, Loss: 141.44059, Residuals: -1.26612, Convergence: 0.001530\n",
      "Epoch: 92, Loss: 141.23508, Residuals: -1.25890, Convergence: 0.001455\n",
      "Epoch: 93, Loss: 141.04008, Residuals: -1.25199, Convergence: 0.001383\n",
      "Epoch: 94, Loss: 140.85519, Residuals: -1.24539, Convergence: 0.001313\n",
      "Epoch: 95, Loss: 140.68002, Residuals: -1.23907, Convergence: 0.001245\n",
      "Epoch: 96, Loss: 140.51425, Residuals: -1.23303, Convergence: 0.001180\n",
      "Epoch: 97, Loss: 140.35758, Residuals: -1.22725, Convergence: 0.001116\n",
      "Epoch: 98, Loss: 140.20972, Residuals: -1.22174, Convergence: 0.001055\n",
      "Epoch: 99, Loss: 140.07043, Residuals: -1.21648, Convergence: 0.000994\n",
      "Evidence -181.289\n",
      "\n",
      "Epoch: 99, Evidence: -181.28857, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.24e-01\n",
      "Epoch: 99, Loss: 1366.35086, Residuals: -1.21648, Convergence:   inf\n",
      "Epoch: 100, Loss: 1305.32352, Residuals: -1.24499, Convergence: 0.046753\n",
      "Epoch: 101, Loss: 1259.01025, Residuals: -1.26697, Convergence: 0.036785\n",
      "Epoch: 102, Loss: 1224.10119, Residuals: -1.28221, Convergence: 0.028518\n",
      "Epoch: 103, Loss: 1196.95378, Residuals: -1.29247, Convergence: 0.022680\n",
      "Epoch: 104, Loss: 1174.98776, Residuals: -1.29978, Convergence: 0.018695\n",
      "Epoch: 105, Loss: 1156.75088, Residuals: -1.30518, Convergence: 0.015766\n",
      "Epoch: 106, Loss: 1141.37755, Residuals: -1.30913, Convergence: 0.013469\n",
      "Epoch: 107, Loss: 1128.27869, Residuals: -1.31185, Convergence: 0.011610\n",
      "Epoch: 108, Loss: 1117.01447, Residuals: -1.31349, Convergence: 0.010084\n",
      "Epoch: 109, Loss: 1107.24139, Residuals: -1.31419, Convergence: 0.008827\n",
      "Epoch: 110, Loss: 1098.68128, Residuals: -1.31403, Convergence: 0.007791\n",
      "Epoch: 111, Loss: 1091.10622, Residuals: -1.31311, Convergence: 0.006943\n",
      "Epoch: 112, Loss: 1084.32459, Residuals: -1.31149, Convergence: 0.006254\n",
      "Epoch: 113, Loss: 1078.17195, Residuals: -1.30925, Convergence: 0.005707\n",
      "Epoch: 114, Loss: 1072.50399, Residuals: -1.30640, Convergence: 0.005285\n",
      "Epoch: 115, Loss: 1067.18730, Residuals: -1.30299, Convergence: 0.004982\n",
      "Epoch: 116, Loss: 1062.09678, Residuals: -1.29901, Convergence: 0.004793\n",
      "Epoch: 117, Loss: 1057.11204, Residuals: -1.29447, Convergence: 0.004715\n",
      "Epoch: 118, Loss: 1052.12421, Residuals: -1.28937, Convergence: 0.004741\n",
      "Epoch: 119, Loss: 1047.05082, Residuals: -1.28373, Convergence: 0.004845\n",
      "Epoch: 120, Loss: 1041.85892, Residuals: -1.27760, Convergence: 0.004983\n",
      "Epoch: 121, Loss: 1036.58182, Residuals: -1.27106, Convergence: 0.005091\n",
      "Epoch: 122, Loss: 1031.30838, Residuals: -1.26417, Convergence: 0.005113\n",
      "Epoch: 123, Loss: 1026.14669, Residuals: -1.25703, Convergence: 0.005030\n",
      "Epoch: 124, Loss: 1021.18341, Residuals: -1.24968, Convergence: 0.004860\n",
      "Epoch: 125, Loss: 1016.46827, Residuals: -1.24221, Convergence: 0.004639\n",
      "Epoch: 126, Loss: 1012.01737, Residuals: -1.23466, Convergence: 0.004398\n",
      "Epoch: 127, Loss: 1007.82701, Residuals: -1.22710, Convergence: 0.004158\n",
      "Epoch: 128, Loss: 1003.88330, Residuals: -1.21958, Convergence: 0.003928\n",
      "Epoch: 129, Loss: 1000.16842, Residuals: -1.21213, Convergence: 0.003714\n",
      "Epoch: 130, Loss: 996.66411, Residuals: -1.20479, Convergence: 0.003516\n",
      "Epoch: 131, Loss: 993.35275, Residuals: -1.19758, Convergence: 0.003334\n",
      "Epoch: 132, Loss: 990.21823, Residuals: -1.19055, Convergence: 0.003165\n",
      "Epoch: 133, Loss: 987.24615, Residuals: -1.18369, Convergence: 0.003010\n",
      "Epoch: 134, Loss: 984.42461, Residuals: -1.17704, Convergence: 0.002866\n",
      "Epoch: 135, Loss: 981.74276, Residuals: -1.17061, Convergence: 0.002732\n",
      "Epoch: 136, Loss: 979.19161, Residuals: -1.16439, Convergence: 0.002605\n",
      "Epoch: 137, Loss: 976.76342, Residuals: -1.15841, Convergence: 0.002486\n",
      "Epoch: 138, Loss: 974.45155, Residuals: -1.15267, Convergence: 0.002372\n",
      "Epoch: 139, Loss: 972.24910, Residuals: -1.14716, Convergence: 0.002265\n",
      "Epoch: 140, Loss: 970.15044, Residuals: -1.14189, Convergence: 0.002163\n",
      "Epoch: 141, Loss: 968.14999, Residuals: -1.13684, Convergence: 0.002066\n",
      "Epoch: 142, Loss: 966.24217, Residuals: -1.13203, Convergence: 0.001974\n",
      "Epoch: 143, Loss: 964.42152, Residuals: -1.12744, Convergence: 0.001888\n",
      "Epoch: 144, Loss: 962.68290, Residuals: -1.12306, Convergence: 0.001806\n",
      "Epoch: 145, Loss: 961.02160, Residuals: -1.11889, Convergence: 0.001729\n",
      "Epoch: 146, Loss: 959.43241, Residuals: -1.11491, Convergence: 0.001656\n",
      "Epoch: 147, Loss: 957.91077, Residuals: -1.11112, Convergence: 0.001588\n",
      "Epoch: 148, Loss: 956.45239, Residuals: -1.10750, Convergence: 0.001525\n",
      "Epoch: 149, Loss: 955.05310, Residuals: -1.10405, Convergence: 0.001465\n",
      "Epoch: 150, Loss: 953.70864, Residuals: -1.10076, Convergence: 0.001410\n",
      "Epoch: 151, Loss: 952.41609, Residuals: -1.09761, Convergence: 0.001357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 152, Loss: 951.17134, Residuals: -1.09461, Convergence: 0.001309\n",
      "Epoch: 153, Loss: 949.97142, Residuals: -1.09173, Convergence: 0.001263\n",
      "Epoch: 154, Loss: 948.81320, Residuals: -1.08898, Convergence: 0.001221\n",
      "Epoch: 155, Loss: 947.69405, Residuals: -1.08635, Convergence: 0.001181\n",
      "Epoch: 156, Loss: 946.61131, Residuals: -1.08382, Convergence: 0.001144\n",
      "Epoch: 157, Loss: 945.56239, Residuals: -1.08139, Convergence: 0.001109\n",
      "Epoch: 158, Loss: 944.54458, Residuals: -1.07905, Convergence: 0.001078\n",
      "Epoch: 159, Loss: 943.55557, Residuals: -1.07680, Convergence: 0.001048\n",
      "Epoch: 160, Loss: 942.59258, Residuals: -1.07462, Convergence: 0.001022\n",
      "Epoch: 161, Loss: 941.65307, Residuals: -1.07252, Convergence: 0.000998\n",
      "Evidence 11212.876\n",
      "\n",
      "Epoch: 161, Evidence: 11212.87598, Convergence: 1.016168\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 5.76e-01\n",
      "Epoch: 161, Loss: 2347.54108, Residuals: -1.07252, Convergence:   inf\n",
      "Epoch: 162, Loss: 2306.58004, Residuals: -1.08472, Convergence: 0.017758\n",
      "Epoch: 163, Loss: 2279.15237, Residuals: -1.08421, Convergence: 0.012034\n",
      "Epoch: 164, Loss: 2256.13660, Residuals: -1.08255, Convergence: 0.010201\n",
      "Epoch: 165, Loss: 2236.46364, Residuals: -1.08034, Convergence: 0.008796\n",
      "Epoch: 166, Loss: 2219.45103, Residuals: -1.07774, Convergence: 0.007665\n",
      "Epoch: 167, Loss: 2204.58944, Residuals: -1.07481, Convergence: 0.006741\n",
      "Epoch: 168, Loss: 2191.48033, Residuals: -1.07161, Convergence: 0.005982\n",
      "Epoch: 169, Loss: 2179.80350, Residuals: -1.06821, Convergence: 0.005357\n",
      "Epoch: 170, Loss: 2169.29557, Residuals: -1.06462, Convergence: 0.004844\n",
      "Epoch: 171, Loss: 2159.74127, Residuals: -1.06087, Convergence: 0.004424\n",
      "Epoch: 172, Loss: 2150.97327, Residuals: -1.05698, Convergence: 0.004076\n",
      "Epoch: 173, Loss: 2142.87690, Residuals: -1.05295, Convergence: 0.003778\n",
      "Epoch: 174, Loss: 2135.38508, Residuals: -1.04883, Convergence: 0.003508\n",
      "Epoch: 175, Loss: 2128.46370, Residuals: -1.04468, Convergence: 0.003252\n",
      "Epoch: 176, Loss: 2122.08939, Residuals: -1.04056, Convergence: 0.003004\n",
      "Epoch: 177, Loss: 2116.23784, Residuals: -1.03652, Convergence: 0.002765\n",
      "Epoch: 178, Loss: 2110.87845, Residuals: -1.03259, Convergence: 0.002539\n",
      "Epoch: 179, Loss: 2105.97588, Residuals: -1.02881, Convergence: 0.002328\n",
      "Epoch: 180, Loss: 2101.49110, Residuals: -1.02519, Convergence: 0.002134\n",
      "Epoch: 181, Loss: 2097.38441, Residuals: -1.02174, Convergence: 0.001958\n",
      "Epoch: 182, Loss: 2093.61645, Residuals: -1.01847, Convergence: 0.001800\n",
      "Epoch: 183, Loss: 2090.15140, Residuals: -1.01536, Convergence: 0.001658\n",
      "Epoch: 184, Loss: 2086.95440, Residuals: -1.01242, Convergence: 0.001532\n",
      "Epoch: 185, Loss: 2083.99491, Residuals: -1.00964, Convergence: 0.001420\n",
      "Epoch: 186, Loss: 2081.24499, Residuals: -1.00702, Convergence: 0.001321\n",
      "Epoch: 187, Loss: 2078.67961, Residuals: -1.00453, Convergence: 0.001234\n",
      "Epoch: 188, Loss: 2076.27648, Residuals: -1.00218, Convergence: 0.001157\n",
      "Epoch: 189, Loss: 2074.01641, Residuals: -0.99996, Convergence: 0.001090\n",
      "Epoch: 190, Loss: 2071.88250, Residuals: -0.99785, Convergence: 0.001030\n",
      "Epoch: 191, Loss: 2069.85989, Residuals: -0.99585, Convergence: 0.000977\n",
      "Evidence 14409.646\n",
      "\n",
      "Epoch: 191, Evidence: 14409.64648, Convergence: 0.221849\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 4.41e-01\n",
      "Epoch: 191, Loss: 2470.30751, Residuals: -0.99585, Convergence:   inf\n",
      "Epoch: 192, Loss: 2455.73396, Residuals: -0.99335, Convergence: 0.005934\n",
      "Epoch: 193, Loss: 2444.01788, Residuals: -0.99010, Convergence: 0.004794\n",
      "Epoch: 194, Loss: 2433.97356, Residuals: -0.98685, Convergence: 0.004127\n",
      "Epoch: 195, Loss: 2425.27898, Residuals: -0.98375, Convergence: 0.003585\n",
      "Epoch: 196, Loss: 2417.70177, Residuals: -0.98083, Convergence: 0.003134\n",
      "Epoch: 197, Loss: 2411.05824, Residuals: -0.97810, Convergence: 0.002755\n",
      "Epoch: 198, Loss: 2405.19656, Residuals: -0.97555, Convergence: 0.002437\n",
      "Epoch: 199, Loss: 2399.99259, Residuals: -0.97318, Convergence: 0.002168\n",
      "Epoch: 200, Loss: 2395.34254, Residuals: -0.97097, Convergence: 0.001941\n",
      "Epoch: 201, Loss: 2391.16118, Residuals: -0.96892, Convergence: 0.001749\n",
      "Epoch: 202, Loss: 2387.37735, Residuals: -0.96701, Convergence: 0.001585\n",
      "Epoch: 203, Loss: 2383.93412, Residuals: -0.96523, Convergence: 0.001444\n",
      "Epoch: 204, Loss: 2380.78319, Residuals: -0.96357, Convergence: 0.001323\n",
      "Epoch: 205, Loss: 2377.88533, Residuals: -0.96203, Convergence: 0.001219\n",
      "Epoch: 206, Loss: 2375.20763, Residuals: -0.96060, Convergence: 0.001127\n",
      "Epoch: 207, Loss: 2372.72441, Residuals: -0.95927, Convergence: 0.001047\n",
      "Epoch: 208, Loss: 2370.41252, Residuals: -0.95804, Convergence: 0.000975\n",
      "Evidence 14821.426\n",
      "\n",
      "Epoch: 208, Evidence: 14821.42578, Convergence: 0.027783\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 3.38e-01\n",
      "Epoch: 208, Loss: 2475.13412, Residuals: -0.95804, Convergence:   inf\n",
      "Epoch: 209, Loss: 2468.64294, Residuals: -0.95526, Convergence: 0.002629\n",
      "Epoch: 210, Loss: 2463.21771, Residuals: -0.95273, Convergence: 0.002202\n",
      "Epoch: 211, Loss: 2458.56366, Residuals: -0.95053, Convergence: 0.001893\n",
      "Epoch: 212, Loss: 2454.52092, Residuals: -0.94861, Convergence: 0.001647\n",
      "Epoch: 213, Loss: 2450.96921, Residuals: -0.94694, Convergence: 0.001449\n",
      "Epoch: 214, Loss: 2447.81627, Residuals: -0.94549, Convergence: 0.001288\n",
      "Epoch: 215, Loss: 2444.98955, Residuals: -0.94424, Convergence: 0.001156\n",
      "Epoch: 216, Loss: 2442.43255, Residuals: -0.94315, Convergence: 0.001047\n",
      "Epoch: 217, Loss: 2440.10064, Residuals: -0.94221, Convergence: 0.000956\n",
      "Evidence 14908.972\n",
      "\n",
      "Epoch: 217, Evidence: 14908.97168, Convergence: 0.005872\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.66e-01\n",
      "Epoch: 217, Loss: 2476.74661, Residuals: -0.94221, Convergence:   inf\n",
      "Epoch: 218, Loss: 2472.87899, Residuals: -0.94018, Convergence: 0.001564\n",
      "Epoch: 219, Loss: 2469.60868, Residuals: -0.93855, Convergence: 0.001324\n",
      "Epoch: 220, Loss: 2466.77439, Residuals: -0.93724, Convergence: 0.001149\n",
      "Epoch: 221, Loss: 2464.27892, Residuals: -0.93619, Convergence: 0.001013\n",
      "Epoch: 222, Loss: 2462.05284, Residuals: -0.93535, Convergence: 0.000904\n",
      "Evidence 14940.121\n",
      "\n",
      "Epoch: 222, Evidence: 14940.12109, Convergence: 0.002085\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.15e-01\n",
      "Epoch: 222, Loss: 2477.76769, Residuals: -0.93535, Convergence:   inf\n",
      "Epoch: 223, Loss: 2474.95226, Residuals: -0.93387, Convergence: 0.001138\n",
      "Epoch: 224, Loss: 2472.55087, Residuals: -0.93278, Convergence: 0.000971\n",
      "Evidence 14952.658\n",
      "\n",
      "Epoch: 224, Evidence: 14952.65820, Convergence: 0.000838\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.80e-01\n",
      "Epoch: 224, Loss: 2478.55626, Residuals: -0.93278, Convergence:   inf\n",
      "Epoch: 225, Loss: 2474.01057, Residuals: -0.93101, Convergence: 0.001837\n",
      "Epoch: 226, Loss: 2470.50815, Residuals: -0.92999, Convergence: 0.001418\n",
      "Epoch: 227, Loss: 2467.65203, Residuals: -0.92941, Convergence: 0.001157\n",
      "Epoch: 228, Loss: 2465.22390, Residuals: -0.92920, Convergence: 0.000985\n",
      "Evidence 14970.681\n",
      "\n",
      "Epoch: 228, Evidence: 14970.68066, Convergence: 0.002041\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.49e-01\n",
      "Epoch: 228, Loss: 2478.91989, Residuals: -0.92920, Convergence:   inf\n",
      "Epoch: 229, Loss: 2475.87814, Residuals: -0.92794, Convergence: 0.001229\n",
      "Epoch: 230, Loss: 2473.43515, Residuals: -0.92756, Convergence: 0.000988\n",
      "Evidence 14981.660\n",
      "\n",
      "Epoch: 230, Evidence: 14981.66016, Convergence: 0.000733\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.27e-01\n",
      "Epoch: 230, Loss: 2479.21180, Residuals: -0.92756, Convergence:   inf\n",
      "Epoch: 231, Loss: 2474.69288, Residuals: -0.92648, Convergence: 0.001826\n",
      "Epoch: 232, Loss: 2471.36432, Residuals: -0.92826, Convergence: 0.001347\n",
      "Epoch: 233, Loss: 2468.69629, Residuals: -0.92917, Convergence: 0.001081\n",
      "Epoch: 234, Loss: 2466.29844, Residuals: -0.93154, Convergence: 0.000972\n",
      "Evidence 14997.812\n",
      "\n",
      "Epoch: 234, Evidence: 14997.81250, Convergence: 0.001809\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.13e-01\n",
      "Epoch: 234, Loss: 2478.75923, Residuals: -0.93154, Convergence:   inf\n",
      "Epoch: 235, Loss: 2477.03325, Residuals: -0.93201, Convergence: 0.000697\n",
      "Evidence 15004.230\n",
      "\n",
      "Epoch: 235, Evidence: 15004.23047, Convergence: 0.000428\n",
      "Updating hyper-parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 183, Updated regularization: 9.24e-02\n",
      "Epoch: 235, Loss: 2479.69825, Residuals: -0.93201, Convergence:   inf\n",
      "Epoch: 236, Loss: 2530.24660, Residuals: -0.97555, Convergence: -0.019978\n",
      "Epoch: 236, Loss: 2477.00333, Residuals: -0.93090, Convergence: 0.001088\n",
      "Epoch: 237, Loss: 2476.28598, Residuals: -0.93308, Convergence: 0.000290\n",
      "Evidence 15009.860\n",
      "\n",
      "Epoch: 237, Evidence: 15009.86035, Convergence: 0.000803\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.63e-02\n",
      "Epoch: 237, Loss: 2479.66675, Residuals: -0.93308, Convergence:   inf\n",
      "Epoch: 238, Loss: 2531.24419, Residuals: -0.98157, Convergence: -0.020376\n",
      "Epoch: 238, Loss: 2477.16766, Residuals: -0.93096, Convergence: 0.001009\n",
      "Epoch: 239, Loss: 2477.14328, Residuals: -0.93219, Convergence: 0.000010\n",
      "Evidence 15014.599\n",
      "\n",
      "Epoch: 239, Evidence: 15014.59863, Convergence: 0.001118\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 6.56e-02\n",
      "Epoch: 239, Loss: 2479.50380, Residuals: -0.93219, Convergence:   inf\n",
      "Epoch: 240, Loss: 2477.29371, Residuals: -0.93106, Convergence: 0.000892\n",
      "Evidence 15018.772\n",
      "\n",
      "Epoch: 240, Evidence: 15018.77246, Convergence: 0.000278\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 6.15e-02\n",
      "Epoch: 240, Loss: 2479.32913, Residuals: -0.93106, Convergence:   inf\n",
      "Epoch: 241, Loss: 2483.23385, Residuals: -0.94344, Convergence: -0.001572\n",
      "Epoch: 241, Loss: 2479.19380, Residuals: -0.93248, Convergence: 0.000055\n",
      "Evidence 15020.107\n",
      "\n",
      "Epoch: 241, Evidence: 15020.10742, Convergence: 0.000367\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 5.32e-02\n",
      "Epoch: 241, Loss: 2479.73736, Residuals: -0.93248, Convergence:   inf\n",
      "Epoch: 242, Loss: 2546.19347, Residuals: -1.00242, Convergence: -0.026100\n",
      "Epoch: 242, Loss: 2477.65130, Residuals: -0.93135, Convergence: 0.000842\n",
      "Evidence 15023.413\n",
      "\n",
      "Epoch: 242, Evidence: 15023.41309, Convergence: 0.000587\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 380.65386, Residuals: -4.51520, Convergence:   inf\n",
      "Epoch: 1, Loss: 355.09072, Residuals: -4.39618, Convergence: 0.071990\n",
      "Epoch: 2, Loss: 334.16554, Residuals: -4.23354, Convergence: 0.062619\n",
      "Epoch: 3, Loss: 318.17606, Residuals: -4.07010, Convergence: 0.050254\n",
      "Epoch: 4, Loss: 305.99281, Residuals: -3.92598, Convergence: 0.039815\n",
      "Epoch: 5, Loss: 296.34160, Residuals: -3.79829, Convergence: 0.032568\n",
      "Epoch: 6, Loss: 288.51772, Residuals: -3.68681, Convergence: 0.027118\n",
      "Epoch: 7, Loss: 282.02902, Residuals: -3.59112, Convergence: 0.023007\n",
      "Epoch: 8, Loss: 276.51136, Residuals: -3.50924, Convergence: 0.019955\n",
      "Epoch: 9, Loss: 271.70836, Residuals: -3.43890, Convergence: 0.017677\n",
      "Epoch: 10, Loss: 267.43849, Residuals: -3.37808, Convergence: 0.015966\n",
      "Epoch: 11, Loss: 263.57121, Residuals: -3.32505, Convergence: 0.014673\n",
      "Epoch: 12, Loss: 260.01236, Residuals: -3.27832, Convergence: 0.013687\n",
      "Epoch: 13, Loss: 256.69466, Residuals: -3.23662, Convergence: 0.012925\n",
      "Epoch: 14, Loss: 253.57142, Residuals: -3.19879, Convergence: 0.012317\n",
      "Epoch: 15, Loss: 250.61285, Residuals: -3.16389, Convergence: 0.011805\n",
      "Epoch: 16, Loss: 247.80322, Residuals: -3.13123, Convergence: 0.011338\n",
      "Epoch: 17, Loss: 245.13185, Residuals: -3.10038, Convergence: 0.010898\n",
      "Epoch: 18, Loss: 242.58008, Residuals: -3.07094, Convergence: 0.010519\n",
      "Epoch: 19, Loss: 240.11794, Residuals: -3.04241, Convergence: 0.010254\n",
      "Epoch: 20, Loss: 237.71061, Residuals: -3.01421, Convergence: 0.010127\n",
      "Epoch: 21, Loss: 235.32656, Residuals: -2.98585, Convergence: 0.010131\n",
      "Epoch: 22, Loss: 232.94229, Residuals: -2.95698, Convergence: 0.010235\n",
      "Epoch: 23, Loss: 230.53605, Residuals: -2.92735, Convergence: 0.010438\n",
      "Epoch: 24, Loss: 228.07181, Residuals: -2.89661, Convergence: 0.010805\n",
      "Epoch: 25, Loss: 225.49522, Residuals: -2.86413, Convergence: 0.011426\n",
      "Epoch: 26, Loss: 222.77932, Residuals: -2.82957, Convergence: 0.012191\n",
      "Epoch: 27, Loss: 220.00907, Residuals: -2.79379, Convergence: 0.012592\n",
      "Epoch: 28, Loss: 217.30688, Residuals: -2.75820, Convergence: 0.012435\n",
      "Epoch: 29, Loss: 214.70715, Residuals: -2.72336, Convergence: 0.012108\n",
      "Epoch: 30, Loss: 212.19460, Residuals: -2.68921, Convergence: 0.011841\n",
      "Epoch: 31, Loss: 209.74752, Residuals: -2.65554, Convergence: 0.011667\n",
      "Epoch: 32, Loss: 207.34897, Residuals: -2.62218, Convergence: 0.011568\n",
      "Epoch: 33, Loss: 204.98798, Residuals: -2.58897, Convergence: 0.011518\n",
      "Epoch: 34, Loss: 202.65868, Residuals: -2.55580, Convergence: 0.011494\n",
      "Epoch: 35, Loss: 200.35912, Residuals: -2.52262, Convergence: 0.011477\n",
      "Epoch: 36, Loss: 198.09000, Residuals: -2.48939, Convergence: 0.011455\n",
      "Epoch: 37, Loss: 195.85369, Residuals: -2.45609, Convergence: 0.011418\n",
      "Epoch: 38, Loss: 193.65331, Residuals: -2.42274, Convergence: 0.011362\n",
      "Epoch: 39, Loss: 191.49230, Residuals: -2.38937, Convergence: 0.011285\n",
      "Epoch: 40, Loss: 189.37404, Residuals: -2.35599, Convergence: 0.011186\n",
      "Epoch: 41, Loss: 187.30173, Residuals: -2.32266, Convergence: 0.011064\n",
      "Epoch: 42, Loss: 185.27846, Residuals: -2.28940, Convergence: 0.010920\n",
      "Epoch: 43, Loss: 183.30724, Residuals: -2.25627, Convergence: 0.010754\n",
      "Epoch: 44, Loss: 181.39101, Residuals: -2.22331, Convergence: 0.010564\n",
      "Epoch: 45, Loss: 179.53257, Residuals: -2.19058, Convergence: 0.010352\n",
      "Epoch: 46, Loss: 177.73458, Residuals: -2.15815, Convergence: 0.010116\n",
      "Epoch: 47, Loss: 175.99926, Residuals: -2.12607, Convergence: 0.009860\n",
      "Epoch: 48, Loss: 174.32838, Residuals: -2.09442, Convergence: 0.009585\n",
      "Epoch: 49, Loss: 172.72311, Residuals: -2.06324, Convergence: 0.009294\n",
      "Epoch: 50, Loss: 171.18394, Residuals: -2.03261, Convergence: 0.008991\n",
      "Epoch: 51, Loss: 169.71068, Residuals: -2.00256, Convergence: 0.008681\n",
      "Epoch: 52, Loss: 168.30247, Residuals: -1.97315, Convergence: 0.008367\n",
      "Epoch: 53, Loss: 166.95779, Residuals: -1.94440, Convergence: 0.008054\n",
      "Epoch: 54, Loss: 165.67455, Residuals: -1.91634, Convergence: 0.007746\n",
      "Epoch: 55, Loss: 164.45021, Residuals: -1.88898, Convergence: 0.007445\n",
      "Epoch: 56, Loss: 163.28191, Residuals: -1.86232, Convergence: 0.007155\n",
      "Epoch: 57, Loss: 162.16665, Residuals: -1.83635, Convergence: 0.006877\n",
      "Epoch: 58, Loss: 161.10143, Residuals: -1.81104, Convergence: 0.006612\n",
      "Epoch: 59, Loss: 160.08330, Residuals: -1.78640, Convergence: 0.006360\n",
      "Epoch: 60, Loss: 159.10953, Residuals: -1.76239, Convergence: 0.006120\n",
      "Epoch: 61, Loss: 158.17753, Residuals: -1.73900, Convergence: 0.005892\n",
      "Epoch: 62, Loss: 157.28495, Residuals: -1.71622, Convergence: 0.005675\n",
      "Epoch: 63, Loss: 156.42963, Residuals: -1.69404, Convergence: 0.005468\n",
      "Epoch: 64, Loss: 155.60967, Residuals: -1.67244, Convergence: 0.005269\n",
      "Epoch: 65, Loss: 154.82338, Residuals: -1.65143, Convergence: 0.005079\n",
      "Epoch: 66, Loss: 154.06929, Residuals: -1.63099, Convergence: 0.004894\n",
      "Epoch: 67, Loss: 153.34613, Residuals: -1.61113, Convergence: 0.004716\n",
      "Epoch: 68, Loss: 152.65279, Residuals: -1.59185, Convergence: 0.004542\n",
      "Epoch: 69, Loss: 151.98828, Residuals: -1.57313, Convergence: 0.004372\n",
      "Epoch: 70, Loss: 151.35168, Residuals: -1.55500, Convergence: 0.004206\n",
      "Epoch: 71, Loss: 150.74215, Residuals: -1.53744, Convergence: 0.004044\n",
      "Epoch: 72, Loss: 150.15881, Residuals: -1.52045, Convergence: 0.003885\n",
      "Epoch: 73, Loss: 149.60081, Residuals: -1.50404, Convergence: 0.003730\n",
      "Epoch: 74, Loss: 149.06730, Residuals: -1.48819, Convergence: 0.003579\n",
      "Epoch: 75, Loss: 148.55736, Residuals: -1.47291, Convergence: 0.003433\n",
      "Epoch: 76, Loss: 148.07009, Residuals: -1.45817, Convergence: 0.003291\n",
      "Epoch: 77, Loss: 147.60458, Residuals: -1.44399, Convergence: 0.003154\n",
      "Epoch: 78, Loss: 147.15989, Residuals: -1.43033, Convergence: 0.003022\n",
      "Epoch: 79, Loss: 146.73510, Residuals: -1.41720, Convergence: 0.002895\n",
      "Epoch: 80, Loss: 146.32928, Residuals: -1.40458, Convergence: 0.002773\n",
      "Epoch: 81, Loss: 145.94154, Residuals: -1.39245, Convergence: 0.002657\n",
      "Epoch: 82, Loss: 145.57100, Residuals: -1.38080, Convergence: 0.002545\n",
      "Epoch: 83, Loss: 145.21681, Residuals: -1.36961, Convergence: 0.002439\n",
      "Epoch: 84, Loss: 144.87814, Residuals: -1.35887, Convergence: 0.002338\n",
      "Epoch: 85, Loss: 144.55421, Residuals: -1.34857, Convergence: 0.002241\n",
      "Epoch: 86, Loss: 144.24427, Residuals: -1.33869, Convergence: 0.002149\n",
      "Epoch: 87, Loss: 143.94759, Residuals: -1.32920, Convergence: 0.002061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 88, Loss: 143.66350, Residuals: -1.32011, Convergence: 0.001977\n",
      "Epoch: 89, Loss: 143.39136, Residuals: -1.31138, Convergence: 0.001898\n",
      "Epoch: 90, Loss: 143.13054, Residuals: -1.30302, Convergence: 0.001822\n",
      "Epoch: 91, Loss: 142.88047, Residuals: -1.29499, Convergence: 0.001750\n",
      "Epoch: 92, Loss: 142.64060, Residuals: -1.28730, Convergence: 0.001682\n",
      "Epoch: 93, Loss: 142.41041, Residuals: -1.27992, Convergence: 0.001616\n",
      "Epoch: 94, Loss: 142.18940, Residuals: -1.27284, Convergence: 0.001554\n",
      "Epoch: 95, Loss: 141.97713, Residuals: -1.26606, Convergence: 0.001495\n",
      "Epoch: 96, Loss: 141.77313, Residuals: -1.25954, Convergence: 0.001439\n",
      "Epoch: 97, Loss: 141.57702, Residuals: -1.25330, Convergence: 0.001385\n",
      "Epoch: 98, Loss: 141.38838, Residuals: -1.24730, Convergence: 0.001334\n",
      "Epoch: 99, Loss: 141.20684, Residuals: -1.24155, Convergence: 0.001286\n",
      "Epoch: 100, Loss: 141.03205, Residuals: -1.23602, Convergence: 0.001239\n",
      "Epoch: 101, Loss: 140.86369, Residuals: -1.23072, Convergence: 0.001195\n",
      "Epoch: 102, Loss: 140.70143, Residuals: -1.22563, Convergence: 0.001153\n",
      "Epoch: 103, Loss: 140.54497, Residuals: -1.22073, Convergence: 0.001113\n",
      "Epoch: 104, Loss: 140.39403, Residuals: -1.21603, Convergence: 0.001075\n",
      "Epoch: 105, Loss: 140.24834, Residuals: -1.21151, Convergence: 0.001039\n",
      "Epoch: 106, Loss: 140.10765, Residuals: -1.20716, Convergence: 0.001004\n",
      "Epoch: 107, Loss: 139.97172, Residuals: -1.20297, Convergence: 0.000971\n",
      "Evidence -180.211\n",
      "\n",
      "Epoch: 107, Evidence: -180.21063, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.23e-01\n",
      "Epoch: 107, Loss: 1372.91755, Residuals: -1.20297, Convergence:   inf\n",
      "Epoch: 108, Loss: 1317.01149, Residuals: -1.22665, Convergence: 0.042449\n",
      "Epoch: 109, Loss: 1273.58951, Residuals: -1.24648, Convergence: 0.034094\n",
      "Epoch: 110, Loss: 1240.12201, Residuals: -1.26213, Convergence: 0.026987\n",
      "Epoch: 111, Loss: 1213.71486, Residuals: -1.27430, Convergence: 0.021757\n",
      "Epoch: 112, Loss: 1192.20063, Residuals: -1.28401, Convergence: 0.018046\n",
      "Epoch: 113, Loss: 1174.29190, Residuals: -1.29182, Convergence: 0.015251\n",
      "Epoch: 114, Loss: 1159.18121, Residuals: -1.29802, Convergence: 0.013036\n",
      "Epoch: 115, Loss: 1146.29918, Residuals: -1.30278, Convergence: 0.011238\n",
      "Epoch: 116, Loss: 1135.21282, Residuals: -1.30626, Convergence: 0.009766\n",
      "Epoch: 117, Loss: 1125.57969, Residuals: -1.30859, Convergence: 0.008558\n",
      "Epoch: 118, Loss: 1117.12388, Residuals: -1.30990, Convergence: 0.007569\n",
      "Epoch: 119, Loss: 1109.61835, Residuals: -1.31031, Convergence: 0.006764\n",
      "Epoch: 120, Loss: 1102.87483, Residuals: -1.30989, Convergence: 0.006114\n",
      "Epoch: 121, Loss: 1096.73479, Residuals: -1.30874, Convergence: 0.005598\n",
      "Epoch: 122, Loss: 1091.06448, Residuals: -1.30691, Convergence: 0.005197\n",
      "Epoch: 123, Loss: 1085.75142, Residuals: -1.30447, Convergence: 0.004893\n",
      "Epoch: 124, Loss: 1080.70391, Residuals: -1.30144, Convergence: 0.004671\n",
      "Epoch: 125, Loss: 1075.84750, Residuals: -1.29786, Convergence: 0.004514\n",
      "Epoch: 126, Loss: 1071.12441, Residuals: -1.29376, Convergence: 0.004409\n",
      "Epoch: 127, Loss: 1066.48737, Residuals: -1.28916, Convergence: 0.004348\n",
      "Epoch: 128, Loss: 1061.89453, Residuals: -1.28407, Convergence: 0.004325\n",
      "Epoch: 129, Loss: 1057.30365, Residuals: -1.27850, Convergence: 0.004342\n",
      "Epoch: 130, Loss: 1052.67043, Residuals: -1.27245, Convergence: 0.004401\n",
      "Epoch: 131, Loss: 1047.95703, Residuals: -1.26594, Convergence: 0.004498\n",
      "Epoch: 132, Loss: 1043.14578, Residuals: -1.25900, Convergence: 0.004612\n",
      "Epoch: 133, Loss: 1038.25916, Residuals: -1.25172, Convergence: 0.004707\n",
      "Epoch: 134, Loss: 1033.36332, Residuals: -1.24419, Convergence: 0.004738\n",
      "Epoch: 135, Loss: 1028.55135, Residuals: -1.23651, Convergence: 0.004678\n",
      "Epoch: 136, Loss: 1023.91192, Residuals: -1.22874, Convergence: 0.004531\n",
      "Epoch: 137, Loss: 1019.50696, Residuals: -1.22097, Convergence: 0.004321\n",
      "Epoch: 138, Loss: 1015.36431, Residuals: -1.21324, Convergence: 0.004080\n",
      "Epoch: 139, Loss: 1011.48669, Residuals: -1.20561, Convergence: 0.003834\n",
      "Epoch: 140, Loss: 1007.86221, Residuals: -1.19813, Convergence: 0.003596\n",
      "Epoch: 141, Loss: 1004.47264, Residuals: -1.19083, Convergence: 0.003374\n",
      "Epoch: 142, Loss: 1001.29821, Residuals: -1.18373, Convergence: 0.003170\n",
      "Epoch: 143, Loss: 998.32028, Residuals: -1.17687, Convergence: 0.002983\n",
      "Epoch: 144, Loss: 995.52185, Residuals: -1.17027, Convergence: 0.002811\n",
      "Epoch: 145, Loss: 992.88756, Residuals: -1.16392, Convergence: 0.002653\n",
      "Epoch: 146, Loss: 990.40418, Residuals: -1.15785, Convergence: 0.002507\n",
      "Epoch: 147, Loss: 988.05917, Residuals: -1.15206, Convergence: 0.002373\n",
      "Epoch: 148, Loss: 985.84172, Residuals: -1.14654, Convergence: 0.002249\n",
      "Epoch: 149, Loss: 983.74154, Residuals: -1.14130, Convergence: 0.002135\n",
      "Epoch: 150, Loss: 981.74934, Residuals: -1.13632, Convergence: 0.002029\n",
      "Epoch: 151, Loss: 979.85634, Residuals: -1.13159, Convergence: 0.001932\n",
      "Epoch: 152, Loss: 978.05407, Residuals: -1.12712, Convergence: 0.001843\n",
      "Epoch: 153, Loss: 976.33471, Residuals: -1.12288, Convergence: 0.001761\n",
      "Epoch: 154, Loss: 974.69063, Residuals: -1.11886, Convergence: 0.001687\n",
      "Epoch: 155, Loss: 973.11437, Residuals: -1.11505, Convergence: 0.001620\n",
      "Epoch: 156, Loss: 971.59877, Residuals: -1.11144, Convergence: 0.001560\n",
      "Epoch: 157, Loss: 970.13687, Residuals: -1.10800, Convergence: 0.001507\n",
      "Epoch: 158, Loss: 968.72205, Residuals: -1.10474, Convergence: 0.001460\n",
      "Epoch: 159, Loss: 967.34748, Residuals: -1.10162, Convergence: 0.001421\n",
      "Epoch: 160, Loss: 966.00693, Residuals: -1.09865, Convergence: 0.001388\n",
      "Epoch: 161, Loss: 964.69409, Residuals: -1.09579, Convergence: 0.001361\n",
      "Epoch: 162, Loss: 963.40287, Residuals: -1.09304, Convergence: 0.001340\n",
      "Epoch: 163, Loss: 962.12858, Residuals: -1.09039, Convergence: 0.001324\n",
      "Epoch: 164, Loss: 960.86570, Residuals: -1.08782, Convergence: 0.001314\n",
      "Epoch: 165, Loss: 959.61049, Residuals: -1.08531, Convergence: 0.001308\n",
      "Epoch: 166, Loss: 958.35949, Residuals: -1.08286, Convergence: 0.001305\n",
      "Epoch: 167, Loss: 957.11091, Residuals: -1.08046, Convergence: 0.001305\n",
      "Epoch: 168, Loss: 955.86351, Residuals: -1.07809, Convergence: 0.001305\n",
      "Epoch: 169, Loss: 954.61780, Residuals: -1.07575, Convergence: 0.001305\n",
      "Epoch: 170, Loss: 953.37560, Residuals: -1.07343, Convergence: 0.001303\n",
      "Epoch: 171, Loss: 952.14058, Residuals: -1.07114, Convergence: 0.001297\n",
      "Epoch: 172, Loss: 950.91784, Residuals: -1.06888, Convergence: 0.001286\n",
      "Epoch: 173, Loss: 949.71393, Residuals: -1.06666, Convergence: 0.001268\n",
      "Epoch: 174, Loss: 948.53607, Residuals: -1.06448, Convergence: 0.001242\n",
      "Epoch: 175, Loss: 947.39209, Residuals: -1.06235, Convergence: 0.001208\n",
      "Epoch: 176, Loss: 946.28801, Residuals: -1.06029, Convergence: 0.001167\n",
      "Epoch: 177, Loss: 945.22983, Residuals: -1.05830, Convergence: 0.001119\n",
      "Epoch: 178, Loss: 944.22076, Residuals: -1.05639, Convergence: 0.001069\n",
      "Epoch: 179, Loss: 943.26254, Residuals: -1.05457, Convergence: 0.001016\n",
      "Epoch: 180, Loss: 942.35542, Residuals: -1.05283, Convergence: 0.000963\n",
      "Evidence 11338.828\n",
      "\n",
      "Epoch: 180, Evidence: 11338.82812, Convergence: 1.015893\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.69e-01\n",
      "Epoch: 180, Loss: 2357.13799, Residuals: -1.05283, Convergence:   inf\n",
      "Epoch: 181, Loss: 2316.40525, Residuals: -1.05754, Convergence: 0.017584\n",
      "Epoch: 182, Loss: 2290.47587, Residuals: -1.05541, Convergence: 0.011321\n",
      "Epoch: 183, Loss: 2268.86374, Residuals: -1.05313, Convergence: 0.009526\n",
      "Epoch: 184, Loss: 2250.68898, Residuals: -1.05088, Convergence: 0.008075\n",
      "Epoch: 185, Loss: 2235.30642, Residuals: -1.04864, Convergence: 0.006882\n",
      "Epoch: 186, Loss: 2222.17767, Residuals: -1.04641, Convergence: 0.005908\n",
      "Epoch: 187, Loss: 2210.85129, Residuals: -1.04417, Convergence: 0.005123\n",
      "Epoch: 188, Loss: 2200.95130, Residuals: -1.04190, Convergence: 0.004498\n",
      "Epoch: 189, Loss: 2192.17752, Residuals: -1.03956, Convergence: 0.004002\n",
      "Epoch: 190, Loss: 2184.30262, Residuals: -1.03714, Convergence: 0.003605\n",
      "Epoch: 191, Loss: 2177.17277, Residuals: -1.03464, Convergence: 0.003275\n",
      "Epoch: 192, Loss: 2170.68908, Residuals: -1.03209, Convergence: 0.002987\n",
      "Epoch: 193, Loss: 2164.78964, Residuals: -1.02953, Convergence: 0.002725\n",
      "Epoch: 194, Loss: 2159.42440, Residuals: -1.02700, Convergence: 0.002485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 195, Loss: 2154.54815, Residuals: -1.02454, Convergence: 0.002263\n",
      "Epoch: 196, Loss: 2150.11515, Residuals: -1.02217, Convergence: 0.002062\n",
      "Epoch: 197, Loss: 2146.07834, Residuals: -1.01992, Convergence: 0.001881\n",
      "Epoch: 198, Loss: 2142.39358, Residuals: -1.01779, Convergence: 0.001720\n",
      "Epoch: 199, Loss: 2139.02021, Residuals: -1.01578, Convergence: 0.001577\n",
      "Epoch: 200, Loss: 2135.91896, Residuals: -1.01389, Convergence: 0.001452\n",
      "Epoch: 201, Loss: 2133.05669, Residuals: -1.01211, Convergence: 0.001342\n",
      "Epoch: 202, Loss: 2130.40163, Residuals: -1.01043, Convergence: 0.001246\n",
      "Epoch: 203, Loss: 2127.92832, Residuals: -1.00885, Convergence: 0.001162\n",
      "Epoch: 204, Loss: 2125.61244, Residuals: -1.00735, Convergence: 0.001090\n",
      "Epoch: 205, Loss: 2123.43345, Residuals: -1.00592, Convergence: 0.001026\n",
      "Epoch: 206, Loss: 2121.37380, Residuals: -1.00456, Convergence: 0.000971\n",
      "Evidence 14397.839\n",
      "\n",
      "Epoch: 206, Evidence: 14397.83887, Convergence: 0.212463\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.31e-01\n",
      "Epoch: 206, Loss: 2476.79805, Residuals: -1.00456, Convergence:   inf\n",
      "Epoch: 207, Loss: 2464.26150, Residuals: -1.00190, Convergence: 0.005087\n",
      "Epoch: 208, Loss: 2453.78956, Residuals: -0.99896, Convergence: 0.004268\n",
      "Epoch: 209, Loss: 2444.45179, Residuals: -0.99607, Convergence: 0.003820\n",
      "Epoch: 210, Loss: 2436.06055, Residuals: -0.99327, Convergence: 0.003445\n",
      "Epoch: 211, Loss: 2428.48782, Residuals: -0.99060, Convergence: 0.003118\n",
      "Epoch: 212, Loss: 2421.64284, Residuals: -0.98809, Convergence: 0.002827\n",
      "Epoch: 213, Loss: 2415.45651, Residuals: -0.98575, Convergence: 0.002561\n",
      "Epoch: 214, Loss: 2409.86740, Residuals: -0.98359, Convergence: 0.002319\n",
      "Epoch: 215, Loss: 2404.81974, Residuals: -0.98159, Convergence: 0.002099\n",
      "Epoch: 216, Loss: 2400.26134, Residuals: -0.97975, Convergence: 0.001899\n",
      "Epoch: 217, Loss: 2396.14131, Residuals: -0.97805, Convergence: 0.001719\n",
      "Epoch: 218, Loss: 2392.41179, Residuals: -0.97649, Convergence: 0.001559\n",
      "Epoch: 219, Loss: 2389.02821, Residuals: -0.97505, Convergence: 0.001416\n",
      "Epoch: 220, Loss: 2385.95206, Residuals: -0.97372, Convergence: 0.001289\n",
      "Epoch: 221, Loss: 2383.14530, Residuals: -0.97249, Convergence: 0.001178\n",
      "Epoch: 222, Loss: 2380.57601, Residuals: -0.97134, Convergence: 0.001079\n",
      "Epoch: 223, Loss: 2378.21622, Residuals: -0.97027, Convergence: 0.000992\n",
      "Evidence 14758.580\n",
      "\n",
      "Epoch: 223, Evidence: 14758.58008, Convergence: 0.024443\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.29e-01\n",
      "Epoch: 223, Loss: 2480.62481, Residuals: -0.97027, Convergence:   inf\n",
      "Epoch: 224, Loss: 2473.27609, Residuals: -0.96701, Convergence: 0.002971\n",
      "Epoch: 225, Loss: 2467.16886, Residuals: -0.96424, Convergence: 0.002475\n",
      "Epoch: 226, Loss: 2461.99222, Residuals: -0.96190, Convergence: 0.002103\n",
      "Epoch: 227, Loss: 2457.55186, Residuals: -0.95992, Convergence: 0.001807\n",
      "Epoch: 228, Loss: 2453.70047, Residuals: -0.95825, Convergence: 0.001570\n",
      "Epoch: 229, Loss: 2450.32410, Residuals: -0.95684, Convergence: 0.001378\n",
      "Epoch: 230, Loss: 2447.33420, Residuals: -0.95562, Convergence: 0.001222\n",
      "Epoch: 231, Loss: 2444.66064, Residuals: -0.95457, Convergence: 0.001094\n",
      "Epoch: 232, Loss: 2442.24831, Residuals: -0.95365, Convergence: 0.000988\n",
      "Evidence 14851.219\n",
      "\n",
      "Epoch: 232, Evidence: 14851.21875, Convergence: 0.006238\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.57e-01\n",
      "Epoch: 232, Loss: 2481.64128, Residuals: -0.95365, Convergence:   inf\n",
      "Epoch: 233, Loss: 2477.14927, Residuals: -0.95136, Convergence: 0.001813\n",
      "Epoch: 234, Loss: 2473.45386, Residuals: -0.94955, Convergence: 0.001494\n",
      "Epoch: 235, Loss: 2470.33533, Residuals: -0.94810, Convergence: 0.001262\n",
      "Epoch: 236, Loss: 2467.65571, Residuals: -0.94692, Convergence: 0.001086\n",
      "Epoch: 237, Loss: 2465.31412, Residuals: -0.94595, Convergence: 0.000950\n",
      "Evidence 14886.201\n",
      "\n",
      "Epoch: 237, Evidence: 14886.20117, Convergence: 0.002350\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.06e-01\n",
      "Epoch: 237, Loss: 2482.22902, Residuals: -0.94595, Convergence:   inf\n",
      "Epoch: 238, Loss: 2479.11807, Residuals: -0.94446, Convergence: 0.001255\n",
      "Epoch: 239, Loss: 2476.56587, Residuals: -0.94327, Convergence: 0.001031\n",
      "Epoch: 240, Loss: 2474.39499, Residuals: -0.94227, Convergence: 0.000877\n",
      "Evidence 14902.445\n",
      "\n",
      "Epoch: 240, Evidence: 14902.44531, Convergence: 0.001090\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.69e-01\n",
      "Epoch: 240, Loss: 2482.68259, Residuals: -0.94227, Convergence:   inf\n",
      "Epoch: 241, Loss: 2480.30037, Residuals: -0.94098, Convergence: 0.000960\n",
      "Evidence 14909.252\n",
      "\n",
      "Epoch: 241, Evidence: 14909.25195, Convergence: 0.000457\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.43e-01\n",
      "Epoch: 241, Loss: 2483.11518, Residuals: -0.94098, Convergence:   inf\n",
      "Epoch: 242, Loss: 2479.21743, Residuals: -0.93937, Convergence: 0.001572\n",
      "Epoch: 243, Loss: 2476.13680, Residuals: -0.93758, Convergence: 0.001244\n",
      "Epoch: 244, Loss: 2473.62507, Residuals: -0.93629, Convergence: 0.001015\n",
      "Epoch: 245, Loss: 2471.48760, Residuals: -0.93550, Convergence: 0.000865\n",
      "Evidence 14923.958\n",
      "\n",
      "Epoch: 245, Evidence: 14923.95801, Convergence: 0.001441\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.20e-01\n",
      "Epoch: 245, Loss: 2483.03178, Residuals: -0.93550, Convergence:   inf\n",
      "Epoch: 246, Loss: 2480.42996, Residuals: -0.93338, Convergence: 0.001049\n",
      "Epoch: 247, Loss: 2478.33374, Residuals: -0.93204, Convergence: 0.000846\n",
      "Evidence 14933.256\n",
      "\n",
      "Epoch: 247, Evidence: 14933.25586, Convergence: 0.000623\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.03e-01\n",
      "Epoch: 247, Loss: 2483.06546, Residuals: -0.93204, Convergence:   inf\n",
      "Epoch: 248, Loss: 2479.24125, Residuals: -0.92850, Convergence: 0.001542\n",
      "Epoch: 249, Loss: 2476.44747, Residuals: -0.92880, Convergence: 0.001128\n",
      "Epoch: 250, Loss: 2474.05904, Residuals: -0.92927, Convergence: 0.000965\n",
      "Evidence 14945.090\n",
      "\n",
      "Epoch: 250, Evidence: 14945.08984, Convergence: 0.001414\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.54e-02\n",
      "Epoch: 250, Loss: 2482.98095, Residuals: -0.92927, Convergence:   inf\n",
      "Epoch: 251, Loss: 2480.62708, Residuals: -0.92677, Convergence: 0.000949\n",
      "Evidence 14950.934\n",
      "\n",
      "Epoch: 251, Evidence: 14950.93359, Convergence: 0.000391\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.85e-02\n",
      "Epoch: 251, Loss: 2482.50013, Residuals: -0.92677, Convergence:   inf\n",
      "Epoch: 252, Loss: 2483.76955, Residuals: -0.92734, Convergence: -0.000511\n",
      "Evidence 14951.441\n",
      "\n",
      "Epoch: 252, Evidence: 14951.44141, Convergence: 0.000425\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.87e-02\n",
      "Epoch: 252, Loss: 2483.72712, Residuals: -0.92734, Convergence:   inf\n",
      "Epoch: 253, Loss: 22266.31881, Residuals: -0.73866, Convergence: -0.888454\n",
      "Epoch: 253, Loss: 2549.58220, Residuals: -0.96714, Convergence: -0.025830\n",
      "Epoch: 253, Loss: 2480.92028, Residuals: -0.92222, Convergence: 0.001131\n",
      "Epoch: 254, Loss: 2479.62493, Residuals: -0.92270, Convergence: 0.000522\n",
      "Evidence 14959.820\n",
      "\n",
      "Epoch: 254, Evidence: 14959.82031, Convergence: 0.000985\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 383.29285, Residuals: -4.52835, Convergence:   inf\n",
      "Epoch: 1, Loss: 357.46559, Residuals: -4.40847, Convergence: 0.072251\n",
      "Epoch: 2, Loss: 336.30017, Residuals: -4.24430, Convergence: 0.062936\n",
      "Epoch: 3, Loss: 320.15345, Residuals: -4.07961, Convergence: 0.050434\n",
      "Epoch: 4, Loss: 307.85504, Residuals: -3.93464, Convergence: 0.039949\n",
      "Epoch: 5, Loss: 298.10733, Residuals: -3.80655, Convergence: 0.032699\n",
      "Epoch: 6, Loss: 290.19516, Residuals: -3.69495, Convergence: 0.027265\n",
      "Epoch: 7, Loss: 283.62873, Residuals: -3.59936, Convergence: 0.023151\n",
      "Epoch: 8, Loss: 278.04506, Residuals: -3.51774, Convergence: 0.020082\n",
      "Epoch: 9, Loss: 273.18724, Residuals: -3.44778, Convergence: 0.017782\n",
      "Epoch: 10, Loss: 268.87261, Residuals: -3.38737, Convergence: 0.016047\n",
      "Epoch: 11, Loss: 264.96887, Residuals: -3.33472, Convergence: 0.014733\n",
      "Epoch: 12, Loss: 261.37959, Residuals: -3.28833, Convergence: 0.013732\n",
      "Epoch: 13, Loss: 258.03485, Residuals: -3.24687, Convergence: 0.012962\n",
      "Epoch: 14, Loss: 254.88470, Residuals: -3.20920, Convergence: 0.012359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Loss: 251.89538, Residuals: -3.17434, Convergence: 0.011867\n",
      "Epoch: 16, Loss: 249.04704, Residuals: -3.14160, Convergence: 0.011437\n",
      "Epoch: 17, Loss: 246.32680, Residuals: -3.11055, Convergence: 0.011043\n",
      "Epoch: 18, Loss: 243.71609, Residuals: -3.08082, Convergence: 0.010712\n",
      "Epoch: 19, Loss: 241.18533, Residuals: -3.05192, Convergence: 0.010493\n",
      "Epoch: 20, Loss: 238.69949, Residuals: -3.02328, Convergence: 0.010414\n",
      "Epoch: 21, Loss: 236.22694, Residuals: -2.99437, Convergence: 0.010467\n",
      "Epoch: 22, Loss: 233.74541, Residuals: -2.96481, Convergence: 0.010616\n",
      "Epoch: 23, Loss: 231.23629, Residuals: -2.93434, Convergence: 0.010851\n",
      "Epoch: 24, Loss: 228.66731, Residuals: -2.90258, Convergence: 0.011235\n",
      "Epoch: 25, Loss: 225.98595, Residuals: -2.86893, Convergence: 0.011865\n",
      "Epoch: 26, Loss: 223.16040, Residuals: -2.83297, Convergence: 0.012662\n",
      "Epoch: 27, Loss: 220.26958, Residuals: -2.79547, Convergence: 0.013124\n",
      "Epoch: 28, Loss: 217.44715, Residuals: -2.75797, Convergence: 0.012980\n",
      "Epoch: 29, Loss: 214.74042, Residuals: -2.72117, Convergence: 0.012605\n",
      "Epoch: 30, Loss: 212.13990, Residuals: -2.68514, Convergence: 0.012259\n",
      "Epoch: 31, Loss: 209.62652, Residuals: -2.64976, Convergence: 0.011990\n",
      "Epoch: 32, Loss: 207.18420, Residuals: -2.61494, Convergence: 0.011788\n",
      "Epoch: 33, Loss: 204.80128, Residuals: -2.58056, Convergence: 0.011635\n",
      "Epoch: 34, Loss: 202.47001, Residuals: -2.54652, Convergence: 0.011514\n",
      "Epoch: 35, Loss: 200.18570, Residuals: -2.51276, Convergence: 0.011411\n",
      "Epoch: 36, Loss: 197.94596, Residuals: -2.47921, Convergence: 0.011315\n",
      "Epoch: 37, Loss: 195.75014, Residuals: -2.44583, Convergence: 0.011217\n",
      "Epoch: 38, Loss: 193.59862, Residuals: -2.41258, Convergence: 0.011113\n",
      "Epoch: 39, Loss: 191.49248, Residuals: -2.37947, Convergence: 0.010999\n",
      "Epoch: 40, Loss: 189.43306, Residuals: -2.34647, Convergence: 0.010871\n",
      "Epoch: 41, Loss: 187.42174, Residuals: -2.31361, Convergence: 0.010732\n",
      "Epoch: 42, Loss: 185.45974, Residuals: -2.28090, Convergence: 0.010579\n",
      "Epoch: 43, Loss: 183.54810, Residuals: -2.24835, Convergence: 0.010415\n",
      "Epoch: 44, Loss: 181.68775, Residuals: -2.21597, Convergence: 0.010239\n",
      "Epoch: 45, Loss: 179.87966, Residuals: -2.18381, Convergence: 0.010052\n",
      "Epoch: 46, Loss: 178.12499, Residuals: -2.15187, Convergence: 0.009851\n",
      "Epoch: 47, Loss: 176.42517, Residuals: -2.12020, Convergence: 0.009635\n",
      "Epoch: 48, Loss: 174.78179, Residuals: -2.08883, Convergence: 0.009402\n",
      "Epoch: 49, Loss: 173.19639, Residuals: -2.05782, Convergence: 0.009154\n",
      "Epoch: 50, Loss: 171.67019, Residuals: -2.02720, Convergence: 0.008890\n",
      "Epoch: 51, Loss: 170.20388, Residuals: -1.99703, Convergence: 0.008615\n",
      "Epoch: 52, Loss: 168.79752, Residuals: -1.96735, Convergence: 0.008332\n",
      "Epoch: 53, Loss: 167.45060, Residuals: -1.93821, Convergence: 0.008044\n",
      "Epoch: 54, Loss: 166.16203, Residuals: -1.90964, Convergence: 0.007755\n",
      "Epoch: 55, Loss: 164.93032, Residuals: -1.88168, Convergence: 0.007468\n",
      "Epoch: 56, Loss: 163.75357, Residuals: -1.85436, Convergence: 0.007186\n",
      "Epoch: 57, Loss: 162.62962, Residuals: -1.82769, Convergence: 0.006911\n",
      "Epoch: 58, Loss: 161.55605, Residuals: -1.80169, Convergence: 0.006645\n",
      "Epoch: 59, Loss: 160.53027, Residuals: -1.77637, Convergence: 0.006390\n",
      "Epoch: 60, Loss: 159.54963, Residuals: -1.75174, Convergence: 0.006146\n",
      "Epoch: 61, Loss: 158.61149, Residuals: -1.72778, Convergence: 0.005915\n",
      "Epoch: 62, Loss: 157.71336, Residuals: -1.70448, Convergence: 0.005695\n",
      "Epoch: 63, Loss: 156.85299, Residuals: -1.68185, Convergence: 0.005485\n",
      "Epoch: 64, Loss: 156.02844, Residuals: -1.65986, Convergence: 0.005285\n",
      "Epoch: 65, Loss: 155.23810, Residuals: -1.63851, Convergence: 0.005091\n",
      "Epoch: 66, Loss: 154.48067, Residuals: -1.61779, Convergence: 0.004903\n",
      "Epoch: 67, Loss: 153.75508, Residuals: -1.59770, Convergence: 0.004719\n",
      "Epoch: 68, Loss: 153.06045, Residuals: -1.57824, Convergence: 0.004538\n",
      "Epoch: 69, Loss: 152.39599, Residuals: -1.55941, Convergence: 0.004360\n",
      "Epoch: 70, Loss: 151.76100, Residuals: -1.54121, Convergence: 0.004184\n",
      "Epoch: 71, Loss: 151.15474, Residuals: -1.52363, Convergence: 0.004011\n",
      "Epoch: 72, Loss: 150.57648, Residuals: -1.50668, Convergence: 0.003840\n",
      "Epoch: 73, Loss: 150.02545, Residuals: -1.49035, Convergence: 0.003673\n",
      "Epoch: 74, Loss: 149.50082, Residuals: -1.47463, Convergence: 0.003509\n",
      "Epoch: 75, Loss: 149.00174, Residuals: -1.45953, Convergence: 0.003349\n",
      "Epoch: 76, Loss: 148.52730, Residuals: -1.44501, Convergence: 0.003194\n",
      "Epoch: 77, Loss: 148.07658, Residuals: -1.43109, Convergence: 0.003044\n",
      "Epoch: 78, Loss: 147.64859, Residuals: -1.41773, Convergence: 0.002899\n",
      "Epoch: 79, Loss: 147.24241, Residuals: -1.40493, Convergence: 0.002759\n",
      "Epoch: 80, Loss: 146.85704, Residuals: -1.39268, Convergence: 0.002624\n",
      "Epoch: 81, Loss: 146.49153, Residuals: -1.38094, Convergence: 0.002495\n",
      "Epoch: 82, Loss: 146.14494, Residuals: -1.36972, Convergence: 0.002372\n",
      "Epoch: 83, Loss: 145.81635, Residuals: -1.35898, Convergence: 0.002253\n",
      "Epoch: 84, Loss: 145.50486, Residuals: -1.34871, Convergence: 0.002141\n",
      "Epoch: 85, Loss: 145.20961, Residuals: -1.33890, Convergence: 0.002033\n",
      "Epoch: 86, Loss: 144.92979, Residuals: -1.32952, Convergence: 0.001931\n",
      "Epoch: 87, Loss: 144.66461, Residuals: -1.32055, Convergence: 0.001833\n",
      "Epoch: 88, Loss: 144.41331, Residuals: -1.31199, Convergence: 0.001740\n",
      "Epoch: 89, Loss: 144.17522, Residuals: -1.30380, Convergence: 0.001651\n",
      "Epoch: 90, Loss: 143.94966, Residuals: -1.29598, Convergence: 0.001567\n",
      "Epoch: 91, Loss: 143.73604, Residuals: -1.28851, Convergence: 0.001486\n",
      "Epoch: 92, Loss: 143.53376, Residuals: -1.28136, Convergence: 0.001409\n",
      "Epoch: 93, Loss: 143.34231, Residuals: -1.27454, Convergence: 0.001336\n",
      "Epoch: 94, Loss: 143.16121, Residuals: -1.26802, Convergence: 0.001265\n",
      "Epoch: 95, Loss: 142.98999, Residuals: -1.26180, Convergence: 0.001197\n",
      "Epoch: 96, Loss: 142.82827, Residuals: -1.25585, Convergence: 0.001132\n",
      "Epoch: 97, Loss: 142.67565, Residuals: -1.25017, Convergence: 0.001070\n",
      "Epoch: 98, Loss: 142.53179, Residuals: -1.24475, Convergence: 0.001009\n",
      "Epoch: 99, Loss: 142.39638, Residuals: -1.23958, Convergence: 0.000951\n",
      "Evidence -184.312\n",
      "\n",
      "Epoch: 99, Evidence: -184.31241, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.24e-01\n",
      "Epoch: 99, Loss: 1368.79439, Residuals: -1.23958, Convergence:   inf\n",
      "Epoch: 100, Loss: 1305.68226, Residuals: -1.27150, Convergence: 0.048337\n",
      "Epoch: 101, Loss: 1258.16517, Residuals: -1.29598, Convergence: 0.037767\n",
      "Epoch: 102, Loss: 1222.54775, Residuals: -1.31305, Convergence: 0.029134\n",
      "Epoch: 103, Loss: 1194.92070, Residuals: -1.32472, Convergence: 0.023120\n",
      "Epoch: 104, Loss: 1172.57625, Residuals: -1.33317, Convergence: 0.019056\n",
      "Epoch: 105, Loss: 1154.00272, Residuals: -1.33951, Convergence: 0.016095\n",
      "Epoch: 106, Loss: 1138.30122, Residuals: -1.34421, Convergence: 0.013794\n",
      "Epoch: 107, Loss: 1124.86195, Residuals: -1.34751, Convergence: 0.011947\n",
      "Epoch: 108, Loss: 1113.23258, Residuals: -1.34958, Convergence: 0.010446\n",
      "Epoch: 109, Loss: 1103.05880, Residuals: -1.35053, Convergence: 0.009223\n",
      "Epoch: 110, Loss: 1094.05542, Residuals: -1.35048, Convergence: 0.008229\n",
      "Epoch: 111, Loss: 1085.98949, Residuals: -1.34952, Convergence: 0.007427\n",
      "Epoch: 112, Loss: 1078.66550, Residuals: -1.34771, Convergence: 0.006790\n",
      "Epoch: 113, Loss: 1071.91993, Residuals: -1.34513, Convergence: 0.006293\n",
      "Epoch: 114, Loss: 1065.61387, Residuals: -1.34182, Convergence: 0.005918\n",
      "Epoch: 115, Loss: 1059.62891, Residuals: -1.33781, Convergence: 0.005648\n",
      "Epoch: 116, Loss: 1053.86415, Residuals: -1.33313, Convergence: 0.005470\n",
      "Epoch: 117, Loss: 1048.23263, Residuals: -1.32779, Convergence: 0.005372\n",
      "Epoch: 118, Loss: 1042.66448, Residuals: -1.32181, Convergence: 0.005340\n",
      "Epoch: 119, Loss: 1037.11355, Residuals: -1.31525, Convergence: 0.005352\n",
      "Epoch: 120, Loss: 1031.56912, Residuals: -1.30816, Convergence: 0.005375\n",
      "Epoch: 121, Loss: 1026.06611, Residuals: -1.30063, Convergence: 0.005363\n",
      "Epoch: 122, Loss: 1020.67531, Residuals: -1.29276, Convergence: 0.005282\n",
      "Epoch: 123, Loss: 1015.48071, Residuals: -1.28465, Convergence: 0.005115\n",
      "Epoch: 124, Loss: 1010.54939, Residuals: -1.27639, Convergence: 0.004880\n",
      "Epoch: 125, Loss: 1005.91696, Residuals: -1.26806, Convergence: 0.004605\n",
      "Epoch: 126, Loss: 1001.58906, Residuals: -1.25973, Convergence: 0.004321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 127, Loss: 997.55160, Residuals: -1.25144, Convergence: 0.004047\n",
      "Epoch: 128, Loss: 993.78065, Residuals: -1.24325, Convergence: 0.003795\n",
      "Epoch: 129, Loss: 990.25037, Residuals: -1.23520, Convergence: 0.003565\n",
      "Epoch: 130, Loss: 986.93601, Residuals: -1.22731, Convergence: 0.003358\n",
      "Epoch: 131, Loss: 983.81611, Residuals: -1.21961, Convergence: 0.003171\n",
      "Epoch: 132, Loss: 980.87222, Residuals: -1.21212, Convergence: 0.003001\n",
      "Epoch: 133, Loss: 978.08960, Residuals: -1.20487, Convergence: 0.002845\n",
      "Epoch: 134, Loss: 975.45514, Residuals: -1.19786, Convergence: 0.002701\n",
      "Epoch: 135, Loss: 972.95852, Residuals: -1.19111, Convergence: 0.002566\n",
      "Epoch: 136, Loss: 970.59043, Residuals: -1.18462, Convergence: 0.002440\n",
      "Epoch: 137, Loss: 968.34284, Residuals: -1.17840, Convergence: 0.002321\n",
      "Epoch: 138, Loss: 966.20817, Residuals: -1.17244, Convergence: 0.002209\n",
      "Epoch: 139, Loss: 964.17912, Residuals: -1.16675, Convergence: 0.002104\n",
      "Epoch: 140, Loss: 962.24926, Residuals: -1.16131, Convergence: 0.002006\n",
      "Epoch: 141, Loss: 960.41220, Residuals: -1.15613, Convergence: 0.001913\n",
      "Epoch: 142, Loss: 958.66116, Residuals: -1.15120, Convergence: 0.001827\n",
      "Epoch: 143, Loss: 956.99017, Residuals: -1.14650, Convergence: 0.001746\n",
      "Epoch: 144, Loss: 955.39412, Residuals: -1.14202, Convergence: 0.001671\n",
      "Epoch: 145, Loss: 953.86655, Residuals: -1.13776, Convergence: 0.001601\n",
      "Epoch: 146, Loss: 952.40287, Residuals: -1.13370, Convergence: 0.001537\n",
      "Epoch: 147, Loss: 950.99780, Residuals: -1.12984, Convergence: 0.001477\n",
      "Epoch: 148, Loss: 949.64717, Residuals: -1.12615, Convergence: 0.001422\n",
      "Epoch: 149, Loss: 948.34636, Residuals: -1.12263, Convergence: 0.001372\n",
      "Epoch: 150, Loss: 947.09167, Residuals: -1.11927, Convergence: 0.001325\n",
      "Epoch: 151, Loss: 945.87924, Residuals: -1.11605, Convergence: 0.001282\n",
      "Epoch: 152, Loss: 944.70596, Residuals: -1.11298, Convergence: 0.001242\n",
      "Epoch: 153, Loss: 943.56828, Residuals: -1.11003, Convergence: 0.001206\n",
      "Epoch: 154, Loss: 942.46333, Residuals: -1.10720, Convergence: 0.001172\n",
      "Epoch: 155, Loss: 941.38843, Residuals: -1.10448, Convergence: 0.001142\n",
      "Epoch: 156, Loss: 940.34060, Residuals: -1.10186, Convergence: 0.001114\n",
      "Epoch: 157, Loss: 939.31722, Residuals: -1.09933, Convergence: 0.001089\n",
      "Epoch: 158, Loss: 938.31506, Residuals: -1.09688, Convergence: 0.001068\n",
      "Epoch: 159, Loss: 937.33222, Residuals: -1.09451, Convergence: 0.001049\n",
      "Epoch: 160, Loss: 936.36571, Residuals: -1.09221, Convergence: 0.001032\n",
      "Epoch: 161, Loss: 935.41255, Residuals: -1.08997, Convergence: 0.001019\n",
      "Epoch: 162, Loss: 934.47088, Residuals: -1.08778, Convergence: 0.001008\n",
      "Epoch: 163, Loss: 933.53782, Residuals: -1.08563, Convergence: 0.000999\n",
      "Evidence 11106.165\n",
      "\n",
      "Epoch: 163, Evidence: 11106.16504, Convergence: 1.016595\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.77e-01\n",
      "Epoch: 163, Loss: 2345.12834, Residuals: -1.08563, Convergence:   inf\n",
      "Epoch: 164, Loss: 2305.93791, Residuals: -1.09407, Convergence: 0.016995\n",
      "Epoch: 165, Loss: 2278.23444, Residuals: -1.09300, Convergence: 0.012160\n",
      "Epoch: 166, Loss: 2255.02513, Residuals: -1.09081, Convergence: 0.010292\n",
      "Epoch: 167, Loss: 2235.29204, Residuals: -1.08823, Convergence: 0.008828\n",
      "Epoch: 168, Loss: 2218.37436, Residuals: -1.08543, Convergence: 0.007626\n",
      "Epoch: 169, Loss: 2203.77269, Residuals: -1.08247, Convergence: 0.006626\n",
      "Epoch: 170, Loss: 2191.07937, Residuals: -1.07941, Convergence: 0.005793\n",
      "Epoch: 171, Loss: 2179.95382, Residuals: -1.07626, Convergence: 0.005104\n",
      "Epoch: 172, Loss: 2170.10422, Residuals: -1.07304, Convergence: 0.004539\n",
      "Epoch: 173, Loss: 2161.28358, Residuals: -1.06975, Convergence: 0.004081\n",
      "Epoch: 174, Loss: 2153.28478, Residuals: -1.06638, Convergence: 0.003715\n",
      "Epoch: 175, Loss: 2145.94859, Residuals: -1.06291, Convergence: 0.003419\n",
      "Epoch: 176, Loss: 2139.16321, Residuals: -1.05936, Convergence: 0.003172\n",
      "Epoch: 177, Loss: 2132.86080, Residuals: -1.05575, Convergence: 0.002955\n",
      "Epoch: 178, Loss: 2127.00247, Residuals: -1.05211, Convergence: 0.002754\n",
      "Epoch: 179, Loss: 2121.56506, Residuals: -1.04851, Convergence: 0.002563\n",
      "Epoch: 180, Loss: 2116.52728, Residuals: -1.04497, Convergence: 0.002380\n",
      "Epoch: 181, Loss: 2111.86706, Residuals: -1.04155, Convergence: 0.002207\n",
      "Epoch: 182, Loss: 2107.55931, Residuals: -1.03825, Convergence: 0.002044\n",
      "Epoch: 183, Loss: 2103.57789, Residuals: -1.03511, Convergence: 0.001893\n",
      "Epoch: 184, Loss: 2099.89672, Residuals: -1.03212, Convergence: 0.001753\n",
      "Epoch: 185, Loss: 2096.49083, Residuals: -1.02930, Convergence: 0.001625\n",
      "Epoch: 186, Loss: 2093.33618, Residuals: -1.02664, Convergence: 0.001507\n",
      "Epoch: 187, Loss: 2090.41212, Residuals: -1.02414, Convergence: 0.001399\n",
      "Epoch: 188, Loss: 2087.69829, Residuals: -1.02180, Convergence: 0.001300\n",
      "Epoch: 189, Loss: 2085.17696, Residuals: -1.01961, Convergence: 0.001209\n",
      "Epoch: 190, Loss: 2082.83193, Residuals: -1.01755, Convergence: 0.001126\n",
      "Epoch: 191, Loss: 2080.64860, Residuals: -1.01563, Convergence: 0.001049\n",
      "Epoch: 192, Loss: 2078.61256, Residuals: -1.01383, Convergence: 0.000980\n",
      "Evidence 14245.929\n",
      "\n",
      "Epoch: 192, Evidence: 14245.92871, Convergence: 0.220397\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.39e-01\n",
      "Epoch: 192, Loss: 2471.14543, Residuals: -1.01383, Convergence:   inf\n",
      "Epoch: 193, Loss: 2457.23978, Residuals: -1.01057, Convergence: 0.005659\n",
      "Epoch: 194, Loss: 2445.91550, Residuals: -1.00690, Convergence: 0.004630\n",
      "Epoch: 195, Loss: 2436.20027, Residuals: -1.00338, Convergence: 0.003988\n",
      "Epoch: 196, Loss: 2427.80340, Residuals: -1.00008, Convergence: 0.003459\n",
      "Epoch: 197, Loss: 2420.49944, Residuals: -0.99703, Convergence: 0.003018\n",
      "Epoch: 198, Loss: 2414.10769, Residuals: -0.99421, Convergence: 0.002648\n",
      "Epoch: 199, Loss: 2408.48027, Residuals: -0.99163, Convergence: 0.002337\n",
      "Epoch: 200, Loss: 2403.49708, Residuals: -0.98926, Convergence: 0.002073\n",
      "Epoch: 201, Loss: 2399.05777, Residuals: -0.98709, Convergence: 0.001850\n",
      "Epoch: 202, Loss: 2395.08047, Residuals: -0.98510, Convergence: 0.001661\n",
      "Epoch: 203, Loss: 2391.49597, Residuals: -0.98328, Convergence: 0.001499\n",
      "Epoch: 204, Loss: 2388.24761, Residuals: -0.98162, Convergence: 0.001360\n",
      "Epoch: 205, Loss: 2385.28894, Residuals: -0.98009, Convergence: 0.001240\n",
      "Epoch: 206, Loss: 2382.57988, Residuals: -0.97870, Convergence: 0.001137\n",
      "Epoch: 207, Loss: 2380.08919, Residuals: -0.97744, Convergence: 0.001046\n",
      "Epoch: 208, Loss: 2377.78937, Residuals: -0.97628, Convergence: 0.000967\n",
      "Evidence 14649.469\n",
      "\n",
      "Epoch: 208, Evidence: 14649.46875, Convergence: 0.027546\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.35e-01\n",
      "Epoch: 208, Loss: 2476.54435, Residuals: -0.97628, Convergence:   inf\n",
      "Epoch: 209, Loss: 2470.13989, Residuals: -0.97295, Convergence: 0.002593\n",
      "Epoch: 210, Loss: 2464.81772, Residuals: -0.97001, Convergence: 0.002159\n",
      "Epoch: 211, Loss: 2460.28731, Residuals: -0.96749, Convergence: 0.001841\n",
      "Epoch: 212, Loss: 2456.37740, Residuals: -0.96533, Convergence: 0.001592\n",
      "Epoch: 213, Loss: 2452.96006, Residuals: -0.96349, Convergence: 0.001393\n",
      "Epoch: 214, Loss: 2449.93611, Residuals: -0.96193, Convergence: 0.001234\n",
      "Epoch: 215, Loss: 2447.23012, Residuals: -0.96061, Convergence: 0.001106\n",
      "Epoch: 216, Loss: 2444.78312, Residuals: -0.95948, Convergence: 0.001001\n",
      "Epoch: 217, Loss: 2442.55194, Residuals: -0.95853, Convergence: 0.000913\n",
      "Evidence 14733.228\n",
      "\n",
      "Epoch: 217, Evidence: 14733.22754, Convergence: 0.005685\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.62e-01\n",
      "Epoch: 217, Loss: 2478.16291, Residuals: -0.95853, Convergence:   inf\n",
      "Epoch: 218, Loss: 2474.40191, Residuals: -0.95606, Convergence: 0.001520\n",
      "Epoch: 219, Loss: 2471.24890, Residuals: -0.95406, Convergence: 0.001276\n",
      "Epoch: 220, Loss: 2468.52908, Residuals: -0.95245, Convergence: 0.001102\n",
      "Epoch: 221, Loss: 2466.13628, Residuals: -0.95115, Convergence: 0.000970\n",
      "Evidence 14761.398\n",
      "\n",
      "Epoch: 221, Evidence: 14761.39844, Convergence: 0.001908\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.12e-01\n",
      "Epoch: 221, Loss: 2479.24777, Residuals: -0.95115, Convergence:   inf\n",
      "Epoch: 222, Loss: 2476.40900, Residuals: -0.94915, Convergence: 0.001146\n",
      "Epoch: 223, Loss: 2474.00633, Residuals: -0.94758, Convergence: 0.000971\n",
      "Evidence 14773.238\n",
      "\n",
      "Epoch: 223, Evidence: 14773.23828, Convergence: 0.000801\n",
      "Updating hyper-parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 182, Updated regularization: 1.76e-01\n",
      "Epoch: 223, Loss: 2480.02527, Residuals: -0.94758, Convergence:   inf\n",
      "Epoch: 224, Loss: 2475.47492, Residuals: -0.94478, Convergence: 0.001838\n",
      "Epoch: 225, Loss: 2471.99329, Residuals: -0.94294, Convergence: 0.001408\n",
      "Epoch: 226, Loss: 2469.14455, Residuals: -0.94183, Convergence: 0.001154\n",
      "Epoch: 227, Loss: 2466.71100, Residuals: -0.94129, Convergence: 0.000987\n",
      "Evidence 14791.078\n",
      "\n",
      "Epoch: 227, Evidence: 14791.07812, Convergence: 0.002007\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.46e-01\n",
      "Epoch: 227, Loss: 2480.25003, Residuals: -0.94129, Convergence:   inf\n",
      "Epoch: 228, Loss: 2477.22379, Residuals: -0.93895, Convergence: 0.001222\n",
      "Epoch: 229, Loss: 2474.80887, Residuals: -0.93787, Convergence: 0.000976\n",
      "Evidence 14801.947\n",
      "\n",
      "Epoch: 229, Evidence: 14801.94727, Convergence: 0.000734\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.24e-01\n",
      "Epoch: 229, Loss: 2480.47867, Residuals: -0.93787, Convergence:   inf\n",
      "Epoch: 230, Loss: 2476.04297, Residuals: -0.93462, Convergence: 0.001791\n",
      "Epoch: 231, Loss: 2472.81758, Residuals: -0.93606, Convergence: 0.001304\n",
      "Epoch: 232, Loss: 2470.11430, Residuals: -0.93686, Convergence: 0.001094\n",
      "Epoch: 233, Loss: 2467.73953, Residuals: -0.93975, Convergence: 0.000962\n",
      "Evidence 14817.862\n",
      "\n",
      "Epoch: 233, Evidence: 14817.86230, Convergence: 0.001808\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.09e-01\n",
      "Epoch: 233, Loss: 2479.80226, Residuals: -0.93975, Convergence:   inf\n",
      "Epoch: 234, Loss: 2477.95471, Residuals: -0.93787, Convergence: 0.000746\n",
      "Evidence 14824.535\n",
      "\n",
      "Epoch: 234, Evidence: 14824.53516, Convergence: 0.000450\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.97e-02\n",
      "Epoch: 234, Loss: 2480.75019, Residuals: -0.93787, Convergence:   inf\n",
      "Epoch: 235, Loss: 2522.16899, Residuals: -0.97547, Convergence: -0.016422\n",
      "Epoch: 235, Loss: 2478.47439, Residuals: -0.93554, Convergence: 0.000918\n",
      "Evidence 14828.717\n",
      "\n",
      "Epoch: 235, Evidence: 14828.71680, Convergence: 0.000732\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.23e-02\n",
      "Epoch: 235, Loss: 2480.23143, Residuals: -0.93554, Convergence:   inf\n",
      "Epoch: 236, Loss: 2483.26145, Residuals: -0.93898, Convergence: -0.001220\n",
      "Epoch: 236, Loss: 2479.76033, Residuals: -0.93484, Convergence: 0.000190\n",
      "Evidence 14830.746\n",
      "\n",
      "Epoch: 236, Evidence: 14830.74609, Convergence: 0.000869\n",
      "Total samples: 184, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 384.05455, Residuals: -4.53659, Convergence:   inf\n",
      "Epoch: 1, Loss: 358.25940, Residuals: -4.41658, Convergence: 0.072001\n",
      "Epoch: 2, Loss: 337.12466, Residuals: -4.25252, Convergence: 0.062691\n",
      "Epoch: 3, Loss: 321.00771, Residuals: -4.08846, Convergence: 0.050207\n",
      "Epoch: 4, Loss: 308.71482, Residuals: -3.94412, Convergence: 0.039820\n",
      "Epoch: 5, Loss: 298.96241, Residuals: -3.81659, Convergence: 0.032621\n",
      "Epoch: 6, Loss: 291.04039, Residuals: -3.70544, Convergence: 0.027220\n",
      "Epoch: 7, Loss: 284.45922, Residuals: -3.61015, Convergence: 0.023136\n",
      "Epoch: 8, Loss: 278.85731, Residuals: -3.52869, Convergence: 0.020089\n",
      "Epoch: 9, Loss: 273.97900, Residuals: -3.45874, Convergence: 0.017805\n",
      "Epoch: 10, Loss: 269.64248, Residuals: -3.39821, Convergence: 0.016082\n",
      "Epoch: 11, Loss: 265.71605, Residuals: -3.34531, Convergence: 0.014777\n",
      "Epoch: 12, Loss: 262.10356, Residuals: -3.29852, Convergence: 0.013783\n",
      "Epoch: 13, Loss: 258.73527, Residuals: -3.25653, Convergence: 0.013018\n",
      "Epoch: 14, Loss: 255.56162, Residuals: -3.21821, Convergence: 0.012418\n",
      "Epoch: 15, Loss: 252.55000, Residuals: -3.18262, Convergence: 0.011925\n",
      "Epoch: 16, Loss: 249.68263, Residuals: -3.14910, Convergence: 0.011484\n",
      "Epoch: 17, Loss: 246.94834, Residuals: -3.11729, Convergence: 0.011072\n",
      "Epoch: 18, Loss: 244.32887, Residuals: -3.08684, Convergence: 0.010721\n",
      "Epoch: 19, Loss: 241.79388, Residuals: -3.05729, Convergence: 0.010484\n",
      "Epoch: 20, Loss: 239.30700, Residuals: -3.02808, Convergence: 0.010392\n",
      "Epoch: 21, Loss: 236.83428, Residuals: -2.99868, Convergence: 0.010441\n",
      "Epoch: 22, Loss: 234.34879, Residuals: -2.96868, Convergence: 0.010606\n",
      "Epoch: 23, Loss: 231.82380, Residuals: -2.93772, Convergence: 0.010892\n",
      "Epoch: 24, Loss: 229.21678, Residuals: -2.90533, Convergence: 0.011374\n",
      "Epoch: 25, Loss: 226.47209, Residuals: -2.87084, Convergence: 0.012119\n",
      "Epoch: 26, Loss: 223.58845, Residuals: -2.83413, Convergence: 0.012897\n",
      "Epoch: 27, Loss: 220.68715, Residuals: -2.79646, Convergence: 0.013147\n",
      "Epoch: 28, Loss: 217.87906, Residuals: -2.75915, Convergence: 0.012888\n",
      "Epoch: 29, Loss: 215.18222, Residuals: -2.72258, Convergence: 0.012533\n",
      "Epoch: 30, Loss: 212.58037, Residuals: -2.68669, Convergence: 0.012239\n",
      "Epoch: 31, Loss: 210.05543, Residuals: -2.65136, Convergence: 0.012020\n",
      "Epoch: 32, Loss: 207.59370, Residuals: -2.61648, Convergence: 0.011858\n",
      "Epoch: 33, Loss: 205.18589, Residuals: -2.58194, Convergence: 0.011735\n",
      "Epoch: 34, Loss: 202.82604, Residuals: -2.54769, Convergence: 0.011635\n",
      "Epoch: 35, Loss: 200.51059, Residuals: -2.51366, Convergence: 0.011548\n",
      "Epoch: 36, Loss: 198.23770, Residuals: -2.47981, Convergence: 0.011466\n",
      "Epoch: 37, Loss: 196.00664, Residuals: -2.44610, Convergence: 0.011383\n",
      "Epoch: 38, Loss: 193.81745, Residuals: -2.41252, Convergence: 0.011295\n",
      "Epoch: 39, Loss: 191.67064, Residuals: -2.37904, Convergence: 0.011201\n",
      "Epoch: 40, Loss: 189.56698, Residuals: -2.34566, Convergence: 0.011097\n",
      "Epoch: 41, Loss: 187.50733, Residuals: -2.31238, Convergence: 0.010984\n",
      "Epoch: 42, Loss: 185.49258, Residuals: -2.27919, Convergence: 0.010862\n",
      "Epoch: 43, Loss: 183.52362, Residuals: -2.24610, Convergence: 0.010729\n",
      "Epoch: 44, Loss: 181.60151, Residuals: -2.21312, Convergence: 0.010584\n",
      "Epoch: 45, Loss: 179.72748, Residuals: -2.18027, Convergence: 0.010427\n",
      "Epoch: 46, Loss: 177.90325, Residuals: -2.14758, Convergence: 0.010254\n",
      "Epoch: 47, Loss: 176.13086, Residuals: -2.11508, Convergence: 0.010063\n",
      "Epoch: 48, Loss: 174.41271, Residuals: -2.08282, Convergence: 0.009851\n",
      "Epoch: 49, Loss: 172.75120, Residuals: -2.05084, Convergence: 0.009618\n",
      "Epoch: 50, Loss: 171.14849, Residuals: -2.01922, Convergence: 0.009364\n",
      "Epoch: 51, Loss: 169.60622, Residuals: -1.98800, Convergence: 0.009093\n",
      "Epoch: 52, Loss: 168.12540, Residuals: -1.95725, Convergence: 0.008808\n",
      "Epoch: 53, Loss: 166.70638, Residuals: -1.92702, Convergence: 0.008512\n",
      "Epoch: 54, Loss: 165.34891, Residuals: -1.89737, Convergence: 0.008210\n",
      "Epoch: 55, Loss: 164.05226, Residuals: -1.86833, Convergence: 0.007904\n",
      "Epoch: 56, Loss: 162.81528, Residuals: -1.83994, Convergence: 0.007597\n",
      "Epoch: 57, Loss: 161.63650, Residuals: -1.81225, Convergence: 0.007293\n",
      "Epoch: 58, Loss: 160.51417, Residuals: -1.78528, Convergence: 0.006992\n",
      "Epoch: 59, Loss: 159.44628, Residuals: -1.75905, Convergence: 0.006697\n",
      "Epoch: 60, Loss: 158.43065, Residuals: -1.73358, Convergence: 0.006411\n",
      "Epoch: 61, Loss: 157.46486, Residuals: -1.70889, Convergence: 0.006133\n",
      "Epoch: 62, Loss: 156.54640, Residuals: -1.68497, Convergence: 0.005867\n",
      "Epoch: 63, Loss: 155.67266, Residuals: -1.66183, Convergence: 0.005613\n",
      "Epoch: 64, Loss: 154.84109, Residuals: -1.63945, Convergence: 0.005370\n",
      "Epoch: 65, Loss: 154.04925, Residuals: -1.61781, Convergence: 0.005140\n",
      "Epoch: 66, Loss: 153.29492, Residuals: -1.59691, Convergence: 0.004921\n",
      "Epoch: 67, Loss: 152.57613, Residuals: -1.57673, Convergence: 0.004711\n",
      "Epoch: 68, Loss: 151.89118, Residuals: -1.55725, Convergence: 0.004510\n",
      "Epoch: 69, Loss: 151.23858, Residuals: -1.53846, Convergence: 0.004315\n",
      "Epoch: 70, Loss: 150.61701, Residuals: -1.52035, Convergence: 0.004127\n",
      "Epoch: 71, Loss: 150.02531, Residuals: -1.50291, Convergence: 0.003944\n",
      "Epoch: 72, Loss: 149.46237, Residuals: -1.48613, Convergence: 0.003766\n",
      "Epoch: 73, Loss: 148.92715, Residuals: -1.46999, Convergence: 0.003594\n",
      "Epoch: 74, Loss: 148.41860, Residuals: -1.45449, Convergence: 0.003426\n",
      "Epoch: 75, Loss: 147.93571, Residuals: -1.43961, Convergence: 0.003264\n",
      "Epoch: 76, Loss: 147.47744, Residuals: -1.42533, Convergence: 0.003107\n",
      "Epoch: 77, Loss: 147.04279, Residuals: -1.41164, Convergence: 0.002956\n",
      "Epoch: 78, Loss: 146.63071, Residuals: -1.39851, Convergence: 0.002810\n",
      "Epoch: 79, Loss: 146.24020, Residuals: -1.38595, Convergence: 0.002670\n",
      "Epoch: 80, Loss: 145.87024, Residuals: -1.37391, Convergence: 0.002536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81, Loss: 145.51986, Residuals: -1.36239, Convergence: 0.002408\n",
      "Epoch: 82, Loss: 145.18808, Residuals: -1.35137, Convergence: 0.002285\n",
      "Epoch: 83, Loss: 144.87396, Residuals: -1.34082, Convergence: 0.002168\n",
      "Epoch: 84, Loss: 144.57661, Residuals: -1.33072, Convergence: 0.002057\n",
      "Epoch: 85, Loss: 144.29516, Residuals: -1.32106, Convergence: 0.001951\n",
      "Epoch: 86, Loss: 144.02879, Residuals: -1.31182, Convergence: 0.001849\n",
      "Epoch: 87, Loss: 143.77670, Residuals: -1.30298, Convergence: 0.001753\n",
      "Epoch: 88, Loss: 143.53817, Residuals: -1.29452, Convergence: 0.001662\n",
      "Epoch: 89, Loss: 143.31250, Residuals: -1.28642, Convergence: 0.001575\n",
      "Epoch: 90, Loss: 143.09905, Residuals: -1.27867, Convergence: 0.001492\n",
      "Epoch: 91, Loss: 142.89723, Residuals: -1.27125, Convergence: 0.001412\n",
      "Epoch: 92, Loss: 142.70646, Residuals: -1.26415, Convergence: 0.001337\n",
      "Epoch: 93, Loss: 142.52625, Residuals: -1.25735, Convergence: 0.001264\n",
      "Epoch: 94, Loss: 142.35614, Residuals: -1.25084, Convergence: 0.001195\n",
      "Epoch: 95, Loss: 142.19567, Residuals: -1.24461, Convergence: 0.001128\n",
      "Epoch: 96, Loss: 142.04447, Residuals: -1.23866, Convergence: 0.001064\n",
      "Epoch: 97, Loss: 141.90216, Residuals: -1.23296, Convergence: 0.001003\n",
      "Epoch: 98, Loss: 141.76840, Residuals: -1.22751, Convergence: 0.000944\n",
      "Evidence -184.005\n",
      "\n",
      "Epoch: 98, Evidence: -184.00510, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 7.25e-01\n",
      "Epoch: 98, Loss: 1368.89039, Residuals: -1.22751, Convergence:   inf\n",
      "Epoch: 99, Loss: 1304.17972, Residuals: -1.25977, Convergence: 0.049618\n",
      "Epoch: 100, Loss: 1255.97536, Residuals: -1.28446, Convergence: 0.038380\n",
      "Epoch: 101, Loss: 1220.21488, Residuals: -1.30145, Convergence: 0.029307\n",
      "Epoch: 102, Loss: 1192.58940, Residuals: -1.31297, Convergence: 0.023164\n",
      "Epoch: 103, Loss: 1170.28236, Residuals: -1.32135, Convergence: 0.019061\n",
      "Epoch: 104, Loss: 1151.78712, Residuals: -1.32767, Convergence: 0.016058\n",
      "Epoch: 105, Loss: 1136.21113, Residuals: -1.33240, Convergence: 0.013709\n",
      "Epoch: 106, Loss: 1122.94295, Residuals: -1.33575, Convergence: 0.011816\n",
      "Epoch: 107, Loss: 1111.52658, Residuals: -1.33790, Convergence: 0.010271\n",
      "Epoch: 108, Loss: 1101.60483, Residuals: -1.33897, Convergence: 0.009007\n",
      "Epoch: 109, Loss: 1092.89002, Residuals: -1.33909, Convergence: 0.007974\n",
      "Epoch: 110, Loss: 1085.14613, Residuals: -1.33835, Convergence: 0.007136\n",
      "Epoch: 111, Loss: 1078.17438, Residuals: -1.33683, Convergence: 0.006466\n",
      "Epoch: 112, Loss: 1071.80462, Residuals: -1.33458, Convergence: 0.005943\n",
      "Epoch: 113, Loss: 1065.88640, Residuals: -1.33166, Convergence: 0.005552\n",
      "Epoch: 114, Loss: 1060.28414, Residuals: -1.32809, Convergence: 0.005284\n",
      "Epoch: 115, Loss: 1054.87441, Residuals: -1.32388, Convergence: 0.005128\n",
      "Epoch: 116, Loss: 1049.54804, Residuals: -1.31902, Convergence: 0.005075\n",
      "Epoch: 117, Loss: 1044.21726, Residuals: -1.31354, Convergence: 0.005105\n",
      "Epoch: 118, Loss: 1038.83191, Residuals: -1.30746, Convergence: 0.005184\n",
      "Epoch: 119, Loss: 1033.39208, Residuals: -1.30085, Convergence: 0.005264\n",
      "Epoch: 120, Loss: 1027.95297, Residuals: -1.29379, Convergence: 0.005291\n",
      "Epoch: 121, Loss: 1022.60777, Residuals: -1.28636, Convergence: 0.005227\n",
      "Epoch: 122, Loss: 1017.45361, Residuals: -1.27866, Convergence: 0.005066\n",
      "Epoch: 123, Loss: 1012.56329, Residuals: -1.27079, Convergence: 0.004830\n",
      "Epoch: 124, Loss: 1007.97241, Residuals: -1.26280, Convergence: 0.004555\n",
      "Epoch: 125, Loss: 1003.68677, Residuals: -1.25479, Convergence: 0.004270\n",
      "Epoch: 126, Loss: 999.69253, Residuals: -1.24679, Convergence: 0.003995\n",
      "Epoch: 127, Loss: 995.96692, Residuals: -1.23886, Convergence: 0.003741\n",
      "Epoch: 128, Loss: 992.48480, Residuals: -1.23104, Convergence: 0.003508\n",
      "Epoch: 129, Loss: 989.22214, Residuals: -1.22336, Convergence: 0.003298\n",
      "Epoch: 130, Loss: 986.15741, Residuals: -1.21584, Convergence: 0.003108\n",
      "Epoch: 131, Loss: 983.27272, Residuals: -1.20851, Convergence: 0.002934\n",
      "Epoch: 132, Loss: 980.55260, Residuals: -1.20139, Convergence: 0.002774\n",
      "Epoch: 133, Loss: 977.98367, Residuals: -1.19449, Convergence: 0.002627\n",
      "Epoch: 134, Loss: 975.55552, Residuals: -1.18783, Convergence: 0.002489\n",
      "Epoch: 135, Loss: 973.25871, Residuals: -1.18141, Convergence: 0.002360\n",
      "Epoch: 136, Loss: 971.08473, Residuals: -1.17524, Convergence: 0.002239\n",
      "Epoch: 137, Loss: 969.02602, Residuals: -1.16932, Convergence: 0.002125\n",
      "Epoch: 138, Loss: 967.07559, Residuals: -1.16367, Convergence: 0.002017\n",
      "Epoch: 139, Loss: 965.22620, Residuals: -1.15827, Convergence: 0.001916\n",
      "Epoch: 140, Loss: 963.47126, Residuals: -1.15312, Convergence: 0.001821\n",
      "Epoch: 141, Loss: 961.80478, Residuals: -1.14822, Convergence: 0.001733\n",
      "Epoch: 142, Loss: 960.21961, Residuals: -1.14356, Convergence: 0.001651\n",
      "Epoch: 143, Loss: 958.71026, Residuals: -1.13913, Convergence: 0.001574\n",
      "Epoch: 144, Loss: 957.27078, Residuals: -1.13493, Convergence: 0.001504\n",
      "Epoch: 145, Loss: 955.89544, Residuals: -1.13094, Convergence: 0.001439\n",
      "Epoch: 146, Loss: 954.57908, Residuals: -1.12715, Convergence: 0.001379\n",
      "Epoch: 147, Loss: 953.31684, Residuals: -1.12355, Convergence: 0.001324\n",
      "Epoch: 148, Loss: 952.10394, Residuals: -1.12013, Convergence: 0.001274\n",
      "Epoch: 149, Loss: 950.93628, Residuals: -1.11689, Convergence: 0.001228\n",
      "Epoch: 150, Loss: 949.81003, Residuals: -1.11380, Convergence: 0.001186\n",
      "Epoch: 151, Loss: 948.72135, Residuals: -1.11086, Convergence: 0.001148\n",
      "Epoch: 152, Loss: 947.66753, Residuals: -1.10806, Convergence: 0.001112\n",
      "Epoch: 153, Loss: 946.64557, Residuals: -1.10538, Convergence: 0.001080\n",
      "Epoch: 154, Loss: 945.65294, Residuals: -1.10283, Convergence: 0.001050\n",
      "Epoch: 155, Loss: 944.68764, Residuals: -1.10039, Convergence: 0.001022\n",
      "Epoch: 156, Loss: 943.74772, Residuals: -1.09806, Convergence: 0.000996\n",
      "Evidence 11147.056\n",
      "\n",
      "Epoch: 156, Evidence: 11147.05566, Convergence: 1.016507\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 5.79e-01\n",
      "Epoch: 156, Loss: 2362.85740, Residuals: -1.09806, Convergence:   inf\n",
      "Epoch: 157, Loss: 2324.21916, Residuals: -1.10692, Convergence: 0.016624\n",
      "Epoch: 158, Loss: 2295.96677, Residuals: -1.10675, Convergence: 0.012305\n",
      "Epoch: 159, Loss: 2272.12497, Residuals: -1.10522, Convergence: 0.010493\n",
      "Epoch: 160, Loss: 2251.70411, Residuals: -1.10319, Convergence: 0.009069\n",
      "Epoch: 161, Loss: 2234.06223, Residuals: -1.10083, Convergence: 0.007897\n",
      "Epoch: 162, Loss: 2218.69791, Residuals: -1.09819, Convergence: 0.006925\n",
      "Epoch: 163, Loss: 2205.19572, Residuals: -1.09529, Convergence: 0.006123\n",
      "Epoch: 164, Loss: 2193.20680, Residuals: -1.09217, Convergence: 0.005466\n",
      "Epoch: 165, Loss: 2182.44319, Residuals: -1.08883, Convergence: 0.004932\n",
      "Epoch: 166, Loss: 2172.66942, Residuals: -1.08529, Convergence: 0.004499\n",
      "Epoch: 167, Loss: 2163.69599, Residuals: -1.08158, Convergence: 0.004147\n",
      "Epoch: 168, Loss: 2155.37757, Residuals: -1.07770, Convergence: 0.003859\n",
      "Epoch: 169, Loss: 2147.61045, Residuals: -1.07370, Convergence: 0.003617\n",
      "Epoch: 170, Loss: 2140.32871, Residuals: -1.06959, Convergence: 0.003402\n",
      "Epoch: 171, Loss: 2133.50159, Residuals: -1.06542, Convergence: 0.003200\n",
      "Epoch: 172, Loss: 2127.12344, Residuals: -1.06124, Convergence: 0.002998\n",
      "Epoch: 173, Loss: 2121.19743, Residuals: -1.05710, Convergence: 0.002794\n",
      "Epoch: 174, Loss: 2115.72075, Residuals: -1.05305, Convergence: 0.002589\n",
      "Epoch: 175, Loss: 2110.68147, Residuals: -1.04914, Convergence: 0.002388\n",
      "Epoch: 176, Loss: 2106.05919, Residuals: -1.04539, Convergence: 0.002195\n",
      "Epoch: 177, Loss: 2101.82333, Residuals: -1.04182, Convergence: 0.002015\n",
      "Epoch: 178, Loss: 2097.94273, Residuals: -1.03844, Convergence: 0.001850\n",
      "Epoch: 179, Loss: 2094.38295, Residuals: -1.03525, Convergence: 0.001700\n",
      "Epoch: 180, Loss: 2091.11181, Residuals: -1.03225, Convergence: 0.001564\n",
      "Epoch: 181, Loss: 2088.09963, Residuals: -1.02944, Convergence: 0.001443\n",
      "Epoch: 182, Loss: 2085.31862, Residuals: -1.02681, Convergence: 0.001334\n",
      "Epoch: 183, Loss: 2082.74512, Residuals: -1.02434, Convergence: 0.001236\n",
      "Epoch: 184, Loss: 2080.35732, Residuals: -1.02204, Convergence: 0.001148\n",
      "Epoch: 185, Loss: 2078.13661, Residuals: -1.01989, Convergence: 0.001069\n",
      "Epoch: 186, Loss: 2076.06689, Residuals: -1.01787, Convergence: 0.000997\n",
      "Evidence 14379.576\n",
      "\n",
      "Epoch: 186, Evidence: 14379.57617, Convergence: 0.224799\n",
      "Updating hyper-parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 184, Updated regularization: 4.40e-01\n",
      "Epoch: 186, Loss: 2494.68212, Residuals: -1.01787, Convergence:   inf\n",
      "Epoch: 187, Loss: 2479.89486, Residuals: -1.01446, Convergence: 0.005963\n",
      "Epoch: 188, Loss: 2467.93694, Residuals: -1.01044, Convergence: 0.004845\n",
      "Epoch: 189, Loss: 2457.75891, Residuals: -1.00652, Convergence: 0.004141\n",
      "Epoch: 190, Loss: 2449.01196, Residuals: -1.00279, Convergence: 0.003572\n",
      "Epoch: 191, Loss: 2441.43434, Residuals: -0.99929, Convergence: 0.003104\n",
      "Epoch: 192, Loss: 2434.82035, Residuals: -0.99604, Convergence: 0.002716\n",
      "Epoch: 193, Loss: 2429.00572, Residuals: -0.99301, Convergence: 0.002394\n",
      "Epoch: 194, Loss: 2423.85575, Residuals: -0.99021, Convergence: 0.002125\n",
      "Epoch: 195, Loss: 2419.26338, Residuals: -0.98761, Convergence: 0.001898\n",
      "Epoch: 196, Loss: 2415.14114, Residuals: -0.98520, Convergence: 0.001707\n",
      "Epoch: 197, Loss: 2411.41643, Residuals: -0.98297, Convergence: 0.001545\n",
      "Epoch: 198, Loss: 2408.03267, Residuals: -0.98090, Convergence: 0.001405\n",
      "Epoch: 199, Loss: 2404.94112, Residuals: -0.97898, Convergence: 0.001285\n",
      "Epoch: 200, Loss: 2402.10243, Residuals: -0.97720, Convergence: 0.001182\n",
      "Epoch: 201, Loss: 2399.48481, Residuals: -0.97555, Convergence: 0.001091\n",
      "Epoch: 202, Loss: 2397.06079, Residuals: -0.97402, Convergence: 0.001011\n",
      "Epoch: 203, Loss: 2394.80771, Residuals: -0.97261, Convergence: 0.000941\n",
      "Evidence 14817.438\n",
      "\n",
      "Epoch: 203, Evidence: 14817.43848, Convergence: 0.029550\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 3.35e-01\n",
      "Epoch: 203, Loss: 2500.30678, Residuals: -0.97261, Convergence:   inf\n",
      "Epoch: 204, Loss: 2493.80469, Residuals: -0.96906, Convergence: 0.002607\n",
      "Epoch: 205, Loss: 2488.45208, Residuals: -0.96583, Convergence: 0.002151\n",
      "Epoch: 206, Loss: 2483.89949, Residuals: -0.96298, Convergence: 0.001833\n",
      "Epoch: 207, Loss: 2479.95495, Residuals: -0.96046, Convergence: 0.001591\n",
      "Epoch: 208, Loss: 2476.48615, Residuals: -0.95824, Convergence: 0.001401\n",
      "Epoch: 209, Loss: 2473.39833, Residuals: -0.95628, Convergence: 0.001248\n",
      "Epoch: 210, Loss: 2470.62182, Residuals: -0.95456, Convergence: 0.001124\n",
      "Epoch: 211, Loss: 2468.10196, Residuals: -0.95305, Convergence: 0.001021\n",
      "Epoch: 212, Loss: 2465.79731, Residuals: -0.95173, Convergence: 0.000935\n",
      "Evidence 14903.830\n",
      "\n",
      "Epoch: 212, Evidence: 14903.83008, Convergence: 0.005797\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 2.62e-01\n",
      "Epoch: 212, Loss: 2502.11747, Residuals: -0.95173, Convergence:   inf\n",
      "Epoch: 213, Loss: 2498.27423, Residuals: -0.94901, Convergence: 0.001538\n",
      "Epoch: 214, Loss: 2495.05245, Residuals: -0.94675, Convergence: 0.001291\n",
      "Epoch: 215, Loss: 2492.25290, Residuals: -0.94485, Convergence: 0.001123\n",
      "Epoch: 216, Loss: 2489.77290, Residuals: -0.94326, Convergence: 0.000996\n",
      "Evidence 14932.148\n",
      "\n",
      "Epoch: 216, Evidence: 14932.14844, Convergence: 0.001896\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 2.12e-01\n",
      "Epoch: 216, Loss: 2503.30114, Residuals: -0.94326, Convergence:   inf\n",
      "Epoch: 217, Loss: 2500.37748, Residuals: -0.94106, Convergence: 0.001169\n",
      "Epoch: 218, Loss: 2497.88933, Residuals: -0.93929, Convergence: 0.000996\n",
      "Evidence 14944.077\n",
      "\n",
      "Epoch: 218, Evidence: 14944.07715, Convergence: 0.000798\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.76e-01\n",
      "Epoch: 218, Loss: 2504.15318, Residuals: -0.93929, Convergence:   inf\n",
      "Epoch: 219, Loss: 2499.40622, Residuals: -0.93619, Convergence: 0.001899\n",
      "Epoch: 220, Loss: 2495.75801, Residuals: -0.93402, Convergence: 0.001462\n",
      "Epoch: 221, Loss: 2492.77155, Residuals: -0.93257, Convergence: 0.001198\n",
      "Epoch: 222, Loss: 2490.23073, Residuals: -0.93177, Convergence: 0.001020\n",
      "Epoch: 223, Loss: 2488.00414, Residuals: -0.93141, Convergence: 0.000895\n",
      "Evidence 14964.938\n",
      "\n",
      "Epoch: 223, Evidence: 14964.93750, Convergence: 0.002191\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.45e-01\n",
      "Epoch: 223, Loss: 2504.25375, Residuals: -0.93141, Convergence:   inf\n",
      "Epoch: 224, Loss: 2501.27329, Residuals: -0.92914, Convergence: 0.001192\n",
      "Epoch: 225, Loss: 2498.88491, Residuals: -0.92807, Convergence: 0.000956\n",
      "Evidence 14976.500\n",
      "\n",
      "Epoch: 225, Evidence: 14976.50000, Convergence: 0.000772\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.23e-01\n",
      "Epoch: 225, Loss: 2504.55037, Residuals: -0.92807, Convergence:   inf\n",
      "Epoch: 226, Loss: 2500.18259, Residuals: -0.92528, Convergence: 0.001747\n",
      "Epoch: 227, Loss: 2496.97875, Residuals: -0.92676, Convergence: 0.001283\n",
      "Epoch: 228, Loss: 2494.35957, Residuals: -0.92806, Convergence: 0.001050\n",
      "Epoch: 229, Loss: 2492.09892, Residuals: -0.93102, Convergence: 0.000907\n",
      "Evidence 14992.099\n",
      "\n",
      "Epoch: 229, Evidence: 14992.09863, Convergence: 0.001812\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.10e-01\n",
      "Epoch: 229, Loss: 2503.98718, Residuals: -0.93102, Convergence:   inf\n",
      "Epoch: 230, Loss: 2502.49393, Residuals: -0.93041, Convergence: 0.000597\n",
      "Evidence 14998.355\n",
      "\n",
      "Epoch: 230, Evidence: 14998.35547, Convergence: 0.000417\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 8.90e-02\n",
      "Epoch: 230, Loss: 2504.98053, Residuals: -0.93041, Convergence:   inf\n",
      "Epoch: 231, Loss: 2550.06925, Residuals: -0.97223, Convergence: -0.017681\n",
      "Epoch: 231, Loss: 2502.64053, Residuals: -0.92834, Convergence: 0.000935\n",
      "Evidence 15002.801\n",
      "\n",
      "Epoch: 231, Evidence: 15002.80078, Convergence: 0.000713\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 8.19e-02\n",
      "Epoch: 231, Loss: 2504.43126, Residuals: -0.92834, Convergence:   inf\n",
      "Epoch: 232, Loss: 2510.54146, Residuals: -0.93138, Convergence: -0.002434\n",
      "Epoch: 232, Loss: 2504.78534, Residuals: -0.92775, Convergence: -0.000141\n",
      "Evidence 15003.943\n",
      "\n",
      "Epoch: 232, Evidence: 15003.94336, Convergence: 0.000789\n",
      "Total samples: 184, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 382.90497, Residuals: -4.53054, Convergence:   inf\n",
      "Epoch: 1, Loss: 357.15377, Residuals: -4.40959, Convergence: 0.072101\n",
      "Epoch: 2, Loss: 336.15526, Residuals: -4.24642, Convergence: 0.062467\n",
      "Epoch: 3, Loss: 320.10328, Residuals: -4.08283, Convergence: 0.050146\n",
      "Epoch: 4, Loss: 307.84759, Residuals: -3.93868, Convergence: 0.039811\n",
      "Epoch: 5, Loss: 298.12307, Residuals: -3.81131, Convergence: 0.032619\n",
      "Epoch: 6, Loss: 290.22646, Residuals: -3.70048, Convergence: 0.027208\n",
      "Epoch: 7, Loss: 283.66862, Residuals: -3.60567, Convergence: 0.023118\n",
      "Epoch: 8, Loss: 278.08737, Residuals: -3.52474, Convergence: 0.020070\n",
      "Epoch: 9, Loss: 273.22691, Residuals: -3.45533, Convergence: 0.017789\n",
      "Epoch: 10, Loss: 268.90464, Residuals: -3.39531, Convergence: 0.016074\n",
      "Epoch: 11, Loss: 264.98746, Residuals: -3.34287, Convergence: 0.014783\n",
      "Epoch: 12, Loss: 261.37744, Residuals: -3.29648, Convergence: 0.013812\n",
      "Epoch: 13, Loss: 258.00321, Residuals: -3.25482, Convergence: 0.013078\n",
      "Epoch: 14, Loss: 254.81512, Residuals: -3.21674, Convergence: 0.012511\n",
      "Epoch: 15, Loss: 251.78333, Residuals: -3.18134, Convergence: 0.012041\n",
      "Epoch: 16, Loss: 248.89423, Residuals: -3.14804, Convergence: 0.011608\n",
      "Epoch: 17, Loss: 246.13707, Residuals: -3.11644, Convergence: 0.011202\n",
      "Epoch: 18, Loss: 243.48970, Residuals: -3.08611, Convergence: 0.010873\n",
      "Epoch: 19, Loss: 240.91835, Residuals: -3.05645, Convergence: 0.010673\n",
      "Epoch: 20, Loss: 238.38657, Residuals: -3.02686, Convergence: 0.010621\n",
      "Epoch: 21, Loss: 235.86401, Residuals: -2.99685, Convergence: 0.010695\n",
      "Epoch: 22, Loss: 233.32788, Residuals: -2.96610, Convergence: 0.010869\n",
      "Epoch: 23, Loss: 230.74967, Residuals: -2.93428, Convergence: 0.011173\n",
      "Epoch: 24, Loss: 228.07891, Residuals: -2.90085, Convergence: 0.011710\n",
      "Epoch: 25, Loss: 225.26031, Residuals: -2.86514, Convergence: 0.012513\n",
      "Epoch: 26, Loss: 222.32443, Residuals: -2.82737, Convergence: 0.013205\n",
      "Epoch: 27, Loss: 219.41687, Residuals: -2.78911, Convergence: 0.013251\n",
      "Epoch: 28, Loss: 216.62514, Residuals: -2.75145, Convergence: 0.012887\n",
      "Epoch: 29, Loss: 213.95019, Residuals: -2.71457, Convergence: 0.012503\n",
      "Epoch: 30, Loss: 211.37160, Residuals: -2.67836, Convergence: 0.012199\n",
      "Epoch: 31, Loss: 208.87090, Residuals: -2.64268, Convergence: 0.011972\n",
      "Epoch: 32, Loss: 206.43486, Residuals: -2.60742, Convergence: 0.011801\n",
      "Epoch: 33, Loss: 204.05477, Residuals: -2.57251, Convergence: 0.011664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34, Loss: 201.72533, Residuals: -2.53789, Convergence: 0.011548\n",
      "Epoch: 35, Loss: 199.44364, Residuals: -2.50351, Convergence: 0.011440\n",
      "Epoch: 36, Loss: 197.20837, Residuals: -2.46935, Convergence: 0.011335\n",
      "Epoch: 37, Loss: 195.01912, Residuals: -2.43537, Convergence: 0.011226\n",
      "Epoch: 38, Loss: 192.87599, Residuals: -2.40157, Convergence: 0.011111\n",
      "Epoch: 39, Loss: 190.77922, Residuals: -2.36792, Convergence: 0.010991\n",
      "Epoch: 40, Loss: 188.72899, Residuals: -2.33441, Convergence: 0.010863\n",
      "Epoch: 41, Loss: 186.72546, Residuals: -2.30103, Convergence: 0.010730\n",
      "Epoch: 42, Loss: 184.76888, Residuals: -2.26777, Convergence: 0.010589\n",
      "Epoch: 43, Loss: 182.85983, Residuals: -2.23464, Convergence: 0.010440\n",
      "Epoch: 44, Loss: 180.99951, Residuals: -2.20166, Convergence: 0.010278\n",
      "Epoch: 45, Loss: 179.18988, Residuals: -2.16884, Convergence: 0.010099\n",
      "Epoch: 46, Loss: 177.43363, Residuals: -2.13624, Convergence: 0.009898\n",
      "Epoch: 47, Loss: 175.73379, Residuals: -2.10392, Convergence: 0.009673\n",
      "Epoch: 48, Loss: 174.09328, Residuals: -2.07194, Convergence: 0.009423\n",
      "Epoch: 49, Loss: 172.51446, Residuals: -2.04038, Convergence: 0.009152\n",
      "Epoch: 50, Loss: 170.99885, Residuals: -2.00929, Convergence: 0.008863\n",
      "Epoch: 51, Loss: 169.54705, Residuals: -1.97873, Convergence: 0.008563\n",
      "Epoch: 52, Loss: 168.15884, Residuals: -1.94876, Convergence: 0.008255\n",
      "Epoch: 53, Loss: 166.83331, Residuals: -1.91942, Convergence: 0.007945\n",
      "Epoch: 54, Loss: 165.56905, Residuals: -1.89072, Convergence: 0.007636\n",
      "Epoch: 55, Loss: 164.36426, Residuals: -1.86271, Convergence: 0.007330\n",
      "Epoch: 56, Loss: 163.21692, Residuals: -1.83541, Convergence: 0.007030\n",
      "Epoch: 57, Loss: 162.12485, Residuals: -1.80882, Convergence: 0.006736\n",
      "Epoch: 58, Loss: 161.08581, Residuals: -1.78296, Convergence: 0.006450\n",
      "Epoch: 59, Loss: 160.09751, Residuals: -1.75784, Convergence: 0.006173\n",
      "Epoch: 60, Loss: 159.15771, Residuals: -1.73346, Convergence: 0.005905\n",
      "Epoch: 61, Loss: 158.26422, Residuals: -1.70982, Convergence: 0.005646\n",
      "Epoch: 62, Loss: 157.41491, Residuals: -1.68693, Convergence: 0.005395\n",
      "Epoch: 63, Loss: 156.60777, Residuals: -1.66477, Convergence: 0.005154\n",
      "Epoch: 64, Loss: 155.84086, Residuals: -1.64335, Convergence: 0.004921\n",
      "Epoch: 65, Loss: 155.11238, Residuals: -1.62266, Convergence: 0.004696\n",
      "Epoch: 66, Loss: 154.42061, Residuals: -1.60270, Convergence: 0.004480\n",
      "Epoch: 67, Loss: 153.76395, Residuals: -1.58345, Convergence: 0.004271\n",
      "Epoch: 68, Loss: 153.14086, Residuals: -1.56492, Convergence: 0.004069\n",
      "Epoch: 69, Loss: 152.54989, Residuals: -1.54709, Convergence: 0.003874\n",
      "Epoch: 70, Loss: 151.98963, Residuals: -1.52996, Convergence: 0.003686\n",
      "Epoch: 71, Loss: 151.45867, Residuals: -1.51351, Convergence: 0.003506\n",
      "Epoch: 72, Loss: 150.95556, Residuals: -1.49774, Convergence: 0.003333\n",
      "Epoch: 73, Loss: 150.47882, Residuals: -1.48262, Convergence: 0.003168\n",
      "Epoch: 74, Loss: 150.02684, Residuals: -1.46815, Convergence: 0.003013\n",
      "Epoch: 75, Loss: 149.59794, Residuals: -1.45430, Convergence: 0.002867\n",
      "Epoch: 76, Loss: 149.19034, Residuals: -1.44105, Convergence: 0.002732\n",
      "Epoch: 77, Loss: 148.80222, Residuals: -1.42836, Convergence: 0.002608\n",
      "Epoch: 78, Loss: 148.43179, Residuals: -1.41620, Convergence: 0.002496\n",
      "Epoch: 79, Loss: 148.07736, Residuals: -1.40454, Convergence: 0.002394\n",
      "Epoch: 80, Loss: 147.73742, Residuals: -1.39335, Convergence: 0.002301\n",
      "Epoch: 81, Loss: 147.41073, Residuals: -1.38258, Convergence: 0.002216\n",
      "Epoch: 82, Loss: 147.09625, Residuals: -1.37223, Convergence: 0.002138\n",
      "Epoch: 83, Loss: 146.79320, Residuals: -1.36225, Convergence: 0.002064\n",
      "Epoch: 84, Loss: 146.50103, Residuals: -1.35264, Convergence: 0.001994\n",
      "Epoch: 85, Loss: 146.21931, Residuals: -1.34337, Convergence: 0.001927\n",
      "Epoch: 86, Loss: 145.94774, Residuals: -1.33444, Convergence: 0.001861\n",
      "Epoch: 87, Loss: 145.68611, Residuals: -1.32583, Convergence: 0.001796\n",
      "Epoch: 88, Loss: 145.43421, Residuals: -1.31754, Convergence: 0.001732\n",
      "Epoch: 89, Loss: 145.19190, Residuals: -1.30955, Convergence: 0.001669\n",
      "Epoch: 90, Loss: 144.95900, Residuals: -1.30186, Convergence: 0.001607\n",
      "Epoch: 91, Loss: 144.73535, Residuals: -1.29446, Convergence: 0.001545\n",
      "Epoch: 92, Loss: 144.52077, Residuals: -1.28734, Convergence: 0.001485\n",
      "Epoch: 93, Loss: 144.31509, Residuals: -1.28049, Convergence: 0.001425\n",
      "Epoch: 94, Loss: 144.11811, Residuals: -1.27392, Convergence: 0.001367\n",
      "Epoch: 95, Loss: 143.92964, Residuals: -1.26760, Convergence: 0.001309\n",
      "Epoch: 96, Loss: 143.74948, Residuals: -1.26153, Convergence: 0.001253\n",
      "Epoch: 97, Loss: 143.57744, Residuals: -1.25570, Convergence: 0.001198\n",
      "Epoch: 98, Loss: 143.41333, Residuals: -1.25011, Convergence: 0.001144\n",
      "Epoch: 99, Loss: 143.25696, Residuals: -1.24475, Convergence: 0.001091\n",
      "Epoch: 100, Loss: 143.10818, Residuals: -1.23962, Convergence: 0.001040\n",
      "Epoch: 101, Loss: 142.96681, Residuals: -1.23470, Convergence: 0.000989\n",
      "Evidence -184.573\n",
      "\n",
      "Epoch: 101, Evidence: -184.57307, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 7.25e-01\n",
      "Epoch: 101, Loss: 1395.44588, Residuals: -1.23470, Convergence:   inf\n",
      "Epoch: 102, Loss: 1330.67639, Residuals: -1.26634, Convergence: 0.048674\n",
      "Epoch: 103, Loss: 1281.61222, Residuals: -1.29084, Convergence: 0.038283\n",
      "Epoch: 104, Loss: 1244.71938, Residuals: -1.30784, Convergence: 0.029639\n",
      "Epoch: 105, Loss: 1216.03926, Residuals: -1.31931, Convergence: 0.023585\n",
      "Epoch: 106, Loss: 1192.80472, Residuals: -1.32756, Convergence: 0.019479\n",
      "Epoch: 107, Loss: 1173.47935, Residuals: -1.33372, Convergence: 0.016468\n",
      "Epoch: 108, Loss: 1157.14705, Residuals: -1.33826, Convergence: 0.014114\n",
      "Epoch: 109, Loss: 1143.17852, Residuals: -1.34142, Convergence: 0.012219\n",
      "Epoch: 110, Loss: 1131.10177, Residuals: -1.34336, Convergence: 0.010677\n",
      "Epoch: 111, Loss: 1120.54423, Residuals: -1.34418, Convergence: 0.009422\n",
      "Epoch: 112, Loss: 1111.20574, Residuals: -1.34401, Convergence: 0.008404\n",
      "Epoch: 113, Loss: 1102.83744, Residuals: -1.34292, Convergence: 0.007588\n",
      "Epoch: 114, Loss: 1095.23315, Residuals: -1.34098, Convergence: 0.006943\n",
      "Epoch: 115, Loss: 1088.21845, Residuals: -1.33826, Convergence: 0.006446\n",
      "Epoch: 116, Loss: 1081.64521, Residuals: -1.33479, Convergence: 0.006077\n",
      "Epoch: 117, Loss: 1075.38719, Residuals: -1.33060, Convergence: 0.005819\n",
      "Epoch: 118, Loss: 1069.33579, Residuals: -1.32572, Convergence: 0.005659\n",
      "Epoch: 119, Loss: 1063.40134, Residuals: -1.32018, Convergence: 0.005581\n",
      "Epoch: 120, Loss: 1057.51896, Residuals: -1.31402, Convergence: 0.005562\n",
      "Epoch: 121, Loss: 1051.65848, Residuals: -1.30729, Convergence: 0.005573\n",
      "Epoch: 122, Loss: 1045.83693, Residuals: -1.30011, Convergence: 0.005566\n",
      "Epoch: 123, Loss: 1040.11666, Residuals: -1.29256, Convergence: 0.005500\n",
      "Epoch: 124, Loss: 1034.58381, Residuals: -1.28476, Convergence: 0.005348\n",
      "Epoch: 125, Loss: 1029.31621, Residuals: -1.27680, Convergence: 0.005118\n",
      "Epoch: 126, Loss: 1024.36092, Residuals: -1.26875, Convergence: 0.004837\n",
      "Epoch: 127, Loss: 1019.73132, Residuals: -1.26067, Convergence: 0.004540\n",
      "Epoch: 128, Loss: 1015.41517, Residuals: -1.25262, Convergence: 0.004251\n",
      "Epoch: 129, Loss: 1011.38893, Residuals: -1.24466, Convergence: 0.003981\n",
      "Epoch: 130, Loss: 1007.62278, Residuals: -1.23681, Convergence: 0.003738\n",
      "Epoch: 131, Loss: 1004.09020, Residuals: -1.22912, Convergence: 0.003518\n",
      "Epoch: 132, Loss: 1000.76656, Residuals: -1.22160, Convergence: 0.003321\n",
      "Epoch: 133, Loss: 997.63150, Residuals: -1.21429, Convergence: 0.003142\n",
      "Epoch: 134, Loss: 994.66779, Residuals: -1.20719, Convergence: 0.002980\n",
      "Epoch: 135, Loss: 991.86154, Residuals: -1.20033, Convergence: 0.002829\n",
      "Epoch: 136, Loss: 989.20049, Residuals: -1.19371, Convergence: 0.002690\n",
      "Epoch: 137, Loss: 986.67518, Residuals: -1.18735, Convergence: 0.002559\n",
      "Epoch: 138, Loss: 984.27589, Residuals: -1.18123, Convergence: 0.002438\n",
      "Epoch: 139, Loss: 981.99461, Residuals: -1.17537, Convergence: 0.002323\n",
      "Epoch: 140, Loss: 979.82382, Residuals: -1.16977, Convergence: 0.002215\n",
      "Epoch: 141, Loss: 977.75639, Residuals: -1.16441, Convergence: 0.002114\n",
      "Epoch: 142, Loss: 975.78539, Residuals: -1.15929, Convergence: 0.002020\n",
      "Epoch: 143, Loss: 973.90426, Residuals: -1.15441, Convergence: 0.001932\n",
      "Epoch: 144, Loss: 972.10669, Residuals: -1.14976, Convergence: 0.001849\n",
      "Epoch: 145, Loss: 970.38679, Residuals: -1.14532, Convergence: 0.001772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 146, Loss: 968.73858, Residuals: -1.14109, Convergence: 0.001701\n",
      "Epoch: 147, Loss: 967.15707, Residuals: -1.13705, Convergence: 0.001635\n",
      "Epoch: 148, Loss: 965.63737, Residuals: -1.13320, Convergence: 0.001574\n",
      "Epoch: 149, Loss: 964.17409, Residuals: -1.12952, Convergence: 0.001518\n",
      "Epoch: 150, Loss: 962.76326, Residuals: -1.12600, Convergence: 0.001465\n",
      "Epoch: 151, Loss: 961.40022, Residuals: -1.12263, Convergence: 0.001418\n",
      "Epoch: 152, Loss: 960.08066, Residuals: -1.11941, Convergence: 0.001374\n",
      "Epoch: 153, Loss: 958.80071, Residuals: -1.11632, Convergence: 0.001335\n",
      "Epoch: 154, Loss: 957.55615, Residuals: -1.11334, Convergence: 0.001300\n",
      "Epoch: 155, Loss: 956.34225, Residuals: -1.11047, Convergence: 0.001269\n",
      "Epoch: 156, Loss: 955.15560, Residuals: -1.10770, Convergence: 0.001242\n",
      "Epoch: 157, Loss: 953.99104, Residuals: -1.10502, Convergence: 0.001221\n",
      "Epoch: 158, Loss: 952.84430, Residuals: -1.10240, Convergence: 0.001203\n",
      "Epoch: 159, Loss: 951.71153, Residuals: -1.09985, Convergence: 0.001190\n",
      "Epoch: 160, Loss: 950.58878, Residuals: -1.09736, Convergence: 0.001181\n",
      "Epoch: 161, Loss: 949.47325, Residuals: -1.09490, Convergence: 0.001175\n",
      "Epoch: 162, Loss: 948.36322, Residuals: -1.09248, Convergence: 0.001170\n",
      "Epoch: 163, Loss: 947.25887, Residuals: -1.09008, Convergence: 0.001166\n",
      "Epoch: 164, Loss: 946.16195, Residuals: -1.08772, Convergence: 0.001159\n",
      "Epoch: 165, Loss: 945.07576, Residuals: -1.08539, Convergence: 0.001149\n",
      "Epoch: 166, Loss: 944.00497, Residuals: -1.08310, Convergence: 0.001134\n",
      "Epoch: 167, Loss: 942.95486, Residuals: -1.08086, Convergence: 0.001114\n",
      "Epoch: 168, Loss: 941.93052, Residuals: -1.07867, Convergence: 0.001087\n",
      "Epoch: 169, Loss: 940.93663, Residuals: -1.07655, Convergence: 0.001056\n",
      "Epoch: 170, Loss: 939.97663, Residuals: -1.07449, Convergence: 0.001021\n",
      "Epoch: 171, Loss: 939.05330, Residuals: -1.07251, Convergence: 0.000983\n",
      "Evidence 11321.559\n",
      "\n",
      "Epoch: 171, Evidence: 11321.55859, Convergence: 1.016303\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 5.74e-01\n",
      "Epoch: 171, Loss: 2375.32420, Residuals: -1.07251, Convergence:   inf\n",
      "Epoch: 172, Loss: 2335.76943, Residuals: -1.08099, Convergence: 0.016934\n",
      "Epoch: 173, Loss: 2308.67195, Residuals: -1.07945, Convergence: 0.011737\n",
      "Epoch: 174, Loss: 2286.24949, Residuals: -1.07715, Convergence: 0.009808\n",
      "Epoch: 175, Loss: 2267.42101, Residuals: -1.07469, Convergence: 0.008304\n",
      "Epoch: 176, Loss: 2251.45990, Residuals: -1.07215, Convergence: 0.007089\n",
      "Epoch: 177, Loss: 2237.80375, Residuals: -1.06957, Convergence: 0.006102\n",
      "Epoch: 178, Loss: 2225.99397, Residuals: -1.06694, Convergence: 0.005305\n",
      "Epoch: 179, Loss: 2215.65336, Residuals: -1.06427, Convergence: 0.004667\n",
      "Epoch: 180, Loss: 2206.47283, Residuals: -1.06151, Convergence: 0.004161\n",
      "Epoch: 181, Loss: 2198.20681, Residuals: -1.05866, Convergence: 0.003760\n",
      "Epoch: 182, Loss: 2190.67363, Residuals: -1.05569, Convergence: 0.003439\n",
      "Epoch: 183, Loss: 2183.74947, Residuals: -1.05263, Convergence: 0.003171\n",
      "Epoch: 184, Loss: 2177.35937, Residuals: -1.04948, Convergence: 0.002935\n",
      "Epoch: 185, Loss: 2171.45644, Residuals: -1.04630, Convergence: 0.002718\n",
      "Epoch: 186, Loss: 2166.00384, Residuals: -1.04314, Convergence: 0.002517\n",
      "Epoch: 187, Loss: 2160.96913, Residuals: -1.04003, Convergence: 0.002330\n",
      "Epoch: 188, Loss: 2156.32073, Residuals: -1.03700, Convergence: 0.002156\n",
      "Epoch: 189, Loss: 2152.02536, Residuals: -1.03408, Convergence: 0.001996\n",
      "Epoch: 190, Loss: 2148.05336, Residuals: -1.03128, Convergence: 0.001849\n",
      "Epoch: 191, Loss: 2144.37702, Residuals: -1.02861, Convergence: 0.001714\n",
      "Epoch: 192, Loss: 2140.96977, Residuals: -1.02606, Convergence: 0.001591\n",
      "Epoch: 193, Loss: 2137.80923, Residuals: -1.02364, Convergence: 0.001478\n",
      "Epoch: 194, Loss: 2134.87340, Residuals: -1.02134, Convergence: 0.001375\n",
      "Epoch: 195, Loss: 2132.14244, Residuals: -1.01916, Convergence: 0.001281\n",
      "Epoch: 196, Loss: 2129.59899, Residuals: -1.01709, Convergence: 0.001194\n",
      "Epoch: 197, Loss: 2127.22626, Residuals: -1.01512, Convergence: 0.001115\n",
      "Epoch: 198, Loss: 2125.00969, Residuals: -1.01324, Convergence: 0.001043\n",
      "Epoch: 199, Loss: 2122.93588, Residuals: -1.01145, Convergence: 0.000977\n",
      "Evidence 14470.578\n",
      "\n",
      "Epoch: 199, Evidence: 14470.57812, Convergence: 0.217615\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 4.36e-01\n",
      "Epoch: 199, Loss: 2496.63720, Residuals: -1.01145, Convergence:   inf\n",
      "Epoch: 200, Loss: 2483.06982, Residuals: -1.00854, Convergence: 0.005464\n",
      "Epoch: 201, Loss: 2471.98087, Residuals: -1.00519, Convergence: 0.004486\n",
      "Epoch: 202, Loss: 2462.45107, Residuals: -1.00196, Convergence: 0.003870\n",
      "Epoch: 203, Loss: 2454.20150, Residuals: -0.99896, Convergence: 0.003361\n",
      "Epoch: 204, Loss: 2447.02431, Residuals: -0.99619, Convergence: 0.002933\n",
      "Epoch: 205, Loss: 2440.75396, Residuals: -0.99366, Convergence: 0.002569\n",
      "Epoch: 206, Loss: 2435.25022, Residuals: -0.99134, Convergence: 0.002260\n",
      "Epoch: 207, Loss: 2430.39488, Residuals: -0.98922, Convergence: 0.001998\n",
      "Epoch: 208, Loss: 2426.08777, Residuals: -0.98729, Convergence: 0.001775\n",
      "Epoch: 209, Loss: 2422.24673, Residuals: -0.98552, Convergence: 0.001586\n",
      "Epoch: 210, Loss: 2418.79996, Residuals: -0.98392, Convergence: 0.001425\n",
      "Epoch: 211, Loss: 2415.68930, Residuals: -0.98245, Convergence: 0.001288\n",
      "Epoch: 212, Loss: 2412.86630, Residuals: -0.98111, Convergence: 0.001170\n",
      "Epoch: 213, Loss: 2410.29066, Residuals: -0.97989, Convergence: 0.001069\n",
      "Epoch: 214, Loss: 2407.92891, Residuals: -0.97877, Convergence: 0.000981\n",
      "Evidence 14841.622\n",
      "\n",
      "Epoch: 214, Evidence: 14841.62207, Convergence: 0.025000\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 3.33e-01\n",
      "Epoch: 214, Loss: 2501.56588, Residuals: -0.97877, Convergence:   inf\n",
      "Epoch: 215, Loss: 2494.88638, Residuals: -0.97561, Convergence: 0.002677\n",
      "Epoch: 216, Loss: 2489.36545, Residuals: -0.97289, Convergence: 0.002218\n",
      "Epoch: 217, Loss: 2484.68667, Residuals: -0.97060, Convergence: 0.001883\n",
      "Epoch: 218, Loss: 2480.67415, Residuals: -0.96868, Convergence: 0.001618\n",
      "Epoch: 219, Loss: 2477.19174, Residuals: -0.96706, Convergence: 0.001406\n",
      "Epoch: 220, Loss: 2474.13564, Residuals: -0.96570, Convergence: 0.001235\n",
      "Epoch: 221, Loss: 2471.42303, Residuals: -0.96454, Convergence: 0.001098\n",
      "Epoch: 222, Loss: 2468.99071, Residuals: -0.96356, Convergence: 0.000985\n",
      "Evidence 14923.510\n",
      "\n",
      "Epoch: 222, Evidence: 14923.50977, Convergence: 0.005487\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 2.60e-01\n",
      "Epoch: 222, Loss: 2502.98640, Residuals: -0.96356, Convergence:   inf\n",
      "Epoch: 223, Loss: 2498.87007, Residuals: -0.96127, Convergence: 0.001647\n",
      "Epoch: 224, Loss: 2495.46526, Residuals: -0.95946, Convergence: 0.001364\n",
      "Epoch: 225, Loss: 2492.57215, Residuals: -0.95803, Convergence: 0.001161\n",
      "Epoch: 226, Loss: 2490.06905, Residuals: -0.95688, Convergence: 0.001005\n",
      "Epoch: 227, Loss: 2487.86822, Residuals: -0.95595, Convergence: 0.000885\n",
      "Evidence 14954.991\n",
      "\n",
      "Epoch: 227, Evidence: 14954.99121, Convergence: 0.002105\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 2.08e-01\n",
      "Epoch: 227, Loss: 2503.76379, Residuals: -0.95595, Convergence:   inf\n",
      "Epoch: 228, Loss: 2500.85431, Residuals: -0.95429, Convergence: 0.001163\n",
      "Epoch: 229, Loss: 2498.43244, Residuals: -0.95301, Convergence: 0.000969\n",
      "Evidence 14968.169\n",
      "\n",
      "Epoch: 229, Evidence: 14968.16895, Convergence: 0.000880\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.72e-01\n",
      "Epoch: 229, Loss: 2504.40961, Residuals: -0.95301, Convergence:   inf\n",
      "Epoch: 230, Loss: 2499.90768, Residuals: -0.95103, Convergence: 0.001801\n",
      "Epoch: 231, Loss: 2496.46878, Residuals: -0.94947, Convergence: 0.001378\n",
      "Epoch: 232, Loss: 2493.71271, Residuals: -0.94842, Convergence: 0.001105\n",
      "Epoch: 233, Loss: 2491.39232, Residuals: -0.94783, Convergence: 0.000931\n",
      "Evidence 14985.898\n",
      "\n",
      "Epoch: 233, Evidence: 14985.89844, Convergence: 0.002062\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.42e-01\n",
      "Epoch: 233, Loss: 2504.49990, Residuals: -0.94783, Convergence:   inf\n",
      "Epoch: 234, Loss: 2501.50833, Residuals: -0.94575, Convergence: 0.001196\n",
      "Epoch: 235, Loss: 2499.16779, Residuals: -0.94468, Convergence: 0.000937\n",
      "Evidence 14997.004\n",
      "\n",
      "Epoch: 235, Evidence: 14997.00391, Convergence: 0.000741\n",
      "Updating hyper-parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 184, Updated regularization: 1.20e-01\n",
      "Epoch: 235, Loss: 2504.66647, Residuals: -0.94468, Convergence:   inf\n",
      "Epoch: 236, Loss: 2500.33589, Residuals: -0.94185, Convergence: 0.001732\n",
      "Epoch: 237, Loss: 2497.29001, Residuals: -0.94293, Convergence: 0.001220\n",
      "Epoch: 238, Loss: 2494.72656, Residuals: -0.94334, Convergence: 0.001028\n",
      "Epoch: 239, Loss: 2492.52008, Residuals: -0.94591, Convergence: 0.000885\n",
      "Evidence 15012.475\n",
      "\n",
      "Epoch: 239, Evidence: 15012.47461, Convergence: 0.001770\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.07e-01\n",
      "Epoch: 239, Loss: 2503.93399, Residuals: -0.94591, Convergence:   inf\n",
      "Epoch: 240, Loss: 2502.34268, Residuals: -0.94466, Convergence: 0.000636\n",
      "Evidence 15019.065\n",
      "\n",
      "Epoch: 240, Evidence: 15019.06543, Convergence: 0.000439\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 8.72e-02\n",
      "Epoch: 240, Loss: 2504.88961, Residuals: -0.94466, Convergence:   inf\n",
      "Epoch: 241, Loss: 2552.15759, Residuals: -0.99307, Convergence: -0.018521\n",
      "Epoch: 241, Loss: 2502.63724, Residuals: -0.94351, Convergence: 0.000900\n",
      "Evidence 15023.547\n",
      "\n",
      "Epoch: 241, Evidence: 15023.54688, Convergence: 0.000737\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 8.03e-02\n",
      "Epoch: 241, Loss: 2504.30633, Residuals: -0.94351, Convergence:   inf\n",
      "Epoch: 242, Loss: 2509.27011, Residuals: -0.94776, Convergence: -0.001978\n",
      "Epoch: 242, Loss: 2504.33799, Residuals: -0.94286, Convergence: -0.000013\n",
      "Evidence 15025.213\n",
      "\n",
      "Epoch: 242, Evidence: 15025.21289, Convergence: 0.000848\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 384.29119, Residuals: -4.56857, Convergence:   inf\n",
      "Epoch: 1, Loss: 358.43553, Residuals: -4.44669, Convergence: 0.072135\n",
      "Epoch: 2, Loss: 337.29213, Residuals: -4.28153, Convergence: 0.062686\n",
      "Epoch: 3, Loss: 321.13494, Residuals: -4.11625, Convergence: 0.050313\n",
      "Epoch: 4, Loss: 308.80309, Residuals: -3.97066, Convergence: 0.039934\n",
      "Epoch: 5, Loss: 299.01619, Residuals: -3.84208, Convergence: 0.032730\n",
      "Epoch: 6, Loss: 291.06552, Residuals: -3.73018, Convergence: 0.027316\n",
      "Epoch: 7, Loss: 284.46180, Residuals: -3.63436, Convergence: 0.023215\n",
      "Epoch: 8, Loss: 278.84258, Residuals: -3.55252, Convergence: 0.020152\n",
      "Epoch: 9, Loss: 273.95135, Residuals: -3.48227, Convergence: 0.017854\n",
      "Epoch: 10, Loss: 269.60515, Residuals: -3.42148, Convergence: 0.016121\n",
      "Epoch: 11, Loss: 265.67097, Residuals: -3.36834, Convergence: 0.014808\n",
      "Epoch: 12, Loss: 262.05132, Residuals: -3.32133, Convergence: 0.013813\n",
      "Epoch: 13, Loss: 258.67525, Residuals: -3.27913, Convergence: 0.013051\n",
      "Epoch: 14, Loss: 255.49244, Residuals: -3.24060, Convergence: 0.012458\n",
      "Epoch: 15, Loss: 252.47018, Residuals: -3.20482, Convergence: 0.011971\n",
      "Epoch: 16, Loss: 249.59082, Residuals: -3.17115, Convergence: 0.011536\n",
      "Epoch: 17, Loss: 246.84205, Residuals: -3.13920, Convergence: 0.011136\n",
      "Epoch: 18, Loss: 244.20315, Residuals: -3.10857, Convergence: 0.010806\n",
      "Epoch: 19, Loss: 241.64190, Residuals: -3.07872, Convergence: 0.010599\n",
      "Epoch: 20, Loss: 239.12204, Residuals: -3.04907, Convergence: 0.010538\n",
      "Epoch: 21, Loss: 236.61262, Residuals: -3.01910, Convergence: 0.010606\n",
      "Epoch: 22, Loss: 234.09147, Residuals: -2.98848, Convergence: 0.010770\n",
      "Epoch: 23, Loss: 231.53389, Residuals: -2.95692, Convergence: 0.011046\n",
      "Epoch: 24, Loss: 228.89459, Residuals: -2.92391, Convergence: 0.011531\n",
      "Epoch: 25, Loss: 226.11375, Residuals: -2.88875, Convergence: 0.012298\n",
      "Epoch: 26, Loss: 223.19116, Residuals: -2.85134, Convergence: 0.013095\n",
      "Epoch: 27, Loss: 220.25622, Residuals: -2.81300, Convergence: 0.013325\n",
      "Epoch: 28, Loss: 217.42129, Residuals: -2.77509, Convergence: 0.013039\n",
      "Epoch: 29, Loss: 214.70137, Residuals: -2.73791, Convergence: 0.012668\n",
      "Epoch: 30, Loss: 212.07826, Residuals: -2.70141, Convergence: 0.012369\n",
      "Epoch: 31, Loss: 209.53299, Residuals: -2.66541, Convergence: 0.012147\n",
      "Epoch: 32, Loss: 207.05173, Residuals: -2.62982, Convergence: 0.011984\n",
      "Epoch: 33, Loss: 204.62555, Residuals: -2.59452, Convergence: 0.011857\n",
      "Epoch: 34, Loss: 202.24935, Residuals: -2.55945, Convergence: 0.011749\n",
      "Epoch: 35, Loss: 199.92075, Residuals: -2.52456, Convergence: 0.011648\n",
      "Epoch: 36, Loss: 197.63932, Residuals: -2.48983, Convergence: 0.011543\n",
      "Epoch: 37, Loss: 195.40575, Residuals: -2.45523, Convergence: 0.011430\n",
      "Epoch: 38, Loss: 193.22150, Residuals: -2.42076, Convergence: 0.011304\n",
      "Epoch: 39, Loss: 191.08827, Residuals: -2.38643, Convergence: 0.011164\n",
      "Epoch: 40, Loss: 189.00783, Residuals: -2.35225, Convergence: 0.011007\n",
      "Epoch: 41, Loss: 186.98178, Residuals: -2.31825, Convergence: 0.010836\n",
      "Epoch: 42, Loss: 185.01145, Residuals: -2.28445, Convergence: 0.010650\n",
      "Epoch: 43, Loss: 183.09792, Residuals: -2.25089, Convergence: 0.010451\n",
      "Epoch: 44, Loss: 181.24203, Residuals: -2.21759, Convergence: 0.010240\n",
      "Epoch: 45, Loss: 179.44451, Residuals: -2.18460, Convergence: 0.010017\n",
      "Epoch: 46, Loss: 177.70608, Residuals: -2.15194, Convergence: 0.009783\n",
      "Epoch: 47, Loss: 176.02749, Residuals: -2.11966, Convergence: 0.009536\n",
      "Epoch: 48, Loss: 174.40947, Residuals: -2.08781, Convergence: 0.009277\n",
      "Epoch: 49, Loss: 172.85258, Residuals: -2.05641, Convergence: 0.009007\n",
      "Epoch: 50, Loss: 171.35703, Residuals: -2.02552, Convergence: 0.008728\n",
      "Epoch: 51, Loss: 169.92254, Residuals: -1.99516, Convergence: 0.008442\n",
      "Epoch: 52, Loss: 168.54837, Residuals: -1.96538, Convergence: 0.008153\n",
      "Epoch: 53, Loss: 167.23338, Residuals: -1.93619, Convergence: 0.007863\n",
      "Epoch: 54, Loss: 165.97609, Residuals: -1.90763, Convergence: 0.007575\n",
      "Epoch: 55, Loss: 164.77483, Residuals: -1.87971, Convergence: 0.007290\n",
      "Epoch: 56, Loss: 163.62780, Residuals: -1.85246, Convergence: 0.007010\n",
      "Epoch: 57, Loss: 162.53314, Residuals: -1.82589, Convergence: 0.006735\n",
      "Epoch: 58, Loss: 161.48899, Residuals: -1.80002, Convergence: 0.006466\n",
      "Epoch: 59, Loss: 160.49347, Residuals: -1.77487, Convergence: 0.006203\n",
      "Epoch: 60, Loss: 159.54474, Residuals: -1.75044, Convergence: 0.005946\n",
      "Epoch: 61, Loss: 158.64092, Residuals: -1.72675, Convergence: 0.005697\n",
      "Epoch: 62, Loss: 157.78014, Residuals: -1.70380, Convergence: 0.005456\n",
      "Epoch: 63, Loss: 156.96046, Residuals: -1.68159, Convergence: 0.005222\n",
      "Epoch: 64, Loss: 156.17986, Residuals: -1.66014, Convergence: 0.004998\n",
      "Epoch: 65, Loss: 155.43623, Residuals: -1.63942, Convergence: 0.004784\n",
      "Epoch: 66, Loss: 154.72740, Residuals: -1.61942, Convergence: 0.004581\n",
      "Epoch: 67, Loss: 154.05113, Residuals: -1.60014, Convergence: 0.004390\n",
      "Epoch: 68, Loss: 153.40524, Residuals: -1.58154, Convergence: 0.004210\n",
      "Epoch: 69, Loss: 152.78760, Residuals: -1.56360, Convergence: 0.004042\n",
      "Epoch: 70, Loss: 152.19631, Residuals: -1.54629, Convergence: 0.003885\n",
      "Epoch: 71, Loss: 151.62970, Residuals: -1.52958, Convergence: 0.003737\n",
      "Epoch: 72, Loss: 151.08635, Residuals: -1.51345, Convergence: 0.003596\n",
      "Epoch: 73, Loss: 150.56512, Residuals: -1.49788, Convergence: 0.003462\n",
      "Epoch: 74, Loss: 150.06505, Residuals: -1.48285, Convergence: 0.003332\n",
      "Epoch: 75, Loss: 149.58539, Residuals: -1.46834, Convergence: 0.003207\n",
      "Epoch: 76, Loss: 149.12546, Residuals: -1.45435, Convergence: 0.003084\n",
      "Epoch: 77, Loss: 148.68468, Residuals: -1.44085, Convergence: 0.002965\n",
      "Epoch: 78, Loss: 148.26251, Residuals: -1.42784, Convergence: 0.002847\n",
      "Epoch: 79, Loss: 147.85839, Residuals: -1.41530, Convergence: 0.002733\n",
      "Epoch: 80, Loss: 147.47180, Residuals: -1.40323, Convergence: 0.002621\n",
      "Epoch: 81, Loss: 147.10219, Residuals: -1.39162, Convergence: 0.002513\n",
      "Epoch: 82, Loss: 146.74899, Residuals: -1.38045, Convergence: 0.002407\n",
      "Epoch: 83, Loss: 146.41162, Residuals: -1.36971, Convergence: 0.002304\n",
      "Epoch: 84, Loss: 146.08952, Residuals: -1.35939, Convergence: 0.002205\n",
      "Epoch: 85, Loss: 145.78210, Residuals: -1.34948, Convergence: 0.002109\n",
      "Epoch: 86, Loss: 145.48878, Residuals: -1.33996, Convergence: 0.002016\n",
      "Epoch: 87, Loss: 145.20899, Residuals: -1.33082, Convergence: 0.001927\n",
      "Epoch: 88, Loss: 144.94219, Residuals: -1.32205, Convergence: 0.001841\n",
      "Epoch: 89, Loss: 144.68783, Residuals: -1.31364, Convergence: 0.001758\n",
      "Epoch: 90, Loss: 144.44540, Residuals: -1.30557, Convergence: 0.001678\n",
      "Epoch: 91, Loss: 144.21442, Residuals: -1.29784, Convergence: 0.001602\n",
      "Epoch: 92, Loss: 143.99443, Residuals: -1.29042, Convergence: 0.001528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 93, Loss: 143.78499, Residuals: -1.28330, Convergence: 0.001457\n",
      "Epoch: 94, Loss: 143.58570, Residuals: -1.27648, Convergence: 0.001388\n",
      "Epoch: 95, Loss: 143.39620, Residuals: -1.26995, Convergence: 0.001322\n",
      "Epoch: 96, Loss: 143.21613, Residuals: -1.26368, Convergence: 0.001257\n",
      "Epoch: 97, Loss: 143.04519, Residuals: -1.25768, Convergence: 0.001195\n",
      "Epoch: 98, Loss: 142.88309, Residuals: -1.25193, Convergence: 0.001135\n",
      "Epoch: 99, Loss: 142.72956, Residuals: -1.24642, Convergence: 0.001076\n",
      "Epoch: 100, Loss: 142.58436, Residuals: -1.24115, Convergence: 0.001018\n",
      "Epoch: 101, Loss: 142.44727, Residuals: -1.23611, Convergence: 0.000962\n",
      "Evidence -184.263\n",
      "\n",
      "Epoch: 101, Evidence: -184.26349, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.25e-01\n",
      "Epoch: 101, Loss: 1377.58359, Residuals: -1.23611, Convergence:   inf\n",
      "Epoch: 102, Loss: 1315.07655, Residuals: -1.26701, Convergence: 0.047531\n",
      "Epoch: 103, Loss: 1267.90561, Residuals: -1.29128, Convergence: 0.037204\n",
      "Epoch: 104, Loss: 1232.51444, Residuals: -1.30863, Convergence: 0.028715\n",
      "Epoch: 105, Loss: 1205.06660, Residuals: -1.32083, Convergence: 0.022777\n",
      "Epoch: 106, Loss: 1182.89949, Residuals: -1.32991, Convergence: 0.018740\n",
      "Epoch: 107, Loss: 1164.51976, Residuals: -1.33693, Convergence: 0.015783\n",
      "Epoch: 108, Loss: 1149.03096, Residuals: -1.34234, Convergence: 0.013480\n",
      "Epoch: 109, Loss: 1135.82204, Residuals: -1.34637, Convergence: 0.011629\n",
      "Epoch: 110, Loss: 1124.43634, Residuals: -1.34918, Convergence: 0.010126\n",
      "Epoch: 111, Loss: 1114.51700, Residuals: -1.35088, Convergence: 0.008900\n",
      "Epoch: 112, Loss: 1105.77615, Residuals: -1.35159, Convergence: 0.007905\n",
      "Epoch: 113, Loss: 1097.97748, Residuals: -1.35140, Convergence: 0.007103\n",
      "Epoch: 114, Loss: 1090.92367, Residuals: -1.35037, Convergence: 0.006466\n",
      "Epoch: 115, Loss: 1084.44870, Residuals: -1.34858, Convergence: 0.005971\n",
      "Epoch: 116, Loss: 1078.40978, Residuals: -1.34606, Convergence: 0.005600\n",
      "Epoch: 117, Loss: 1072.68510, Residuals: -1.34285, Convergence: 0.005337\n",
      "Epoch: 118, Loss: 1067.16863, Residuals: -1.33896, Convergence: 0.005169\n",
      "Epoch: 119, Loss: 1061.76893, Residuals: -1.33442, Convergence: 0.005086\n",
      "Epoch: 120, Loss: 1056.40879, Residuals: -1.32923, Convergence: 0.005074\n",
      "Epoch: 121, Loss: 1051.02777, Residuals: -1.32344, Convergence: 0.005120\n",
      "Epoch: 122, Loss: 1045.59332, Residuals: -1.31708, Convergence: 0.005197\n",
      "Epoch: 123, Loss: 1040.11491, Residuals: -1.31023, Convergence: 0.005267\n",
      "Epoch: 124, Loss: 1034.65202, Residuals: -1.30299, Convergence: 0.005280\n",
      "Epoch: 125, Loss: 1029.30165, Residuals: -1.29546, Convergence: 0.005198\n",
      "Epoch: 126, Loss: 1024.16415, Residuals: -1.28774, Convergence: 0.005016\n",
      "Epoch: 127, Loss: 1019.30949, Residuals: -1.27990, Convergence: 0.004763\n",
      "Epoch: 128, Loss: 1014.76798, Residuals: -1.27201, Convergence: 0.004475\n",
      "Epoch: 129, Loss: 1010.53700, Residuals: -1.26415, Convergence: 0.004187\n",
      "Epoch: 130, Loss: 1006.59588, Residuals: -1.25635, Convergence: 0.003915\n",
      "Epoch: 131, Loss: 1002.91725, Residuals: -1.24866, Convergence: 0.003668\n",
      "Epoch: 132, Loss: 999.47280, Residuals: -1.24112, Convergence: 0.003446\n",
      "Epoch: 133, Loss: 996.23790, Residuals: -1.23374, Convergence: 0.003247\n",
      "Epoch: 134, Loss: 993.19081, Residuals: -1.22656, Convergence: 0.003068\n",
      "Epoch: 135, Loss: 990.31405, Residuals: -1.21959, Convergence: 0.002905\n",
      "Epoch: 136, Loss: 987.59248, Residuals: -1.21285, Convergence: 0.002756\n",
      "Epoch: 137, Loss: 985.01451, Residuals: -1.20635, Convergence: 0.002617\n",
      "Epoch: 138, Loss: 982.56933, Residuals: -1.20009, Convergence: 0.002489\n",
      "Epoch: 139, Loss: 980.24818, Residuals: -1.19408, Convergence: 0.002368\n",
      "Epoch: 140, Loss: 978.04260, Residuals: -1.18832, Convergence: 0.002255\n",
      "Epoch: 141, Loss: 975.94565, Residuals: -1.18281, Convergence: 0.002149\n",
      "Epoch: 142, Loss: 973.95012, Residuals: -1.17754, Convergence: 0.002049\n",
      "Epoch: 143, Loss: 972.04938, Residuals: -1.17252, Convergence: 0.001955\n",
      "Epoch: 144, Loss: 970.23687, Residuals: -1.16773, Convergence: 0.001868\n",
      "Epoch: 145, Loss: 968.50678, Residuals: -1.16316, Convergence: 0.001786\n",
      "Epoch: 146, Loss: 966.85307, Residuals: -1.15881, Convergence: 0.001710\n",
      "Epoch: 147, Loss: 965.27046, Residuals: -1.15467, Convergence: 0.001640\n",
      "Epoch: 148, Loss: 963.75336, Residuals: -1.15072, Convergence: 0.001574\n",
      "Epoch: 149, Loss: 962.29714, Residuals: -1.14695, Convergence: 0.001513\n",
      "Epoch: 150, Loss: 960.89760, Residuals: -1.14336, Convergence: 0.001456\n",
      "Epoch: 151, Loss: 959.54994, Residuals: -1.13993, Convergence: 0.001404\n",
      "Epoch: 152, Loss: 958.25037, Residuals: -1.13665, Convergence: 0.001356\n",
      "Epoch: 153, Loss: 956.99499, Residuals: -1.13351, Convergence: 0.001312\n",
      "Epoch: 154, Loss: 955.78000, Residuals: -1.13051, Convergence: 0.001271\n",
      "Epoch: 155, Loss: 954.60155, Residuals: -1.12762, Convergence: 0.001234\n",
      "Epoch: 156, Loss: 953.45637, Residuals: -1.12484, Convergence: 0.001201\n",
      "Epoch: 157, Loss: 952.33974, Residuals: -1.12215, Convergence: 0.001173\n",
      "Epoch: 158, Loss: 951.24848, Residuals: -1.11956, Convergence: 0.001147\n",
      "Epoch: 159, Loss: 950.17811, Residuals: -1.11704, Convergence: 0.001126\n",
      "Epoch: 160, Loss: 949.12475, Residuals: -1.11458, Convergence: 0.001110\n",
      "Epoch: 161, Loss: 948.08447, Residuals: -1.11217, Convergence: 0.001097\n",
      "Epoch: 162, Loss: 947.05374, Residuals: -1.10981, Convergence: 0.001088\n",
      "Epoch: 163, Loss: 946.02982, Residuals: -1.10748, Convergence: 0.001082\n",
      "Epoch: 164, Loss: 945.01093, Residuals: -1.10517, Convergence: 0.001078\n",
      "Epoch: 165, Loss: 943.99660, Residuals: -1.10289, Convergence: 0.001075\n",
      "Epoch: 166, Loss: 942.98813, Residuals: -1.10063, Convergence: 0.001069\n",
      "Epoch: 167, Loss: 941.98819, Residuals: -1.09839, Convergence: 0.001062\n",
      "Epoch: 168, Loss: 941.00097, Residuals: -1.09619, Convergence: 0.001049\n",
      "Epoch: 169, Loss: 940.03045, Residuals: -1.09402, Convergence: 0.001032\n",
      "Epoch: 170, Loss: 939.08129, Residuals: -1.09190, Convergence: 0.001011\n",
      "Epoch: 171, Loss: 938.15721, Residuals: -1.08983, Convergence: 0.000985\n",
      "Evidence 11177.923\n",
      "\n",
      "Epoch: 171, Evidence: 11177.92285, Convergence: 1.016485\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.74e-01\n",
      "Epoch: 171, Loss: 2354.12185, Residuals: -1.08983, Convergence:   inf\n",
      "Epoch: 172, Loss: 2315.75989, Residuals: -1.09708, Convergence: 0.016566\n",
      "Epoch: 173, Loss: 2288.91518, Residuals: -1.09535, Convergence: 0.011728\n",
      "Epoch: 174, Loss: 2266.62392, Residuals: -1.09285, Convergence: 0.009835\n",
      "Epoch: 175, Loss: 2247.86087, Residuals: -1.09018, Convergence: 0.008347\n",
      "Epoch: 176, Loss: 2231.92913, Residuals: -1.08744, Convergence: 0.007138\n",
      "Epoch: 177, Loss: 2218.28860, Residuals: -1.08467, Convergence: 0.006149\n",
      "Epoch: 178, Loss: 2206.49679, Residuals: -1.08188, Convergence: 0.005344\n",
      "Epoch: 179, Loss: 2196.18772, Residuals: -1.07905, Convergence: 0.004694\n",
      "Epoch: 180, Loss: 2187.05491, Residuals: -1.07617, Convergence: 0.004176\n",
      "Epoch: 181, Loss: 2178.84841, Residuals: -1.07322, Convergence: 0.003766\n",
      "Epoch: 182, Loss: 2171.37525, Residuals: -1.07018, Convergence: 0.003442\n",
      "Epoch: 183, Loss: 2164.49833, Residuals: -1.06703, Convergence: 0.003177\n",
      "Epoch: 184, Loss: 2158.13343, Residuals: -1.06379, Convergence: 0.002949\n",
      "Epoch: 185, Loss: 2152.22919, Residuals: -1.06050, Convergence: 0.002743\n",
      "Epoch: 186, Loss: 2146.75437, Residuals: -1.05720, Convergence: 0.002550\n",
      "Epoch: 187, Loss: 2141.67955, Residuals: -1.05393, Convergence: 0.002370\n",
      "Epoch: 188, Loss: 2136.97717, Residuals: -1.05074, Convergence: 0.002200\n",
      "Epoch: 189, Loss: 2132.61666, Residuals: -1.04764, Convergence: 0.002045\n",
      "Epoch: 190, Loss: 2128.57162, Residuals: -1.04465, Convergence: 0.001900\n",
      "Epoch: 191, Loss: 2124.81394, Residuals: -1.04180, Convergence: 0.001768\n",
      "Epoch: 192, Loss: 2121.32035, Residuals: -1.03908, Convergence: 0.001647\n",
      "Epoch: 193, Loss: 2118.06935, Residuals: -1.03649, Convergence: 0.001535\n",
      "Epoch: 194, Loss: 2115.04074, Residuals: -1.03405, Convergence: 0.001432\n",
      "Epoch: 195, Loss: 2112.21680, Residuals: -1.03173, Convergence: 0.001337\n",
      "Epoch: 196, Loss: 2109.58335, Residuals: -1.02955, Convergence: 0.001248\n",
      "Epoch: 197, Loss: 2107.12366, Residuals: -1.02749, Convergence: 0.001167\n",
      "Epoch: 198, Loss: 2104.82596, Residuals: -1.02554, Convergence: 0.001092\n",
      "Epoch: 199, Loss: 2102.67589, Residuals: -1.02371, Convergence: 0.001023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200, Loss: 2100.66320, Residuals: -1.02197, Convergence: 0.000958\n",
      "Evidence 14292.870\n",
      "\n",
      "Epoch: 200, Evidence: 14292.87012, Convergence: 0.217937\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.36e-01\n",
      "Epoch: 200, Loss: 2474.74748, Residuals: -1.02197, Convergence:   inf\n",
      "Epoch: 201, Loss: 2461.45267, Residuals: -1.01853, Convergence: 0.005401\n",
      "Epoch: 202, Loss: 2450.58530, Residuals: -1.01485, Convergence: 0.004435\n",
      "Epoch: 203, Loss: 2441.24831, Residuals: -1.01137, Convergence: 0.003825\n",
      "Epoch: 204, Loss: 2433.17280, Residuals: -1.00817, Convergence: 0.003319\n",
      "Epoch: 205, Loss: 2426.15396, Residuals: -1.00526, Convergence: 0.002893\n",
      "Epoch: 206, Loss: 2420.02276, Residuals: -1.00263, Convergence: 0.002534\n",
      "Epoch: 207, Loss: 2414.63869, Residuals: -1.00025, Convergence: 0.002230\n",
      "Epoch: 208, Loss: 2409.88361, Residuals: -0.99811, Convergence: 0.001973\n",
      "Epoch: 209, Loss: 2405.65831, Residuals: -0.99617, Convergence: 0.001756\n",
      "Epoch: 210, Loss: 2401.88223, Residuals: -0.99442, Convergence: 0.001572\n",
      "Epoch: 211, Loss: 2398.48612, Residuals: -0.99284, Convergence: 0.001416\n",
      "Epoch: 212, Loss: 2395.41453, Residuals: -0.99141, Convergence: 0.001282\n",
      "Epoch: 213, Loss: 2392.62093, Residuals: -0.99011, Convergence: 0.001168\n",
      "Epoch: 214, Loss: 2390.06703, Residuals: -0.98894, Convergence: 0.001069\n",
      "Epoch: 215, Loss: 2387.72064, Residuals: -0.98787, Convergence: 0.000983\n",
      "Evidence 14662.396\n",
      "\n",
      "Epoch: 215, Evidence: 14662.39648, Convergence: 0.025202\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.32e-01\n",
      "Epoch: 215, Loss: 2480.42260, Residuals: -0.98787, Convergence:   inf\n",
      "Epoch: 216, Loss: 2473.91506, Residuals: -0.98461, Convergence: 0.002630\n",
      "Epoch: 217, Loss: 2468.54699, Residuals: -0.98186, Convergence: 0.002175\n",
      "Epoch: 218, Loss: 2464.00841, Residuals: -0.97958, Convergence: 0.001842\n",
      "Epoch: 219, Loss: 2460.11393, Residuals: -0.97770, Convergence: 0.001583\n",
      "Epoch: 220, Loss: 2456.72510, Residuals: -0.97613, Convergence: 0.001379\n",
      "Epoch: 221, Loss: 2453.73759, Residuals: -0.97482, Convergence: 0.001218\n",
      "Epoch: 222, Loss: 2451.07130, Residuals: -0.97372, Convergence: 0.001088\n",
      "Epoch: 223, Loss: 2448.66604, Residuals: -0.97279, Convergence: 0.000982\n",
      "Evidence 14741.467\n",
      "\n",
      "Epoch: 223, Evidence: 14741.46680, Convergence: 0.005364\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.59e-01\n",
      "Epoch: 223, Loss: 2482.14356, Residuals: -0.97279, Convergence:   inf\n",
      "Epoch: 224, Loss: 2478.19159, Residuals: -0.97047, Convergence: 0.001595\n",
      "Epoch: 225, Loss: 2474.92009, Residuals: -0.96867, Convergence: 0.001322\n",
      "Epoch: 226, Loss: 2472.12200, Residuals: -0.96725, Convergence: 0.001132\n",
      "Epoch: 227, Loss: 2469.67788, Residuals: -0.96612, Convergence: 0.000990\n",
      "Evidence 14769.459\n",
      "\n",
      "Epoch: 227, Evidence: 14769.45898, Convergence: 0.001895\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.08e-01\n",
      "Epoch: 227, Loss: 2483.16691, Residuals: -0.96612, Convergence:   inf\n",
      "Epoch: 228, Loss: 2480.22513, Residuals: -0.96432, Convergence: 0.001186\n",
      "Epoch: 229, Loss: 2477.76300, Residuals: -0.96294, Convergence: 0.000994\n",
      "Evidence 14781.669\n",
      "\n",
      "Epoch: 229, Evidence: 14781.66895, Convergence: 0.000826\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.71e-01\n",
      "Epoch: 229, Loss: 2483.89007, Residuals: -0.96294, Convergence:   inf\n",
      "Epoch: 230, Loss: 2479.25340, Residuals: -0.96060, Convergence: 0.001870\n",
      "Epoch: 231, Loss: 2475.71194, Residuals: -0.95891, Convergence: 0.001430\n",
      "Epoch: 232, Loss: 2472.83677, Residuals: -0.95769, Convergence: 0.001163\n",
      "Epoch: 233, Loss: 2470.39476, Residuals: -0.95693, Convergence: 0.000989\n",
      "Evidence 14799.660\n",
      "\n",
      "Epoch: 233, Evidence: 14799.66016, Convergence: 0.002041\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.42e-01\n",
      "Epoch: 233, Loss: 2483.99871, Residuals: -0.95693, Convergence:   inf\n",
      "Epoch: 234, Loss: 2480.89905, Residuals: -0.95455, Convergence: 0.001249\n",
      "Epoch: 235, Loss: 2478.44029, Residuals: -0.95328, Convergence: 0.000992\n",
      "Evidence 14810.753\n",
      "\n",
      "Epoch: 235, Evidence: 14810.75293, Convergence: 0.000749\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.20e-01\n",
      "Epoch: 235, Loss: 2484.16821, Residuals: -0.95328, Convergence:   inf\n",
      "Epoch: 236, Loss: 2479.60208, Residuals: -0.94999, Convergence: 0.001841\n",
      "Epoch: 237, Loss: 2476.33593, Residuals: -0.95058, Convergence: 0.001319\n",
      "Epoch: 238, Loss: 2473.62906, Residuals: -0.95064, Convergence: 0.001094\n",
      "Epoch: 239, Loss: 2471.25783, Residuals: -0.95318, Convergence: 0.000960\n",
      "Evidence 14826.889\n",
      "\n",
      "Epoch: 239, Evidence: 14826.88867, Convergence: 0.001836\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.07e-01\n",
      "Epoch: 239, Loss: 2483.41030, Residuals: -0.95318, Convergence:   inf\n",
      "Epoch: 240, Loss: 2481.48433, Residuals: -0.95080, Convergence: 0.000776\n",
      "Evidence 14833.727\n",
      "\n",
      "Epoch: 240, Evidence: 14833.72656, Convergence: 0.000461\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.71e-02\n",
      "Epoch: 240, Loss: 2484.40945, Residuals: -0.95080, Convergence:   inf\n",
      "Epoch: 241, Loss: 2528.93547, Residuals: -0.99645, Convergence: -0.017607\n",
      "Epoch: 241, Loss: 2481.97572, Residuals: -0.94984, Convergence: 0.000981\n",
      "Evidence 14838.235\n",
      "\n",
      "Epoch: 241, Evidence: 14838.23535, Convergence: 0.000765\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.05e-02\n",
      "Epoch: 241, Loss: 2483.77903, Residuals: -0.94984, Convergence:   inf\n",
      "Epoch: 242, Loss: 2487.07902, Residuals: -0.95142, Convergence: -0.001327\n",
      "Epoch: 242, Loss: 2483.26777, Residuals: -0.94802, Convergence: 0.000206\n",
      "Evidence 14840.475\n",
      "\n",
      "Epoch: 242, Evidence: 14840.47461, Convergence: 0.000915\n",
      "Total samples: 184, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 382.40800, Residuals: -4.51153, Convergence:   inf\n",
      "Epoch: 1, Loss: 356.67173, Residuals: -4.39164, Convergence: 0.072157\n",
      "Epoch: 2, Loss: 335.58419, Residuals: -4.22772, Convergence: 0.062838\n",
      "Epoch: 3, Loss: 319.51509, Residuals: -4.06378, Convergence: 0.050292\n",
      "Epoch: 4, Loss: 307.25459, Residuals: -3.91910, Convergence: 0.039903\n",
      "Epoch: 5, Loss: 297.52996, Residuals: -3.79108, Convergence: 0.032685\n",
      "Epoch: 6, Loss: 289.63456, Residuals: -3.67952, Convergence: 0.027260\n",
      "Epoch: 7, Loss: 283.07785, Residuals: -3.58394, Convergence: 0.023162\n",
      "Epoch: 8, Loss: 277.49603, Residuals: -3.50230, Convergence: 0.020115\n",
      "Epoch: 9, Loss: 272.63229, Residuals: -3.43228, Convergence: 0.017840\n",
      "Epoch: 10, Loss: 268.30411, Residuals: -3.37182, Convergence: 0.016132\n",
      "Epoch: 11, Loss: 264.37934, Residuals: -3.31911, Convergence: 0.014845\n",
      "Epoch: 12, Loss: 260.76169, Residuals: -3.27263, Convergence: 0.013873\n",
      "Epoch: 13, Loss: 257.38146, Residuals: -3.23104, Convergence: 0.013133\n",
      "Epoch: 14, Loss: 254.18957, Residuals: -3.19318, Convergence: 0.012557\n",
      "Epoch: 15, Loss: 251.15472, Residuals: -3.15812, Convergence: 0.012084\n",
      "Epoch: 16, Loss: 248.26092, Residuals: -3.12523, Convergence: 0.011656\n",
      "Epoch: 17, Loss: 245.49730, Residuals: -3.09414, Convergence: 0.011257\n",
      "Epoch: 18, Loss: 242.84383, Residuals: -3.06448, Convergence: 0.010927\n",
      "Epoch: 19, Loss: 240.26818, Residuals: -3.03573, Convergence: 0.010720\n",
      "Epoch: 20, Loss: 237.73326, Residuals: -3.00728, Convergence: 0.010663\n",
      "Epoch: 21, Loss: 235.20593, Residuals: -2.97859, Convergence: 0.010745\n",
      "Epoch: 22, Loss: 232.66130, Residuals: -2.94926, Convergence: 0.010937\n",
      "Epoch: 23, Loss: 230.07486, Residuals: -2.91896, Convergence: 0.011242\n",
      "Epoch: 24, Loss: 227.40592, Residuals: -2.88722, Convergence: 0.011736\n",
      "Epoch: 25, Loss: 224.59895, Residuals: -2.85334, Convergence: 0.012498\n",
      "Epoch: 26, Loss: 221.64517, Residuals: -2.81710, Convergence: 0.013327\n",
      "Epoch: 27, Loss: 218.65742, Residuals: -2.77962, Convergence: 0.013664\n",
      "Epoch: 28, Loss: 215.75222, Residuals: -2.74224, Convergence: 0.013465\n",
      "Epoch: 29, Loss: 212.95367, Residuals: -2.70534, Convergence: 0.013142\n",
      "Epoch: 30, Loss: 210.24786, Residuals: -2.66889, Convergence: 0.012870\n",
      "Epoch: 31, Loss: 207.61791, Residuals: -2.63278, Convergence: 0.012667\n",
      "Epoch: 32, Loss: 205.05124, Residuals: -2.59692, Convergence: 0.012517\n",
      "Epoch: 33, Loss: 202.53973, Residuals: -2.56124, Convergence: 0.012400\n",
      "Epoch: 34, Loss: 200.07872, Residuals: -2.52573, Convergence: 0.012300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35, Loss: 197.66609, Residuals: -2.49037, Convergence: 0.012206\n",
      "Epoch: 36, Loss: 195.30146, Residuals: -2.45515, Convergence: 0.012108\n",
      "Epoch: 37, Loss: 192.98553, Residuals: -2.42007, Convergence: 0.012001\n",
      "Epoch: 38, Loss: 190.71957, Residuals: -2.38514, Convergence: 0.011881\n",
      "Epoch: 39, Loss: 188.50508, Residuals: -2.35035, Convergence: 0.011748\n",
      "Epoch: 40, Loss: 186.34347, Residuals: -2.31573, Convergence: 0.011600\n",
      "Epoch: 41, Loss: 184.23599, Residuals: -2.28128, Convergence: 0.011439\n",
      "Epoch: 42, Loss: 182.18362, Residuals: -2.24701, Convergence: 0.011265\n",
      "Epoch: 43, Loss: 180.18728, Residuals: -2.21293, Convergence: 0.011079\n",
      "Epoch: 44, Loss: 178.24786, Residuals: -2.17906, Convergence: 0.010880\n",
      "Epoch: 45, Loss: 176.36651, Residuals: -2.14544, Convergence: 0.010667\n",
      "Epoch: 46, Loss: 174.54467, Residuals: -2.11209, Convergence: 0.010438\n",
      "Epoch: 47, Loss: 172.78409, Residuals: -2.07906, Convergence: 0.010190\n",
      "Epoch: 48, Loss: 171.08660, Residuals: -2.04640, Convergence: 0.009922\n",
      "Epoch: 49, Loss: 169.45392, Residuals: -2.01416, Convergence: 0.009635\n",
      "Epoch: 50, Loss: 167.88736, Residuals: -1.98241, Convergence: 0.009331\n",
      "Epoch: 51, Loss: 166.38769, Residuals: -1.95120, Convergence: 0.009013\n",
      "Epoch: 52, Loss: 164.95507, Residuals: -1.92057, Convergence: 0.008685\n",
      "Epoch: 53, Loss: 163.58910, Residuals: -1.89059, Convergence: 0.008350\n",
      "Epoch: 54, Loss: 162.28892, Residuals: -1.86128, Convergence: 0.008012\n",
      "Epoch: 55, Loss: 161.05322, Residuals: -1.83268, Convergence: 0.007673\n",
      "Epoch: 56, Loss: 159.88041, Residuals: -1.80484, Convergence: 0.007336\n",
      "Epoch: 57, Loss: 158.76857, Residuals: -1.77777, Convergence: 0.007003\n",
      "Epoch: 58, Loss: 157.71551, Residuals: -1.75149, Convergence: 0.006677\n",
      "Epoch: 59, Loss: 156.71878, Residuals: -1.72603, Convergence: 0.006360\n",
      "Epoch: 60, Loss: 155.77568, Residuals: -1.70139, Convergence: 0.006054\n",
      "Epoch: 61, Loss: 154.88329, Residuals: -1.67757, Convergence: 0.005762\n",
      "Epoch: 62, Loss: 154.03855, Residuals: -1.65456, Convergence: 0.005484\n",
      "Epoch: 63, Loss: 153.23830, Residuals: -1.63234, Convergence: 0.005222\n",
      "Epoch: 64, Loss: 152.47940, Residuals: -1.61089, Convergence: 0.004977\n",
      "Epoch: 65, Loss: 151.75884, Residuals: -1.59018, Convergence: 0.004748\n",
      "Epoch: 66, Loss: 151.07382, Residuals: -1.57018, Convergence: 0.004534\n",
      "Epoch: 67, Loss: 150.42185, Residuals: -1.55086, Convergence: 0.004334\n",
      "Epoch: 68, Loss: 149.80077, Residuals: -1.53221, Convergence: 0.004146\n",
      "Epoch: 69, Loss: 149.20878, Residuals: -1.51419, Convergence: 0.003968\n",
      "Epoch: 70, Loss: 148.64435, Residuals: -1.49679, Convergence: 0.003797\n",
      "Epoch: 71, Loss: 148.10625, Residuals: -1.47999, Convergence: 0.003633\n",
      "Epoch: 72, Loss: 147.59338, Residuals: -1.46378, Convergence: 0.003475\n",
      "Epoch: 73, Loss: 147.10480, Residuals: -1.44815, Convergence: 0.003321\n",
      "Epoch: 74, Loss: 146.63964, Residuals: -1.43309, Convergence: 0.003172\n",
      "Epoch: 75, Loss: 146.19707, Residuals: -1.41860, Convergence: 0.003027\n",
      "Epoch: 76, Loss: 145.77631, Residuals: -1.40465, Convergence: 0.002886\n",
      "Epoch: 77, Loss: 145.37655, Residuals: -1.39125, Convergence: 0.002750\n",
      "Epoch: 78, Loss: 144.99702, Residuals: -1.37838, Convergence: 0.002617\n",
      "Epoch: 79, Loss: 144.63696, Residuals: -1.36602, Convergence: 0.002489\n",
      "Epoch: 80, Loss: 144.29560, Residuals: -1.35416, Convergence: 0.002366\n",
      "Epoch: 81, Loss: 143.97222, Residuals: -1.34280, Convergence: 0.002246\n",
      "Epoch: 82, Loss: 143.66609, Residuals: -1.33191, Convergence: 0.002131\n",
      "Epoch: 83, Loss: 143.37651, Residuals: -1.32148, Convergence: 0.002020\n",
      "Epoch: 84, Loss: 143.10282, Residuals: -1.31149, Convergence: 0.001913\n",
      "Epoch: 85, Loss: 142.84438, Residuals: -1.30194, Convergence: 0.001809\n",
      "Epoch: 86, Loss: 142.60057, Residuals: -1.29282, Convergence: 0.001710\n",
      "Epoch: 87, Loss: 142.37081, Residuals: -1.28409, Convergence: 0.001614\n",
      "Epoch: 88, Loss: 142.15454, Residuals: -1.27577, Convergence: 0.001521\n",
      "Epoch: 89, Loss: 141.95119, Residuals: -1.26783, Convergence: 0.001433\n",
      "Epoch: 90, Loss: 141.76022, Residuals: -1.26027, Convergence: 0.001347\n",
      "Epoch: 91, Loss: 141.58105, Residuals: -1.25307, Convergence: 0.001265\n",
      "Epoch: 92, Loss: 141.41309, Residuals: -1.24623, Convergence: 0.001188\n",
      "Epoch: 93, Loss: 141.25568, Residuals: -1.23974, Convergence: 0.001114\n",
      "Epoch: 94, Loss: 141.10810, Residuals: -1.23359, Convergence: 0.001046\n",
      "Epoch: 95, Loss: 140.96952, Residuals: -1.22777, Convergence: 0.000983\n",
      "Evidence -182.424\n",
      "\n",
      "Epoch: 95, Evidence: -182.42403, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 7.24e-01\n",
      "Epoch: 95, Loss: 1379.53910, Residuals: -1.22777, Convergence:   inf\n",
      "Epoch: 96, Loss: 1316.94278, Residuals: -1.25647, Convergence: 0.047532\n",
      "Epoch: 97, Loss: 1269.45990, Residuals: -1.27923, Convergence: 0.037404\n",
      "Epoch: 98, Loss: 1233.70054, Residuals: -1.29547, Convergence: 0.028985\n",
      "Epoch: 99, Loss: 1205.87778, Residuals: -1.30665, Convergence: 0.023073\n",
      "Epoch: 100, Loss: 1183.35470, Residuals: -1.31467, Convergence: 0.019033\n",
      "Epoch: 101, Loss: 1164.65484, Residuals: -1.32056, Convergence: 0.016056\n",
      "Epoch: 102, Loss: 1148.88723, Residuals: -1.32483, Convergence: 0.013724\n",
      "Epoch: 103, Loss: 1135.44070, Residuals: -1.32773, Convergence: 0.011843\n",
      "Epoch: 104, Loss: 1123.86090, Residuals: -1.32946, Convergence: 0.010304\n",
      "Epoch: 105, Loss: 1113.79042, Residuals: -1.33015, Convergence: 0.009042\n",
      "Epoch: 106, Loss: 1104.94094, Residuals: -1.32991, Convergence: 0.008009\n",
      "Epoch: 107, Loss: 1097.07325, Residuals: -1.32885, Convergence: 0.007172\n",
      "Epoch: 108, Loss: 1089.98317, Residuals: -1.32703, Convergence: 0.006505\n",
      "Epoch: 109, Loss: 1083.49150, Residuals: -1.32452, Convergence: 0.005991\n",
      "Epoch: 110, Loss: 1077.43558, Residuals: -1.32134, Convergence: 0.005621\n",
      "Epoch: 111, Loss: 1071.66247, Residuals: -1.31750, Convergence: 0.005387\n",
      "Epoch: 112, Loss: 1066.02775, Residuals: -1.31300, Convergence: 0.005286\n",
      "Epoch: 113, Loss: 1060.39916, Residuals: -1.30784, Convergence: 0.005308\n",
      "Epoch: 114, Loss: 1054.67184, Residuals: -1.30203, Convergence: 0.005430\n",
      "Epoch: 115, Loss: 1048.79279, Residuals: -1.29560, Convergence: 0.005606\n",
      "Epoch: 116, Loss: 1042.78667, Residuals: -1.28864, Convergence: 0.005760\n",
      "Epoch: 117, Loss: 1036.75686, Residuals: -1.28124, Convergence: 0.005816\n",
      "Epoch: 118, Loss: 1030.85043, Residuals: -1.27349, Convergence: 0.005730\n",
      "Epoch: 119, Loss: 1025.20147, Residuals: -1.26548, Convergence: 0.005510\n",
      "Epoch: 120, Loss: 1019.89371, Residuals: -1.25729, Convergence: 0.005204\n",
      "Epoch: 121, Loss: 1014.95799, Residuals: -1.24900, Convergence: 0.004863\n",
      "Epoch: 122, Loss: 1010.38649, Residuals: -1.24068, Convergence: 0.004525\n",
      "Epoch: 123, Loss: 1006.15399, Residuals: -1.23239, Convergence: 0.004207\n",
      "Epoch: 124, Loss: 1002.22781, Residuals: -1.22420, Convergence: 0.003917\n",
      "Epoch: 125, Loss: 998.57591, Residuals: -1.21614, Convergence: 0.003657\n",
      "Epoch: 126, Loss: 995.16942, Residuals: -1.20825, Convergence: 0.003423\n",
      "Epoch: 127, Loss: 991.98344, Residuals: -1.20056, Convergence: 0.003212\n",
      "Epoch: 128, Loss: 988.99757, Residuals: -1.19310, Convergence: 0.003019\n",
      "Epoch: 129, Loss: 986.19392, Residuals: -1.18588, Convergence: 0.002843\n",
      "Epoch: 130, Loss: 983.55820, Residuals: -1.17892, Convergence: 0.002680\n",
      "Epoch: 131, Loss: 981.07818, Residuals: -1.17223, Convergence: 0.002528\n",
      "Epoch: 132, Loss: 978.74224, Residuals: -1.16581, Convergence: 0.002387\n",
      "Epoch: 133, Loss: 976.54088, Residuals: -1.15968, Convergence: 0.002254\n",
      "Epoch: 134, Loss: 974.46446, Residuals: -1.15382, Convergence: 0.002131\n",
      "Epoch: 135, Loss: 972.50488, Residuals: -1.14823, Convergence: 0.002015\n",
      "Epoch: 136, Loss: 970.65317, Residuals: -1.14292, Convergence: 0.001908\n",
      "Epoch: 137, Loss: 968.90198, Residuals: -1.13787, Convergence: 0.001807\n",
      "Epoch: 138, Loss: 967.24375, Residuals: -1.13308, Convergence: 0.001714\n",
      "Epoch: 139, Loss: 965.67093, Residuals: -1.12852, Convergence: 0.001629\n",
      "Epoch: 140, Loss: 964.17720, Residuals: -1.12420, Convergence: 0.001549\n",
      "Epoch: 141, Loss: 962.75588, Residuals: -1.12011, Convergence: 0.001476\n",
      "Epoch: 142, Loss: 961.40128, Residuals: -1.11622, Convergence: 0.001409\n",
      "Epoch: 143, Loss: 960.10776, Residuals: -1.11253, Convergence: 0.001347\n",
      "Epoch: 144, Loss: 958.87037, Residuals: -1.10902, Convergence: 0.001290\n",
      "Epoch: 145, Loss: 957.68436, Residuals: -1.10570, Convergence: 0.001238\n",
      "Epoch: 146, Loss: 956.54545, Residuals: -1.10253, Convergence: 0.001191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 147, Loss: 955.44996, Residuals: -1.09952, Convergence: 0.001147\n",
      "Epoch: 148, Loss: 954.39405, Residuals: -1.09665, Convergence: 0.001106\n",
      "Epoch: 149, Loss: 953.37448, Residuals: -1.09392, Convergence: 0.001069\n",
      "Epoch: 150, Loss: 952.38844, Residuals: -1.09131, Convergence: 0.001035\n",
      "Epoch: 151, Loss: 951.43298, Residuals: -1.08881, Convergence: 0.001004\n",
      "Epoch: 152, Loss: 950.50578, Residuals: -1.08643, Convergence: 0.000975\n",
      "Evidence 11163.214\n",
      "\n",
      "Epoch: 152, Evidence: 11163.21387, Convergence: 1.016342\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 5.77e-01\n",
      "Epoch: 152, Loss: 2365.86523, Residuals: -1.08643, Convergence:   inf\n",
      "Epoch: 153, Loss: 2324.24297, Residuals: -1.09606, Convergence: 0.017908\n",
      "Epoch: 154, Loss: 2295.88317, Residuals: -1.09637, Convergence: 0.012352\n",
      "Epoch: 155, Loss: 2271.86726, Residuals: -1.09553, Convergence: 0.010571\n",
      "Epoch: 156, Loss: 2251.24427, Residuals: -1.09411, Convergence: 0.009161\n",
      "Epoch: 157, Loss: 2233.42643, Residuals: -1.09230, Convergence: 0.007978\n",
      "Epoch: 158, Loss: 2217.95275, Residuals: -1.09017, Convergence: 0.006977\n",
      "Epoch: 159, Loss: 2204.43565, Residuals: -1.08778, Convergence: 0.006132\n",
      "Epoch: 160, Loss: 2192.54353, Residuals: -1.08517, Convergence: 0.005424\n",
      "Epoch: 161, Loss: 2181.98674, Residuals: -1.08236, Convergence: 0.004838\n",
      "Epoch: 162, Loss: 2172.51417, Residuals: -1.07937, Convergence: 0.004360\n",
      "Epoch: 163, Loss: 2163.90808, Residuals: -1.07619, Convergence: 0.003977\n",
      "Epoch: 164, Loss: 2155.98527, Residuals: -1.07282, Convergence: 0.003675\n",
      "Epoch: 165, Loss: 2148.60235, Residuals: -1.06926, Convergence: 0.003436\n",
      "Epoch: 166, Loss: 2141.66211, Residuals: -1.06552, Convergence: 0.003241\n",
      "Epoch: 167, Loss: 2135.11146, Residuals: -1.06162, Convergence: 0.003068\n",
      "Epoch: 168, Loss: 2128.93605, Residuals: -1.05762, Convergence: 0.002901\n",
      "Epoch: 169, Loss: 2123.13775, Residuals: -1.05357, Convergence: 0.002731\n",
      "Epoch: 170, Loss: 2117.71992, Residuals: -1.04953, Convergence: 0.002558\n",
      "Epoch: 171, Loss: 2112.67783, Residuals: -1.04556, Convergence: 0.002387\n",
      "Epoch: 172, Loss: 2107.99824, Residuals: -1.04169, Convergence: 0.002220\n",
      "Epoch: 173, Loss: 2103.66024, Residuals: -1.03794, Convergence: 0.002062\n",
      "Epoch: 174, Loss: 2099.63917, Residuals: -1.03434, Convergence: 0.001915\n",
      "Epoch: 175, Loss: 2095.90778, Residuals: -1.03089, Convergence: 0.001780\n",
      "Epoch: 176, Loss: 2092.44028, Residuals: -1.02760, Convergence: 0.001657\n",
      "Epoch: 177, Loss: 2089.21050, Residuals: -1.02448, Convergence: 0.001546\n",
      "Epoch: 178, Loss: 2086.19504, Residuals: -1.02151, Convergence: 0.001445\n",
      "Epoch: 179, Loss: 2083.37400, Residuals: -1.01871, Convergence: 0.001354\n",
      "Epoch: 180, Loss: 2080.72920, Residuals: -1.01606, Convergence: 0.001271\n",
      "Epoch: 181, Loss: 2078.24383, Residuals: -1.01356, Convergence: 0.001196\n",
      "Epoch: 182, Loss: 2075.90516, Residuals: -1.01120, Convergence: 0.001127\n",
      "Epoch: 183, Loss: 2073.70189, Residuals: -1.00899, Convergence: 0.001062\n",
      "Epoch: 184, Loss: 2071.62426, Residuals: -1.00690, Convergence: 0.001003\n",
      "Epoch: 185, Loss: 2069.66313, Residuals: -1.00495, Convergence: 0.000948\n",
      "Evidence 14364.331\n",
      "\n",
      "Epoch: 185, Evidence: 14364.33105, Convergence: 0.222852\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 4.40e-01\n",
      "Epoch: 185, Loss: 2494.48598, Residuals: -1.00495, Convergence:   inf\n",
      "Epoch: 186, Loss: 2479.13771, Residuals: -1.00291, Convergence: 0.006191\n",
      "Epoch: 187, Loss: 2466.76926, Residuals: -1.00008, Convergence: 0.005014\n",
      "Epoch: 188, Loss: 2456.21495, Residuals: -0.99711, Convergence: 0.004297\n",
      "Epoch: 189, Loss: 2447.11831, Residuals: -0.99414, Convergence: 0.003717\n",
      "Epoch: 190, Loss: 2439.21761, Residuals: -0.99128, Convergence: 0.003239\n",
      "Epoch: 191, Loss: 2432.31049, Residuals: -0.98855, Convergence: 0.002840\n",
      "Epoch: 192, Loss: 2426.23306, Residuals: -0.98597, Convergence: 0.002505\n",
      "Epoch: 193, Loss: 2420.85484, Residuals: -0.98357, Convergence: 0.002222\n",
      "Epoch: 194, Loss: 2416.06518, Residuals: -0.98132, Convergence: 0.001982\n",
      "Epoch: 195, Loss: 2411.77602, Residuals: -0.97922, Convergence: 0.001778\n",
      "Epoch: 196, Loss: 2407.91133, Residuals: -0.97725, Convergence: 0.001605\n",
      "Epoch: 197, Loss: 2404.40957, Residuals: -0.97542, Convergence: 0.001456\n",
      "Epoch: 198, Loss: 2401.21862, Residuals: -0.97370, Convergence: 0.001329\n",
      "Epoch: 199, Loss: 2398.29537, Residuals: -0.97210, Convergence: 0.001219\n",
      "Epoch: 200, Loss: 2395.60448, Residuals: -0.97059, Convergence: 0.001123\n",
      "Epoch: 201, Loss: 2393.11551, Residuals: -0.96917, Convergence: 0.001040\n",
      "Epoch: 202, Loss: 2390.80389, Residuals: -0.96784, Convergence: 0.000967\n",
      "Evidence 14821.097\n",
      "\n",
      "Epoch: 202, Evidence: 14821.09668, Convergence: 0.030819\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 3.37e-01\n",
      "Epoch: 202, Loss: 2500.40900, Residuals: -0.96784, Convergence:   inf\n",
      "Epoch: 203, Loss: 2493.51053, Residuals: -0.96529, Convergence: 0.002767\n",
      "Epoch: 204, Loss: 2487.83571, Residuals: -0.96266, Convergence: 0.002281\n",
      "Epoch: 205, Loss: 2483.01218, Residuals: -0.96018, Convergence: 0.001943\n",
      "Epoch: 206, Loss: 2478.84119, Residuals: -0.95792, Convergence: 0.001683\n",
      "Epoch: 207, Loss: 2475.18394, Residuals: -0.95587, Convergence: 0.001478\n",
      "Epoch: 208, Loss: 2471.93739, Residuals: -0.95403, Convergence: 0.001313\n",
      "Epoch: 209, Loss: 2469.02441, Residuals: -0.95237, Convergence: 0.001180\n",
      "Epoch: 210, Loss: 2466.38501, Residuals: -0.95087, Convergence: 0.001070\n",
      "Epoch: 211, Loss: 2463.97363, Residuals: -0.94952, Convergence: 0.000979\n",
      "Evidence 14912.829\n",
      "\n",
      "Epoch: 211, Evidence: 14912.82910, Convergence: 0.006151\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 2.66e-01\n",
      "Epoch: 211, Loss: 2502.03315, Residuals: -0.94952, Convergence:   inf\n",
      "Epoch: 212, Loss: 2498.03330, Residuals: -0.94710, Convergence: 0.001601\n",
      "Epoch: 213, Loss: 2494.67722, Residuals: -0.94487, Convergence: 0.001345\n",
      "Epoch: 214, Loss: 2491.76401, Residuals: -0.94290, Convergence: 0.001169\n",
      "Epoch: 215, Loss: 2489.18700, Residuals: -0.94117, Convergence: 0.001035\n",
      "Epoch: 216, Loss: 2486.87369, Residuals: -0.93966, Convergence: 0.000930\n",
      "Evidence 14944.661\n",
      "\n",
      "Epoch: 216, Evidence: 14944.66113, Convergence: 0.002130\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 2.15e-01\n",
      "Epoch: 216, Loss: 2502.98889, Residuals: -0.93966, Convergence:   inf\n",
      "Epoch: 217, Loss: 2500.12552, Residuals: -0.93750, Convergence: 0.001145\n",
      "Epoch: 218, Loss: 2497.67617, Residuals: -0.93560, Convergence: 0.000981\n",
      "Evidence 14957.170\n",
      "\n",
      "Epoch: 218, Evidence: 14957.16992, Convergence: 0.000836\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.80e-01\n",
      "Epoch: 218, Loss: 2503.75109, Residuals: -0.93560, Convergence:   inf\n",
      "Epoch: 219, Loss: 2499.12281, Residuals: -0.93226, Convergence: 0.001852\n",
      "Epoch: 220, Loss: 2495.49570, Residuals: -0.92940, Convergence: 0.001453\n",
      "Epoch: 221, Loss: 2492.49404, Residuals: -0.92724, Convergence: 0.001204\n",
      "Epoch: 222, Loss: 2489.91683, Residuals: -0.92572, Convergence: 0.001035\n",
      "Epoch: 223, Loss: 2487.63997, Residuals: -0.92465, Convergence: 0.000915\n",
      "Evidence 14978.218\n",
      "\n",
      "Epoch: 223, Evidence: 14978.21777, Convergence: 0.002240\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.50e-01\n",
      "Epoch: 223, Loss: 2503.81212, Residuals: -0.92465, Convergence:   inf\n",
      "Epoch: 224, Loss: 2500.89638, Residuals: -0.92174, Convergence: 0.001166\n",
      "Epoch: 225, Loss: 2498.52913, Residuals: -0.91987, Convergence: 0.000947\n",
      "Evidence 14989.525\n",
      "\n",
      "Epoch: 225, Evidence: 14989.52539, Convergence: 0.000754\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.28e-01\n",
      "Epoch: 225, Loss: 2504.10218, Residuals: -0.91987, Convergence:   inf\n",
      "Epoch: 226, Loss: 2499.74952, Residuals: -0.91566, Convergence: 0.001741\n",
      "Epoch: 227, Loss: 2496.54295, Residuals: -0.91578, Convergence: 0.001284\n",
      "Epoch: 228, Loss: 2493.81410, Residuals: -0.91629, Convergence: 0.001094\n",
      "Epoch: 229, Loss: 2491.50606, Residuals: -0.91859, Convergence: 0.000926\n",
      "Evidence 15005.807\n",
      "\n",
      "Epoch: 229, Evidence: 15005.80664, Convergence: 0.001839\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.14e-01\n",
      "Epoch: 229, Loss: 2503.77073, Residuals: -0.91859, Convergence:   inf\n",
      "Epoch: 230, Loss: 2502.12444, Residuals: -0.91742, Convergence: 0.000658\n",
      "Evidence 15012.027\n",
      "\n",
      "Epoch: 230, Evidence: 15012.02734, Convergence: 0.000414\n",
      "Updating hyper-parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 184, Updated regularization: 9.32e-02\n",
      "Epoch: 230, Loss: 2504.70078, Residuals: -0.91742, Convergence:   inf\n",
      "Epoch: 231, Loss: 2547.08118, Residuals: -0.95228, Convergence: -0.016639\n",
      "Epoch: 231, Loss: 2502.42512, Residuals: -0.91430, Convergence: 0.000909\n",
      "Evidence 15016.499\n",
      "\n",
      "Epoch: 231, Evidence: 15016.49902, Convergence: 0.000712\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 8.55e-02\n",
      "Epoch: 231, Loss: 2504.28759, Residuals: -0.91430, Convergence:   inf\n",
      "Epoch: 232, Loss: 2508.38622, Residuals: -0.91528, Convergence: -0.001634\n",
      "Epoch: 232, Loss: 2504.20361, Residuals: -0.91303, Convergence: 0.000034\n",
      "Evidence 15018.127\n",
      "\n",
      "Epoch: 232, Evidence: 15018.12695, Convergence: 0.000820\n",
      "Total samples: 185, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 388.20096, Residuals: -4.53576, Convergence:   inf\n",
      "Epoch: 1, Loss: 362.29677, Residuals: -4.41487, Convergence: 0.071500\n",
      "Epoch: 2, Loss: 341.05314, Residuals: -4.24975, Convergence: 0.062288\n",
      "Epoch: 3, Loss: 324.82640, Residuals: -4.08428, Convergence: 0.049955\n",
      "Epoch: 4, Loss: 312.43700, Residuals: -3.93842, Convergence: 0.039654\n",
      "Epoch: 5, Loss: 302.59445, Residuals: -3.80936, Convergence: 0.032527\n",
      "Epoch: 6, Loss: 294.58900, Residuals: -3.69696, Convergence: 0.027175\n",
      "Epoch: 7, Loss: 287.92673, Residuals: -3.60071, Convergence: 0.023139\n",
      "Epoch: 8, Loss: 282.24176, Residuals: -3.51845, Convergence: 0.020142\n",
      "Epoch: 9, Loss: 277.27596, Residuals: -3.44780, Convergence: 0.017909\n",
      "Epoch: 10, Loss: 272.84525, Residuals: -3.38661, Convergence: 0.016239\n",
      "Epoch: 11, Loss: 268.81589, Residuals: -3.33306, Convergence: 0.014989\n",
      "Epoch: 12, Loss: 265.09013, Residuals: -3.28559, Convergence: 0.014055\n",
      "Epoch: 13, Loss: 261.59739, Residuals: -3.24286, Convergence: 0.013352\n",
      "Epoch: 14, Loss: 258.28917, Residuals: -3.20367, Convergence: 0.012808\n",
      "Epoch: 15, Loss: 255.13743, Residuals: -3.16715, Convergence: 0.012353\n",
      "Epoch: 16, Loss: 252.13065, Residuals: -3.13276, Convergence: 0.011925\n",
      "Epoch: 17, Loss: 249.25879, Residuals: -3.10016, Convergence: 0.011522\n",
      "Epoch: 18, Loss: 246.49877, Residuals: -3.06895, Convergence: 0.011197\n",
      "Epoch: 19, Loss: 243.81586, Residuals: -3.03860, Convergence: 0.011004\n",
      "Epoch: 20, Loss: 241.17332, Residuals: -3.00852, Convergence: 0.010957\n",
      "Epoch: 21, Loss: 238.54052, Residuals: -2.97822, Convergence: 0.011037\n",
      "Epoch: 22, Loss: 235.89505, Residuals: -2.94737, Convergence: 0.011215\n",
      "Epoch: 23, Loss: 233.21315, Residuals: -2.91568, Convergence: 0.011500\n",
      "Epoch: 24, Loss: 230.45436, Residuals: -2.88271, Convergence: 0.011971\n",
      "Epoch: 25, Loss: 227.55919, Residuals: -2.84777, Convergence: 0.012723\n",
      "Epoch: 26, Loss: 224.49506, Residuals: -2.81041, Convergence: 0.013649\n",
      "Epoch: 27, Loss: 221.35463, Residuals: -2.77148, Convergence: 0.014187\n",
      "Epoch: 28, Loss: 218.28410, Residuals: -2.73257, Convergence: 0.014067\n",
      "Epoch: 29, Loss: 215.33138, Residuals: -2.69435, Convergence: 0.013712\n",
      "Epoch: 30, Loss: 212.48713, Residuals: -2.65684, Convergence: 0.013386\n",
      "Epoch: 31, Loss: 209.73459, Residuals: -2.61992, Convergence: 0.013124\n",
      "Epoch: 32, Loss: 207.06105, Residuals: -2.58347, Convergence: 0.012912\n",
      "Epoch: 33, Loss: 204.45851, Residuals: -2.54741, Convergence: 0.012729\n",
      "Epoch: 34, Loss: 201.92257, Residuals: -2.51167, Convergence: 0.012559\n",
      "Epoch: 35, Loss: 199.45120, Residuals: -2.47620, Convergence: 0.012391\n",
      "Epoch: 36, Loss: 197.04381, Residuals: -2.44096, Convergence: 0.012218\n",
      "Epoch: 37, Loss: 194.70055, Residuals: -2.40594, Convergence: 0.012035\n",
      "Epoch: 38, Loss: 192.42186, Residuals: -2.37114, Convergence: 0.011842\n",
      "Epoch: 39, Loss: 190.20820, Residuals: -2.33654, Convergence: 0.011638\n",
      "Epoch: 40, Loss: 188.05988, Residuals: -2.30217, Convergence: 0.011424\n",
      "Epoch: 41, Loss: 185.97703, Residuals: -2.26803, Convergence: 0.011199\n",
      "Epoch: 42, Loss: 183.95960, Residuals: -2.23415, Convergence: 0.010967\n",
      "Epoch: 43, Loss: 182.00744, Residuals: -2.20054, Convergence: 0.010726\n",
      "Epoch: 44, Loss: 180.12042, Residuals: -2.16724, Convergence: 0.010476\n",
      "Epoch: 45, Loss: 178.29860, Residuals: -2.13427, Convergence: 0.010218\n",
      "Epoch: 46, Loss: 176.54231, Residuals: -2.10169, Convergence: 0.009948\n",
      "Epoch: 47, Loss: 174.85225, Residuals: -2.06952, Convergence: 0.009666\n",
      "Epoch: 48, Loss: 173.22934, Residuals: -2.03782, Convergence: 0.009369\n",
      "Epoch: 49, Loss: 171.67445, Residuals: -2.00665, Convergence: 0.009057\n",
      "Epoch: 50, Loss: 170.18818, Residuals: -1.97605, Convergence: 0.008733\n",
      "Epoch: 51, Loss: 168.77050, Residuals: -1.94609, Convergence: 0.008400\n",
      "Epoch: 52, Loss: 167.42073, Residuals: -1.91680, Convergence: 0.008062\n",
      "Epoch: 53, Loss: 166.13737, Residuals: -1.88823, Convergence: 0.007725\n",
      "Epoch: 54, Loss: 164.91826, Residuals: -1.86040, Convergence: 0.007392\n",
      "Epoch: 55, Loss: 163.76057, Residuals: -1.83334, Convergence: 0.007069\n",
      "Epoch: 56, Loss: 162.66097, Residuals: -1.80704, Convergence: 0.006760\n",
      "Epoch: 57, Loss: 161.61567, Residuals: -1.78153, Convergence: 0.006468\n",
      "Epoch: 58, Loss: 160.62065, Residuals: -1.75677, Convergence: 0.006195\n",
      "Epoch: 59, Loss: 159.67180, Residuals: -1.73274, Convergence: 0.005942\n",
      "Epoch: 60, Loss: 158.76513, Residuals: -1.70942, Convergence: 0.005711\n",
      "Epoch: 61, Loss: 157.89696, Residuals: -1.68678, Convergence: 0.005498\n",
      "Epoch: 62, Loss: 157.06409, Residuals: -1.66478, Convergence: 0.005303\n",
      "Epoch: 63, Loss: 156.26385, Residuals: -1.64339, Convergence: 0.005121\n",
      "Epoch: 64, Loss: 155.49412, Residuals: -1.62259, Convergence: 0.004950\n",
      "Epoch: 65, Loss: 154.75328, Residuals: -1.60236, Convergence: 0.004787\n",
      "Epoch: 66, Loss: 154.04012, Residuals: -1.58269, Convergence: 0.004630\n",
      "Epoch: 67, Loss: 153.35374, Residuals: -1.56358, Convergence: 0.004476\n",
      "Epoch: 68, Loss: 152.69343, Residuals: -1.54502, Convergence: 0.004324\n",
      "Epoch: 69, Loss: 152.05862, Residuals: -1.52701, Convergence: 0.004175\n",
      "Epoch: 70, Loss: 151.44882, Residuals: -1.50955, Convergence: 0.004026\n",
      "Epoch: 71, Loss: 150.86354, Residuals: -1.49264, Convergence: 0.003879\n",
      "Epoch: 72, Loss: 150.30232, Residuals: -1.47628, Convergence: 0.003734\n",
      "Epoch: 73, Loss: 149.76462, Residuals: -1.46046, Convergence: 0.003590\n",
      "Epoch: 74, Loss: 149.24992, Residuals: -1.44518, Convergence: 0.003449\n",
      "Epoch: 75, Loss: 148.75760, Residuals: -1.43044, Convergence: 0.003310\n",
      "Epoch: 76, Loss: 148.28704, Residuals: -1.41622, Convergence: 0.003173\n",
      "Epoch: 77, Loss: 147.83756, Residuals: -1.40253, Convergence: 0.003040\n",
      "Epoch: 78, Loss: 147.40846, Residuals: -1.38934, Convergence: 0.002911\n",
      "Epoch: 79, Loss: 146.99902, Residuals: -1.37665, Convergence: 0.002785\n",
      "Epoch: 80, Loss: 146.60848, Residuals: -1.36445, Convergence: 0.002664\n",
      "Epoch: 81, Loss: 146.23610, Residuals: -1.35272, Convergence: 0.002546\n",
      "Epoch: 82, Loss: 145.88111, Residuals: -1.34146, Convergence: 0.002433\n",
      "Epoch: 83, Loss: 145.54277, Residuals: -1.33063, Convergence: 0.002325\n",
      "Epoch: 84, Loss: 145.22035, Residuals: -1.32025, Convergence: 0.002220\n",
      "Epoch: 85, Loss: 144.91313, Residuals: -1.31027, Convergence: 0.002120\n",
      "Epoch: 86, Loss: 144.62042, Residuals: -1.30070, Convergence: 0.002024\n",
      "Epoch: 87, Loss: 144.34158, Residuals: -1.29152, Convergence: 0.001932\n",
      "Epoch: 88, Loss: 144.07594, Residuals: -1.28272, Convergence: 0.001844\n",
      "Epoch: 89, Loss: 143.82295, Residuals: -1.27427, Convergence: 0.001759\n",
      "Epoch: 90, Loss: 143.58201, Residuals: -1.26616, Convergence: 0.001678\n",
      "Epoch: 91, Loss: 143.35261, Residuals: -1.25839, Convergence: 0.001600\n",
      "Epoch: 92, Loss: 143.13425, Residuals: -1.25093, Convergence: 0.001526\n",
      "Epoch: 93, Loss: 142.92647, Residuals: -1.24378, Convergence: 0.001454\n",
      "Epoch: 94, Loss: 142.72883, Residuals: -1.23691, Convergence: 0.001385\n",
      "Epoch: 95, Loss: 142.54095, Residuals: -1.23033, Convergence: 0.001318\n",
      "Epoch: 96, Loss: 142.36245, Residuals: -1.22401, Convergence: 0.001254\n",
      "Epoch: 97, Loss: 142.19300, Residuals: -1.21795, Convergence: 0.001192\n",
      "Epoch: 98, Loss: 142.03229, Residuals: -1.21213, Convergence: 0.001132\n",
      "Epoch: 99, Loss: 141.88003, Residuals: -1.20655, Convergence: 0.001073\n",
      "Epoch: 100, Loss: 141.73596, Residuals: -1.20119, Convergence: 0.001016\n",
      "Epoch: 101, Loss: 141.59986, Residuals: -1.19605, Convergence: 0.000961\n",
      "Evidence -183.700\n",
      "\n",
      "Epoch: 101, Evidence: -183.69958, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 7.25e-01\n",
      "Epoch: 101, Loss: 1380.81490, Residuals: -1.19605, Convergence:   inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 102, Loss: 1317.85985, Residuals: -1.22729, Convergence: 0.047771\n",
      "Epoch: 103, Loss: 1270.18666, Residuals: -1.25187, Convergence: 0.037532\n",
      "Epoch: 104, Loss: 1234.20101, Residuals: -1.26986, Convergence: 0.029157\n",
      "Epoch: 105, Loss: 1206.21251, Residuals: -1.28293, Convergence: 0.023204\n",
      "Epoch: 106, Loss: 1183.63058, Residuals: -1.29289, Convergence: 0.019079\n",
      "Epoch: 107, Loss: 1164.94774, Residuals: -1.30074, Convergence: 0.016037\n",
      "Epoch: 108, Loss: 1149.24238, Residuals: -1.30693, Convergence: 0.013666\n",
      "Epoch: 109, Loss: 1135.88388, Residuals: -1.31168, Convergence: 0.011760\n",
      "Epoch: 110, Loss: 1124.40208, Residuals: -1.31516, Convergence: 0.010211\n",
      "Epoch: 111, Loss: 1114.43110, Residuals: -1.31750, Convergence: 0.008947\n",
      "Epoch: 112, Loss: 1105.67624, Residuals: -1.31882, Convergence: 0.007918\n",
      "Epoch: 113, Loss: 1097.89657, Residuals: -1.31921, Convergence: 0.007086\n",
      "Epoch: 114, Loss: 1090.89081, Residuals: -1.31875, Convergence: 0.006422\n",
      "Epoch: 115, Loss: 1084.48695, Residuals: -1.31750, Convergence: 0.005905\n",
      "Epoch: 116, Loss: 1078.53539, Residuals: -1.31550, Convergence: 0.005518\n",
      "Epoch: 117, Loss: 1072.90195, Residuals: -1.31279, Convergence: 0.005251\n",
      "Epoch: 118, Loss: 1067.46682, Residuals: -1.30938, Convergence: 0.005092\n",
      "Epoch: 119, Loss: 1062.12372, Residuals: -1.30527, Convergence: 0.005031\n",
      "Epoch: 120, Loss: 1056.78339, Residuals: -1.30047, Convergence: 0.005053\n",
      "Epoch: 121, Loss: 1051.38405, Residuals: -1.29500, Convergence: 0.005135\n",
      "Epoch: 122, Loss: 1045.90625, Residuals: -1.28892, Convergence: 0.005237\n",
      "Epoch: 123, Loss: 1040.38311, Residuals: -1.28231, Convergence: 0.005309\n",
      "Epoch: 124, Loss: 1034.90011, Residuals: -1.27527, Convergence: 0.005298\n",
      "Epoch: 125, Loss: 1029.56558, Residuals: -1.26790, Convergence: 0.005181\n",
      "Epoch: 126, Loss: 1024.47382, Residuals: -1.26031, Convergence: 0.004970\n",
      "Epoch: 127, Loss: 1019.68060, Residuals: -1.25257, Convergence: 0.004701\n",
      "Epoch: 128, Loss: 1015.20090, Residuals: -1.24477, Convergence: 0.004413\n",
      "Epoch: 129, Loss: 1011.02376, Residuals: -1.23696, Convergence: 0.004132\n",
      "Epoch: 130, Loss: 1007.12377, Residuals: -1.22920, Convergence: 0.003872\n",
      "Epoch: 131, Loss: 1003.47162, Residuals: -1.22153, Convergence: 0.003640\n",
      "Epoch: 132, Loss: 1000.03898, Residuals: -1.21398, Convergence: 0.003433\n",
      "Epoch: 133, Loss: 996.80031, Residuals: -1.20658, Convergence: 0.003249\n",
      "Epoch: 134, Loss: 993.73418, Residuals: -1.19936, Convergence: 0.003085\n",
      "Epoch: 135, Loss: 990.82357, Residuals: -1.19232, Convergence: 0.002938\n",
      "Epoch: 136, Loss: 988.05311, Residuals: -1.18548, Convergence: 0.002804\n",
      "Epoch: 137, Loss: 985.41210, Residuals: -1.17886, Convergence: 0.002680\n",
      "Epoch: 138, Loss: 982.89062, Residuals: -1.17246, Convergence: 0.002565\n",
      "Epoch: 139, Loss: 980.48168, Residuals: -1.16629, Convergence: 0.002457\n",
      "Epoch: 140, Loss: 978.17801, Residuals: -1.16035, Convergence: 0.002355\n",
      "Epoch: 141, Loss: 975.97425, Residuals: -1.15465, Convergence: 0.002258\n",
      "Epoch: 142, Loss: 973.86486, Residuals: -1.14917, Convergence: 0.002166\n",
      "Epoch: 143, Loss: 971.84492, Residuals: -1.14393, Convergence: 0.002078\n",
      "Epoch: 144, Loss: 969.90991, Residuals: -1.13890, Convergence: 0.001995\n",
      "Epoch: 145, Loss: 968.05524, Residuals: -1.13409, Convergence: 0.001916\n",
      "Epoch: 146, Loss: 966.27669, Residuals: -1.12950, Convergence: 0.001841\n",
      "Epoch: 147, Loss: 964.56947, Residuals: -1.12510, Convergence: 0.001770\n",
      "Epoch: 148, Loss: 962.92995, Residuals: -1.12089, Convergence: 0.001703\n",
      "Epoch: 149, Loss: 961.35381, Residuals: -1.11687, Convergence: 0.001640\n",
      "Epoch: 150, Loss: 959.83727, Residuals: -1.11302, Convergence: 0.001580\n",
      "Epoch: 151, Loss: 958.37655, Residuals: -1.10933, Convergence: 0.001524\n",
      "Epoch: 152, Loss: 956.96758, Residuals: -1.10579, Convergence: 0.001472\n",
      "Epoch: 153, Loss: 955.60616, Residuals: -1.10240, Convergence: 0.001425\n",
      "Epoch: 154, Loss: 954.28846, Residuals: -1.09914, Convergence: 0.001381\n",
      "Epoch: 155, Loss: 953.01012, Residuals: -1.09601, Convergence: 0.001341\n",
      "Epoch: 156, Loss: 951.76672, Residuals: -1.09298, Convergence: 0.001306\n",
      "Epoch: 157, Loss: 950.55359, Residuals: -1.09006, Convergence: 0.001276\n",
      "Epoch: 158, Loss: 949.36589, Residuals: -1.08722, Convergence: 0.001251\n",
      "Epoch: 159, Loss: 948.19845, Residuals: -1.08447, Convergence: 0.001231\n",
      "Epoch: 160, Loss: 947.04645, Residuals: -1.08178, Convergence: 0.001216\n",
      "Epoch: 161, Loss: 945.90517, Residuals: -1.07914, Convergence: 0.001207\n",
      "Epoch: 162, Loss: 944.77088, Residuals: -1.07654, Convergence: 0.001201\n",
      "Epoch: 163, Loss: 943.64026, Residuals: -1.07397, Convergence: 0.001198\n",
      "Epoch: 164, Loss: 942.51256, Residuals: -1.07143, Convergence: 0.001196\n",
      "Epoch: 165, Loss: 941.38847, Residuals: -1.06891, Convergence: 0.001194\n",
      "Epoch: 166, Loss: 940.27106, Residuals: -1.06643, Convergence: 0.001188\n",
      "Epoch: 167, Loss: 939.16503, Residuals: -1.06397, Convergence: 0.001178\n",
      "Epoch: 168, Loss: 938.07655, Residuals: -1.06156, Convergence: 0.001160\n",
      "Epoch: 169, Loss: 937.01196, Residuals: -1.05920, Convergence: 0.001136\n",
      "Epoch: 170, Loss: 935.97672, Residuals: -1.05690, Convergence: 0.001106\n",
      "Epoch: 171, Loss: 934.97630, Residuals: -1.05467, Convergence: 0.001070\n",
      "Epoch: 172, Loss: 934.01403, Residuals: -1.05252, Convergence: 0.001030\n",
      "Epoch: 173, Loss: 933.09230, Residuals: -1.05046, Convergence: 0.000988\n",
      "Evidence 11388.217\n",
      "\n",
      "Epoch: 173, Evidence: 11388.21680, Convergence: 1.016131\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 5.73e-01\n",
      "Epoch: 173, Loss: 2376.36191, Residuals: -1.05046, Convergence:   inf\n",
      "Epoch: 174, Loss: 2340.94833, Residuals: -1.05790, Convergence: 0.015128\n",
      "Epoch: 175, Loss: 2314.57822, Residuals: -1.05639, Convergence: 0.011393\n",
      "Epoch: 176, Loss: 2292.55883, Residuals: -1.05407, Convergence: 0.009605\n",
      "Epoch: 177, Loss: 2274.03334, Residuals: -1.05158, Convergence: 0.008147\n",
      "Epoch: 178, Loss: 2258.30991, Residuals: -1.04902, Convergence: 0.006962\n",
      "Epoch: 179, Loss: 2244.84452, Residuals: -1.04642, Convergence: 0.005998\n",
      "Epoch: 180, Loss: 2233.18856, Residuals: -1.04378, Convergence: 0.005219\n",
      "Epoch: 181, Loss: 2222.96920, Residuals: -1.04109, Convergence: 0.004597\n",
      "Epoch: 182, Loss: 2213.87440, Residuals: -1.03832, Convergence: 0.004108\n",
      "Epoch: 183, Loss: 2205.65280, Residuals: -1.03544, Convergence: 0.003728\n",
      "Epoch: 184, Loss: 2198.11601, Residuals: -1.03242, Convergence: 0.003429\n",
      "Epoch: 185, Loss: 2191.14113, Residuals: -1.02927, Convergence: 0.003183\n",
      "Epoch: 186, Loss: 2184.66034, Residuals: -1.02600, Convergence: 0.002966\n",
      "Epoch: 187, Loss: 2178.64054, Residuals: -1.02266, Convergence: 0.002763\n",
      "Epoch: 188, Loss: 2173.05997, Residuals: -1.01932, Convergence: 0.002568\n",
      "Epoch: 189, Loss: 2167.89627, Residuals: -1.01601, Convergence: 0.002382\n",
      "Epoch: 190, Loss: 2163.12207, Residuals: -1.01278, Convergence: 0.002207\n",
      "Epoch: 191, Loss: 2158.70836, Residuals: -1.00965, Convergence: 0.002045\n",
      "Epoch: 192, Loss: 2154.62423, Residuals: -1.00664, Convergence: 0.001896\n",
      "Epoch: 193, Loss: 2150.83939, Residuals: -1.00376, Convergence: 0.001760\n",
      "Epoch: 194, Loss: 2147.32526, Residuals: -1.00102, Convergence: 0.001637\n",
      "Epoch: 195, Loss: 2144.05511, Residuals: -0.99842, Convergence: 0.001525\n",
      "Epoch: 196, Loss: 2141.00603, Residuals: -0.99597, Convergence: 0.001424\n",
      "Epoch: 197, Loss: 2138.15571, Residuals: -0.99365, Convergence: 0.001333\n",
      "Epoch: 198, Loss: 2135.48620, Residuals: -0.99148, Convergence: 0.001250\n",
      "Epoch: 199, Loss: 2132.98091, Residuals: -0.98944, Convergence: 0.001175\n",
      "Epoch: 200, Loss: 2130.62615, Residuals: -0.98753, Convergence: 0.001105\n",
      "Epoch: 201, Loss: 2128.40833, Residuals: -0.98575, Convergence: 0.001042\n",
      "Epoch: 202, Loss: 2126.31877, Residuals: -0.98409, Convergence: 0.000983\n",
      "Evidence 14638.801\n",
      "\n",
      "Epoch: 202, Evidence: 14638.80078, Convergence: 0.222053\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 4.34e-01\n",
      "Epoch: 202, Loss: 2507.80011, Residuals: -0.98409, Convergence:   inf\n",
      "Epoch: 203, Loss: 2494.74204, Residuals: -0.98048, Convergence: 0.005234\n",
      "Epoch: 204, Loss: 2484.01999, Residuals: -0.97675, Convergence: 0.004316\n",
      "Epoch: 205, Loss: 2474.81059, Residuals: -0.97336, Convergence: 0.003721\n",
      "Epoch: 206, Loss: 2466.85243, Residuals: -0.97032, Convergence: 0.003226\n",
      "Epoch: 207, Loss: 2459.93649, Residuals: -0.96761, Convergence: 0.002811\n",
      "Epoch: 208, Loss: 2453.88963, Residuals: -0.96522, Convergence: 0.002464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 209, Loss: 2448.57154, Residuals: -0.96310, Convergence: 0.002172\n",
      "Epoch: 210, Loss: 2443.86445, Residuals: -0.96125, Convergence: 0.001926\n",
      "Epoch: 211, Loss: 2439.67286, Residuals: -0.95961, Convergence: 0.001718\n",
      "Epoch: 212, Loss: 2435.91804, Residuals: -0.95816, Convergence: 0.001541\n",
      "Epoch: 213, Loss: 2432.53421, Residuals: -0.95688, Convergence: 0.001391\n",
      "Epoch: 214, Loss: 2429.46782, Residuals: -0.95574, Convergence: 0.001262\n",
      "Epoch: 215, Loss: 2426.67397, Residuals: -0.95472, Convergence: 0.001151\n",
      "Epoch: 216, Loss: 2424.11646, Residuals: -0.95380, Convergence: 0.001055\n",
      "Epoch: 217, Loss: 2421.76381, Residuals: -0.95298, Convergence: 0.000971\n",
      "Evidence 15002.631\n",
      "\n",
      "Epoch: 217, Evidence: 15002.63086, Convergence: 0.024251\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 3.30e-01\n",
      "Epoch: 217, Loss: 2513.55296, Residuals: -0.95298, Convergence:   inf\n",
      "Epoch: 218, Loss: 2507.13603, Residuals: -0.95003, Convergence: 0.002559\n",
      "Epoch: 219, Loss: 2501.84694, Residuals: -0.94761, Convergence: 0.002114\n",
      "Epoch: 220, Loss: 2497.37645, Residuals: -0.94565, Convergence: 0.001790\n",
      "Epoch: 221, Loss: 2493.53817, Residuals: -0.94406, Convergence: 0.001539\n",
      "Epoch: 222, Loss: 2490.19496, Residuals: -0.94276, Convergence: 0.001343\n",
      "Epoch: 223, Loss: 2487.24410, Residuals: -0.94171, Convergence: 0.001186\n",
      "Epoch: 224, Loss: 2484.60772, Residuals: -0.94085, Convergence: 0.001061\n",
      "Epoch: 225, Loss: 2482.22752, Residuals: -0.94015, Convergence: 0.000959\n",
      "Evidence 15081.034\n",
      "\n",
      "Epoch: 225, Evidence: 15081.03418, Convergence: 0.005199\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 2.57e-01\n",
      "Epoch: 225, Loss: 2515.29470, Residuals: -0.94015, Convergence:   inf\n",
      "Epoch: 226, Loss: 2511.39127, Residuals: -0.93807, Convergence: 0.001554\n",
      "Epoch: 227, Loss: 2508.15605, Residuals: -0.93652, Convergence: 0.001290\n",
      "Epoch: 228, Loss: 2505.38521, Residuals: -0.93535, Convergence: 0.001106\n",
      "Epoch: 229, Loss: 2502.96106, Residuals: -0.93445, Convergence: 0.000969\n",
      "Evidence 15109.072\n",
      "\n",
      "Epoch: 229, Evidence: 15109.07227, Convergence: 0.001856\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 2.07e-01\n",
      "Epoch: 229, Loss: 2516.36773, Residuals: -0.93445, Convergence:   inf\n",
      "Epoch: 230, Loss: 2513.44548, Residuals: -0.93286, Convergence: 0.001163\n",
      "Epoch: 231, Loss: 2510.99716, Residuals: -0.93171, Convergence: 0.000975\n",
      "Evidence 15121.364\n",
      "\n",
      "Epoch: 231, Evidence: 15121.36426, Convergence: 0.000813\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 1.71e-01\n",
      "Epoch: 231, Loss: 2517.14429, Residuals: -0.93171, Convergence:   inf\n",
      "Epoch: 232, Loss: 2512.49114, Residuals: -0.92970, Convergence: 0.001852\n",
      "Epoch: 233, Loss: 2508.99210, Residuals: -0.92860, Convergence: 0.001395\n",
      "Epoch: 234, Loss: 2506.14403, Residuals: -0.92798, Convergence: 0.001136\n",
      "Epoch: 235, Loss: 2503.71454, Residuals: -0.92776, Convergence: 0.000970\n",
      "Evidence 15139.404\n",
      "\n",
      "Epoch: 235, Evidence: 15139.40430, Convergence: 0.002004\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 1.42e-01\n",
      "Epoch: 235, Loss: 2517.26231, Residuals: -0.92776, Convergence:   inf\n",
      "Epoch: 236, Loss: 2514.19026, Residuals: -0.92601, Convergence: 0.001222\n",
      "Epoch: 237, Loss: 2511.75208, Residuals: -0.92536, Convergence: 0.000971\n",
      "Evidence 15150.621\n",
      "\n",
      "Epoch: 237, Evidence: 15150.62109, Convergence: 0.000740\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 1.20e-01\n",
      "Epoch: 237, Loss: 2517.45319, Residuals: -0.92536, Convergence:   inf\n",
      "Epoch: 238, Loss: 2512.94417, Residuals: -0.92356, Convergence: 0.001794\n",
      "Epoch: 239, Loss: 2509.65957, Residuals: -0.92515, Convergence: 0.001309\n",
      "Epoch: 240, Loss: 2506.93636, Residuals: -0.92558, Convergence: 0.001086\n",
      "Epoch: 241, Loss: 2504.50184, Residuals: -0.92828, Convergence: 0.000972\n",
      "Evidence 15166.824\n",
      "\n",
      "Epoch: 241, Evidence: 15166.82422, Convergence: 0.001808\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 1.07e-01\n",
      "Epoch: 241, Loss: 2516.68850, Residuals: -0.92828, Convergence:   inf\n",
      "Epoch: 242, Loss: 2514.45149, Residuals: -0.92727, Convergence: 0.000890\n",
      "Evidence 15174.084\n",
      "\n",
      "Epoch: 242, Evidence: 15174.08398, Convergence: 0.000478\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 8.75e-02\n",
      "Epoch: 242, Loss: 2517.62547, Residuals: -0.92727, Convergence:   inf\n",
      "Epoch: 243, Loss: 2556.94300, Residuals: -0.97266, Convergence: -0.015377\n",
      "Epoch: 243, Loss: 2515.32432, Residuals: -0.92853, Convergence: 0.000915\n",
      "Evidence 15178.352\n",
      "\n",
      "Epoch: 243, Evidence: 15178.35156, Convergence: 0.000759\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 8.11e-02\n",
      "Epoch: 243, Loss: 2517.02410, Residuals: -0.92853, Convergence:   inf\n",
      "Epoch: 244, Loss: 2520.27435, Residuals: -0.93355, Convergence: -0.001290\n",
      "Epoch: 244, Loss: 2516.46440, Residuals: -0.92857, Convergence: 0.000222\n",
      "Evidence 15180.676\n",
      "\n",
      "Epoch: 244, Evidence: 15180.67578, Convergence: 0.000912\n",
      "Total samples: 181, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 381.73091, Residuals: -4.53644, Convergence:   inf\n",
      "Epoch: 1, Loss: 356.00603, Residuals: -4.41523, Convergence: 0.072260\n",
      "Epoch: 2, Loss: 334.90953, Residuals: -4.24954, Convergence: 0.062992\n",
      "Epoch: 3, Loss: 318.83865, Residuals: -4.08391, Convergence: 0.050404\n",
      "Epoch: 4, Loss: 306.58084, Residuals: -3.93786, Convergence: 0.039982\n",
      "Epoch: 5, Loss: 296.86055, Residuals: -3.80868, Convergence: 0.032744\n",
      "Epoch: 6, Loss: 288.96949, Residuals: -3.69617, Convergence: 0.027308\n",
      "Epoch: 7, Loss: 282.41712, Residuals: -3.59985, Convergence: 0.023201\n",
      "Epoch: 8, Loss: 276.84123, Residuals: -3.51767, Convergence: 0.020141\n",
      "Epoch: 9, Loss: 271.98653, Residuals: -3.44731, Convergence: 0.017849\n",
      "Epoch: 10, Loss: 267.67189, Residuals: -3.38666, Convergence: 0.016119\n",
      "Epoch: 11, Loss: 263.76658, Residuals: -3.33393, Convergence: 0.014806\n",
      "Epoch: 12, Loss: 260.17554, Residuals: -3.28758, Convergence: 0.013802\n",
      "Epoch: 13, Loss: 256.82971, Residuals: -3.24626, Convergence: 0.013027\n",
      "Epoch: 14, Loss: 253.67927, Residuals: -3.20880, Convergence: 0.012419\n",
      "Epoch: 15, Loss: 250.68941, Residuals: -3.17421, Convergence: 0.011927\n",
      "Epoch: 16, Loss: 247.83853, Residuals: -3.14177, Convergence: 0.011503\n",
      "Epoch: 17, Loss: 245.11272, Residuals: -3.11101, Convergence: 0.011121\n",
      "Epoch: 18, Loss: 242.49425, Residuals: -3.08155, Convergence: 0.010798\n",
      "Epoch: 19, Loss: 239.95527, Residuals: -3.05292, Convergence: 0.010581\n",
      "Epoch: 20, Loss: 237.46206, Residuals: -3.02458, Convergence: 0.010499\n",
      "Epoch: 21, Loss: 234.98335, Residuals: -2.99602, Convergence: 0.010548\n",
      "Epoch: 22, Loss: 232.49625, Residuals: -2.96687, Convergence: 0.010697\n",
      "Epoch: 23, Loss: 229.98188, Residuals: -2.93690, Convergence: 0.010933\n",
      "Epoch: 24, Loss: 227.40921, Residuals: -2.90577, Convergence: 0.011313\n",
      "Epoch: 25, Loss: 224.72634, Residuals: -2.87285, Convergence: 0.011938\n",
      "Epoch: 26, Loss: 221.89095, Residuals: -2.83761, Convergence: 0.012778\n",
      "Epoch: 27, Loss: 218.95853, Residuals: -2.80052, Convergence: 0.013393\n",
      "Epoch: 28, Loss: 216.06468, Residuals: -2.76305, Convergence: 0.013393\n",
      "Epoch: 29, Loss: 213.27321, Residuals: -2.72603, Convergence: 0.013089\n",
      "Epoch: 30, Loss: 210.58189, Residuals: -2.68954, Convergence: 0.012780\n",
      "Epoch: 31, Loss: 207.97511, Residuals: -2.65349, Convergence: 0.012534\n",
      "Epoch: 32, Loss: 205.44003, Residuals: -2.61778, Convergence: 0.012340\n",
      "Epoch: 33, Loss: 202.96834, Residuals: -2.58236, Convergence: 0.012178\n",
      "Epoch: 34, Loss: 200.55536, Residuals: -2.54720, Convergence: 0.012031\n",
      "Epoch: 35, Loss: 198.19890, Residuals: -2.51227, Convergence: 0.011889\n",
      "Epoch: 36, Loss: 195.89835, Residuals: -2.47758, Convergence: 0.011744\n",
      "Epoch: 37, Loss: 193.65396, Residuals: -2.44313, Convergence: 0.011590\n",
      "Epoch: 38, Loss: 191.46649, Residuals: -2.40892, Convergence: 0.011425\n",
      "Epoch: 39, Loss: 189.33679, Residuals: -2.37495, Convergence: 0.011248\n",
      "Epoch: 40, Loss: 187.26571, Residuals: -2.34124, Convergence: 0.011060\n",
      "Epoch: 41, Loss: 185.25385, Residuals: -2.30778, Convergence: 0.010860\n",
      "Epoch: 42, Loss: 183.30165, Residuals: -2.27460, Convergence: 0.010650\n",
      "Epoch: 43, Loss: 181.40925, Residuals: -2.24170, Convergence: 0.010432\n",
      "Epoch: 44, Loss: 179.57661, Residuals: -2.20910, Convergence: 0.010205\n",
      "Epoch: 45, Loss: 177.80354, Residuals: -2.17682, Convergence: 0.009972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46, Loss: 176.08985, Residuals: -2.14488, Convergence: 0.009732\n",
      "Epoch: 47, Loss: 174.43548, Residuals: -2.11330, Convergence: 0.009484\n",
      "Epoch: 48, Loss: 172.84063, Residuals: -2.08211, Convergence: 0.009227\n",
      "Epoch: 49, Loss: 171.30579, Residuals: -2.05136, Convergence: 0.008960\n",
      "Epoch: 50, Loss: 169.83156, Residuals: -2.02108, Convergence: 0.008681\n",
      "Epoch: 51, Loss: 168.41841, Residuals: -1.99133, Convergence: 0.008391\n",
      "Epoch: 52, Loss: 167.06635, Residuals: -1.96214, Convergence: 0.008093\n",
      "Epoch: 53, Loss: 165.77480, Residuals: -1.93355, Convergence: 0.007791\n",
      "Epoch: 54, Loss: 164.54254, Residuals: -1.90558, Convergence: 0.007489\n",
      "Epoch: 55, Loss: 163.36786, Residuals: -1.87827, Convergence: 0.007190\n",
      "Epoch: 56, Loss: 162.24869, Residuals: -1.85161, Convergence: 0.006898\n",
      "Epoch: 57, Loss: 161.18287, Residuals: -1.82563, Convergence: 0.006612\n",
      "Epoch: 58, Loss: 160.16821, Residuals: -1.80031, Convergence: 0.006335\n",
      "Epoch: 59, Loss: 159.20258, Residuals: -1.77568, Convergence: 0.006065\n",
      "Epoch: 60, Loss: 158.28391, Residuals: -1.75174, Convergence: 0.005804\n",
      "Epoch: 61, Loss: 157.41011, Residuals: -1.72849, Convergence: 0.005551\n",
      "Epoch: 62, Loss: 156.57913, Residuals: -1.70594, Convergence: 0.005307\n",
      "Epoch: 63, Loss: 155.78879, Residuals: -1.68408, Convergence: 0.005073\n",
      "Epoch: 64, Loss: 155.03689, Residuals: -1.66292, Convergence: 0.004850\n",
      "Epoch: 65, Loss: 154.32114, Residuals: -1.64245, Convergence: 0.004638\n",
      "Epoch: 66, Loss: 153.63924, Residuals: -1.62264, Convergence: 0.004438\n",
      "Epoch: 67, Loss: 152.98892, Residuals: -1.60348, Convergence: 0.004251\n",
      "Epoch: 68, Loss: 152.36806, Residuals: -1.58496, Convergence: 0.004075\n",
      "Epoch: 69, Loss: 151.77467, Residuals: -1.56705, Convergence: 0.003910\n",
      "Epoch: 70, Loss: 151.20700, Residuals: -1.54972, Convergence: 0.003754\n",
      "Epoch: 71, Loss: 150.66354, Residuals: -1.53297, Convergence: 0.003607\n",
      "Epoch: 72, Loss: 150.14301, Residuals: -1.51676, Convergence: 0.003467\n",
      "Epoch: 73, Loss: 149.64430, Residuals: -1.50110, Convergence: 0.003333\n",
      "Epoch: 74, Loss: 149.16649, Residuals: -1.48595, Convergence: 0.003203\n",
      "Epoch: 75, Loss: 148.70878, Residuals: -1.47133, Convergence: 0.003078\n",
      "Epoch: 76, Loss: 148.27045, Residuals: -1.45720, Convergence: 0.002956\n",
      "Epoch: 77, Loss: 147.85083, Residuals: -1.44357, Convergence: 0.002838\n",
      "Epoch: 78, Loss: 147.44930, Residuals: -1.43042, Convergence: 0.002723\n",
      "Epoch: 79, Loss: 147.06523, Residuals: -1.41775, Convergence: 0.002612\n",
      "Epoch: 80, Loss: 146.69803, Residuals: -1.40554, Convergence: 0.002503\n",
      "Epoch: 81, Loss: 146.34708, Residuals: -1.39379, Convergence: 0.002398\n",
      "Epoch: 82, Loss: 146.01178, Residuals: -1.38248, Convergence: 0.002296\n",
      "Epoch: 83, Loss: 145.69153, Residuals: -1.37160, Convergence: 0.002198\n",
      "Epoch: 84, Loss: 145.38573, Residuals: -1.36115, Convergence: 0.002103\n",
      "Epoch: 85, Loss: 145.09380, Residuals: -1.35110, Convergence: 0.002012\n",
      "Epoch: 86, Loss: 144.81514, Residuals: -1.34146, Convergence: 0.001924\n",
      "Epoch: 87, Loss: 144.54922, Residuals: -1.33219, Convergence: 0.001840\n",
      "Epoch: 88, Loss: 144.29548, Residuals: -1.32330, Convergence: 0.001758\n",
      "Epoch: 89, Loss: 144.05342, Residuals: -1.31477, Convergence: 0.001680\n",
      "Epoch: 90, Loss: 143.82255, Residuals: -1.30658, Convergence: 0.001605\n",
      "Epoch: 91, Loss: 143.60242, Residuals: -1.29873, Convergence: 0.001533\n",
      "Epoch: 92, Loss: 143.39259, Residuals: -1.29120, Convergence: 0.001463\n",
      "Epoch: 93, Loss: 143.19268, Residuals: -1.28398, Convergence: 0.001396\n",
      "Epoch: 94, Loss: 143.00232, Residuals: -1.27707, Convergence: 0.001331\n",
      "Epoch: 95, Loss: 142.82118, Residuals: -1.27044, Convergence: 0.001268\n",
      "Epoch: 96, Loss: 142.64896, Residuals: -1.26409, Convergence: 0.001207\n",
      "Epoch: 97, Loss: 142.48540, Residuals: -1.25801, Convergence: 0.001148\n",
      "Epoch: 98, Loss: 142.33024, Residuals: -1.25220, Convergence: 0.001090\n",
      "Epoch: 99, Loss: 142.18326, Residuals: -1.24665, Convergence: 0.001034\n",
      "Epoch: 100, Loss: 142.04422, Residuals: -1.24134, Convergence: 0.000979\n",
      "Evidence -182.989\n",
      "\n",
      "Epoch: 100, Evidence: -182.98871, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 7.24e-01\n",
      "Epoch: 100, Loss: 1374.65264, Residuals: -1.24134, Convergence:   inf\n",
      "Epoch: 101, Loss: 1312.01710, Residuals: -1.27250, Convergence: 0.047740\n",
      "Epoch: 102, Loss: 1264.50807, Residuals: -1.29705, Convergence: 0.037571\n",
      "Epoch: 103, Loss: 1228.76743, Residuals: -1.31461, Convergence: 0.029087\n",
      "Epoch: 104, Loss: 1201.06966, Residuals: -1.32673, Convergence: 0.023061\n",
      "Epoch: 105, Loss: 1178.71325, Residuals: -1.33546, Convergence: 0.018967\n",
      "Epoch: 106, Loss: 1160.15019, Residuals: -1.34192, Convergence: 0.016001\n",
      "Epoch: 107, Loss: 1144.45900, Residuals: -1.34665, Convergence: 0.013711\n",
      "Epoch: 108, Loss: 1131.01951, Residuals: -1.34991, Convergence: 0.011883\n",
      "Epoch: 109, Loss: 1119.37587, Residuals: -1.35186, Convergence: 0.010402\n",
      "Epoch: 110, Loss: 1109.17461, Residuals: -1.35266, Convergence: 0.009197\n",
      "Epoch: 111, Loss: 1100.13471, Residuals: -1.35243, Convergence: 0.008217\n",
      "Epoch: 112, Loss: 1092.02684, Residuals: -1.35126, Convergence: 0.007425\n",
      "Epoch: 113, Loss: 1084.66351, Residuals: -1.34925, Convergence: 0.006789\n",
      "Epoch: 114, Loss: 1077.88916, Residuals: -1.34646, Convergence: 0.006285\n",
      "Epoch: 115, Loss: 1071.57609, Residuals: -1.34297, Convergence: 0.005891\n",
      "Epoch: 116, Loss: 1065.61927, Residuals: -1.33883, Convergence: 0.005590\n",
      "Epoch: 117, Loss: 1059.93224, Residuals: -1.33406, Convergence: 0.005365\n",
      "Epoch: 118, Loss: 1054.44365, Residuals: -1.32873, Convergence: 0.005205\n",
      "Epoch: 119, Loss: 1049.09167, Residuals: -1.32284, Convergence: 0.005102\n",
      "Epoch: 120, Loss: 1043.82165, Residuals: -1.31643, Convergence: 0.005049\n",
      "Epoch: 121, Loss: 1038.58746, Residuals: -1.30955, Convergence: 0.005040\n",
      "Epoch: 122, Loss: 1033.36026, Residuals: -1.30223, Convergence: 0.005058\n",
      "Epoch: 123, Loss: 1028.14247, Residuals: -1.29457, Convergence: 0.005075\n",
      "Epoch: 124, Loss: 1022.97525, Residuals: -1.28664, Convergence: 0.005051\n",
      "Epoch: 125, Loss: 1017.93006, Residuals: -1.27853, Convergence: 0.004956\n",
      "Epoch: 126, Loss: 1013.08480, Residuals: -1.27032, Convergence: 0.004783\n",
      "Epoch: 127, Loss: 1008.49647, Residuals: -1.26208, Convergence: 0.004550\n",
      "Epoch: 128, Loss: 1004.19093, Residuals: -1.25387, Convergence: 0.004288\n",
      "Epoch: 129, Loss: 1000.16805, Residuals: -1.24573, Convergence: 0.004022\n",
      "Epoch: 130, Loss: 996.41128, Residuals: -1.23770, Convergence: 0.003770\n",
      "Epoch: 131, Loss: 992.89782, Residuals: -1.22981, Convergence: 0.003539\n",
      "Epoch: 132, Loss: 989.60357, Residuals: -1.22210, Convergence: 0.003329\n",
      "Epoch: 133, Loss: 986.50632, Residuals: -1.21459, Convergence: 0.003140\n",
      "Epoch: 134, Loss: 983.58677, Residuals: -1.20729, Convergence: 0.002968\n",
      "Epoch: 135, Loss: 980.82811, Residuals: -1.20022, Convergence: 0.002813\n",
      "Epoch: 136, Loss: 978.21614, Residuals: -1.19339, Convergence: 0.002670\n",
      "Epoch: 137, Loss: 975.73911, Residuals: -1.18681, Convergence: 0.002539\n",
      "Epoch: 138, Loss: 973.38622, Residuals: -1.18048, Convergence: 0.002417\n",
      "Epoch: 139, Loss: 971.14827, Residuals: -1.17441, Convergence: 0.002304\n",
      "Epoch: 140, Loss: 969.01755, Residuals: -1.16859, Convergence: 0.002199\n",
      "Epoch: 141, Loss: 966.98639, Residuals: -1.16302, Convergence: 0.002101\n",
      "Epoch: 142, Loss: 965.04830, Residuals: -1.15770, Convergence: 0.002008\n",
      "Epoch: 143, Loss: 963.19699, Residuals: -1.15261, Convergence: 0.001922\n",
      "Epoch: 144, Loss: 961.42677, Residuals: -1.14776, Convergence: 0.001841\n",
      "Epoch: 145, Loss: 959.73187, Residuals: -1.14314, Convergence: 0.001766\n",
      "Epoch: 146, Loss: 958.10758, Residuals: -1.13873, Convergence: 0.001695\n",
      "Epoch: 147, Loss: 956.54924, Residuals: -1.13453, Convergence: 0.001629\n",
      "Epoch: 148, Loss: 955.05206, Residuals: -1.13052, Convergence: 0.001568\n",
      "Epoch: 149, Loss: 953.61182, Residuals: -1.12670, Convergence: 0.001510\n",
      "Epoch: 150, Loss: 952.22469, Residuals: -1.12306, Convergence: 0.001457\n",
      "Epoch: 151, Loss: 950.88658, Residuals: -1.11958, Convergence: 0.001407\n",
      "Epoch: 152, Loss: 949.59442, Residuals: -1.11626, Convergence: 0.001361\n",
      "Epoch: 153, Loss: 948.34428, Residuals: -1.11309, Convergence: 0.001318\n",
      "Epoch: 154, Loss: 947.13291, Residuals: -1.11005, Convergence: 0.001279\n",
      "Epoch: 155, Loss: 945.95692, Residuals: -1.10714, Convergence: 0.001243\n",
      "Epoch: 156, Loss: 944.81338, Residuals: -1.10435, Convergence: 0.001210\n",
      "Epoch: 157, Loss: 943.69862, Residuals: -1.10166, Convergence: 0.001181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 158, Loss: 942.60964, Residuals: -1.09908, Convergence: 0.001155\n",
      "Epoch: 159, Loss: 941.54281, Residuals: -1.09658, Convergence: 0.001133\n",
      "Epoch: 160, Loss: 940.49477, Residuals: -1.09417, Convergence: 0.001114\n",
      "Epoch: 161, Loss: 939.46179, Residuals: -1.09182, Convergence: 0.001100\n",
      "Epoch: 162, Loss: 938.44070, Residuals: -1.08954, Convergence: 0.001088\n",
      "Epoch: 163, Loss: 937.42777, Residuals: -1.08731, Convergence: 0.001081\n",
      "Epoch: 164, Loss: 936.42046, Residuals: -1.08513, Convergence: 0.001076\n",
      "Epoch: 165, Loss: 935.41633, Residuals: -1.08298, Convergence: 0.001073\n",
      "Epoch: 166, Loss: 934.41451, Residuals: -1.08085, Convergence: 0.001072\n",
      "Epoch: 167, Loss: 933.41509, Residuals: -1.07876, Convergence: 0.001071\n",
      "Epoch: 168, Loss: 932.42004, Residuals: -1.07669, Convergence: 0.001067\n",
      "Epoch: 169, Loss: 931.43247, Residuals: -1.07465, Convergence: 0.001060\n",
      "Epoch: 170, Loss: 930.45694, Residuals: -1.07264, Convergence: 0.001048\n",
      "Epoch: 171, Loss: 929.49818, Residuals: -1.07068, Convergence: 0.001031\n",
      "Epoch: 172, Loss: 928.56117, Residuals: -1.06876, Convergence: 0.001009\n",
      "Epoch: 173, Loss: 927.65016, Residuals: -1.06689, Convergence: 0.000982\n",
      "Evidence 11125.204\n",
      "\n",
      "Epoch: 173, Evidence: 11125.20410, Convergence: 1.016448\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 5.74e-01\n",
      "Epoch: 173, Loss: 2341.70499, Residuals: -1.06689, Convergence:   inf\n",
      "Epoch: 174, Loss: 2301.81416, Residuals: -1.07578, Convergence: 0.017330\n",
      "Epoch: 175, Loss: 2274.59562, Residuals: -1.07430, Convergence: 0.011966\n",
      "Epoch: 176, Loss: 2252.09425, Residuals: -1.07202, Convergence: 0.009991\n",
      "Epoch: 177, Loss: 2233.24256, Residuals: -1.06954, Convergence: 0.008441\n",
      "Epoch: 178, Loss: 2217.30720, Residuals: -1.06698, Convergence: 0.007187\n",
      "Epoch: 179, Loss: 2203.72491, Residuals: -1.06438, Convergence: 0.006163\n",
      "Epoch: 180, Loss: 2192.04657, Residuals: -1.06179, Convergence: 0.005328\n",
      "Epoch: 181, Loss: 2181.90391, Residuals: -1.05921, Convergence: 0.004649\n",
      "Epoch: 182, Loss: 2172.99392, Residuals: -1.05663, Convergence: 0.004100\n",
      "Epoch: 183, Loss: 2165.06780, Residuals: -1.05403, Convergence: 0.003661\n",
      "Epoch: 184, Loss: 2157.92170, Residuals: -1.05141, Convergence: 0.003312\n",
      "Epoch: 185, Loss: 2151.39721, Residuals: -1.04872, Convergence: 0.003033\n",
      "Epoch: 186, Loss: 2145.37960, Residuals: -1.04598, Convergence: 0.002805\n",
      "Epoch: 187, Loss: 2139.79425, Residuals: -1.04319, Convergence: 0.002610\n",
      "Epoch: 188, Loss: 2134.59742, Residuals: -1.04036, Convergence: 0.002435\n",
      "Epoch: 189, Loss: 2129.76253, Residuals: -1.03753, Convergence: 0.002270\n",
      "Epoch: 190, Loss: 2125.26719, Residuals: -1.03473, Convergence: 0.002115\n",
      "Epoch: 191, Loss: 2121.09064, Residuals: -1.03199, Convergence: 0.001969\n",
      "Epoch: 192, Loss: 2117.20943, Residuals: -1.02932, Convergence: 0.001833\n",
      "Epoch: 193, Loss: 2113.59943, Residuals: -1.02674, Convergence: 0.001708\n",
      "Epoch: 194, Loss: 2110.23824, Residuals: -1.02425, Convergence: 0.001593\n",
      "Epoch: 195, Loss: 2107.10326, Residuals: -1.02185, Convergence: 0.001488\n",
      "Epoch: 196, Loss: 2104.17705, Residuals: -1.01955, Convergence: 0.001391\n",
      "Epoch: 197, Loss: 2101.44217, Residuals: -1.01735, Convergence: 0.001301\n",
      "Epoch: 198, Loss: 2098.88444, Residuals: -1.01524, Convergence: 0.001219\n",
      "Epoch: 199, Loss: 2096.49101, Residuals: -1.01323, Convergence: 0.001142\n",
      "Epoch: 200, Loss: 2094.24913, Residuals: -1.01131, Convergence: 0.001070\n",
      "Epoch: 201, Loss: 2092.14884, Residuals: -1.00948, Convergence: 0.001004\n",
      "Epoch: 202, Loss: 2090.17900, Residuals: -1.00773, Convergence: 0.000942\n",
      "Evidence 14243.932\n",
      "\n",
      "Epoch: 202, Evidence: 14243.93164, Convergence: 0.218951\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 4.35e-01\n",
      "Epoch: 202, Loss: 2464.67738, Residuals: -1.00773, Convergence:   inf\n",
      "Epoch: 203, Loss: 2451.14970, Residuals: -1.00519, Convergence: 0.005519\n",
      "Epoch: 204, Loss: 2440.14600, Residuals: -1.00196, Convergence: 0.004509\n",
      "Epoch: 205, Loss: 2430.62826, Residuals: -0.99879, Convergence: 0.003916\n",
      "Epoch: 206, Loss: 2422.33769, Residuals: -0.99584, Convergence: 0.003423\n",
      "Epoch: 207, Loss: 2415.08420, Residuals: -0.99315, Convergence: 0.003003\n",
      "Epoch: 208, Loss: 2408.71546, Residuals: -0.99071, Convergence: 0.002644\n",
      "Epoch: 209, Loss: 2403.10160, Residuals: -0.98849, Convergence: 0.002336\n",
      "Epoch: 210, Loss: 2398.13173, Residuals: -0.98648, Convergence: 0.002072\n",
      "Epoch: 211, Loss: 2393.71197, Residuals: -0.98465, Convergence: 0.001846\n",
      "Epoch: 212, Loss: 2389.76156, Residuals: -0.98300, Convergence: 0.001653\n",
      "Epoch: 213, Loss: 2386.21184, Residuals: -0.98149, Convergence: 0.001488\n",
      "Epoch: 214, Loss: 2383.00459, Residuals: -0.98013, Convergence: 0.001346\n",
      "Epoch: 215, Loss: 2380.09235, Residuals: -0.97888, Convergence: 0.001224\n",
      "Epoch: 216, Loss: 2377.43319, Residuals: -0.97775, Convergence: 0.001118\n",
      "Epoch: 217, Loss: 2374.99364, Residuals: -0.97671, Convergence: 0.001027\n",
      "Epoch: 218, Loss: 2372.74445, Residuals: -0.97577, Convergence: 0.000948\n",
      "Evidence 14621.020\n",
      "\n",
      "Epoch: 218, Evidence: 14621.01953, Convergence: 0.025791\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 3.33e-01\n",
      "Epoch: 218, Loss: 2469.73785, Residuals: -0.97577, Convergence:   inf\n",
      "Epoch: 219, Loss: 2463.08513, Residuals: -0.97260, Convergence: 0.002701\n",
      "Epoch: 220, Loss: 2457.59888, Residuals: -0.96989, Convergence: 0.002232\n",
      "Epoch: 221, Loss: 2452.93788, Residuals: -0.96767, Convergence: 0.001900\n",
      "Epoch: 222, Loss: 2448.92462, Residuals: -0.96585, Convergence: 0.001639\n",
      "Epoch: 223, Loss: 2445.42710, Residuals: -0.96434, Convergence: 0.001430\n",
      "Epoch: 224, Loss: 2442.34469, Residuals: -0.96309, Convergence: 0.001262\n",
      "Epoch: 225, Loss: 2439.59755, Residuals: -0.96204, Convergence: 0.001126\n",
      "Epoch: 226, Loss: 2437.12529, Residuals: -0.96114, Convergence: 0.001014\n",
      "Epoch: 227, Loss: 2434.87987, Residuals: -0.96039, Convergence: 0.000922\n",
      "Evidence 14706.327\n",
      "\n",
      "Epoch: 227, Evidence: 14706.32715, Convergence: 0.005801\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 2.61e-01\n",
      "Epoch: 227, Loss: 2471.13638, Residuals: -0.96039, Convergence:   inf\n",
      "Epoch: 228, Loss: 2467.17124, Residuals: -0.95809, Convergence: 0.001607\n",
      "Epoch: 229, Loss: 2463.87493, Residuals: -0.95636, Convergence: 0.001338\n",
      "Epoch: 230, Loss: 2461.05214, Residuals: -0.95504, Convergence: 0.001147\n",
      "Epoch: 231, Loss: 2458.59169, Residuals: -0.95402, Convergence: 0.001001\n",
      "Epoch: 232, Loss: 2456.41241, Residuals: -0.95322, Convergence: 0.000887\n",
      "Evidence 14738.029\n",
      "\n",
      "Epoch: 232, Evidence: 14738.02930, Convergence: 0.002151\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 2.09e-01\n",
      "Epoch: 232, Loss: 2472.00467, Residuals: -0.95322, Convergence:   inf\n",
      "Epoch: 233, Loss: 2469.15571, Residuals: -0.95159, Convergence: 0.001154\n",
      "Epoch: 234, Loss: 2466.76583, Residuals: -0.95044, Convergence: 0.000969\n",
      "Evidence 14750.850\n",
      "\n",
      "Epoch: 234, Evidence: 14750.84961, Convergence: 0.000869\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.74e-01\n",
      "Epoch: 234, Loss: 2472.69348, Residuals: -0.95044, Convergence:   inf\n",
      "Epoch: 235, Loss: 2468.20682, Residuals: -0.94850, Convergence: 0.001818\n",
      "Epoch: 236, Loss: 2464.76805, Residuals: -0.94745, Convergence: 0.001395\n",
      "Epoch: 237, Loss: 2461.99832, Residuals: -0.94677, Convergence: 0.001125\n",
      "Epoch: 238, Loss: 2459.66145, Residuals: -0.94644, Convergence: 0.000950\n",
      "Evidence 14768.437\n",
      "\n",
      "Epoch: 238, Evidence: 14768.43652, Convergence: 0.002059\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.44e-01\n",
      "Epoch: 238, Loss: 2472.87279, Residuals: -0.94644, Convergence:   inf\n",
      "Epoch: 239, Loss: 2469.85782, Residuals: -0.94473, Convergence: 0.001221\n",
      "Epoch: 240, Loss: 2467.48108, Residuals: -0.94406, Convergence: 0.000963\n",
      "Evidence 14779.426\n",
      "\n",
      "Epoch: 240, Evidence: 14779.42578, Convergence: 0.000744\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.21e-01\n",
      "Epoch: 240, Loss: 2473.08664, Residuals: -0.94406, Convergence:   inf\n",
      "Epoch: 241, Loss: 2468.67973, Residuals: -0.94194, Convergence: 0.001785\n",
      "Epoch: 242, Loss: 2465.54431, Residuals: -0.94337, Convergence: 0.001272\n",
      "Epoch: 243, Loss: 2462.96658, Residuals: -0.94373, Convergence: 0.001047\n",
      "Epoch: 244, Loss: 2460.73546, Residuals: -0.94620, Convergence: 0.000907\n",
      "Evidence 14794.938\n",
      "\n",
      "Epoch: 244, Evidence: 14794.93848, Convergence: 0.001791\n",
      "Updating hyper-parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 181, Updated regularization: 1.08e-01\n",
      "Epoch: 244, Loss: 2472.40010, Residuals: -0.94620, Convergence:   inf\n",
      "Epoch: 245, Loss: 2470.56273, Residuals: -0.94481, Convergence: 0.000744\n",
      "Evidence 14801.676\n",
      "\n",
      "Epoch: 245, Evidence: 14801.67578, Convergence: 0.000455\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 8.84e-02\n",
      "Epoch: 245, Loss: 2473.41264, Residuals: -0.94481, Convergence:   inf\n",
      "Epoch: 246, Loss: 2520.61282, Residuals: -0.99022, Convergence: -0.018726\n",
      "Epoch: 246, Loss: 2471.06380, Residuals: -0.94379, Convergence: 0.000951\n",
      "Evidence 14805.999\n",
      "\n",
      "Epoch: 246, Evidence: 14805.99902, Convergence: 0.000747\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 8.16e-02\n",
      "Epoch: 246, Loss: 2472.83259, Residuals: -0.94379, Convergence:   inf\n",
      "Epoch: 247, Loss: 2476.05461, Residuals: -0.94785, Convergence: -0.001301\n",
      "Epoch: 247, Loss: 2472.38547, Residuals: -0.94313, Convergence: 0.000181\n",
      "Evidence 14808.085\n",
      "\n",
      "Epoch: 247, Evidence: 14808.08496, Convergence: 0.000888\n",
      "Total samples: 183, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 379.77830, Residuals: -4.50296, Convergence:   inf\n",
      "Epoch: 1, Loss: 354.08370, Residuals: -4.38270, Convergence: 0.072566\n",
      "Epoch: 2, Loss: 333.08070, Residuals: -4.21942, Convergence: 0.063057\n",
      "Epoch: 3, Loss: 317.00947, Residuals: -4.05533, Convergence: 0.050696\n",
      "Epoch: 4, Loss: 304.74729, Residuals: -3.91047, Convergence: 0.040237\n",
      "Epoch: 5, Loss: 295.02373, Residuals: -3.78227, Convergence: 0.032959\n",
      "Epoch: 6, Loss: 287.13491, Residuals: -3.67061, Convergence: 0.027474\n",
      "Epoch: 7, Loss: 280.59125, Residuals: -3.57505, Convergence: 0.023321\n",
      "Epoch: 8, Loss: 275.03001, Residuals: -3.49351, Convergence: 0.020220\n",
      "Epoch: 9, Loss: 270.19559, Residuals: -3.42363, Convergence: 0.017892\n",
      "Epoch: 10, Loss: 265.90638, Residuals: -3.36332, Convergence: 0.016131\n",
      "Epoch: 11, Loss: 262.03086, Residuals: -3.31080, Convergence: 0.014790\n",
      "Epoch: 12, Loss: 258.47320, Residuals: -3.26458, Convergence: 0.013764\n",
      "Epoch: 13, Loss: 255.16401, Residuals: -3.22336, Convergence: 0.012969\n",
      "Epoch: 14, Loss: 252.05393, Residuals: -3.18601, Convergence: 0.012339\n",
      "Epoch: 15, Loss: 249.10962, Residuals: -3.15157, Convergence: 0.011819\n",
      "Epoch: 16, Loss: 246.31100, Residuals: -3.11933, Convergence: 0.011362\n",
      "Epoch: 17, Loss: 243.64416, Residuals: -3.08883, Convergence: 0.010946\n",
      "Epoch: 18, Loss: 241.08983, Residuals: -3.05964, Convergence: 0.010595\n",
      "Epoch: 19, Loss: 238.61867, Residuals: -3.03128, Convergence: 0.010356\n",
      "Epoch: 20, Loss: 236.19614, Residuals: -3.00319, Convergence: 0.010256\n",
      "Epoch: 21, Loss: 233.78965, Residuals: -2.97484, Convergence: 0.010293\n",
      "Epoch: 22, Loss: 231.37262, Residuals: -2.94583, Convergence: 0.010446\n",
      "Epoch: 23, Loss: 228.91900, Residuals: -2.91584, Convergence: 0.010718\n",
      "Epoch: 24, Loss: 226.38821, Residuals: -2.88442, Convergence: 0.011179\n",
      "Epoch: 25, Loss: 223.72512, Residuals: -2.85095, Convergence: 0.011903\n",
      "Epoch: 26, Loss: 220.92307, Residuals: -2.81527, Convergence: 0.012683\n",
      "Epoch: 27, Loss: 218.10034, Residuals: -2.77862, Convergence: 0.012942\n",
      "Epoch: 28, Loss: 215.37260, Residuals: -2.74238, Convergence: 0.012665\n",
      "Epoch: 29, Loss: 212.75827, Residuals: -2.70693, Convergence: 0.012288\n",
      "Epoch: 30, Loss: 210.23821, Residuals: -2.67216, Convergence: 0.011987\n",
      "Epoch: 31, Loss: 207.79155, Residuals: -2.63790, Convergence: 0.011775\n",
      "Epoch: 32, Loss: 205.40283, Residuals: -2.60402, Convergence: 0.011629\n",
      "Epoch: 33, Loss: 203.06218, Residuals: -2.57038, Convergence: 0.011527\n",
      "Epoch: 34, Loss: 200.76423, Residuals: -2.53692, Convergence: 0.011446\n",
      "Epoch: 35, Loss: 198.50690, Residuals: -2.50359, Convergence: 0.011372\n",
      "Epoch: 36, Loss: 196.29038, Residuals: -2.47036, Convergence: 0.011292\n",
      "Epoch: 37, Loss: 194.11622, Residuals: -2.43723, Convergence: 0.011200\n",
      "Epoch: 38, Loss: 191.98661, Residuals: -2.40420, Convergence: 0.011093\n",
      "Epoch: 39, Loss: 189.90390, Residuals: -2.37130, Convergence: 0.010967\n",
      "Epoch: 40, Loss: 187.87025, Residuals: -2.33853, Convergence: 0.010825\n",
      "Epoch: 41, Loss: 185.88738, Residuals: -2.30593, Convergence: 0.010667\n",
      "Epoch: 42, Loss: 183.95647, Residuals: -2.27352, Convergence: 0.010497\n",
      "Epoch: 43, Loss: 182.07814, Residuals: -2.24131, Convergence: 0.010316\n",
      "Epoch: 44, Loss: 180.25244, Residuals: -2.20932, Convergence: 0.010129\n",
      "Epoch: 45, Loss: 178.47904, Residuals: -2.17757, Convergence: 0.009936\n",
      "Epoch: 46, Loss: 176.75732, Residuals: -2.14606, Convergence: 0.009741\n",
      "Epoch: 47, Loss: 175.08665, Residuals: -2.11481, Convergence: 0.009542\n",
      "Epoch: 48, Loss: 173.46661, Residuals: -2.08382, Convergence: 0.009339\n",
      "Epoch: 49, Loss: 171.89717, Residuals: -2.05313, Convergence: 0.009130\n",
      "Epoch: 50, Loss: 170.37884, Residuals: -2.02276, Convergence: 0.008911\n",
      "Epoch: 51, Loss: 168.91261, Residuals: -1.99274, Convergence: 0.008680\n",
      "Epoch: 52, Loss: 167.49973, Residuals: -1.96312, Convergence: 0.008435\n",
      "Epoch: 53, Loss: 166.14143, Residuals: -1.93395, Convergence: 0.008176\n",
      "Epoch: 54, Loss: 164.83854, Residuals: -1.90528, Convergence: 0.007904\n",
      "Epoch: 55, Loss: 163.59143, Residuals: -1.87715, Convergence: 0.007623\n",
      "Epoch: 56, Loss: 162.39981, Residuals: -1.84961, Convergence: 0.007338\n",
      "Epoch: 57, Loss: 161.26290, Residuals: -1.82268, Convergence: 0.007050\n",
      "Epoch: 58, Loss: 160.17948, Residuals: -1.79640, Convergence: 0.006764\n",
      "Epoch: 59, Loss: 159.14802, Residuals: -1.77079, Convergence: 0.006481\n",
      "Epoch: 60, Loss: 158.16680, Residuals: -1.74588, Convergence: 0.006204\n",
      "Epoch: 61, Loss: 157.23396, Residuals: -1.72167, Convergence: 0.005933\n",
      "Epoch: 62, Loss: 156.34762, Residuals: -1.69818, Convergence: 0.005669\n",
      "Epoch: 63, Loss: 155.50584, Residuals: -1.67541, Convergence: 0.005413\n",
      "Epoch: 64, Loss: 154.70669, Residuals: -1.65337, Convergence: 0.005166\n",
      "Epoch: 65, Loss: 153.94826, Residuals: -1.63207, Convergence: 0.004927\n",
      "Epoch: 66, Loss: 153.22857, Residuals: -1.61150, Convergence: 0.004697\n",
      "Epoch: 67, Loss: 152.54563, Residuals: -1.59166, Convergence: 0.004477\n",
      "Epoch: 68, Loss: 151.89739, Residuals: -1.57255, Convergence: 0.004268\n",
      "Epoch: 69, Loss: 151.28172, Residuals: -1.55415, Convergence: 0.004070\n",
      "Epoch: 70, Loss: 150.69647, Residuals: -1.53644, Convergence: 0.003884\n",
      "Epoch: 71, Loss: 150.13948, Residuals: -1.51940, Convergence: 0.003710\n",
      "Epoch: 72, Loss: 149.60868, Residuals: -1.50300, Convergence: 0.003548\n",
      "Epoch: 73, Loss: 149.10214, Residuals: -1.48722, Convergence: 0.003397\n",
      "Epoch: 74, Loss: 148.61815, Residuals: -1.47203, Convergence: 0.003257\n",
      "Epoch: 75, Loss: 148.15523, Residuals: -1.45739, Convergence: 0.003125\n",
      "Epoch: 76, Loss: 147.71219, Residuals: -1.44330, Convergence: 0.002999\n",
      "Epoch: 77, Loss: 147.28802, Residuals: -1.42972, Convergence: 0.002880\n",
      "Epoch: 78, Loss: 146.88194, Residuals: -1.41665, Convergence: 0.002765\n",
      "Epoch: 79, Loss: 146.49328, Residuals: -1.40406, Convergence: 0.002653\n",
      "Epoch: 80, Loss: 146.12146, Residuals: -1.39195, Convergence: 0.002545\n",
      "Epoch: 81, Loss: 145.76598, Residuals: -1.38030, Convergence: 0.002439\n",
      "Epoch: 82, Loss: 145.42637, Residuals: -1.36910, Convergence: 0.002335\n",
      "Epoch: 83, Loss: 145.10215, Residuals: -1.35834, Convergence: 0.002234\n",
      "Epoch: 84, Loss: 144.79287, Residuals: -1.34801, Convergence: 0.002136\n",
      "Epoch: 85, Loss: 144.49808, Residuals: -1.33810, Convergence: 0.002040\n",
      "Epoch: 86, Loss: 144.21733, Residuals: -1.32860, Convergence: 0.001947\n",
      "Epoch: 87, Loss: 143.95017, Residuals: -1.31949, Convergence: 0.001856\n",
      "Epoch: 88, Loss: 143.69618, Residuals: -1.31076, Convergence: 0.001768\n",
      "Epoch: 89, Loss: 143.45492, Residuals: -1.30241, Convergence: 0.001682\n",
      "Epoch: 90, Loss: 143.22600, Residuals: -1.29441, Convergence: 0.001598\n",
      "Epoch: 91, Loss: 143.00902, Residuals: -1.28677, Convergence: 0.001517\n",
      "Epoch: 92, Loss: 142.80359, Residuals: -1.27947, Convergence: 0.001439\n",
      "Epoch: 93, Loss: 142.60936, Residuals: -1.27250, Convergence: 0.001362\n",
      "Epoch: 94, Loss: 142.42597, Residuals: -1.26584, Convergence: 0.001288\n",
      "Epoch: 95, Loss: 142.25309, Residuals: -1.25951, Convergence: 0.001215\n",
      "Epoch: 96, Loss: 142.09036, Residuals: -1.25347, Convergence: 0.001145\n",
      "Epoch: 97, Loss: 141.93742, Residuals: -1.24774, Convergence: 0.001077\n",
      "Epoch: 98, Loss: 141.79388, Residuals: -1.24231, Convergence: 0.001012\n",
      "Epoch: 99, Loss: 141.65932, Residuals: -1.23716, Convergence: 0.000950\n",
      "Evidence -183.234\n",
      "\n",
      "Epoch: 99, Evidence: -183.23354, Convergence:   inf\n",
      "Updating hyper-parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 183, Updated regularization: 7.24e-01\n",
      "Epoch: 99, Loss: 1373.11552, Residuals: -1.23716, Convergence:   inf\n",
      "Epoch: 100, Loss: 1308.87392, Residuals: -1.26717, Convergence: 0.049082\n",
      "Epoch: 101, Loss: 1260.39652, Residuals: -1.29053, Convergence: 0.038462\n",
      "Epoch: 102, Loss: 1224.10541, Residuals: -1.30668, Convergence: 0.029647\n",
      "Epoch: 103, Loss: 1195.94034, Residuals: -1.31746, Convergence: 0.023551\n",
      "Epoch: 104, Loss: 1173.13391, Residuals: -1.32508, Convergence: 0.019441\n",
      "Epoch: 105, Loss: 1154.17183, Residuals: -1.33067, Convergence: 0.016429\n",
      "Epoch: 106, Loss: 1138.14775, Residuals: -1.33468, Convergence: 0.014079\n",
      "Epoch: 107, Loss: 1124.43792, Residuals: -1.33739, Convergence: 0.012193\n",
      "Epoch: 108, Loss: 1112.57546, Residuals: -1.33894, Convergence: 0.010662\n",
      "Epoch: 109, Loss: 1102.19227, Residuals: -1.33946, Convergence: 0.009420\n",
      "Epoch: 110, Loss: 1092.99049, Residuals: -1.33906, Convergence: 0.008419\n",
      "Epoch: 111, Loss: 1084.72221, Residuals: -1.33780, Convergence: 0.007622\n",
      "Epoch: 112, Loss: 1077.17694, Residuals: -1.33575, Convergence: 0.007005\n",
      "Epoch: 113, Loss: 1070.17182, Residuals: -1.33295, Convergence: 0.006546\n",
      "Epoch: 114, Loss: 1063.54612, Residuals: -1.32939, Convergence: 0.006230\n",
      "Epoch: 115, Loss: 1057.16015, Residuals: -1.32510, Convergence: 0.006041\n",
      "Epoch: 116, Loss: 1050.89751, Residuals: -1.32008, Convergence: 0.005959\n",
      "Epoch: 117, Loss: 1044.67835, Residuals: -1.31437, Convergence: 0.005953\n",
      "Epoch: 118, Loss: 1038.47425, Residuals: -1.30802, Convergence: 0.005974\n",
      "Epoch: 119, Loss: 1032.31917, Residuals: -1.30114, Convergence: 0.005962\n",
      "Epoch: 120, Loss: 1026.29540, Residuals: -1.29383, Convergence: 0.005869\n",
      "Epoch: 121, Loss: 1020.50237, Residuals: -1.28621, Convergence: 0.005677\n",
      "Epoch: 122, Loss: 1015.02009, Residuals: -1.27837, Convergence: 0.005401\n",
      "Epoch: 123, Loss: 1009.88959, Residuals: -1.27040, Convergence: 0.005080\n",
      "Epoch: 124, Loss: 1005.11752, Residuals: -1.26238, Convergence: 0.004748\n",
      "Epoch: 125, Loss: 1000.68643, Residuals: -1.25436, Convergence: 0.004428\n",
      "Epoch: 126, Loss: 996.56858, Residuals: -1.24640, Convergence: 0.004132\n",
      "Epoch: 127, Loss: 992.73266, Residuals: -1.23854, Convergence: 0.003864\n",
      "Epoch: 128, Loss: 989.14947, Residuals: -1.23082, Convergence: 0.003622\n",
      "Epoch: 129, Loss: 985.79219, Residuals: -1.22327, Convergence: 0.003406\n",
      "Epoch: 130, Loss: 982.63817, Residuals: -1.21591, Convergence: 0.003210\n",
      "Epoch: 131, Loss: 979.66769, Residuals: -1.20876, Convergence: 0.003032\n",
      "Epoch: 132, Loss: 976.86497, Residuals: -1.20183, Convergence: 0.002869\n",
      "Epoch: 133, Loss: 974.21554, Residuals: -1.19513, Convergence: 0.002720\n",
      "Epoch: 134, Loss: 971.70803, Residuals: -1.18868, Convergence: 0.002581\n",
      "Epoch: 135, Loss: 969.33240, Residuals: -1.18248, Convergence: 0.002451\n",
      "Epoch: 136, Loss: 967.07935, Residuals: -1.17652, Convergence: 0.002330\n",
      "Epoch: 137, Loss: 964.94086, Residuals: -1.17082, Convergence: 0.002216\n",
      "Epoch: 138, Loss: 962.90967, Residuals: -1.16537, Convergence: 0.002109\n",
      "Epoch: 139, Loss: 960.97858, Residuals: -1.16016, Convergence: 0.002010\n",
      "Epoch: 140, Loss: 959.14103, Residuals: -1.15519, Convergence: 0.001916\n",
      "Epoch: 141, Loss: 957.39096, Residuals: -1.15046, Convergence: 0.001828\n",
      "Epoch: 142, Loss: 955.72243, Residuals: -1.14594, Convergence: 0.001746\n",
      "Epoch: 143, Loss: 954.12950, Residuals: -1.14165, Convergence: 0.001670\n",
      "Epoch: 144, Loss: 952.60715, Residuals: -1.13756, Convergence: 0.001598\n",
      "Epoch: 145, Loss: 951.15051, Residuals: -1.13367, Convergence: 0.001531\n",
      "Epoch: 146, Loss: 949.75425, Residuals: -1.12996, Convergence: 0.001470\n",
      "Epoch: 147, Loss: 948.41450, Residuals: -1.12643, Convergence: 0.001413\n",
      "Epoch: 148, Loss: 947.12666, Residuals: -1.12306, Convergence: 0.001360\n",
      "Epoch: 149, Loss: 945.88670, Residuals: -1.11985, Convergence: 0.001311\n",
      "Epoch: 150, Loss: 944.69106, Residuals: -1.11678, Convergence: 0.001266\n",
      "Epoch: 151, Loss: 943.53580, Residuals: -1.11385, Convergence: 0.001224\n",
      "Epoch: 152, Loss: 942.41773, Residuals: -1.11104, Convergence: 0.001186\n",
      "Epoch: 153, Loss: 941.33366, Residuals: -1.10835, Convergence: 0.001152\n",
      "Epoch: 154, Loss: 940.27995, Residuals: -1.10577, Convergence: 0.001121\n",
      "Epoch: 155, Loss: 939.25361, Residuals: -1.10328, Convergence: 0.001093\n",
      "Epoch: 156, Loss: 938.25137, Residuals: -1.10088, Convergence: 0.001068\n",
      "Epoch: 157, Loss: 937.26965, Residuals: -1.09856, Convergence: 0.001047\n",
      "Epoch: 158, Loss: 936.30546, Residuals: -1.09631, Convergence: 0.001030\n",
      "Epoch: 159, Loss: 935.35505, Residuals: -1.09413, Convergence: 0.001016\n",
      "Epoch: 160, Loss: 934.41508, Residuals: -1.09199, Convergence: 0.001006\n",
      "Epoch: 161, Loss: 933.48198, Residuals: -1.08989, Convergence: 0.001000\n",
      "Evidence 11178.729\n",
      "\n",
      "Epoch: 161, Evidence: 11178.72852, Convergence: 1.016391\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 5.77e-01\n",
      "Epoch: 161, Loss: 2355.11405, Residuals: -1.08989, Convergence:   inf\n",
      "Epoch: 162, Loss: 2314.13750, Residuals: -1.09912, Convergence: 0.017707\n",
      "Epoch: 163, Loss: 2285.89243, Residuals: -1.09750, Convergence: 0.012356\n",
      "Epoch: 164, Loss: 2262.17669, Residuals: -1.09494, Convergence: 0.010484\n",
      "Epoch: 165, Loss: 2242.00374, Residuals: -1.09203, Convergence: 0.008998\n",
      "Epoch: 166, Loss: 2224.70885, Residuals: -1.08891, Convergence: 0.007774\n",
      "Epoch: 167, Loss: 2209.78194, Residuals: -1.08565, Convergence: 0.006755\n",
      "Epoch: 168, Loss: 2196.81283, Residuals: -1.08231, Convergence: 0.005904\n",
      "Epoch: 169, Loss: 2185.45608, Residuals: -1.07891, Convergence: 0.005197\n",
      "Epoch: 170, Loss: 2175.41862, Residuals: -1.07546, Convergence: 0.004614\n",
      "Epoch: 171, Loss: 2166.45297, Residuals: -1.07195, Convergence: 0.004138\n",
      "Epoch: 172, Loss: 2158.35784, Residuals: -1.06836, Convergence: 0.003751\n",
      "Epoch: 173, Loss: 2150.98319, Residuals: -1.06471, Convergence: 0.003429\n",
      "Epoch: 174, Loss: 2144.22619, Residuals: -1.06099, Convergence: 0.003151\n",
      "Epoch: 175, Loss: 2138.02181, Residuals: -1.05726, Convergence: 0.002902\n",
      "Epoch: 176, Loss: 2132.32424, Residuals: -1.05356, Convergence: 0.002672\n",
      "Epoch: 177, Loss: 2127.09510, Residuals: -1.04993, Convergence: 0.002458\n",
      "Epoch: 178, Loss: 2122.29645, Residuals: -1.04640, Convergence: 0.002261\n",
      "Epoch: 179, Loss: 2117.88893, Residuals: -1.04301, Convergence: 0.002081\n",
      "Epoch: 180, Loss: 2113.83318, Residuals: -1.03976, Convergence: 0.001919\n",
      "Epoch: 181, Loss: 2110.09096, Residuals: -1.03667, Convergence: 0.001773\n",
      "Epoch: 182, Loss: 2106.62751, Residuals: -1.03372, Convergence: 0.001644\n",
      "Epoch: 183, Loss: 2103.41146, Residuals: -1.03093, Convergence: 0.001529\n",
      "Epoch: 184, Loss: 2100.41620, Residuals: -1.02827, Convergence: 0.001426\n",
      "Epoch: 185, Loss: 2097.61696, Residuals: -1.02575, Convergence: 0.001334\n",
      "Epoch: 186, Loss: 2094.99528, Residuals: -1.02336, Convergence: 0.001251\n",
      "Epoch: 187, Loss: 2092.53470, Residuals: -1.02109, Convergence: 0.001176\n",
      "Epoch: 188, Loss: 2090.22148, Residuals: -1.01893, Convergence: 0.001107\n",
      "Epoch: 189, Loss: 2088.04412, Residuals: -1.01687, Convergence: 0.001043\n",
      "Epoch: 190, Loss: 2085.99358, Residuals: -1.01492, Convergence: 0.000983\n",
      "Evidence 14311.176\n",
      "\n",
      "Epoch: 190, Evidence: 14311.17578, Convergence: 0.218881\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 4.40e-01\n",
      "Epoch: 190, Loss: 2478.32607, Residuals: -1.01492, Convergence:   inf\n",
      "Epoch: 191, Loss: 2464.59837, Residuals: -1.01213, Convergence: 0.005570\n",
      "Epoch: 192, Loss: 2453.27274, Residuals: -1.00881, Convergence: 0.004617\n",
      "Epoch: 193, Loss: 2443.42567, Residuals: -1.00555, Convergence: 0.004030\n",
      "Epoch: 194, Loss: 2434.81131, Residuals: -1.00244, Convergence: 0.003538\n",
      "Epoch: 195, Loss: 2427.24416, Residuals: -0.99952, Convergence: 0.003118\n",
      "Epoch: 196, Loss: 2420.57384, Residuals: -0.99681, Convergence: 0.002756\n",
      "Epoch: 197, Loss: 2414.67211, Residuals: -0.99429, Convergence: 0.002444\n",
      "Epoch: 198, Loss: 2409.43113, Residuals: -0.99195, Convergence: 0.002175\n",
      "Epoch: 199, Loss: 2404.75753, Residuals: -0.98977, Convergence: 0.001943\n",
      "Epoch: 200, Loss: 2400.56973, Residuals: -0.98776, Convergence: 0.001745\n",
      "Epoch: 201, Loss: 2396.80102, Residuals: -0.98589, Convergence: 0.001572\n",
      "Epoch: 202, Loss: 2393.39209, Residuals: -0.98415, Convergence: 0.001424\n",
      "Epoch: 203, Loss: 2390.29477, Residuals: -0.98254, Convergence: 0.001296\n",
      "Epoch: 204, Loss: 2387.46700, Residuals: -0.98105, Convergence: 0.001184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 205, Loss: 2384.87328, Residuals: -0.97966, Convergence: 0.001088\n",
      "Epoch: 206, Loss: 2382.48492, Residuals: -0.97837, Convergence: 0.001002\n",
      "Epoch: 207, Loss: 2380.27558, Residuals: -0.97718, Convergence: 0.000928\n",
      "Evidence 14715.830\n",
      "\n",
      "Epoch: 207, Evidence: 14715.83008, Convergence: 0.027498\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 3.37e-01\n",
      "Epoch: 207, Loss: 2483.62064, Residuals: -0.97718, Convergence:   inf\n",
      "Epoch: 208, Loss: 2476.88741, Residuals: -0.97393, Convergence: 0.002718\n",
      "Epoch: 209, Loss: 2471.27963, Residuals: -0.97108, Convergence: 0.002269\n",
      "Epoch: 210, Loss: 2466.49832, Residuals: -0.96865, Convergence: 0.001939\n",
      "Epoch: 211, Loss: 2462.37211, Residuals: -0.96658, Convergence: 0.001676\n",
      "Epoch: 212, Loss: 2458.77138, Residuals: -0.96480, Convergence: 0.001464\n",
      "Epoch: 213, Loss: 2455.59448, Residuals: -0.96327, Convergence: 0.001294\n",
      "Epoch: 214, Loss: 2452.76295, Residuals: -0.96195, Convergence: 0.001154\n",
      "Epoch: 215, Loss: 2450.21441, Residuals: -0.96082, Convergence: 0.001040\n",
      "Epoch: 216, Loss: 2447.90005, Residuals: -0.95984, Convergence: 0.000945\n",
      "Evidence 14805.148\n",
      "\n",
      "Epoch: 216, Evidence: 14805.14844, Convergence: 0.006033\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.64e-01\n",
      "Epoch: 216, Loss: 2485.26977, Residuals: -0.95984, Convergence:   inf\n",
      "Epoch: 217, Loss: 2481.24869, Residuals: -0.95743, Convergence: 0.001621\n",
      "Epoch: 218, Loss: 2477.88746, Residuals: -0.95550, Convergence: 0.001356\n",
      "Epoch: 219, Loss: 2475.00084, Residuals: -0.95394, Convergence: 0.001166\n",
      "Epoch: 220, Loss: 2472.47985, Residuals: -0.95268, Convergence: 0.001020\n",
      "Epoch: 221, Loss: 2470.24442, Residuals: -0.95164, Convergence: 0.000905\n",
      "Evidence 14837.229\n",
      "\n",
      "Epoch: 221, Evidence: 14837.22852, Convergence: 0.002162\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.12e-01\n",
      "Epoch: 221, Loss: 2486.24478, Residuals: -0.95164, Convergence:   inf\n",
      "Epoch: 222, Loss: 2483.36597, Residuals: -0.94989, Convergence: 0.001159\n",
      "Epoch: 223, Loss: 2480.94684, Residuals: -0.94853, Convergence: 0.000975\n",
      "Evidence 14850.080\n",
      "\n",
      "Epoch: 223, Evidence: 14850.08008, Convergence: 0.000865\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.76e-01\n",
      "Epoch: 223, Loss: 2486.97405, Residuals: -0.94853, Convergence:   inf\n",
      "Epoch: 224, Loss: 2482.47721, Residuals: -0.94648, Convergence: 0.001811\n",
      "Epoch: 225, Loss: 2479.02092, Residuals: -0.94482, Convergence: 0.001394\n",
      "Epoch: 226, Loss: 2476.21164, Residuals: -0.94369, Convergence: 0.001135\n",
      "Epoch: 227, Loss: 2473.82008, Residuals: -0.94303, Convergence: 0.000967\n",
      "Evidence 14867.812\n",
      "\n",
      "Epoch: 227, Evidence: 14867.81250, Convergence: 0.002057\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.46e-01\n",
      "Epoch: 227, Loss: 2487.21206, Residuals: -0.94303, Convergence:   inf\n",
      "Epoch: 228, Loss: 2484.22453, Residuals: -0.94098, Convergence: 0.001203\n",
      "Epoch: 229, Loss: 2481.84455, Residuals: -0.93991, Convergence: 0.000959\n",
      "Evidence 14878.675\n",
      "\n",
      "Epoch: 229, Evidence: 14878.67480, Convergence: 0.000730\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.24e-01\n",
      "Epoch: 229, Loss: 2487.42983, Residuals: -0.93991, Convergence:   inf\n",
      "Epoch: 230, Loss: 2483.03221, Residuals: -0.93690, Convergence: 0.001771\n",
      "Epoch: 231, Loss: 2479.84825, Residuals: -0.93816, Convergence: 0.001284\n",
      "Epoch: 232, Loss: 2477.23346, Residuals: -0.93865, Convergence: 0.001056\n",
      "Epoch: 233, Loss: 2474.94857, Residuals: -0.94108, Convergence: 0.000923\n",
      "Evidence 14894.341\n",
      "\n",
      "Epoch: 233, Evidence: 14894.34082, Convergence: 0.001781\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.10e-01\n",
      "Epoch: 233, Loss: 2486.77956, Residuals: -0.94108, Convergence:   inf\n",
      "Epoch: 234, Loss: 2485.19773, Residuals: -0.93975, Convergence: 0.000637\n",
      "Evidence 14900.658\n",
      "\n",
      "Epoch: 234, Evidence: 14900.65820, Convergence: 0.000424\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 9.00e-02\n",
      "Epoch: 234, Loss: 2487.75952, Residuals: -0.93975, Convergence:   inf\n",
      "Epoch: 235, Loss: 2539.32240, Residuals: -0.98159, Convergence: -0.020306\n",
      "Epoch: 235, Loss: 2485.19665, Residuals: -0.93758, Convergence: 0.001031\n",
      "Epoch: 236, Loss: 2484.64576, Residuals: -0.93923, Convergence: 0.000222\n",
      "Evidence 14906.106\n",
      "\n",
      "Epoch: 236, Evidence: 14906.10645, Convergence: 0.000789\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.47e-02\n",
      "Epoch: 236, Loss: 2487.63637, Residuals: -0.93923, Convergence:   inf\n",
      "Epoch: 237, Loss: 2544.52027, Residuals: -0.98884, Convergence: -0.022355\n",
      "Epoch: 237, Loss: 2485.24048, Residuals: -0.93669, Convergence: 0.000964\n",
      "Evidence 14910.697\n",
      "\n",
      "Epoch: 237, Evidence: 14910.69727, Convergence: 0.001097\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 6.90e-02\n",
      "Epoch: 237, Loss: 2487.29122, Residuals: -0.93669, Convergence:   inf\n",
      "Epoch: 238, Loss: 2487.42597, Residuals: -0.93542, Convergence: -0.000054\n",
      "Evidence 14911.918\n",
      "\n",
      "Epoch: 238, Evidence: 14911.91797, Convergence: 0.000082\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 5.96e-02\n",
      "Epoch: 238, Loss: 2487.75488, Residuals: -0.93542, Convergence:   inf\n",
      "Epoch: 239, Loss: 2547.73816, Residuals: -0.99196, Convergence: -0.023544\n",
      "Epoch: 239, Loss: 2485.62265, Residuals: -0.93448, Convergence: 0.000858\n",
      "Evidence 14915.434\n",
      "\n",
      "Epoch: 239, Evidence: 14915.43359, Convergence: 0.000318\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 5.63e-02\n",
      "Epoch: 239, Loss: 2487.44911, Residuals: -0.93448, Convergence:   inf\n",
      "Epoch: 240, Loss: 2493.20571, Residuals: -0.94950, Convergence: -0.002309\n",
      "Epoch: 240, Loss: 2487.68460, Residuals: -0.93701, Convergence: -0.000095\n",
      "Evidence 14916.257\n",
      "\n",
      "Epoch: 240, Evidence: 14916.25684, Convergence: 0.000373\n",
      "Total samples: 183, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 380.88441, Residuals: -4.49136, Convergence:   inf\n",
      "Epoch: 1, Loss: 355.27814, Residuals: -4.37355, Convergence: 0.072074\n",
      "Epoch: 2, Loss: 334.27634, Residuals: -4.21127, Convergence: 0.062828\n",
      "Epoch: 3, Loss: 318.27089, Residuals: -4.04822, Convergence: 0.050289\n",
      "Epoch: 4, Loss: 306.06249, Residuals: -3.90430, Convergence: 0.039889\n",
      "Epoch: 5, Loss: 296.37733, Residuals: -3.77679, Convergence: 0.032678\n",
      "Epoch: 6, Loss: 288.51347, Residuals: -3.66558, Convergence: 0.027256\n",
      "Epoch: 7, Loss: 281.98215, Residuals: -3.57029, Convergence: 0.023162\n",
      "Epoch: 8, Loss: 276.42098, Residuals: -3.48888, Convergence: 0.020118\n",
      "Epoch: 9, Loss: 271.57460, Residuals: -3.41906, Convergence: 0.017846\n",
      "Epoch: 10, Loss: 267.26198, Residuals: -3.35876, Convergence: 0.016136\n",
      "Epoch: 11, Loss: 263.35271, Residuals: -3.30621, Convergence: 0.014844\n",
      "Epoch: 12, Loss: 259.75239, Residuals: -3.25988, Convergence: 0.013861\n",
      "Epoch: 13, Loss: 256.39331, Residuals: -3.21845, Convergence: 0.013101\n",
      "Epoch: 14, Loss: 253.22779, Residuals: -3.18076, Convergence: 0.012501\n",
      "Epoch: 15, Loss: 250.22432, Residuals: -3.14582, Convergence: 0.012003\n",
      "Epoch: 16, Loss: 247.36513, Residuals: -3.11297, Convergence: 0.011559\n",
      "Epoch: 17, Loss: 244.63836, Residuals: -3.08179, Convergence: 0.011146\n",
      "Epoch: 18, Loss: 242.02533, Residuals: -3.05192, Convergence: 0.010797\n",
      "Epoch: 19, Loss: 239.49618, Residuals: -3.02289, Convergence: 0.010560\n",
      "Epoch: 20, Loss: 237.01641, Residuals: -2.99414, Convergence: 0.010462\n",
      "Epoch: 21, Loss: 234.55634, Residuals: -2.96519, Convergence: 0.010488\n",
      "Epoch: 22, Loss: 232.09648, Residuals: -2.93577, Convergence: 0.010598\n",
      "Epoch: 23, Loss: 229.61962, Residuals: -2.90567, Convergence: 0.010787\n",
      "Epoch: 24, Loss: 227.09352, Residuals: -2.87456, Convergence: 0.011124\n",
      "Epoch: 25, Loss: 224.46481, Residuals: -2.84183, Convergence: 0.011711\n",
      "Epoch: 26, Loss: 221.68956, Residuals: -2.80693, Convergence: 0.012519\n",
      "Epoch: 27, Loss: 218.81857, Residuals: -2.77032, Convergence: 0.013120\n",
      "Epoch: 28, Loss: 215.98359, Residuals: -2.73345, Convergence: 0.013126\n",
      "Epoch: 29, Loss: 213.24839, Residuals: -2.69715, Convergence: 0.012826\n",
      "Epoch: 30, Loss: 210.61011, Residuals: -2.66151, Convergence: 0.012527\n",
      "Epoch: 31, Loss: 208.05150, Residuals: -2.62642, Convergence: 0.012298\n",
      "Epoch: 32, Loss: 205.55790, Residuals: -2.59174, Convergence: 0.012131\n",
      "Epoch: 33, Loss: 203.11957, Residuals: -2.55735, Convergence: 0.012004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34, Loss: 200.73102, Residuals: -2.52319, Convergence: 0.011899\n",
      "Epoch: 35, Loss: 198.38984, Residuals: -2.48920, Convergence: 0.011801\n",
      "Epoch: 36, Loss: 196.09572, Residuals: -2.45536, Convergence: 0.011699\n",
      "Epoch: 37, Loss: 193.84961, Residuals: -2.42166, Convergence: 0.011587\n",
      "Epoch: 38, Loss: 191.65317, Residuals: -2.38809, Convergence: 0.011460\n",
      "Epoch: 39, Loss: 189.50831, Residuals: -2.35468, Convergence: 0.011318\n",
      "Epoch: 40, Loss: 187.41688, Residuals: -2.32143, Convergence: 0.011159\n",
      "Epoch: 41, Loss: 185.38048, Residuals: -2.28838, Convergence: 0.010985\n",
      "Epoch: 42, Loss: 183.40039, Residuals: -2.25554, Convergence: 0.010797\n",
      "Epoch: 43, Loss: 181.47753, Residuals: -2.22295, Convergence: 0.010596\n",
      "Epoch: 44, Loss: 179.61243, Residuals: -2.19064, Convergence: 0.010384\n",
      "Epoch: 45, Loss: 177.80538, Residuals: -2.15864, Convergence: 0.010163\n",
      "Epoch: 46, Loss: 176.05644, Residuals: -2.12697, Convergence: 0.009934\n",
      "Epoch: 47, Loss: 174.36561, Residuals: -2.09568, Convergence: 0.009697\n",
      "Epoch: 48, Loss: 172.73280, Residuals: -2.06480, Convergence: 0.009453\n",
      "Epoch: 49, Loss: 171.15781, Residuals: -2.03435, Convergence: 0.009202\n",
      "Epoch: 50, Loss: 169.64020, Residuals: -2.00436, Convergence: 0.008946\n",
      "Epoch: 51, Loss: 168.17910, Residuals: -1.97485, Convergence: 0.008688\n",
      "Epoch: 52, Loss: 166.77325, Residuals: -1.94583, Convergence: 0.008430\n",
      "Epoch: 53, Loss: 165.42101, Residuals: -1.91730, Convergence: 0.008175\n",
      "Epoch: 54, Loss: 164.12059, Residuals: -1.88927, Convergence: 0.007924\n",
      "Epoch: 55, Loss: 162.87023, Residuals: -1.86174, Convergence: 0.007677\n",
      "Epoch: 56, Loss: 161.66828, Residuals: -1.83471, Convergence: 0.007435\n",
      "Epoch: 57, Loss: 160.51334, Residuals: -1.80820, Convergence: 0.007195\n",
      "Epoch: 58, Loss: 159.40418, Residuals: -1.78221, Convergence: 0.006958\n",
      "Epoch: 59, Loss: 158.33976, Residuals: -1.75678, Convergence: 0.006722\n",
      "Epoch: 60, Loss: 157.31915, Residuals: -1.73191, Convergence: 0.006488\n",
      "Epoch: 61, Loss: 156.34147, Residuals: -1.70763, Convergence: 0.006253\n",
      "Epoch: 62, Loss: 155.40588, Residuals: -1.68396, Convergence: 0.006020\n",
      "Epoch: 63, Loss: 154.51153, Residuals: -1.66091, Convergence: 0.005788\n",
      "Epoch: 64, Loss: 153.65753, Residuals: -1.63850, Convergence: 0.005558\n",
      "Epoch: 65, Loss: 152.84296, Residuals: -1.61675, Convergence: 0.005329\n",
      "Epoch: 66, Loss: 152.06685, Residuals: -1.59567, Convergence: 0.005104\n",
      "Epoch: 67, Loss: 151.32818, Residuals: -1.57526, Convergence: 0.004881\n",
      "Epoch: 68, Loss: 150.62585, Residuals: -1.55553, Convergence: 0.004663\n",
      "Epoch: 69, Loss: 149.95875, Residuals: -1.53648, Convergence: 0.004449\n",
      "Epoch: 70, Loss: 149.32570, Residuals: -1.51811, Convergence: 0.004239\n",
      "Epoch: 71, Loss: 148.72549, Residuals: -1.50042, Convergence: 0.004036\n",
      "Epoch: 72, Loss: 148.15688, Residuals: -1.48340, Convergence: 0.003838\n",
      "Epoch: 73, Loss: 147.61861, Residuals: -1.46704, Convergence: 0.003646\n",
      "Epoch: 74, Loss: 147.10941, Residuals: -1.45133, Convergence: 0.003461\n",
      "Epoch: 75, Loss: 146.62799, Residuals: -1.43627, Convergence: 0.003283\n",
      "Epoch: 76, Loss: 146.17307, Residuals: -1.42183, Convergence: 0.003112\n",
      "Epoch: 77, Loss: 145.74340, Residuals: -1.40800, Convergence: 0.002948\n",
      "Epoch: 78, Loss: 145.33772, Residuals: -1.39475, Convergence: 0.002791\n",
      "Epoch: 79, Loss: 144.95483, Residuals: -1.38208, Convergence: 0.002641\n",
      "Epoch: 80, Loss: 144.59351, Residuals: -1.36997, Convergence: 0.002499\n",
      "Epoch: 81, Loss: 144.25264, Residuals: -1.35838, Convergence: 0.002363\n",
      "Epoch: 82, Loss: 143.93109, Residuals: -1.34731, Convergence: 0.002234\n",
      "Epoch: 83, Loss: 143.62783, Residuals: -1.33674, Convergence: 0.002111\n",
      "Epoch: 84, Loss: 143.34184, Residuals: -1.32663, Convergence: 0.001995\n",
      "Epoch: 85, Loss: 143.07217, Residuals: -1.31698, Convergence: 0.001885\n",
      "Epoch: 86, Loss: 142.81793, Residuals: -1.30776, Convergence: 0.001780\n",
      "Epoch: 87, Loss: 142.57827, Residuals: -1.29895, Convergence: 0.001681\n",
      "Epoch: 88, Loss: 142.35242, Residuals: -1.29054, Convergence: 0.001587\n",
      "Epoch: 89, Loss: 142.13965, Residuals: -1.28251, Convergence: 0.001497\n",
      "Epoch: 90, Loss: 141.93930, Residuals: -1.27485, Convergence: 0.001412\n",
      "Epoch: 91, Loss: 141.75075, Residuals: -1.26753, Convergence: 0.001330\n",
      "Epoch: 92, Loss: 141.57342, Residuals: -1.26055, Convergence: 0.001253\n",
      "Epoch: 93, Loss: 141.40681, Residuals: -1.25389, Convergence: 0.001178\n",
      "Epoch: 94, Loss: 141.25041, Residuals: -1.24755, Convergence: 0.001107\n",
      "Epoch: 95, Loss: 141.10377, Residuals: -1.24150, Convergence: 0.001039\n",
      "Epoch: 96, Loss: 140.96645, Residuals: -1.23575, Convergence: 0.000974\n",
      "Evidence -182.652\n",
      "\n",
      "Epoch: 96, Evidence: -182.65195, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.25e-01\n",
      "Epoch: 96, Loss: 1356.17524, Residuals: -1.23575, Convergence:   inf\n",
      "Epoch: 97, Loss: 1293.48463, Residuals: -1.26690, Convergence: 0.048466\n",
      "Epoch: 98, Loss: 1245.41810, Residuals: -1.29129, Convergence: 0.038595\n",
      "Epoch: 99, Loss: 1208.97656, Residuals: -1.30880, Convergence: 0.030142\n",
      "Epoch: 100, Loss: 1180.72129, Residuals: -1.32099, Convergence: 0.023931\n",
      "Epoch: 101, Loss: 1158.02134, Residuals: -1.32989, Convergence: 0.019602\n",
      "Epoch: 102, Loss: 1139.29971, Residuals: -1.33665, Convergence: 0.016433\n",
      "Epoch: 103, Loss: 1123.58927, Residuals: -1.34178, Convergence: 0.013982\n",
      "Epoch: 104, Loss: 1110.23165, Residuals: -1.34554, Convergence: 0.012031\n",
      "Epoch: 105, Loss: 1098.74064, Residuals: -1.34810, Convergence: 0.010458\n",
      "Epoch: 106, Loss: 1088.73818, Residuals: -1.34957, Convergence: 0.009187\n",
      "Epoch: 107, Loss: 1079.92165, Residuals: -1.35004, Convergence: 0.008164\n",
      "Epoch: 108, Loss: 1072.04333, Residuals: -1.34959, Convergence: 0.007349\n",
      "Epoch: 109, Loss: 1064.89560, Residuals: -1.34827, Convergence: 0.006712\n",
      "Epoch: 110, Loss: 1058.30292, Residuals: -1.34612, Convergence: 0.006229\n",
      "Epoch: 111, Loss: 1052.11573, Residuals: -1.34318, Convergence: 0.005881\n",
      "Epoch: 112, Loss: 1046.20450, Residuals: -1.33945, Convergence: 0.005650\n",
      "Epoch: 113, Loss: 1040.46068, Residuals: -1.33496, Convergence: 0.005520\n",
      "Epoch: 114, Loss: 1034.79793, Residuals: -1.32972, Convergence: 0.005472\n",
      "Epoch: 115, Loss: 1029.15875, Residuals: -1.32378, Convergence: 0.005479\n",
      "Epoch: 116, Loss: 1023.52634, Residuals: -1.31720, Convergence: 0.005503\n",
      "Epoch: 117, Loss: 1017.93006, Residuals: -1.31009, Convergence: 0.005498\n",
      "Epoch: 118, Loss: 1012.43859, Residuals: -1.30255, Convergence: 0.005424\n",
      "Epoch: 119, Loss: 1007.13392, Residuals: -1.29467, Convergence: 0.005267\n",
      "Epoch: 120, Loss: 1002.08358, Residuals: -1.28657, Convergence: 0.005040\n",
      "Epoch: 121, Loss: 997.32331, Residuals: -1.27831, Convergence: 0.004773\n",
      "Epoch: 122, Loss: 992.85954, Residuals: -1.26998, Convergence: 0.004496\n",
      "Epoch: 123, Loss: 988.67854, Residuals: -1.26162, Convergence: 0.004229\n",
      "Epoch: 124, Loss: 984.75683, Residuals: -1.25330, Convergence: 0.003982\n",
      "Epoch: 125, Loss: 981.06844, Residuals: -1.24505, Convergence: 0.003760\n",
      "Epoch: 126, Loss: 977.58876, Residuals: -1.23690, Convergence: 0.003559\n",
      "Epoch: 127, Loss: 974.29604, Residuals: -1.22888, Convergence: 0.003380\n",
      "Epoch: 128, Loss: 971.17254, Residuals: -1.22103, Convergence: 0.003216\n",
      "Epoch: 129, Loss: 968.20384, Residuals: -1.21335, Convergence: 0.003066\n",
      "Epoch: 130, Loss: 965.37849, Residuals: -1.20588, Convergence: 0.002927\n",
      "Epoch: 131, Loss: 962.68767, Residuals: -1.19862, Convergence: 0.002795\n",
      "Epoch: 132, Loss: 960.12376, Residuals: -1.19160, Convergence: 0.002670\n",
      "Epoch: 133, Loss: 957.68123, Residuals: -1.18481, Convergence: 0.002550\n",
      "Epoch: 134, Loss: 955.35427, Residuals: -1.17828, Convergence: 0.002436\n",
      "Epoch: 135, Loss: 953.13782, Residuals: -1.17200, Convergence: 0.002325\n",
      "Epoch: 136, Loss: 951.02660, Residuals: -1.16598, Convergence: 0.002220\n",
      "Epoch: 137, Loss: 949.01559, Residuals: -1.16021, Convergence: 0.002119\n",
      "Epoch: 138, Loss: 947.09894, Residuals: -1.15470, Convergence: 0.002024\n",
      "Epoch: 139, Loss: 945.27166, Residuals: -1.14943, Convergence: 0.001933\n",
      "Epoch: 140, Loss: 943.52738, Residuals: -1.14441, Convergence: 0.001849\n",
      "Epoch: 141, Loss: 941.86112, Residuals: -1.13961, Convergence: 0.001769\n",
      "Epoch: 142, Loss: 940.26699, Residuals: -1.13504, Convergence: 0.001695\n",
      "Epoch: 143, Loss: 938.73997, Residuals: -1.13069, Convergence: 0.001627\n",
      "Epoch: 144, Loss: 937.27427, Residuals: -1.12653, Convergence: 0.001564\n",
      "Epoch: 145, Loss: 935.86516, Residuals: -1.12256, Convergence: 0.001506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 146, Loss: 934.50787, Residuals: -1.11878, Convergence: 0.001452\n",
      "Epoch: 147, Loss: 933.19744, Residuals: -1.11516, Convergence: 0.001404\n",
      "Epoch: 148, Loss: 931.92933, Residuals: -1.11170, Convergence: 0.001361\n",
      "Epoch: 149, Loss: 930.69896, Residuals: -1.10839, Convergence: 0.001322\n",
      "Epoch: 150, Loss: 929.50187, Residuals: -1.10521, Convergence: 0.001288\n",
      "Epoch: 151, Loss: 928.33354, Residuals: -1.10216, Convergence: 0.001259\n",
      "Epoch: 152, Loss: 927.18918, Residuals: -1.09921, Convergence: 0.001234\n",
      "Epoch: 153, Loss: 926.06448, Residuals: -1.09637, Convergence: 0.001214\n",
      "Epoch: 154, Loss: 924.95473, Residuals: -1.09361, Convergence: 0.001200\n",
      "Epoch: 155, Loss: 923.85587, Residuals: -1.09093, Convergence: 0.001189\n",
      "Epoch: 156, Loss: 922.76399, Residuals: -1.08832, Convergence: 0.001183\n",
      "Epoch: 157, Loss: 921.67612, Residuals: -1.08575, Convergence: 0.001180\n",
      "Epoch: 158, Loss: 920.59091, Residuals: -1.08324, Convergence: 0.001179\n",
      "Epoch: 159, Loss: 919.50792, Residuals: -1.08077, Convergence: 0.001178\n",
      "Epoch: 160, Loss: 918.42827, Residuals: -1.07834, Convergence: 0.001176\n",
      "Epoch: 161, Loss: 917.35533, Residuals: -1.07595, Convergence: 0.001170\n",
      "Epoch: 162, Loss: 916.29252, Residuals: -1.07360, Convergence: 0.001160\n",
      "Epoch: 163, Loss: 915.24490, Residuals: -1.07131, Convergence: 0.001145\n",
      "Epoch: 164, Loss: 914.21680, Residuals: -1.06907, Convergence: 0.001125\n",
      "Epoch: 165, Loss: 913.21210, Residuals: -1.06689, Convergence: 0.001100\n",
      "Epoch: 166, Loss: 912.23501, Residuals: -1.06478, Convergence: 0.001071\n",
      "Epoch: 167, Loss: 911.28766, Residuals: -1.06274, Convergence: 0.001040\n",
      "Epoch: 168, Loss: 910.37197, Residuals: -1.06076, Convergence: 0.001006\n",
      "Epoch: 169, Loss: 909.48949, Residuals: -1.05886, Convergence: 0.000970\n",
      "Evidence 11122.823\n",
      "\n",
      "Epoch: 169, Evidence: 11122.82324, Convergence: 1.016421\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 5.77e-01\n",
      "Epoch: 169, Loss: 2345.88043, Residuals: -1.05886, Convergence:   inf\n",
      "Epoch: 170, Loss: 2304.42408, Residuals: -1.06667, Convergence: 0.017990\n",
      "Epoch: 171, Loss: 2276.09572, Residuals: -1.06489, Convergence: 0.012446\n",
      "Epoch: 172, Loss: 2252.40894, Residuals: -1.06225, Convergence: 0.010516\n",
      "Epoch: 173, Loss: 2232.34893, Residuals: -1.05940, Convergence: 0.008986\n",
      "Epoch: 174, Loss: 2215.24150, Residuals: -1.05648, Convergence: 0.007723\n",
      "Epoch: 175, Loss: 2200.55593, Residuals: -1.05352, Convergence: 0.006674\n",
      "Epoch: 176, Loss: 2187.85349, Residuals: -1.05054, Convergence: 0.005806\n",
      "Epoch: 177, Loss: 2176.76306, Residuals: -1.04754, Convergence: 0.005095\n",
      "Epoch: 178, Loss: 2166.97314, Residuals: -1.04449, Convergence: 0.004518\n",
      "Epoch: 179, Loss: 2158.22737, Residuals: -1.04139, Convergence: 0.004052\n",
      "Epoch: 180, Loss: 2150.32437, Residuals: -1.03821, Convergence: 0.003675\n",
      "Epoch: 181, Loss: 2143.11941, Residuals: -1.03496, Convergence: 0.003362\n",
      "Epoch: 182, Loss: 2136.51579, Residuals: -1.03164, Convergence: 0.003091\n",
      "Epoch: 183, Loss: 2130.44973, Residuals: -1.02830, Convergence: 0.002847\n",
      "Epoch: 184, Loss: 2124.87376, Residuals: -1.02497, Convergence: 0.002624\n",
      "Epoch: 185, Loss: 2119.74495, Residuals: -1.02168, Convergence: 0.002420\n",
      "Epoch: 186, Loss: 2115.02043, Residuals: -1.01848, Convergence: 0.002234\n",
      "Epoch: 187, Loss: 2110.65915, Residuals: -1.01538, Convergence: 0.002066\n",
      "Epoch: 188, Loss: 2106.62037, Residuals: -1.01238, Convergence: 0.001917\n",
      "Epoch: 189, Loss: 2102.86927, Residuals: -1.00951, Convergence: 0.001784\n",
      "Epoch: 190, Loss: 2099.37362, Residuals: -1.00676, Convergence: 0.001665\n",
      "Epoch: 191, Loss: 2096.10780, Residuals: -1.00414, Convergence: 0.001558\n",
      "Epoch: 192, Loss: 2093.05055, Residuals: -1.00165, Convergence: 0.001461\n",
      "Epoch: 193, Loss: 2090.18406, Residuals: -0.99929, Convergence: 0.001371\n",
      "Epoch: 194, Loss: 2087.49392, Residuals: -0.99705, Convergence: 0.001289\n",
      "Epoch: 195, Loss: 2084.96846, Residuals: -0.99494, Convergence: 0.001211\n",
      "Epoch: 196, Loss: 2082.59730, Residuals: -0.99295, Convergence: 0.001139\n",
      "Epoch: 197, Loss: 2080.37187, Residuals: -0.99107, Convergence: 0.001070\n",
      "Epoch: 198, Loss: 2078.28200, Residuals: -0.98929, Convergence: 0.001006\n",
      "Epoch: 199, Loss: 2076.31974, Residuals: -0.98761, Convergence: 0.000945\n",
      "Evidence 14404.707\n",
      "\n",
      "Epoch: 199, Evidence: 14404.70703, Convergence: 0.227834\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 4.40e-01\n",
      "Epoch: 199, Loss: 2472.27358, Residuals: -0.98761, Convergence:   inf\n",
      "Epoch: 200, Loss: 2457.92518, Residuals: -0.98483, Convergence: 0.005838\n",
      "Epoch: 201, Loss: 2446.17409, Residuals: -0.98153, Convergence: 0.004804\n",
      "Epoch: 202, Loss: 2436.03430, Residuals: -0.97836, Convergence: 0.004162\n",
      "Epoch: 203, Loss: 2427.23024, Residuals: -0.97539, Convergence: 0.003627\n",
      "Epoch: 204, Loss: 2419.55178, Residuals: -0.97267, Convergence: 0.003174\n",
      "Epoch: 205, Loss: 2412.82795, Residuals: -0.97020, Convergence: 0.002787\n",
      "Epoch: 206, Loss: 2406.91609, Residuals: -0.96794, Convergence: 0.002456\n",
      "Epoch: 207, Loss: 2401.69734, Residuals: -0.96588, Convergence: 0.002173\n",
      "Epoch: 208, Loss: 2397.06823, Residuals: -0.96399, Convergence: 0.001931\n",
      "Epoch: 209, Loss: 2392.94333, Residuals: -0.96226, Convergence: 0.001724\n",
      "Epoch: 210, Loss: 2389.24830, Residuals: -0.96066, Convergence: 0.001547\n",
      "Epoch: 211, Loss: 2385.92283, Residuals: -0.95919, Convergence: 0.001394\n",
      "Epoch: 212, Loss: 2382.91371, Residuals: -0.95783, Convergence: 0.001263\n",
      "Epoch: 213, Loss: 2380.17666, Residuals: -0.95657, Convergence: 0.001150\n",
      "Epoch: 214, Loss: 2377.67511, Residuals: -0.95541, Convergence: 0.001052\n",
      "Epoch: 215, Loss: 2375.37890, Residuals: -0.95434, Convergence: 0.000967\n",
      "Evidence 14830.982\n",
      "\n",
      "Epoch: 215, Evidence: 14830.98242, Convergence: 0.028742\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 3.37e-01\n",
      "Epoch: 215, Loss: 2477.07428, Residuals: -0.95434, Convergence:   inf\n",
      "Epoch: 216, Loss: 2470.32921, Residuals: -0.95143, Convergence: 0.002730\n",
      "Epoch: 217, Loss: 2464.74106, Residuals: -0.94889, Convergence: 0.002267\n",
      "Epoch: 218, Loss: 2459.99997, Residuals: -0.94676, Convergence: 0.001927\n",
      "Epoch: 219, Loss: 2455.92590, Residuals: -0.94498, Convergence: 0.001659\n",
      "Epoch: 220, Loss: 2452.38641, Residuals: -0.94348, Convergence: 0.001443\n",
      "Epoch: 221, Loss: 2449.27641, Residuals: -0.94222, Convergence: 0.001270\n",
      "Epoch: 222, Loss: 2446.51375, Residuals: -0.94115, Convergence: 0.001129\n",
      "Epoch: 223, Loss: 2444.03568, Residuals: -0.94024, Convergence: 0.001014\n",
      "Epoch: 224, Loss: 2441.79237, Residuals: -0.93947, Convergence: 0.000919\n",
      "Evidence 14920.377\n",
      "\n",
      "Epoch: 224, Evidence: 14920.37695, Convergence: 0.005991\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.65e-01\n",
      "Epoch: 224, Loss: 2478.39479, Residuals: -0.93947, Convergence:   inf\n",
      "Epoch: 225, Loss: 2474.52017, Residuals: -0.93739, Convergence: 0.001566\n",
      "Epoch: 226, Loss: 2471.29893, Residuals: -0.93574, Convergence: 0.001303\n",
      "Epoch: 227, Loss: 2468.54615, Residuals: -0.93444, Convergence: 0.001115\n",
      "Epoch: 228, Loss: 2466.14801, Residuals: -0.93339, Convergence: 0.000972\n",
      "Evidence 14949.658\n",
      "\n",
      "Epoch: 228, Evidence: 14949.65820, Convergence: 0.001959\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.14e-01\n",
      "Epoch: 228, Loss: 2479.30562, Residuals: -0.93339, Convergence:   inf\n",
      "Epoch: 229, Loss: 2476.44797, Residuals: -0.93176, Convergence: 0.001154\n",
      "Epoch: 230, Loss: 2474.05205, Residuals: -0.93046, Convergence: 0.000968\n",
      "Evidence 14961.749\n",
      "\n",
      "Epoch: 230, Evidence: 14961.74902, Convergence: 0.000808\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.79e-01\n",
      "Epoch: 230, Loss: 2479.96783, Residuals: -0.93046, Convergence:   inf\n",
      "Epoch: 231, Loss: 2475.55847, Residuals: -0.92829, Convergence: 0.001781\n",
      "Epoch: 232, Loss: 2472.11211, Residuals: -0.92655, Convergence: 0.001394\n",
      "Epoch: 233, Loss: 2469.31961, Residuals: -0.92531, Convergence: 0.001131\n",
      "Epoch: 234, Loss: 2466.95388, Residuals: -0.92460, Convergence: 0.000959\n",
      "Evidence 14979.433\n",
      "\n",
      "Epoch: 234, Evidence: 14979.43262, Convergence: 0.001988\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.48e-01\n",
      "Epoch: 234, Loss: 2480.03962, Residuals: -0.92460, Convergence:   inf\n",
      "Epoch: 235, Loss: 2477.06979, Residuals: -0.92220, Convergence: 0.001199\n",
      "Epoch: 236, Loss: 2474.70879, Residuals: -0.92086, Convergence: 0.000954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence 14990.389\n",
      "\n",
      "Epoch: 236, Evidence: 14990.38867, Convergence: 0.000731\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.26e-01\n",
      "Epoch: 236, Loss: 2480.15548, Residuals: -0.92086, Convergence:   inf\n",
      "Epoch: 237, Loss: 2475.77749, Residuals: -0.91738, Convergence: 0.001768\n",
      "Epoch: 238, Loss: 2472.65522, Residuals: -0.91786, Convergence: 0.001263\n",
      "Epoch: 239, Loss: 2470.10639, Residuals: -0.91870, Convergence: 0.001032\n",
      "Epoch: 240, Loss: 2467.84554, Residuals: -0.92116, Convergence: 0.000916\n",
      "Evidence 15006.146\n",
      "\n",
      "Epoch: 240, Evidence: 15006.14648, Convergence: 0.001780\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.12e-01\n",
      "Epoch: 240, Loss: 2479.38875, Residuals: -0.92116, Convergence:   inf\n",
      "Epoch: 241, Loss: 2477.63788, Residuals: -0.91877, Convergence: 0.000707\n",
      "Evidence 15012.890\n",
      "\n",
      "Epoch: 241, Evidence: 15012.88965, Convergence: 0.000449\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 9.23e-02\n",
      "Epoch: 241, Loss: 2480.18943, Residuals: -0.91877, Convergence:   inf\n",
      "Epoch: 242, Loss: 2523.61916, Residuals: -0.96002, Convergence: -0.017209\n",
      "Epoch: 242, Loss: 2478.02345, Residuals: -0.91686, Convergence: 0.000874\n",
      "Evidence 15017.207\n",
      "\n",
      "Epoch: 242, Evidence: 15017.20703, Convergence: 0.000737\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 8.54e-02\n",
      "Epoch: 242, Loss: 2479.70796, Residuals: -0.91686, Convergence:   inf\n",
      "Epoch: 243, Loss: 2484.44954, Residuals: -0.91966, Convergence: -0.001909\n",
      "Epoch: 243, Loss: 2479.67070, Residuals: -0.91567, Convergence: 0.000015\n",
      "Evidence 15018.907\n",
      "\n",
      "Epoch: 243, Evidence: 15018.90723, Convergence: 0.000850\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 382.06498, Residuals: -4.52842, Convergence:   inf\n",
      "Epoch: 1, Loss: 356.33183, Residuals: -4.40788, Convergence: 0.072217\n",
      "Epoch: 2, Loss: 335.28359, Residuals: -4.24323, Convergence: 0.062777\n",
      "Epoch: 3, Loss: 319.20409, Residuals: -4.07822, Convergence: 0.050374\n",
      "Epoch: 4, Loss: 306.93847, Residuals: -3.93274, Convergence: 0.039961\n",
      "Epoch: 5, Loss: 297.21242, Residuals: -3.80413, Convergence: 0.032724\n",
      "Epoch: 6, Loss: 289.31788, Residuals: -3.69200, Convergence: 0.027287\n",
      "Epoch: 7, Loss: 282.76690, Residuals: -3.59587, Convergence: 0.023167\n",
      "Epoch: 8, Loss: 277.19737, Residuals: -3.51373, Convergence: 0.020092\n",
      "Epoch: 9, Loss: 272.35282, Residuals: -3.44326, Convergence: 0.017788\n",
      "Epoch: 10, Loss: 268.05085, Residuals: -3.38236, Convergence: 0.016049\n",
      "Epoch: 11, Loss: 264.15951, Residuals: -3.32923, Convergence: 0.014731\n",
      "Epoch: 12, Loss: 260.58284, Residuals: -3.28239, Convergence: 0.013726\n",
      "Epoch: 13, Loss: 257.25164, Residuals: -3.24053, Convergence: 0.012949\n",
      "Epoch: 14, Loss: 254.11690, Residuals: -3.20253, Convergence: 0.012336\n",
      "Epoch: 15, Loss: 251.14593, Residuals: -3.16742, Convergence: 0.011830\n",
      "Epoch: 16, Loss: 248.32030, Residuals: -3.13452, Convergence: 0.011379\n",
      "Epoch: 17, Loss: 245.62874, Residuals: -3.10341, Convergence: 0.010958\n",
      "Epoch: 18, Loss: 243.05375, Residuals: -3.07373, Convergence: 0.010594\n",
      "Epoch: 19, Loss: 240.56593, Residuals: -3.04502, Convergence: 0.010342\n",
      "Epoch: 20, Loss: 238.12970, Residuals: -3.01672, Convergence: 0.010231\n",
      "Epoch: 21, Loss: 235.71243, Residuals: -2.98832, Convergence: 0.010255\n",
      "Epoch: 22, Loss: 233.29022, Residuals: -2.95944, Convergence: 0.010383\n",
      "Epoch: 23, Loss: 230.84262, Residuals: -2.92984, Convergence: 0.010603\n",
      "Epoch: 24, Loss: 228.33651, Residuals: -2.89915, Convergence: 0.010976\n",
      "Epoch: 25, Loss: 225.71865, Residuals: -2.86676, Convergence: 0.011598\n",
      "Epoch: 26, Loss: 222.94776, Residuals: -2.83214, Convergence: 0.012428\n",
      "Epoch: 27, Loss: 220.07974, Residuals: -2.79580, Convergence: 0.013032\n",
      "Epoch: 28, Loss: 217.24341, Residuals: -2.75915, Convergence: 0.013056\n",
      "Epoch: 29, Loss: 214.49638, Residuals: -2.72295, Convergence: 0.012807\n",
      "Epoch: 30, Loss: 211.83452, Residuals: -2.68725, Convergence: 0.012566\n",
      "Epoch: 31, Loss: 209.24189, Residuals: -2.65194, Convergence: 0.012391\n",
      "Epoch: 32, Loss: 206.70589, Residuals: -2.61688, Convergence: 0.012269\n",
      "Epoch: 33, Loss: 204.21916, Residuals: -2.58199, Convergence: 0.012177\n",
      "Epoch: 34, Loss: 201.77848, Residuals: -2.54721, Convergence: 0.012096\n",
      "Epoch: 35, Loss: 199.38345, Residuals: -2.51250, Convergence: 0.012012\n",
      "Epoch: 36, Loss: 197.03529, Residuals: -2.47786, Convergence: 0.011917\n",
      "Epoch: 37, Loss: 194.73600, Residuals: -2.44329, Convergence: 0.011807\n",
      "Epoch: 38, Loss: 192.48767, Residuals: -2.40880, Convergence: 0.011680\n",
      "Epoch: 39, Loss: 190.29229, Residuals: -2.37440, Convergence: 0.011537\n",
      "Epoch: 40, Loss: 188.15151, Residuals: -2.34012, Convergence: 0.011378\n",
      "Epoch: 41, Loss: 186.06669, Residuals: -2.30596, Convergence: 0.011205\n",
      "Epoch: 42, Loss: 184.03904, Residuals: -2.27196, Convergence: 0.011017\n",
      "Epoch: 43, Loss: 182.06976, Residuals: -2.23813, Convergence: 0.010816\n",
      "Epoch: 44, Loss: 180.16019, Residuals: -2.20452, Convergence: 0.010599\n",
      "Epoch: 45, Loss: 178.31184, Residuals: -2.17117, Convergence: 0.010366\n",
      "Epoch: 46, Loss: 176.52629, Residuals: -2.13811, Convergence: 0.010115\n",
      "Epoch: 47, Loss: 174.80505, Residuals: -2.10542, Convergence: 0.009847\n",
      "Epoch: 48, Loss: 173.14931, Residuals: -2.07313, Convergence: 0.009562\n",
      "Epoch: 49, Loss: 171.55979, Residuals: -2.04130, Convergence: 0.009265\n",
      "Epoch: 50, Loss: 170.03667, Residuals: -2.00998, Convergence: 0.008958\n",
      "Epoch: 51, Loss: 168.57959, Residuals: -1.97922, Convergence: 0.008643\n",
      "Epoch: 52, Loss: 167.18769, Residuals: -1.94905, Convergence: 0.008325\n",
      "Epoch: 53, Loss: 165.85974, Residuals: -1.91951, Convergence: 0.008006\n",
      "Epoch: 54, Loss: 164.59416, Residuals: -1.89064, Convergence: 0.007689\n",
      "Epoch: 55, Loss: 163.38916, Residuals: -1.86246, Convergence: 0.007375\n",
      "Epoch: 56, Loss: 162.24272, Residuals: -1.83500, Convergence: 0.007066\n",
      "Epoch: 57, Loss: 161.15268, Residuals: -1.80827, Convergence: 0.006764\n",
      "Epoch: 58, Loss: 160.11673, Residuals: -1.78229, Convergence: 0.006470\n",
      "Epoch: 59, Loss: 159.13250, Residuals: -1.75707, Convergence: 0.006185\n",
      "Epoch: 60, Loss: 158.19750, Residuals: -1.73262, Convergence: 0.005910\n",
      "Epoch: 61, Loss: 157.30926, Residuals: -1.70892, Convergence: 0.005646\n",
      "Epoch: 62, Loss: 156.46530, Residuals: -1.68599, Convergence: 0.005394\n",
      "Epoch: 63, Loss: 155.66323, Residuals: -1.66379, Convergence: 0.005153\n",
      "Epoch: 64, Loss: 154.90075, Residuals: -1.64234, Convergence: 0.004922\n",
      "Epoch: 65, Loss: 154.17571, Residuals: -1.62160, Convergence: 0.004703\n",
      "Epoch: 66, Loss: 153.48603, Residuals: -1.60158, Convergence: 0.004493\n",
      "Epoch: 67, Loss: 152.82971, Residuals: -1.58225, Convergence: 0.004294\n",
      "Epoch: 68, Loss: 152.20479, Residuals: -1.56360, Convergence: 0.004106\n",
      "Epoch: 69, Loss: 151.60935, Residuals: -1.54561, Convergence: 0.003927\n",
      "Epoch: 70, Loss: 151.04154, Residuals: -1.52826, Convergence: 0.003759\n",
      "Epoch: 71, Loss: 150.49954, Residuals: -1.51152, Convergence: 0.003601\n",
      "Epoch: 72, Loss: 149.98170, Residuals: -1.49536, Convergence: 0.003453\n",
      "Epoch: 73, Loss: 149.48657, Residuals: -1.47977, Convergence: 0.003312\n",
      "Epoch: 74, Loss: 149.01285, Residuals: -1.46472, Convergence: 0.003179\n",
      "Epoch: 75, Loss: 148.55950, Residuals: -1.45018, Convergence: 0.003052\n",
      "Epoch: 76, Loss: 148.12564, Residuals: -1.43615, Convergence: 0.002929\n",
      "Epoch: 77, Loss: 147.71052, Residuals: -1.42261, Convergence: 0.002810\n",
      "Epoch: 78, Loss: 147.31352, Residuals: -1.40954, Convergence: 0.002695\n",
      "Epoch: 79, Loss: 146.93406, Residuals: -1.39694, Convergence: 0.002583\n",
      "Epoch: 80, Loss: 146.57162, Residuals: -1.38480, Convergence: 0.002473\n",
      "Epoch: 81, Loss: 146.22568, Residuals: -1.37310, Convergence: 0.002366\n",
      "Epoch: 82, Loss: 145.89573, Residuals: -1.36185, Convergence: 0.002262\n",
      "Epoch: 83, Loss: 145.58128, Residuals: -1.35102, Convergence: 0.002160\n",
      "Epoch: 84, Loss: 145.28183, Residuals: -1.34061, Convergence: 0.002061\n",
      "Epoch: 85, Loss: 144.99688, Residuals: -1.33062, Convergence: 0.001965\n",
      "Epoch: 86, Loss: 144.72594, Residuals: -1.32103, Convergence: 0.001872\n",
      "Epoch: 87, Loss: 144.46854, Residuals: -1.31183, Convergence: 0.001782\n",
      "Epoch: 88, Loss: 144.22424, Residuals: -1.30301, Convergence: 0.001694\n",
      "Epoch: 89, Loss: 143.99254, Residuals: -1.29457, Convergence: 0.001609\n",
      "Epoch: 90, Loss: 143.77300, Residuals: -1.28650, Convergence: 0.001527\n",
      "Epoch: 91, Loss: 143.56516, Residuals: -1.27879, Convergence: 0.001448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 92, Loss: 143.36853, Residuals: -1.27144, Convergence: 0.001372\n",
      "Epoch: 93, Loss: 143.18257, Residuals: -1.26444, Convergence: 0.001299\n",
      "Epoch: 94, Loss: 143.00672, Residuals: -1.25779, Convergence: 0.001230\n",
      "Epoch: 95, Loss: 142.84029, Residuals: -1.25148, Convergence: 0.001165\n",
      "Epoch: 96, Loss: 142.68254, Residuals: -1.24549, Convergence: 0.001106\n",
      "Epoch: 97, Loss: 142.53257, Residuals: -1.23984, Convergence: 0.001052\n",
      "Epoch: 98, Loss: 142.38939, Residuals: -1.23449, Convergence: 0.001006\n",
      "Epoch: 99, Loss: 142.25194, Residuals: -1.22943, Convergence: 0.000966\n",
      "Evidence -183.464\n",
      "\n",
      "Epoch: 99, Evidence: -183.46365, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.24e-01\n",
      "Epoch: 99, Loss: 1366.14465, Residuals: -1.22943, Convergence:   inf\n",
      "Epoch: 100, Loss: 1304.94093, Residuals: -1.25870, Convergence: 0.046902\n",
      "Epoch: 101, Loss: 1258.17455, Residuals: -1.28248, Convergence: 0.037170\n",
      "Epoch: 102, Loss: 1222.69509, Residuals: -1.30016, Convergence: 0.029017\n",
      "Epoch: 103, Loss: 1194.99758, Residuals: -1.31290, Convergence: 0.023178\n",
      "Epoch: 104, Loss: 1172.57021, Residuals: -1.32240, Convergence: 0.019127\n",
      "Epoch: 105, Loss: 1153.96211, Residuals: -1.32969, Convergence: 0.016125\n",
      "Epoch: 106, Loss: 1138.28243, Residuals: -1.33524, Convergence: 0.013775\n",
      "Epoch: 107, Loss: 1124.91464, Residuals: -1.33935, Convergence: 0.011883\n",
      "Epoch: 108, Loss: 1113.39745, Residuals: -1.34218, Convergence: 0.010344\n",
      "Epoch: 109, Loss: 1103.36844, Residuals: -1.34389, Convergence: 0.009089\n",
      "Epoch: 110, Loss: 1094.53530, Residuals: -1.34457, Convergence: 0.008070\n",
      "Epoch: 111, Loss: 1086.65535, Residuals: -1.34432, Convergence: 0.007252\n",
      "Epoch: 112, Loss: 1079.52461, Residuals: -1.34321, Convergence: 0.006605\n",
      "Epoch: 113, Loss: 1072.96574, Residuals: -1.34129, Convergence: 0.006113\n",
      "Epoch: 114, Loss: 1066.82227, Residuals: -1.33860, Convergence: 0.005759\n",
      "Epoch: 115, Loss: 1060.95375, Residuals: -1.33513, Convergence: 0.005531\n",
      "Epoch: 116, Loss: 1055.23385, Residuals: -1.33091, Convergence: 0.005421\n",
      "Epoch: 117, Loss: 1049.55597, Residuals: -1.32594, Convergence: 0.005410\n",
      "Epoch: 118, Loss: 1043.84389, Residuals: -1.32024, Convergence: 0.005472\n",
      "Epoch: 119, Loss: 1038.07261, Residuals: -1.31389, Convergence: 0.005560\n",
      "Epoch: 120, Loss: 1032.28012, Residuals: -1.30696, Convergence: 0.005611\n",
      "Epoch: 121, Loss: 1026.56048, Residuals: -1.29957, Convergence: 0.005572\n",
      "Epoch: 122, Loss: 1021.02876, Residuals: -1.29184, Convergence: 0.005418\n",
      "Epoch: 123, Loss: 1015.77716, Residuals: -1.28387, Convergence: 0.005170\n",
      "Epoch: 124, Loss: 1010.85396, Residuals: -1.27577, Convergence: 0.004870\n",
      "Epoch: 125, Loss: 1006.26797, Residuals: -1.26761, Convergence: 0.004557\n",
      "Epoch: 126, Loss: 1002.00192, Residuals: -1.25947, Convergence: 0.004258\n",
      "Epoch: 127, Loss: 998.02826, Residuals: -1.25140, Convergence: 0.003982\n",
      "Epoch: 128, Loss: 994.31641, Residuals: -1.24345, Convergence: 0.003733\n",
      "Epoch: 129, Loss: 990.83827, Residuals: -1.23566, Convergence: 0.003510\n",
      "Epoch: 130, Loss: 987.56866, Residuals: -1.22806, Convergence: 0.003311\n",
      "Epoch: 131, Loss: 984.48672, Residuals: -1.22067, Convergence: 0.003131\n",
      "Epoch: 132, Loss: 981.57491, Residuals: -1.21351, Convergence: 0.002966\n",
      "Epoch: 133, Loss: 978.81871, Residuals: -1.20660, Convergence: 0.002816\n",
      "Epoch: 134, Loss: 976.20608, Residuals: -1.19993, Convergence: 0.002676\n",
      "Epoch: 135, Loss: 973.72693, Residuals: -1.19353, Convergence: 0.002546\n",
      "Epoch: 136, Loss: 971.37201, Residuals: -1.18739, Convergence: 0.002424\n",
      "Epoch: 137, Loss: 969.13358, Residuals: -1.18151, Convergence: 0.002310\n",
      "Epoch: 138, Loss: 967.00390, Residuals: -1.17590, Convergence: 0.002202\n",
      "Epoch: 139, Loss: 964.97654, Residuals: -1.17054, Convergence: 0.002101\n",
      "Epoch: 140, Loss: 963.04484, Residuals: -1.16543, Convergence: 0.002006\n",
      "Epoch: 141, Loss: 961.20267, Residuals: -1.16057, Convergence: 0.001917\n",
      "Epoch: 142, Loss: 959.44429, Residuals: -1.15594, Convergence: 0.001833\n",
      "Epoch: 143, Loss: 957.76390, Residuals: -1.15153, Convergence: 0.001754\n",
      "Epoch: 144, Loss: 956.15612, Residuals: -1.14734, Convergence: 0.001682\n",
      "Epoch: 145, Loss: 954.61598, Residuals: -1.14335, Convergence: 0.001613\n",
      "Epoch: 146, Loss: 953.13813, Residuals: -1.13956, Convergence: 0.001551\n",
      "Epoch: 147, Loss: 951.71874, Residuals: -1.13595, Convergence: 0.001491\n",
      "Epoch: 148, Loss: 950.35311, Residuals: -1.13250, Convergence: 0.001437\n",
      "Epoch: 149, Loss: 949.03737, Residuals: -1.12922, Convergence: 0.001386\n",
      "Epoch: 150, Loss: 947.76772, Residuals: -1.12609, Convergence: 0.001340\n",
      "Epoch: 151, Loss: 946.54057, Residuals: -1.12309, Convergence: 0.001296\n",
      "Epoch: 152, Loss: 945.35286, Residuals: -1.12023, Convergence: 0.001256\n",
      "Epoch: 153, Loss: 944.20124, Residuals: -1.11749, Convergence: 0.001220\n",
      "Epoch: 154, Loss: 943.08242, Residuals: -1.11485, Convergence: 0.001186\n",
      "Epoch: 155, Loss: 941.99379, Residuals: -1.11232, Convergence: 0.001156\n",
      "Epoch: 156, Loss: 940.93205, Residuals: -1.10989, Convergence: 0.001128\n",
      "Epoch: 157, Loss: 939.89437, Residuals: -1.10753, Convergence: 0.001104\n",
      "Epoch: 158, Loss: 938.87770, Residuals: -1.10526, Convergence: 0.001083\n",
      "Epoch: 159, Loss: 937.87879, Residuals: -1.10305, Convergence: 0.001065\n",
      "Epoch: 160, Loss: 936.89464, Residuals: -1.10090, Convergence: 0.001050\n",
      "Epoch: 161, Loss: 935.92217, Residuals: -1.09880, Convergence: 0.001039\n",
      "Epoch: 162, Loss: 934.95803, Residuals: -1.09675, Convergence: 0.001031\n",
      "Epoch: 163, Loss: 933.99992, Residuals: -1.09473, Convergence: 0.001026\n",
      "Epoch: 164, Loss: 933.04525, Residuals: -1.09273, Convergence: 0.001023\n",
      "Epoch: 165, Loss: 932.09243, Residuals: -1.09075, Convergence: 0.001022\n",
      "Epoch: 166, Loss: 931.14047, Residuals: -1.08879, Convergence: 0.001022\n",
      "Epoch: 167, Loss: 930.18988, Residuals: -1.08684, Convergence: 0.001022\n",
      "Epoch: 168, Loss: 929.24253, Residuals: -1.08491, Convergence: 0.001019\n",
      "Epoch: 169, Loss: 928.30101, Residuals: -1.08298, Convergence: 0.001014\n",
      "Epoch: 170, Loss: 927.37005, Residuals: -1.08108, Convergence: 0.001004\n",
      "Epoch: 171, Loss: 926.45407, Residuals: -1.07920, Convergence: 0.000989\n",
      "Evidence 11144.834\n",
      "\n",
      "Epoch: 171, Evidence: 11144.83398, Convergence: 1.016462\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.76e-01\n",
      "Epoch: 171, Loss: 2344.73447, Residuals: -1.07920, Convergence:   inf\n",
      "Epoch: 172, Loss: 2307.75634, Residuals: -1.08661, Convergence: 0.016023\n",
      "Epoch: 173, Loss: 2282.10365, Residuals: -1.08475, Convergence: 0.011241\n",
      "Epoch: 174, Loss: 2260.69326, Residuals: -1.08227, Convergence: 0.009471\n",
      "Epoch: 175, Loss: 2242.67331, Residuals: -1.07966, Convergence: 0.008035\n",
      "Epoch: 176, Loss: 2227.38906, Residuals: -1.07699, Convergence: 0.006862\n",
      "Epoch: 177, Loss: 2214.31941, Residuals: -1.07431, Convergence: 0.005902\n",
      "Epoch: 178, Loss: 2203.03725, Residuals: -1.07163, Convergence: 0.005121\n",
      "Epoch: 179, Loss: 2193.18451, Residuals: -1.06895, Convergence: 0.004492\n",
      "Epoch: 180, Loss: 2184.46586, Residuals: -1.06624, Convergence: 0.003991\n",
      "Epoch: 181, Loss: 2176.63664, Residuals: -1.06348, Convergence: 0.003597\n",
      "Epoch: 182, Loss: 2169.50651, Residuals: -1.06065, Convergence: 0.003287\n",
      "Epoch: 183, Loss: 2162.94122, Residuals: -1.05774, Convergence: 0.003035\n",
      "Epoch: 184, Loss: 2156.85484, Residuals: -1.05476, Convergence: 0.002822\n",
      "Epoch: 185, Loss: 2151.19814, Residuals: -1.05172, Convergence: 0.002630\n",
      "Epoch: 186, Loss: 2145.93970, Residuals: -1.04868, Convergence: 0.002450\n",
      "Epoch: 187, Loss: 2141.05256, Residuals: -1.04567, Convergence: 0.002283\n",
      "Epoch: 188, Loss: 2136.51125, Residuals: -1.04272, Convergence: 0.002126\n",
      "Epoch: 189, Loss: 2132.28861, Residuals: -1.03985, Convergence: 0.001980\n",
      "Epoch: 190, Loss: 2128.35816, Residuals: -1.03708, Convergence: 0.001847\n",
      "Epoch: 191, Loss: 2124.69572, Residuals: -1.03442, Convergence: 0.001724\n",
      "Epoch: 192, Loss: 2121.27982, Residuals: -1.03189, Convergence: 0.001610\n",
      "Epoch: 193, Loss: 2118.09081, Residuals: -1.02947, Convergence: 0.001506\n",
      "Epoch: 194, Loss: 2115.11310, Residuals: -1.02718, Convergence: 0.001408\n",
      "Epoch: 195, Loss: 2112.33076, Residuals: -1.02501, Convergence: 0.001317\n",
      "Epoch: 196, Loss: 2109.73115, Residuals: -1.02295, Convergence: 0.001232\n",
      "Epoch: 197, Loss: 2107.30133, Residuals: -1.02101, Convergence: 0.001153\n",
      "Epoch: 198, Loss: 2105.02944, Residuals: -1.01917, Convergence: 0.001079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 199, Loss: 2102.90325, Residuals: -1.01743, Convergence: 0.001011\n",
      "Epoch: 200, Loss: 2100.91269, Residuals: -1.01577, Convergence: 0.000947\n",
      "Evidence 14272.543\n",
      "\n",
      "Epoch: 200, Evidence: 14272.54297, Convergence: 0.219142\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.39e-01\n",
      "Epoch: 200, Loss: 2470.05379, Residuals: -1.01577, Convergence:   inf\n",
      "Epoch: 201, Loss: 2456.83807, Residuals: -1.01296, Convergence: 0.005379\n",
      "Epoch: 202, Loss: 2446.05560, Residuals: -1.00961, Convergence: 0.004408\n",
      "Epoch: 203, Loss: 2436.78293, Residuals: -1.00635, Convergence: 0.003805\n",
      "Epoch: 204, Loss: 2428.75218, Residuals: -1.00329, Convergence: 0.003307\n",
      "Epoch: 205, Loss: 2421.75966, Residuals: -1.00047, Convergence: 0.002887\n",
      "Epoch: 206, Loss: 2415.64151, Residuals: -0.99789, Convergence: 0.002533\n",
      "Epoch: 207, Loss: 2410.26122, Residuals: -0.99553, Convergence: 0.002232\n",
      "Epoch: 208, Loss: 2405.50465, Residuals: -0.99337, Convergence: 0.001977\n",
      "Epoch: 209, Loss: 2401.27522, Residuals: -0.99140, Convergence: 0.001761\n",
      "Epoch: 210, Loss: 2397.49239, Residuals: -0.98958, Convergence: 0.001578\n",
      "Epoch: 211, Loss: 2394.08987, Residuals: -0.98791, Convergence: 0.001421\n",
      "Epoch: 212, Loss: 2391.01108, Residuals: -0.98635, Convergence: 0.001288\n",
      "Epoch: 213, Loss: 2388.20999, Residuals: -0.98491, Convergence: 0.001173\n",
      "Epoch: 214, Loss: 2385.64786, Residuals: -0.98356, Convergence: 0.001074\n",
      "Epoch: 215, Loss: 2383.29289, Residuals: -0.98230, Convergence: 0.000988\n",
      "Evidence 14637.221\n",
      "\n",
      "Epoch: 215, Evidence: 14637.22070, Convergence: 0.024914\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.36e-01\n",
      "Epoch: 215, Loss: 2475.65717, Residuals: -0.98230, Convergence:   inf\n",
      "Epoch: 216, Loss: 2469.06663, Residuals: -0.97911, Convergence: 0.002669\n",
      "Epoch: 217, Loss: 2463.63613, Residuals: -0.97623, Convergence: 0.002204\n",
      "Epoch: 218, Loss: 2459.04568, Residuals: -0.97374, Convergence: 0.001867\n",
      "Epoch: 219, Loss: 2455.11027, Residuals: -0.97160, Convergence: 0.001603\n",
      "Epoch: 220, Loss: 2451.69100, Residuals: -0.96974, Convergence: 0.001395\n",
      "Epoch: 221, Loss: 2448.68300, Residuals: -0.96812, Convergence: 0.001228\n",
      "Epoch: 222, Loss: 2446.00417, Residuals: -0.96669, Convergence: 0.001095\n",
      "Epoch: 223, Loss: 2443.59255, Residuals: -0.96543, Convergence: 0.000987\n",
      "Evidence 14716.751\n",
      "\n",
      "Epoch: 223, Evidence: 14716.75098, Convergence: 0.005404\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.63e-01\n",
      "Epoch: 223, Loss: 2477.23585, Residuals: -0.96543, Convergence:   inf\n",
      "Epoch: 224, Loss: 2473.18497, Residuals: -0.96294, Convergence: 0.001638\n",
      "Epoch: 225, Loss: 2469.84416, Residuals: -0.96087, Convergence: 0.001353\n",
      "Epoch: 226, Loss: 2467.00530, Residuals: -0.95916, Convergence: 0.001151\n",
      "Epoch: 227, Loss: 2464.54016, Residuals: -0.95772, Convergence: 0.001000\n",
      "Epoch: 228, Loss: 2462.36172, Residuals: -0.95650, Convergence: 0.000885\n",
      "Evidence 14747.562\n",
      "\n",
      "Epoch: 228, Evidence: 14747.56250, Convergence: 0.002089\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.11e-01\n",
      "Epoch: 228, Loss: 2478.08196, Residuals: -0.95650, Convergence:   inf\n",
      "Epoch: 229, Loss: 2475.21877, Residuals: -0.95455, Convergence: 0.001157\n",
      "Epoch: 230, Loss: 2472.83461, Residuals: -0.95297, Convergence: 0.000964\n",
      "Evidence 14760.452\n",
      "\n",
      "Epoch: 230, Evidence: 14760.45215, Convergence: 0.000873\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.75e-01\n",
      "Epoch: 230, Loss: 2478.74609, Residuals: -0.95297, Convergence:   inf\n",
      "Epoch: 231, Loss: 2474.24247, Residuals: -0.95028, Convergence: 0.001820\n",
      "Epoch: 232, Loss: 2470.83465, Residuals: -0.94821, Convergence: 0.001379\n",
      "Epoch: 233, Loss: 2468.06974, Residuals: -0.94664, Convergence: 0.001120\n",
      "Epoch: 234, Loss: 2465.71823, Residuals: -0.94551, Convergence: 0.000954\n",
      "Evidence 14778.232\n",
      "\n",
      "Epoch: 234, Evidence: 14778.23242, Convergence: 0.002075\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.45e-01\n",
      "Epoch: 234, Loss: 2478.89981, Residuals: -0.94551, Convergence:   inf\n",
      "Epoch: 235, Loss: 2475.88902, Residuals: -0.94289, Convergence: 0.001216\n",
      "Epoch: 236, Loss: 2473.51498, Residuals: -0.94139, Convergence: 0.000960\n",
      "Evidence 14789.251\n",
      "\n",
      "Epoch: 236, Evidence: 14789.25098, Convergence: 0.000745\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.22e-01\n",
      "Epoch: 236, Loss: 2479.09794, Residuals: -0.94139, Convergence:   inf\n",
      "Epoch: 237, Loss: 2474.66931, Residuals: -0.93773, Convergence: 0.001790\n",
      "Epoch: 238, Loss: 2471.50511, Residuals: -0.93824, Convergence: 0.001280\n",
      "Epoch: 239, Loss: 2468.85153, Residuals: -0.93824, Convergence: 0.001075\n",
      "Epoch: 240, Loss: 2466.52416, Residuals: -0.94025, Convergence: 0.000944\n",
      "Evidence 14805.161\n",
      "\n",
      "Epoch: 240, Evidence: 14805.16113, Convergence: 0.001819\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.09e-01\n",
      "Epoch: 240, Loss: 2478.45075, Residuals: -0.94025, Convergence:   inf\n",
      "Epoch: 241, Loss: 2476.67396, Residuals: -0.93769, Convergence: 0.000717\n",
      "Evidence 14811.900\n",
      "\n",
      "Epoch: 241, Evidence: 14811.90039, Convergence: 0.000455\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.90e-02\n",
      "Epoch: 241, Loss: 2479.38286, Residuals: -0.93769, Convergence:   inf\n",
      "Epoch: 242, Loss: 2521.59147, Residuals: -0.97744, Convergence: -0.016739\n",
      "Epoch: 242, Loss: 2477.05512, Residuals: -0.93553, Convergence: 0.000940\n",
      "Evidence 14816.275\n",
      "\n",
      "Epoch: 242, Evidence: 14816.27539, Convergence: 0.000750\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.19e-02\n",
      "Epoch: 242, Loss: 2478.83700, Residuals: -0.93553, Convergence:   inf\n",
      "Epoch: 243, Loss: 2484.04208, Residuals: -0.93699, Convergence: -0.002095\n",
      "Epoch: 243, Loss: 2478.86521, Residuals: -0.93424, Convergence: -0.000011\n",
      "Evidence 14817.914\n",
      "\n",
      "Epoch: 243, Evidence: 14817.91406, Convergence: 0.000861\n",
      "Total samples: 181, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 382.50226, Residuals: -4.54427, Convergence:   inf\n",
      "Epoch: 1, Loss: 356.76430, Residuals: -4.42377, Convergence: 0.072143\n",
      "Epoch: 2, Loss: 335.66008, Residuals: -4.25875, Convergence: 0.062874\n",
      "Epoch: 3, Loss: 319.53685, Residuals: -4.09355, Convergence: 0.050458\n",
      "Epoch: 4, Loss: 307.24114, Residuals: -3.94785, Convergence: 0.040020\n",
      "Epoch: 5, Loss: 297.48810, Residuals: -3.81887, Convergence: 0.032785\n",
      "Epoch: 6, Loss: 289.56935, Residuals: -3.70639, Convergence: 0.027347\n",
      "Epoch: 7, Loss: 282.99553, Residuals: -3.61004, Convergence: 0.023229\n",
      "Epoch: 8, Loss: 277.40492, Residuals: -3.52780, Convergence: 0.020153\n",
      "Epoch: 9, Loss: 272.54230, Residuals: -3.45735, Convergence: 0.017842\n",
      "Epoch: 10, Loss: 268.22647, Residuals: -3.39657, Convergence: 0.016090\n",
      "Epoch: 11, Loss: 264.32646, Residuals: -3.34366, Convergence: 0.014755\n",
      "Epoch: 12, Loss: 260.74688, Residuals: -3.29709, Convergence: 0.013728\n",
      "Epoch: 13, Loss: 257.41819, Residuals: -3.25551, Convergence: 0.012931\n",
      "Epoch: 14, Loss: 254.28952, Residuals: -3.21776, Convergence: 0.012304\n",
      "Epoch: 15, Loss: 251.32433, Residuals: -3.18283, Convergence: 0.011798\n",
      "Epoch: 16, Loss: 248.49898, Residuals: -3.14996, Convergence: 0.011370\n",
      "Epoch: 17, Loss: 245.79870, Residuals: -3.11869, Convergence: 0.010986\n",
      "Epoch: 18, Loss: 243.20625, Residuals: -3.08868, Convergence: 0.010659\n",
      "Epoch: 19, Loss: 240.69401, Residuals: -3.05947, Convergence: 0.010438\n",
      "Epoch: 20, Loss: 238.22774, Residuals: -3.03052, Convergence: 0.010353\n",
      "Epoch: 21, Loss: 235.77573, Residuals: -3.00131, Convergence: 0.010400\n",
      "Epoch: 22, Loss: 233.31547, Residuals: -2.97148, Convergence: 0.010545\n",
      "Epoch: 23, Loss: 230.82889, Residuals: -2.94080, Convergence: 0.010772\n",
      "Epoch: 24, Loss: 228.28559, Residuals: -2.90894, Convergence: 0.011141\n",
      "Epoch: 25, Loss: 225.63464, Residuals: -2.87531, Convergence: 0.011749\n",
      "Epoch: 26, Loss: 222.83591, Residuals: -2.83941, Convergence: 0.012560\n",
      "Epoch: 27, Loss: 219.94663, Residuals: -2.80177, Convergence: 0.013136\n",
      "Epoch: 28, Loss: 217.09994, Residuals: -2.76387, Convergence: 0.013112\n",
      "Epoch: 29, Loss: 214.35704, Residuals: -2.72657, Convergence: 0.012796\n",
      "Epoch: 30, Loss: 211.71439, Residuals: -2.68997, Convergence: 0.012482\n",
      "Epoch: 31, Loss: 209.15476, Residuals: -2.65401, Convergence: 0.012238\n",
      "Epoch: 32, Loss: 206.66321, Residuals: -2.61855, Convergence: 0.012056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33, Loss: 204.22928, Residuals: -2.58351, Convergence: 0.011918\n",
      "Epoch: 34, Loss: 201.84652, Residuals: -2.54879, Convergence: 0.011805\n",
      "Epoch: 35, Loss: 199.51156, Residuals: -2.51434, Convergence: 0.011703\n",
      "Epoch: 36, Loss: 197.22332, Residuals: -2.48011, Convergence: 0.011602\n",
      "Epoch: 37, Loss: 194.98219, Residuals: -2.44607, Convergence: 0.011494\n",
      "Epoch: 38, Loss: 192.78946, Residuals: -2.41220, Convergence: 0.011374\n",
      "Epoch: 39, Loss: 190.64681, Residuals: -2.37851, Convergence: 0.011239\n",
      "Epoch: 40, Loss: 188.55605, Residuals: -2.34499, Convergence: 0.011088\n",
      "Epoch: 41, Loss: 186.51883, Residuals: -2.31168, Convergence: 0.010922\n",
      "Epoch: 42, Loss: 184.53657, Residuals: -2.27859, Convergence: 0.010742\n",
      "Epoch: 43, Loss: 182.61039, Residuals: -2.24574, Convergence: 0.010548\n",
      "Epoch: 44, Loss: 180.74121, Residuals: -2.21317, Convergence: 0.010342\n",
      "Epoch: 45, Loss: 178.92977, Residuals: -2.18092, Convergence: 0.010124\n",
      "Epoch: 46, Loss: 177.17673, Residuals: -2.14901, Convergence: 0.009894\n",
      "Epoch: 47, Loss: 175.48276, Residuals: -2.11749, Convergence: 0.009653\n",
      "Epoch: 48, Loss: 173.84849, Residuals: -2.08638, Convergence: 0.009401\n",
      "Epoch: 49, Loss: 172.27443, Residuals: -2.05573, Convergence: 0.009137\n",
      "Epoch: 50, Loss: 170.76083, Residuals: -2.02556, Convergence: 0.008864\n",
      "Epoch: 51, Loss: 169.30763, Residuals: -1.99591, Convergence: 0.008583\n",
      "Epoch: 52, Loss: 167.91435, Residuals: -1.96680, Convergence: 0.008298\n",
      "Epoch: 53, Loss: 166.58011, Residuals: -1.93826, Convergence: 0.008010\n",
      "Epoch: 54, Loss: 165.30355, Residuals: -1.91033, Convergence: 0.007722\n",
      "Epoch: 55, Loss: 164.08298, Residuals: -1.88301, Convergence: 0.007439\n",
      "Epoch: 56, Loss: 162.91629, Residuals: -1.85632, Convergence: 0.007161\n",
      "Epoch: 57, Loss: 161.80115, Residuals: -1.83027, Convergence: 0.006892\n",
      "Epoch: 58, Loss: 160.73508, Residuals: -1.80487, Convergence: 0.006632\n",
      "Epoch: 59, Loss: 159.71554, Residuals: -1.78011, Convergence: 0.006383\n",
      "Epoch: 60, Loss: 158.74007, Residuals: -1.75599, Convergence: 0.006145\n",
      "Epoch: 61, Loss: 157.80634, Residuals: -1.73251, Convergence: 0.005917\n",
      "Epoch: 62, Loss: 156.91228, Residuals: -1.70965, Convergence: 0.005698\n",
      "Epoch: 63, Loss: 156.05602, Residuals: -1.68740, Convergence: 0.005487\n",
      "Epoch: 64, Loss: 155.23594, Residuals: -1.66577, Convergence: 0.005283\n",
      "Epoch: 65, Loss: 154.45061, Residuals: -1.64476, Convergence: 0.005085\n",
      "Epoch: 66, Loss: 153.69877, Residuals: -1.62435, Convergence: 0.004892\n",
      "Epoch: 67, Loss: 152.97933, Residuals: -1.60454, Convergence: 0.004703\n",
      "Epoch: 68, Loss: 152.29125, Residuals: -1.58534, Convergence: 0.004518\n",
      "Epoch: 69, Loss: 151.63357, Residuals: -1.56674, Convergence: 0.004337\n",
      "Epoch: 70, Loss: 151.00535, Residuals: -1.54874, Convergence: 0.004160\n",
      "Epoch: 71, Loss: 150.40568, Residuals: -1.53134, Convergence: 0.003987\n",
      "Epoch: 72, Loss: 149.83365, Residuals: -1.51453, Convergence: 0.003818\n",
      "Epoch: 73, Loss: 149.28833, Residuals: -1.49830, Convergence: 0.003653\n",
      "Epoch: 74, Loss: 148.76876, Residuals: -1.48265, Convergence: 0.003492\n",
      "Epoch: 75, Loss: 148.27400, Residuals: -1.46757, Convergence: 0.003337\n",
      "Epoch: 76, Loss: 147.80307, Residuals: -1.45305, Convergence: 0.003186\n",
      "Epoch: 77, Loss: 147.35499, Residuals: -1.43908, Convergence: 0.003041\n",
      "Epoch: 78, Loss: 146.92877, Residuals: -1.42564, Convergence: 0.002901\n",
      "Epoch: 79, Loss: 146.52345, Residuals: -1.41272, Convergence: 0.002766\n",
      "Epoch: 80, Loss: 146.13806, Residuals: -1.40032, Convergence: 0.002637\n",
      "Epoch: 81, Loss: 145.77168, Residuals: -1.38841, Convergence: 0.002513\n",
      "Epoch: 82, Loss: 145.42340, Residuals: -1.37698, Convergence: 0.002395\n",
      "Epoch: 83, Loss: 145.09235, Residuals: -1.36601, Convergence: 0.002282\n",
      "Epoch: 84, Loss: 144.77771, Residuals: -1.35549, Convergence: 0.002173\n",
      "Epoch: 85, Loss: 144.47868, Residuals: -1.34540, Convergence: 0.002070\n",
      "Epoch: 86, Loss: 144.19454, Residuals: -1.33573, Convergence: 0.001971\n",
      "Epoch: 87, Loss: 143.92458, Residuals: -1.32646, Convergence: 0.001876\n",
      "Epoch: 88, Loss: 143.66815, Residuals: -1.31758, Convergence: 0.001785\n",
      "Epoch: 89, Loss: 143.42464, Residuals: -1.30907, Convergence: 0.001698\n",
      "Epoch: 90, Loss: 143.19349, Residuals: -1.30092, Convergence: 0.001614\n",
      "Epoch: 91, Loss: 142.97418, Residuals: -1.29311, Convergence: 0.001534\n",
      "Epoch: 92, Loss: 142.76622, Residuals: -1.28563, Convergence: 0.001457\n",
      "Epoch: 93, Loss: 142.56917, Residuals: -1.27847, Convergence: 0.001382\n",
      "Epoch: 94, Loss: 142.38261, Residuals: -1.27161, Convergence: 0.001310\n",
      "Epoch: 95, Loss: 142.20617, Residuals: -1.26505, Convergence: 0.001241\n",
      "Epoch: 96, Loss: 142.03950, Residuals: -1.25878, Convergence: 0.001173\n",
      "Epoch: 97, Loss: 141.88226, Residuals: -1.25277, Convergence: 0.001108\n",
      "Epoch: 98, Loss: 141.73417, Residuals: -1.24704, Convergence: 0.001045\n",
      "Epoch: 99, Loss: 141.59494, Residuals: -1.24156, Convergence: 0.000983\n",
      "Evidence -183.459\n",
      "\n",
      "Epoch: 99, Evidence: -183.45935, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 7.25e-01\n",
      "Epoch: 99, Loss: 1358.40627, Residuals: -1.24156, Convergence:   inf\n",
      "Epoch: 100, Loss: 1295.86110, Residuals: -1.27238, Convergence: 0.048265\n",
      "Epoch: 101, Loss: 1248.31892, Residuals: -1.29624, Convergence: 0.038085\n",
      "Epoch: 102, Loss: 1212.38896, Residuals: -1.31322, Convergence: 0.029636\n",
      "Epoch: 103, Loss: 1184.43770, Residuals: -1.32513, Convergence: 0.023599\n",
      "Epoch: 104, Loss: 1161.84981, Residuals: -1.33392, Convergence: 0.019441\n",
      "Epoch: 105, Loss: 1143.11161, Residuals: -1.34063, Convergence: 0.016392\n",
      "Epoch: 106, Loss: 1127.31028, Residuals: -1.34574, Convergence: 0.014017\n",
      "Epoch: 107, Loss: 1113.82480, Residuals: -1.34946, Convergence: 0.012107\n",
      "Epoch: 108, Loss: 1102.19520, Residuals: -1.35196, Convergence: 0.010551\n",
      "Epoch: 109, Loss: 1092.06484, Residuals: -1.35337, Convergence: 0.009276\n",
      "Epoch: 110, Loss: 1083.14658, Residuals: -1.35378, Convergence: 0.008234\n",
      "Epoch: 111, Loss: 1075.20520, Residuals: -1.35330, Convergence: 0.007386\n",
      "Epoch: 112, Loss: 1068.04474, Residuals: -1.35200, Convergence: 0.006704\n",
      "Epoch: 113, Loss: 1061.49790, Residuals: -1.34993, Convergence: 0.006168\n",
      "Epoch: 114, Loss: 1055.41854, Residuals: -1.34714, Convergence: 0.005760\n",
      "Epoch: 115, Loss: 1049.67863, Residuals: -1.34365, Convergence: 0.005468\n",
      "Epoch: 116, Loss: 1044.16266, Residuals: -1.33948, Convergence: 0.005283\n",
      "Epoch: 117, Loss: 1038.77101, Residuals: -1.33463, Convergence: 0.005190\n",
      "Epoch: 118, Loss: 1033.42166, Residuals: -1.32912, Convergence: 0.005176\n",
      "Epoch: 119, Loss: 1028.05879, Residuals: -1.32297, Convergence: 0.005216\n",
      "Epoch: 120, Loss: 1022.66499, Residuals: -1.31623, Convergence: 0.005274\n",
      "Epoch: 121, Loss: 1017.26861, Residuals: -1.30899, Convergence: 0.005305\n",
      "Epoch: 122, Loss: 1011.93859, Residuals: -1.30134, Convergence: 0.005267\n",
      "Epoch: 123, Loss: 1006.76212, Residuals: -1.29339, Convergence: 0.005142\n",
      "Epoch: 124, Loss: 1001.81562, Residuals: -1.28523, Convergence: 0.004938\n",
      "Epoch: 125, Loss: 997.14698, Residuals: -1.27695, Convergence: 0.004682\n",
      "Epoch: 126, Loss: 992.77303, Residuals: -1.26862, Convergence: 0.004406\n",
      "Epoch: 127, Loss: 988.68917, Residuals: -1.26030, Convergence: 0.004131\n",
      "Epoch: 128, Loss: 984.87758, Residuals: -1.25205, Convergence: 0.003870\n",
      "Epoch: 129, Loss: 981.31601, Residuals: -1.24392, Convergence: 0.003629\n",
      "Epoch: 130, Loss: 977.98218, Residuals: -1.23593, Convergence: 0.003409\n",
      "Epoch: 131, Loss: 974.85415, Residuals: -1.22812, Convergence: 0.003209\n",
      "Epoch: 132, Loss: 971.91331, Residuals: -1.22050, Convergence: 0.003026\n",
      "Epoch: 133, Loss: 969.14304, Residuals: -1.21310, Convergence: 0.002858\n",
      "Epoch: 134, Loss: 966.52913, Residuals: -1.20593, Convergence: 0.002704\n",
      "Epoch: 135, Loss: 964.05807, Residuals: -1.19901, Convergence: 0.002563\n",
      "Epoch: 136, Loss: 961.71993, Residuals: -1.19233, Convergence: 0.002431\n",
      "Epoch: 137, Loss: 959.50421, Residuals: -1.18591, Convergence: 0.002309\n",
      "Epoch: 138, Loss: 957.40214, Residuals: -1.17975, Convergence: 0.002196\n",
      "Epoch: 139, Loss: 955.40599, Residuals: -1.17385, Convergence: 0.002089\n",
      "Epoch: 140, Loss: 953.50794, Residuals: -1.16820, Convergence: 0.001991\n",
      "Epoch: 141, Loss: 951.70114, Residuals: -1.16280, Convergence: 0.001899\n",
      "Epoch: 142, Loss: 949.97914, Residuals: -1.15765, Convergence: 0.001813\n",
      "Epoch: 143, Loss: 948.33526, Residuals: -1.15274, Convergence: 0.001733\n",
      "Epoch: 144, Loss: 946.76412, Residuals: -1.14806, Convergence: 0.001659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 145, Loss: 945.25967, Residuals: -1.14360, Convergence: 0.001592\n",
      "Epoch: 146, Loss: 943.81711, Residuals: -1.13935, Convergence: 0.001528\n",
      "Epoch: 147, Loss: 942.43114, Residuals: -1.13530, Convergence: 0.001471\n",
      "Epoch: 148, Loss: 941.09738, Residuals: -1.13144, Convergence: 0.001417\n",
      "Epoch: 149, Loss: 939.81158, Residuals: -1.12776, Convergence: 0.001368\n",
      "Epoch: 150, Loss: 938.56977, Residuals: -1.12425, Convergence: 0.001323\n",
      "Epoch: 151, Loss: 937.36807, Residuals: -1.12089, Convergence: 0.001282\n",
      "Epoch: 152, Loss: 936.20318, Residuals: -1.11768, Convergence: 0.001244\n",
      "Epoch: 153, Loss: 935.07195, Residuals: -1.11461, Convergence: 0.001210\n",
      "Epoch: 154, Loss: 933.97126, Residuals: -1.11167, Convergence: 0.001179\n",
      "Epoch: 155, Loss: 932.89815, Residuals: -1.10885, Convergence: 0.001150\n",
      "Epoch: 156, Loss: 931.84955, Residuals: -1.10613, Convergence: 0.001125\n",
      "Epoch: 157, Loss: 930.82259, Residuals: -1.10351, Convergence: 0.001103\n",
      "Epoch: 158, Loss: 929.81459, Residuals: -1.10098, Convergence: 0.001084\n",
      "Epoch: 159, Loss: 928.82205, Residuals: -1.09853, Convergence: 0.001069\n",
      "Epoch: 160, Loss: 927.84228, Residuals: -1.09615, Convergence: 0.001056\n",
      "Epoch: 161, Loss: 926.87164, Residuals: -1.09382, Convergence: 0.001047\n",
      "Epoch: 162, Loss: 925.90714, Residuals: -1.09155, Convergence: 0.001042\n",
      "Epoch: 163, Loss: 924.94588, Residuals: -1.08932, Convergence: 0.001039\n",
      "Epoch: 164, Loss: 923.98504, Residuals: -1.08712, Convergence: 0.001040\n",
      "Epoch: 165, Loss: 923.02260, Residuals: -1.08495, Convergence: 0.001043\n",
      "Epoch: 166, Loss: 922.05776, Residuals: -1.08280, Convergence: 0.001046\n",
      "Epoch: 167, Loss: 921.09056, Residuals: -1.08066, Convergence: 0.001050\n",
      "Epoch: 168, Loss: 920.12247, Residuals: -1.07854, Convergence: 0.001052\n",
      "Epoch: 169, Loss: 919.15589, Residuals: -1.07644, Convergence: 0.001052\n",
      "Epoch: 170, Loss: 918.19413, Residuals: -1.07437, Convergence: 0.001047\n",
      "Epoch: 171, Loss: 917.24053, Residuals: -1.07233, Convergence: 0.001040\n",
      "Epoch: 172, Loss: 916.29893, Residuals: -1.07033, Convergence: 0.001028\n",
      "Epoch: 173, Loss: 915.37225, Residuals: -1.06837, Convergence: 0.001012\n",
      "Epoch: 174, Loss: 914.46298, Residuals: -1.06646, Convergence: 0.000994\n",
      "Evidence 11060.044\n",
      "\n",
      "Epoch: 174, Evidence: 11060.04395, Convergence: 1.016587\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 5.76e-01\n",
      "Epoch: 174, Loss: 2328.34817, Residuals: -1.06646, Convergence:   inf\n",
      "Epoch: 175, Loss: 2292.44907, Residuals: -1.07371, Convergence: 0.015660\n",
      "Epoch: 176, Loss: 2266.46308, Residuals: -1.07338, Convergence: 0.011465\n",
      "Epoch: 177, Loss: 2244.78671, Residuals: -1.07190, Convergence: 0.009656\n",
      "Epoch: 178, Loss: 2226.42750, Residuals: -1.07013, Convergence: 0.008246\n",
      "Epoch: 179, Loss: 2210.72331, Residuals: -1.06819, Convergence: 0.007104\n",
      "Epoch: 180, Loss: 2197.16171, Residuals: -1.06612, Convergence: 0.006172\n",
      "Epoch: 181, Loss: 2185.32443, Residuals: -1.06392, Convergence: 0.005417\n",
      "Epoch: 182, Loss: 2174.86355, Residuals: -1.06160, Convergence: 0.004810\n",
      "Epoch: 183, Loss: 2165.49123, Residuals: -1.05913, Convergence: 0.004328\n",
      "Epoch: 184, Loss: 2156.98165, Residuals: -1.05648, Convergence: 0.003945\n",
      "Epoch: 185, Loss: 2149.17343, Residuals: -1.05367, Convergence: 0.003633\n",
      "Epoch: 186, Loss: 2141.96562, Residuals: -1.05069, Convergence: 0.003365\n",
      "Epoch: 187, Loss: 2135.30429, Residuals: -1.04760, Convergence: 0.003120\n",
      "Epoch: 188, Loss: 2129.16224, Residuals: -1.04445, Convergence: 0.002885\n",
      "Epoch: 189, Loss: 2123.52154, Residuals: -1.04129, Convergence: 0.002656\n",
      "Epoch: 190, Loss: 2118.36234, Residuals: -1.03818, Convergence: 0.002435\n",
      "Epoch: 191, Loss: 2113.65626, Residuals: -1.03515, Convergence: 0.002227\n",
      "Epoch: 192, Loss: 2109.37329, Residuals: -1.03222, Convergence: 0.002030\n",
      "Epoch: 193, Loss: 2105.47826, Residuals: -1.02942, Convergence: 0.001850\n",
      "Epoch: 194, Loss: 2101.93480, Residuals: -1.02675, Convergence: 0.001686\n",
      "Epoch: 195, Loss: 2098.70694, Residuals: -1.02420, Convergence: 0.001538\n",
      "Epoch: 196, Loss: 2095.76102, Residuals: -1.02179, Convergence: 0.001406\n",
      "Epoch: 197, Loss: 2093.06471, Residuals: -1.01950, Convergence: 0.001288\n",
      "Epoch: 198, Loss: 2090.59006, Residuals: -1.01734, Convergence: 0.001184\n",
      "Epoch: 199, Loss: 2088.31008, Residuals: -1.01530, Convergence: 0.001092\n",
      "Epoch: 200, Loss: 2086.20285, Residuals: -1.01336, Convergence: 0.001010\n",
      "Epoch: 201, Loss: 2084.24794, Residuals: -1.01154, Convergence: 0.000938\n",
      "Evidence 14280.091\n",
      "\n",
      "Epoch: 201, Evidence: 14280.09082, Convergence: 0.225492\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 4.38e-01\n",
      "Epoch: 201, Loss: 2463.22956, Residuals: -1.01154, Convergence:   inf\n",
      "Epoch: 202, Loss: 2450.29581, Residuals: -1.00894, Convergence: 0.005278\n",
      "Epoch: 203, Loss: 2439.80267, Residuals: -1.00583, Convergence: 0.004301\n",
      "Epoch: 204, Loss: 2430.83623, Residuals: -1.00275, Convergence: 0.003689\n",
      "Epoch: 205, Loss: 2423.09856, Residuals: -0.99981, Convergence: 0.003193\n",
      "Epoch: 206, Loss: 2416.36717, Residuals: -0.99705, Convergence: 0.002786\n",
      "Epoch: 207, Loss: 2410.47035, Residuals: -0.99450, Convergence: 0.002446\n",
      "Epoch: 208, Loss: 2405.27045, Residuals: -0.99213, Convergence: 0.002162\n",
      "Epoch: 209, Loss: 2400.65499, Residuals: -0.98995, Convergence: 0.001923\n",
      "Epoch: 210, Loss: 2396.53416, Residuals: -0.98794, Convergence: 0.001719\n",
      "Epoch: 211, Loss: 2392.83146, Residuals: -0.98609, Convergence: 0.001547\n",
      "Epoch: 212, Loss: 2389.48698, Residuals: -0.98440, Convergence: 0.001400\n",
      "Epoch: 213, Loss: 2386.44840, Residuals: -0.98285, Convergence: 0.001273\n",
      "Epoch: 214, Loss: 2383.67473, Residuals: -0.98144, Convergence: 0.001164\n",
      "Epoch: 215, Loss: 2381.13023, Residuals: -0.98015, Convergence: 0.001069\n",
      "Epoch: 216, Loss: 2378.78752, Residuals: -0.97897, Convergence: 0.000985\n",
      "Evidence 14646.279\n",
      "\n",
      "Epoch: 216, Evidence: 14646.27930, Convergence: 0.025002\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 3.36e-01\n",
      "Epoch: 216, Loss: 2469.03230, Residuals: -0.97897, Convergence:   inf\n",
      "Epoch: 217, Loss: 2462.90655, Residuals: -0.97603, Convergence: 0.002487\n",
      "Epoch: 218, Loss: 2457.83794, Residuals: -0.97342, Convergence: 0.002062\n",
      "Epoch: 219, Loss: 2453.52286, Residuals: -0.97118, Convergence: 0.001759\n",
      "Epoch: 220, Loss: 2449.78957, Residuals: -0.96925, Convergence: 0.001524\n",
      "Epoch: 221, Loss: 2446.51520, Residuals: -0.96759, Convergence: 0.001338\n",
      "Epoch: 222, Loss: 2443.60922, Residuals: -0.96616, Convergence: 0.001189\n",
      "Epoch: 223, Loss: 2441.00226, Residuals: -0.96492, Convergence: 0.001068\n",
      "Epoch: 224, Loss: 2438.64255, Residuals: -0.96384, Convergence: 0.000968\n",
      "Evidence 14721.619\n",
      "\n",
      "Epoch: 224, Evidence: 14721.61914, Convergence: 0.005118\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 2.64e-01\n",
      "Epoch: 224, Loss: 2470.74189, Residuals: -0.96384, Convergence:   inf\n",
      "Epoch: 225, Loss: 2466.95797, Residuals: -0.96166, Convergence: 0.001534\n",
      "Epoch: 226, Loss: 2463.78522, Residuals: -0.95987, Convergence: 0.001288\n",
      "Epoch: 227, Loss: 2461.03992, Residuals: -0.95840, Convergence: 0.001116\n",
      "Epoch: 228, Loss: 2458.61906, Residuals: -0.95718, Convergence: 0.000985\n",
      "Evidence 14748.746\n",
      "\n",
      "Epoch: 228, Evidence: 14748.74609, Convergence: 0.001839\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 2.14e-01\n",
      "Epoch: 228, Loss: 2471.80732, Residuals: -0.95718, Convergence:   inf\n",
      "Epoch: 229, Loss: 2468.91750, Residuals: -0.95540, Convergence: 0.001170\n",
      "Epoch: 230, Loss: 2466.46134, Residuals: -0.95399, Convergence: 0.000996\n",
      "Evidence 14760.746\n",
      "\n",
      "Epoch: 230, Evidence: 14760.74609, Convergence: 0.000813\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.78e-01\n",
      "Epoch: 230, Loss: 2472.59151, Residuals: -0.95399, Convergence:   inf\n",
      "Epoch: 231, Loss: 2467.88802, Residuals: -0.95150, Convergence: 0.001906\n",
      "Epoch: 232, Loss: 2464.26795, Residuals: -0.94977, Convergence: 0.001469\n",
      "Epoch: 233, Loss: 2461.32524, Residuals: -0.94858, Convergence: 0.001196\n",
      "Epoch: 234, Loss: 2458.83635, Residuals: -0.94793, Convergence: 0.001012\n",
      "Epoch: 235, Loss: 2456.66454, Residuals: -0.94764, Convergence: 0.000884\n",
      "Evidence 14781.482\n",
      "\n",
      "Epoch: 235, Evidence: 14781.48242, Convergence: 0.002215\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.47e-01\n",
      "Epoch: 235, Loss: 2472.51245, Residuals: -0.94764, Convergence:   inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 236, Loss: 2469.47702, Residuals: -0.94557, Convergence: 0.001229\n",
      "Epoch: 237, Loss: 2467.05795, Residuals: -0.94458, Convergence: 0.000981\n",
      "Evidence 14793.405\n",
      "\n",
      "Epoch: 237, Evidence: 14793.40527, Convergence: 0.000806\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.25e-01\n",
      "Epoch: 237, Loss: 2472.71663, Residuals: -0.94458, Convergence:   inf\n",
      "Epoch: 238, Loss: 2468.16958, Residuals: -0.94207, Convergence: 0.001842\n",
      "Epoch: 239, Loss: 2464.92563, Residuals: -0.94312, Convergence: 0.001316\n",
      "Epoch: 240, Loss: 2462.29630, Residuals: -0.94402, Convergence: 0.001068\n",
      "Epoch: 241, Loss: 2459.99791, Residuals: -0.94679, Convergence: 0.000934\n",
      "Evidence 14809.512\n",
      "\n",
      "Epoch: 241, Evidence: 14809.51172, Convergence: 0.001893\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.12e-01\n",
      "Epoch: 241, Loss: 2472.00502, Residuals: -0.94679, Convergence:   inf\n",
      "Epoch: 242, Loss: 2469.79838, Residuals: -0.94620, Convergence: 0.000893\n",
      "Evidence 14816.760\n",
      "\n",
      "Epoch: 242, Evidence: 14816.75977, Convergence: 0.000489\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 9.14e-02\n",
      "Epoch: 242, Loss: 2472.90494, Residuals: -0.94620, Convergence:   inf\n",
      "Epoch: 243, Loss: 2508.85291, Residuals: -0.99191, Convergence: -0.014328\n",
      "Epoch: 243, Loss: 2470.41568, Residuals: -0.94575, Convergence: 0.001008\n",
      "Epoch: 244, Loss: 2470.07882, Residuals: -0.94905, Convergence: 0.000136\n",
      "Evidence 14821.907\n",
      "\n",
      "Epoch: 244, Evidence: 14821.90723, Convergence: 0.000836\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 7.59e-02\n",
      "Epoch: 244, Loss: 2472.86924, Residuals: -0.94905, Convergence:   inf\n",
      "Epoch: 245, Loss: 2526.22989, Residuals: -1.01800, Convergence: -0.021123\n",
      "Epoch: 245, Loss: 2470.42362, Residuals: -0.94769, Convergence: 0.000990\n",
      "Evidence 14826.623\n",
      "\n",
      "Epoch: 245, Evidence: 14826.62305, Convergence: 0.001154\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 7.12e-02\n",
      "Epoch: 245, Loss: 2472.40431, Residuals: -0.94769, Convergence:   inf\n",
      "Epoch: 246, Loss: 2472.12797, Residuals: -0.94798, Convergence: 0.000112\n",
      "Evidence 14828.384\n",
      "\n",
      "Epoch: 246, Evidence: 14828.38379, Convergence: 0.000119\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 5.98e-02\n",
      "Epoch: 246, Loss: 2473.01508, Residuals: -0.94798, Convergence:   inf\n",
      "Epoch: 247, Loss: 2527.39967, Residuals: -1.01145, Convergence: -0.021518\n",
      "Epoch: 247, Loss: 2471.09757, Residuals: -0.94600, Convergence: 0.000776\n",
      "Evidence 14831.867\n",
      "\n",
      "Epoch: 247, Evidence: 14831.86719, Convergence: 0.000354\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 5.65e-02\n",
      "Epoch: 247, Loss: 2472.54168, Residuals: -0.94600, Convergence:   inf\n",
      "Epoch: 248, Loss: 2479.76376, Residuals: -0.95396, Convergence: -0.002912\n",
      "Epoch: 248, Loss: 2473.23052, Residuals: -0.94522, Convergence: -0.000279\n",
      "Evidence 14832.195\n",
      "\n",
      "Epoch: 248, Evidence: 14832.19531, Convergence: 0.000376\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 381.68953, Residuals: -4.51727, Convergence:   inf\n",
      "Epoch: 1, Loss: 355.96894, Residuals: -4.39731, Convergence: 0.072255\n",
      "Epoch: 2, Loss: 334.86581, Residuals: -4.23265, Convergence: 0.063020\n",
      "Epoch: 3, Loss: 318.76325, Residuals: -4.06779, Convergence: 0.050516\n",
      "Epoch: 4, Loss: 306.47158, Residuals: -3.92220, Convergence: 0.040107\n",
      "Epoch: 5, Loss: 296.71991, Residuals: -3.79320, Convergence: 0.032865\n",
      "Epoch: 6, Loss: 288.80202, Residuals: -3.68062, Convergence: 0.027416\n",
      "Epoch: 7, Loss: 282.22555, Residuals: -3.58405, Convergence: 0.023302\n",
      "Epoch: 8, Loss: 276.62659, Residuals: -3.50147, Convergence: 0.020240\n",
      "Epoch: 9, Loss: 271.74898, Residuals: -3.43058, Convergence: 0.017949\n",
      "Epoch: 10, Loss: 267.41094, Residuals: -3.36932, Convergence: 0.016222\n",
      "Epoch: 11, Loss: 263.48131, Residuals: -3.31592, Convergence: 0.014914\n",
      "Epoch: 12, Loss: 259.86505, Residuals: -3.26887, Convergence: 0.013916\n",
      "Epoch: 13, Loss: 256.49390, Residuals: -3.22687, Convergence: 0.013143\n",
      "Epoch: 14, Loss: 253.31969, Residuals: -3.18877, Convergence: 0.012530\n",
      "Epoch: 15, Loss: 250.30984, Residuals: -3.15359, Convergence: 0.012024\n",
      "Epoch: 16, Loss: 247.44428, Residuals: -3.12063, Convergence: 0.011581\n",
      "Epoch: 17, Loss: 244.70824, Residuals: -3.08943, Convergence: 0.011181\n",
      "Epoch: 18, Loss: 242.08090, Residuals: -3.05960, Convergence: 0.010853\n",
      "Epoch: 19, Loss: 239.53132, Residuals: -3.03065, Convergence: 0.010644\n",
      "Epoch: 20, Loss: 237.02465, Residuals: -3.00205, Convergence: 0.010576\n",
      "Epoch: 21, Loss: 234.53164, Residuals: -2.97331, Convergence: 0.010630\n",
      "Epoch: 22, Loss: 232.03301, Residuals: -2.94415, Convergence: 0.010768\n",
      "Epoch: 23, Loss: 229.50991, Residuals: -2.91433, Convergence: 0.010993\n",
      "Epoch: 24, Loss: 226.92694, Residuals: -2.88347, Convergence: 0.011382\n",
      "Epoch: 25, Loss: 224.22956, Residuals: -2.85089, Convergence: 0.012030\n",
      "Epoch: 26, Loss: 221.38145, Residuals: -2.81607, Convergence: 0.012865\n",
      "Epoch: 27, Loss: 218.44770, Residuals: -2.77960, Convergence: 0.013430\n",
      "Epoch: 28, Loss: 215.55508, Residuals: -2.74287, Convergence: 0.013419\n",
      "Epoch: 29, Loss: 212.75496, Residuals: -2.70657, Convergence: 0.013161\n",
      "Epoch: 30, Loss: 210.04162, Residuals: -2.67072, Convergence: 0.012918\n",
      "Epoch: 31, Loss: 207.39922, Residuals: -2.63520, Convergence: 0.012741\n",
      "Epoch: 32, Loss: 204.81536, Residuals: -2.59990, Convergence: 0.012616\n",
      "Epoch: 33, Loss: 202.28248, Residuals: -2.56474, Convergence: 0.012521\n",
      "Epoch: 34, Loss: 199.79693, Residuals: -2.52967, Convergence: 0.012440\n",
      "Epoch: 35, Loss: 197.35776, Residuals: -2.49468, Convergence: 0.012359\n",
      "Epoch: 36, Loss: 194.96570, Residuals: -2.45976, Convergence: 0.012269\n",
      "Epoch: 37, Loss: 192.62237, Residuals: -2.42491, Convergence: 0.012165\n",
      "Epoch: 38, Loss: 190.32981, Residuals: -2.39016, Convergence: 0.012045\n",
      "Epoch: 39, Loss: 188.09010, Residuals: -2.35551, Convergence: 0.011908\n",
      "Epoch: 40, Loss: 185.90515, Residuals: -2.32100, Convergence: 0.011753\n",
      "Epoch: 41, Loss: 183.77661, Residuals: -2.28664, Convergence: 0.011582\n",
      "Epoch: 42, Loss: 181.70585, Residuals: -2.25247, Convergence: 0.011396\n",
      "Epoch: 43, Loss: 179.69394, Residuals: -2.21850, Convergence: 0.011196\n",
      "Epoch: 44, Loss: 177.74175, Residuals: -2.18477, Convergence: 0.010983\n",
      "Epoch: 45, Loss: 175.84993, Residuals: -2.15130, Convergence: 0.010758\n",
      "Epoch: 46, Loss: 174.01905, Residuals: -2.11812, Convergence: 0.010521\n",
      "Epoch: 47, Loss: 172.24969, Residuals: -2.08525, Convergence: 0.010272\n",
      "Epoch: 48, Loss: 170.54246, Residuals: -2.05275, Convergence: 0.010011\n",
      "Epoch: 49, Loss: 168.89816, Residuals: -2.02063, Convergence: 0.009735\n",
      "Epoch: 50, Loss: 167.31772, Residuals: -1.98894, Convergence: 0.009446\n",
      "Epoch: 51, Loss: 165.80216, Residuals: -1.95772, Convergence: 0.009141\n",
      "Epoch: 52, Loss: 164.35242, Residuals: -1.92702, Convergence: 0.008821\n",
      "Epoch: 53, Loss: 162.96918, Residuals: -1.89688, Convergence: 0.008488\n",
      "Epoch: 54, Loss: 161.65273, Residuals: -1.86736, Convergence: 0.008144\n",
      "Epoch: 55, Loss: 160.40286, Residuals: -1.83850, Convergence: 0.007792\n",
      "Epoch: 56, Loss: 159.21884, Residuals: -1.81034, Convergence: 0.007436\n",
      "Epoch: 57, Loss: 158.09946, Residuals: -1.78291, Convergence: 0.007080\n",
      "Epoch: 58, Loss: 157.04312, Residuals: -1.75626, Convergence: 0.006726\n",
      "Epoch: 59, Loss: 156.04784, Residuals: -1.73040, Convergence: 0.006378\n",
      "Epoch: 60, Loss: 155.11133, Residuals: -1.70535, Convergence: 0.006038\n",
      "Epoch: 61, Loss: 154.23104, Residuals: -1.68114, Convergence: 0.005708\n",
      "Epoch: 62, Loss: 153.40418, Residuals: -1.65777, Convergence: 0.005390\n",
      "Epoch: 63, Loss: 152.62776, Residuals: -1.63524, Convergence: 0.005087\n",
      "Epoch: 64, Loss: 151.89865, Residuals: -1.61354, Convergence: 0.004800\n",
      "Epoch: 65, Loss: 151.21362, Residuals: -1.59267, Convergence: 0.004530\n",
      "Epoch: 66, Loss: 150.56939, Residuals: -1.57260, Convergence: 0.004279\n",
      "Epoch: 67, Loss: 149.96271, Residuals: -1.55331, Convergence: 0.004046\n",
      "Epoch: 68, Loss: 149.39039, Residuals: -1.53478, Convergence: 0.003831\n",
      "Epoch: 69, Loss: 148.84935, Residuals: -1.51697, Convergence: 0.003635\n",
      "Epoch: 70, Loss: 148.33670, Residuals: -1.49985, Convergence: 0.003456\n",
      "Epoch: 71, Loss: 147.84974, Residuals: -1.48337, Convergence: 0.003294\n",
      "Epoch: 72, Loss: 147.38606, Residuals: -1.46752, Convergence: 0.003146\n",
      "Epoch: 73, Loss: 146.94354, Residuals: -1.45225, Convergence: 0.003012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 74, Loss: 146.52042, Residuals: -1.43753, Convergence: 0.002888\n",
      "Epoch: 75, Loss: 146.11527, Residuals: -1.42334, Convergence: 0.002773\n",
      "Epoch: 76, Loss: 145.72699, Residuals: -1.40965, Convergence: 0.002664\n",
      "Epoch: 77, Loss: 145.35477, Residuals: -1.39644, Convergence: 0.002561\n",
      "Epoch: 78, Loss: 144.99801, Residuals: -1.38371, Convergence: 0.002460\n",
      "Epoch: 79, Loss: 144.65631, Residuals: -1.37144, Convergence: 0.002362\n",
      "Epoch: 80, Loss: 144.32934, Residuals: -1.35964, Convergence: 0.002265\n",
      "Epoch: 81, Loss: 144.01688, Residuals: -1.34830, Convergence: 0.002170\n",
      "Epoch: 82, Loss: 143.71873, Residuals: -1.33741, Convergence: 0.002075\n",
      "Epoch: 83, Loss: 143.43462, Residuals: -1.32697, Convergence: 0.001981\n",
      "Epoch: 84, Loss: 143.16427, Residuals: -1.31699, Convergence: 0.001888\n",
      "Epoch: 85, Loss: 142.90726, Residuals: -1.30747, Convergence: 0.001798\n",
      "Epoch: 86, Loss: 142.66308, Residuals: -1.29839, Convergence: 0.001712\n",
      "Epoch: 87, Loss: 142.43102, Residuals: -1.28974, Convergence: 0.001629\n",
      "Epoch: 88, Loss: 142.21024, Residuals: -1.28153, Convergence: 0.001553\n",
      "Epoch: 89, Loss: 141.99971, Residuals: -1.27372, Convergence: 0.001483\n",
      "Epoch: 90, Loss: 141.79826, Residuals: -1.26630, Convergence: 0.001421\n",
      "Epoch: 91, Loss: 141.60467, Residuals: -1.25924, Convergence: 0.001367\n",
      "Epoch: 92, Loss: 141.41770, Residuals: -1.25250, Convergence: 0.001322\n",
      "Epoch: 93, Loss: 141.23620, Residuals: -1.24606, Convergence: 0.001285\n",
      "Epoch: 94, Loss: 141.05919, Residuals: -1.23988, Convergence: 0.001255\n",
      "Epoch: 95, Loss: 140.88591, Residuals: -1.23394, Convergence: 0.001230\n",
      "Epoch: 96, Loss: 140.71578, Residuals: -1.22821, Convergence: 0.001209\n",
      "Epoch: 97, Loss: 140.54845, Residuals: -1.22266, Convergence: 0.001190\n",
      "Epoch: 98, Loss: 140.38376, Residuals: -1.21729, Convergence: 0.001173\n",
      "Epoch: 99, Loss: 140.22165, Residuals: -1.21208, Convergence: 0.001156\n",
      "Epoch: 100, Loss: 140.06215, Residuals: -1.20702, Convergence: 0.001139\n",
      "Epoch: 101, Loss: 139.90539, Residuals: -1.20211, Convergence: 0.001121\n",
      "Epoch: 102, Loss: 139.75146, Residuals: -1.19734, Convergence: 0.001101\n",
      "Epoch: 103, Loss: 139.60050, Residuals: -1.19271, Convergence: 0.001081\n",
      "Epoch: 104, Loss: 139.45263, Residuals: -1.18821, Convergence: 0.001060\n",
      "Epoch: 105, Loss: 139.30795, Residuals: -1.18384, Convergence: 0.001039\n",
      "Epoch: 106, Loss: 139.16654, Residuals: -1.17960, Convergence: 0.001016\n",
      "Epoch: 107, Loss: 139.02845, Residuals: -1.17548, Convergence: 0.000993\n",
      "Evidence -179.536\n",
      "\n",
      "Epoch: 107, Evidence: -179.53616, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.24e-01\n",
      "Epoch: 107, Loss: 1362.73241, Residuals: -1.17548, Convergence:   inf\n",
      "Epoch: 108, Loss: 1306.84197, Residuals: -1.20182, Convergence: 0.042768\n",
      "Epoch: 109, Loss: 1263.15250, Residuals: -1.22402, Convergence: 0.034588\n",
      "Epoch: 110, Loss: 1229.23126, Residuals: -1.24178, Convergence: 0.027595\n",
      "Epoch: 111, Loss: 1202.31554, Residuals: -1.25572, Convergence: 0.022387\n",
      "Epoch: 112, Loss: 1180.32008, Residuals: -1.26694, Convergence: 0.018635\n",
      "Epoch: 113, Loss: 1161.98769, Residuals: -1.27607, Convergence: 0.015777\n",
      "Epoch: 114, Loss: 1146.51961, Residuals: -1.28344, Convergence: 0.013491\n",
      "Epoch: 115, Loss: 1133.34482, Residuals: -1.28926, Convergence: 0.011625\n",
      "Epoch: 116, Loss: 1122.02413, Residuals: -1.29369, Convergence: 0.010090\n",
      "Epoch: 117, Loss: 1112.20713, Residuals: -1.29689, Convergence: 0.008827\n",
      "Epoch: 118, Loss: 1103.60774, Residuals: -1.29899, Convergence: 0.007792\n",
      "Epoch: 119, Loss: 1095.99021, Residuals: -1.30010, Convergence: 0.006950\n",
      "Epoch: 120, Loss: 1089.15696, Residuals: -1.30032, Convergence: 0.006274\n",
      "Epoch: 121, Loss: 1082.94307, Residuals: -1.29973, Convergence: 0.005738\n",
      "Epoch: 122, Loss: 1077.20899, Residuals: -1.29840, Convergence: 0.005323\n",
      "Epoch: 123, Loss: 1071.83947, Residuals: -1.29639, Convergence: 0.005010\n",
      "Epoch: 124, Loss: 1066.73977, Residuals: -1.29373, Convergence: 0.004781\n",
      "Epoch: 125, Loss: 1061.83440, Residuals: -1.29048, Convergence: 0.004620\n",
      "Epoch: 126, Loss: 1057.06266, Residuals: -1.28664, Convergence: 0.004514\n",
      "Epoch: 127, Loss: 1052.37307, Residuals: -1.28225, Convergence: 0.004456\n",
      "Epoch: 128, Loss: 1047.71591, Residuals: -1.27732, Convergence: 0.004445\n",
      "Epoch: 129, Loss: 1043.03847, Residuals: -1.27183, Convergence: 0.004484\n",
      "Epoch: 130, Loss: 1038.28732, Residuals: -1.26581, Convergence: 0.004576\n",
      "Epoch: 131, Loss: 1033.42064, Residuals: -1.25928, Convergence: 0.004709\n",
      "Epoch: 132, Loss: 1028.43237, Residuals: -1.25229, Convergence: 0.004850\n",
      "Epoch: 133, Loss: 1023.37155, Residuals: -1.24495, Convergence: 0.004945\n",
      "Epoch: 134, Loss: 1018.33557, Residuals: -1.23734, Convergence: 0.004945\n",
      "Epoch: 135, Loss: 1013.43550, Residuals: -1.22956, Convergence: 0.004835\n",
      "Epoch: 136, Loss: 1008.75825, Residuals: -1.22169, Convergence: 0.004637\n",
      "Epoch: 137, Loss: 1004.34902, Residuals: -1.21380, Convergence: 0.004390\n",
      "Epoch: 138, Loss: 1000.21827, Residuals: -1.20595, Convergence: 0.004130\n",
      "Epoch: 139, Loss: 996.35416, Residuals: -1.19818, Convergence: 0.003878\n",
      "Epoch: 140, Loss: 992.73728, Residuals: -1.19056, Convergence: 0.003643\n",
      "Epoch: 141, Loss: 989.34598, Residuals: -1.18311, Convergence: 0.003428\n",
      "Epoch: 142, Loss: 986.16016, Residuals: -1.17588, Convergence: 0.003231\n",
      "Epoch: 143, Loss: 983.16226, Residuals: -1.16888, Convergence: 0.003049\n",
      "Epoch: 144, Loss: 980.33751, Residuals: -1.16214, Convergence: 0.002881\n",
      "Epoch: 145, Loss: 977.67309, Residuals: -1.15567, Convergence: 0.002725\n",
      "Epoch: 146, Loss: 975.15732, Residuals: -1.14948, Convergence: 0.002580\n",
      "Epoch: 147, Loss: 972.78021, Residuals: -1.14357, Convergence: 0.002444\n",
      "Epoch: 148, Loss: 970.53192, Residuals: -1.13794, Convergence: 0.002317\n",
      "Epoch: 149, Loss: 968.40350, Residuals: -1.13259, Convergence: 0.002198\n",
      "Epoch: 150, Loss: 966.38629, Residuals: -1.12752, Convergence: 0.002087\n",
      "Epoch: 151, Loss: 964.47218, Residuals: -1.12270, Convergence: 0.001985\n",
      "Epoch: 152, Loss: 962.65331, Residuals: -1.11814, Convergence: 0.001889\n",
      "Epoch: 153, Loss: 960.92239, Residuals: -1.11381, Convergence: 0.001801\n",
      "Epoch: 154, Loss: 959.27225, Residuals: -1.10972, Convergence: 0.001720\n",
      "Epoch: 155, Loss: 957.69687, Residuals: -1.10583, Convergence: 0.001645\n",
      "Epoch: 156, Loss: 956.18985, Residuals: -1.10215, Convergence: 0.001576\n",
      "Epoch: 157, Loss: 954.74539, Residuals: -1.09865, Convergence: 0.001513\n",
      "Epoch: 158, Loss: 953.35881, Residuals: -1.09533, Convergence: 0.001454\n",
      "Epoch: 159, Loss: 952.02445, Residuals: -1.09217, Convergence: 0.001402\n",
      "Epoch: 160, Loss: 950.73824, Residuals: -1.08915, Convergence: 0.001353\n",
      "Epoch: 161, Loss: 949.49552, Residuals: -1.08628, Convergence: 0.001309\n",
      "Epoch: 162, Loss: 948.29232, Residuals: -1.08354, Convergence: 0.001269\n",
      "Epoch: 163, Loss: 947.12472, Residuals: -1.08091, Convergence: 0.001233\n",
      "Epoch: 164, Loss: 945.98966, Residuals: -1.07839, Convergence: 0.001200\n",
      "Epoch: 165, Loss: 944.88410, Residuals: -1.07597, Convergence: 0.001170\n",
      "Epoch: 166, Loss: 943.80555, Residuals: -1.07364, Convergence: 0.001143\n",
      "Epoch: 167, Loss: 942.75211, Residuals: -1.07140, Convergence: 0.001117\n",
      "Epoch: 168, Loss: 941.72236, Residuals: -1.06924, Convergence: 0.001093\n",
      "Epoch: 169, Loss: 940.71545, Residuals: -1.06716, Convergence: 0.001070\n",
      "Epoch: 170, Loss: 939.73010, Residuals: -1.06514, Convergence: 0.001049\n",
      "Epoch: 171, Loss: 938.76524, Residuals: -1.06320, Convergence: 0.001028\n",
      "Epoch: 172, Loss: 937.82000, Residuals: -1.06131, Convergence: 0.001008\n",
      "Epoch: 173, Loss: 936.89223, Residuals: -1.05948, Convergence: 0.000990\n",
      "Evidence 11384.194\n",
      "\n",
      "Epoch: 173, Evidence: 11384.19434, Convergence: 1.015771\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.71e-01\n",
      "Epoch: 173, Loss: 2347.12020, Residuals: -1.05948, Convergence:   inf\n",
      "Epoch: 174, Loss: 2310.04105, Residuals: -1.06843, Convergence: 0.016051\n",
      "Epoch: 175, Loss: 2285.15582, Residuals: -1.06739, Convergence: 0.010890\n",
      "Epoch: 176, Loss: 2264.15641, Residuals: -1.06590, Convergence: 0.009275\n",
      "Epoch: 177, Loss: 2246.32563, Residuals: -1.06418, Convergence: 0.007938\n",
      "Epoch: 178, Loss: 2231.08350, Residuals: -1.06231, Convergence: 0.006832\n",
      "Epoch: 179, Loss: 2217.95059, Residuals: -1.06032, Convergence: 0.005921\n",
      "Epoch: 180, Loss: 2206.53032, Residuals: -1.05824, Convergence: 0.005176\n",
      "Epoch: 181, Loss: 2196.48742, Residuals: -1.05609, Convergence: 0.004572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 182, Loss: 2187.53978, Residuals: -1.05386, Convergence: 0.004090\n",
      "Epoch: 183, Loss: 2179.45670, Residuals: -1.05155, Convergence: 0.003709\n",
      "Epoch: 184, Loss: 2172.06212, Residuals: -1.04913, Convergence: 0.003404\n",
      "Epoch: 185, Loss: 2165.24179, Residuals: -1.04662, Convergence: 0.003150\n",
      "Epoch: 186, Loss: 2158.93256, Residuals: -1.04402, Convergence: 0.002922\n",
      "Epoch: 187, Loss: 2153.10240, Residuals: -1.04138, Convergence: 0.002708\n",
      "Epoch: 188, Loss: 2147.72841, Residuals: -1.03875, Convergence: 0.002502\n",
      "Epoch: 189, Loss: 2142.78531, Residuals: -1.03615, Convergence: 0.002307\n",
      "Epoch: 190, Loss: 2138.24254, Residuals: -1.03362, Convergence: 0.002125\n",
      "Epoch: 191, Loss: 2134.06581, Residuals: -1.03117, Convergence: 0.001957\n",
      "Epoch: 192, Loss: 2130.21919, Residuals: -1.02881, Convergence: 0.001806\n",
      "Epoch: 193, Loss: 2126.66912, Residuals: -1.02655, Convergence: 0.001669\n",
      "Epoch: 194, Loss: 2123.38320, Residuals: -1.02439, Convergence: 0.001547\n",
      "Epoch: 195, Loss: 2120.33230, Residuals: -1.02233, Convergence: 0.001439\n",
      "Epoch: 196, Loss: 2117.49122, Residuals: -1.02035, Convergence: 0.001342\n",
      "Epoch: 197, Loss: 2114.83689, Residuals: -1.01847, Convergence: 0.001255\n",
      "Epoch: 198, Loss: 2112.34896, Residuals: -1.01667, Convergence: 0.001178\n",
      "Epoch: 199, Loss: 2110.00967, Residuals: -1.01495, Convergence: 0.001109\n",
      "Epoch: 200, Loss: 2107.80389, Residuals: -1.01330, Convergence: 0.001046\n",
      "Epoch: 201, Loss: 2105.71879, Residuals: -1.01172, Convergence: 0.000990\n",
      "Evidence 14442.906\n",
      "\n",
      "Epoch: 201, Evidence: 14442.90625, Convergence: 0.211780\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.34e-01\n",
      "Epoch: 201, Loss: 2468.00697, Residuals: -1.01172, Convergence:   inf\n",
      "Epoch: 202, Loss: 2454.21704, Residuals: -1.00845, Convergence: 0.005619\n",
      "Epoch: 203, Loss: 2443.05414, Residuals: -1.00481, Convergence: 0.004569\n",
      "Epoch: 204, Loss: 2433.46527, Residuals: -1.00141, Convergence: 0.003940\n",
      "Epoch: 205, Loss: 2425.16917, Residuals: -0.99830, Convergence: 0.003421\n",
      "Epoch: 206, Loss: 2417.94643, Residuals: -0.99549, Convergence: 0.002987\n",
      "Epoch: 207, Loss: 2411.61929, Residuals: -0.99294, Convergence: 0.002624\n",
      "Epoch: 208, Loss: 2406.04309, Residuals: -0.99064, Convergence: 0.002318\n",
      "Epoch: 209, Loss: 2401.09987, Residuals: -0.98855, Convergence: 0.002059\n",
      "Epoch: 210, Loss: 2396.69009, Residuals: -0.98665, Convergence: 0.001840\n",
      "Epoch: 211, Loss: 2392.73457, Residuals: -0.98492, Convergence: 0.001653\n",
      "Epoch: 212, Loss: 2389.16538, Residuals: -0.98334, Convergence: 0.001494\n",
      "Epoch: 213, Loss: 2385.92739, Residuals: -0.98188, Convergence: 0.001357\n",
      "Epoch: 214, Loss: 2382.97495, Residuals: -0.98054, Convergence: 0.001239\n",
      "Epoch: 215, Loss: 2380.26954, Residuals: -0.97931, Convergence: 0.001137\n",
      "Epoch: 216, Loss: 2377.77972, Residuals: -0.97817, Convergence: 0.001047\n",
      "Epoch: 217, Loss: 2375.47893, Residuals: -0.97712, Convergence: 0.000969\n",
      "Evidence 14798.881\n",
      "\n",
      "Epoch: 217, Evidence: 14798.88086, Convergence: 0.024054\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.32e-01\n",
      "Epoch: 217, Loss: 2472.26974, Residuals: -0.97712, Convergence:   inf\n",
      "Epoch: 218, Loss: 2465.48484, Residuals: -0.97368, Convergence: 0.002752\n",
      "Epoch: 219, Loss: 2459.86830, Residuals: -0.97068, Convergence: 0.002283\n",
      "Epoch: 220, Loss: 2455.08436, Residuals: -0.96816, Convergence: 0.001949\n",
      "Epoch: 221, Loss: 2450.95059, Residuals: -0.96600, Convergence: 0.001687\n",
      "Epoch: 222, Loss: 2447.33607, Residuals: -0.96415, Convergence: 0.001477\n",
      "Epoch: 223, Loss: 2444.14084, Residuals: -0.96254, Convergence: 0.001307\n",
      "Epoch: 224, Loss: 2441.28823, Residuals: -0.96113, Convergence: 0.001168\n",
      "Epoch: 225, Loss: 2438.71785, Residuals: -0.95990, Convergence: 0.001054\n",
      "Epoch: 226, Loss: 2436.38318, Residuals: -0.95882, Convergence: 0.000958\n",
      "Evidence 14885.631\n",
      "\n",
      "Epoch: 226, Evidence: 14885.63086, Convergence: 0.005828\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.60e-01\n",
      "Epoch: 226, Loss: 2473.54069, Residuals: -0.95882, Convergence:   inf\n",
      "Epoch: 227, Loss: 2469.54415, Residuals: -0.95614, Convergence: 0.001618\n",
      "Epoch: 228, Loss: 2466.20049, Residuals: -0.95398, Convergence: 0.001356\n",
      "Epoch: 229, Loss: 2463.32059, Residuals: -0.95221, Convergence: 0.001169\n",
      "Epoch: 230, Loss: 2460.79655, Residuals: -0.95073, Convergence: 0.001026\n",
      "Epoch: 231, Loss: 2458.55390, Residuals: -0.94949, Convergence: 0.000912\n",
      "Evidence 14917.499\n",
      "\n",
      "Epoch: 231, Evidence: 14917.49902, Convergence: 0.002136\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.10e-01\n",
      "Epoch: 231, Loss: 2474.42469, Residuals: -0.94949, Convergence:   inf\n",
      "Epoch: 232, Loss: 2471.58459, Residuals: -0.94742, Convergence: 0.001149\n",
      "Epoch: 233, Loss: 2469.18780, Residuals: -0.94577, Convergence: 0.000971\n",
      "Evidence 14930.084\n",
      "\n",
      "Epoch: 233, Evidence: 14930.08398, Convergence: 0.000843\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.75e-01\n",
      "Epoch: 233, Loss: 2475.13594, Residuals: -0.94577, Convergence:   inf\n",
      "Epoch: 234, Loss: 2470.72443, Residuals: -0.94310, Convergence: 0.001786\n",
      "Epoch: 235, Loss: 2467.30307, Residuals: -0.94095, Convergence: 0.001387\n",
      "Epoch: 236, Loss: 2464.52270, Residuals: -0.93935, Convergence: 0.001128\n",
      "Epoch: 237, Loss: 2462.16147, Residuals: -0.93834, Convergence: 0.000959\n",
      "Evidence 14947.587\n",
      "\n",
      "Epoch: 237, Evidence: 14947.58691, Convergence: 0.002013\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.45e-01\n",
      "Epoch: 237, Loss: 2475.32835, Residuals: -0.93834, Convergence:   inf\n",
      "Epoch: 238, Loss: 2472.46709, Residuals: -0.93569, Convergence: 0.001157\n",
      "Epoch: 239, Loss: 2470.18169, Residuals: -0.93407, Convergence: 0.000925\n",
      "Evidence 14958.186\n",
      "\n",
      "Epoch: 239, Evidence: 14958.18555, Convergence: 0.000709\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.23e-01\n",
      "Epoch: 239, Loss: 2475.52316, Residuals: -0.93407, Convergence:   inf\n",
      "Epoch: 240, Loss: 2471.43131, Residuals: -0.92986, Convergence: 0.001656\n",
      "Epoch: 241, Loss: 2468.43577, Residuals: -0.93034, Convergence: 0.001214\n",
      "Epoch: 242, Loss: 2465.87831, Residuals: -0.93133, Convergence: 0.001037\n",
      "Epoch: 243, Loss: 2463.61386, Residuals: -0.93355, Convergence: 0.000919\n",
      "Evidence 14973.305\n",
      "\n",
      "Epoch: 243, Evidence: 14973.30469, Convergence: 0.001718\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.09e-01\n",
      "Epoch: 243, Loss: 2474.93131, Residuals: -0.93355, Convergence:   inf\n",
      "Epoch: 244, Loss: 2473.56644, Residuals: -0.93094, Convergence: 0.000552\n",
      "Evidence 14979.330\n",
      "\n",
      "Epoch: 244, Evidence: 14979.33008, Convergence: 0.000402\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.91e-02\n",
      "Epoch: 244, Loss: 2475.83657, Residuals: -0.93094, Convergence:   inf\n",
      "Epoch: 245, Loss: 2520.94516, Residuals: -0.97337, Convergence: -0.017894\n",
      "Epoch: 245, Loss: 2473.87503, Residuals: -0.92719, Convergence: 0.000793\n",
      "Evidence 14983.307\n",
      "\n",
      "Epoch: 245, Evidence: 14983.30664, Convergence: 0.000668\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.18e-02\n",
      "Epoch: 245, Loss: 2475.34529, Residuals: -0.92719, Convergence:   inf\n",
      "Epoch: 246, Loss: 2480.02536, Residuals: -0.92886, Convergence: -0.001887\n",
      "Epoch: 246, Loss: 2475.33134, Residuals: -0.92556, Convergence: 0.000006\n",
      "Evidence 14984.804\n",
      "\n",
      "Epoch: 246, Evidence: 14984.80371, Convergence: 0.000767\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 382.12448, Residuals: -4.52192, Convergence:   inf\n",
      "Epoch: 1, Loss: 356.42850, Residuals: -4.40205, Convergence: 0.072093\n",
      "Epoch: 2, Loss: 335.30564, Residuals: -4.23696, Convergence: 0.062996\n",
      "Epoch: 3, Loss: 319.18668, Residuals: -4.07141, Convergence: 0.050500\n",
      "Epoch: 4, Loss: 306.90361, Residuals: -3.92553, Convergence: 0.040023\n",
      "Epoch: 5, Loss: 297.16369, Residuals: -3.79636, Convergence: 0.032776\n",
      "Epoch: 6, Loss: 289.25891, Residuals: -3.68377, Convergence: 0.027328\n",
      "Epoch: 7, Loss: 282.69779, Residuals: -3.58735, Convergence: 0.023209\n",
      "Epoch: 8, Loss: 277.11665, Residuals: -3.50504, Convergence: 0.020140\n",
      "Epoch: 9, Loss: 272.25930, Residuals: -3.43450, Convergence: 0.017841\n",
      "Epoch: 10, Loss: 267.94387, Residuals: -3.37364, Convergence: 0.016106\n",
      "Epoch: 11, Loss: 264.03897, Residuals: -3.32064, Convergence: 0.014789\n",
      "Epoch: 12, Loss: 260.44911, Residuals: -3.27402, Convergence: 0.013783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Loss: 257.10529, Residuals: -3.23246, Convergence: 0.013006\n",
      "Epoch: 14, Loss: 253.95839, Residuals: -3.19479, Convergence: 0.012391\n",
      "Epoch: 15, Loss: 250.97530, Residuals: -3.16005, Convergence: 0.011886\n",
      "Epoch: 16, Loss: 248.13703, Residuals: -3.12751, Convergence: 0.011438\n",
      "Epoch: 17, Loss: 245.43202, Residuals: -3.09673, Convergence: 0.011021\n",
      "Epoch: 18, Loss: 242.84311, Residuals: -3.06733, Convergence: 0.010661\n",
      "Epoch: 19, Loss: 240.34164, Residuals: -3.03883, Convergence: 0.010408\n",
      "Epoch: 20, Loss: 237.89272, Residuals: -3.01068, Convergence: 0.010294\n",
      "Epoch: 21, Loss: 235.46340, Residuals: -2.98235, Convergence: 0.010317\n",
      "Epoch: 22, Loss: 233.02876, Residuals: -2.95347, Convergence: 0.010448\n",
      "Epoch: 23, Loss: 230.56870, Residuals: -2.92378, Convergence: 0.010670\n",
      "Epoch: 24, Loss: 228.05255, Residuals: -2.89298, Convergence: 0.011033\n",
      "Epoch: 25, Loss: 225.42863, Residuals: -2.86049, Convergence: 0.011640\n",
      "Epoch: 26, Loss: 222.65269, Residuals: -2.82579, Convergence: 0.012468\n",
      "Epoch: 27, Loss: 219.77537, Residuals: -2.78933, Convergence: 0.013092\n",
      "Epoch: 28, Loss: 216.92873, Residuals: -2.75257, Convergence: 0.013122\n",
      "Epoch: 29, Loss: 214.17542, Residuals: -2.71632, Convergence: 0.012855\n",
      "Epoch: 30, Loss: 211.51162, Residuals: -2.68064, Convergence: 0.012594\n",
      "Epoch: 31, Loss: 208.92015, Residuals: -2.64541, Convergence: 0.012404\n",
      "Epoch: 32, Loss: 206.38715, Residuals: -2.61049, Convergence: 0.012273\n",
      "Epoch: 33, Loss: 203.90416, Residuals: -2.57577, Convergence: 0.012177\n",
      "Epoch: 34, Loss: 201.46705, Residuals: -2.54119, Convergence: 0.012097\n",
      "Epoch: 35, Loss: 199.07466, Residuals: -2.50671, Convergence: 0.012018\n",
      "Epoch: 36, Loss: 196.72759, Residuals: -2.47230, Convergence: 0.011931\n",
      "Epoch: 37, Loss: 194.42731, Residuals: -2.43798, Convergence: 0.011831\n",
      "Epoch: 38, Loss: 192.17560, Residuals: -2.40375, Convergence: 0.011717\n",
      "Epoch: 39, Loss: 189.97426, Residuals: -2.36962, Convergence: 0.011588\n",
      "Epoch: 40, Loss: 187.82500, Residuals: -2.33561, Convergence: 0.011443\n",
      "Epoch: 41, Loss: 185.72948, Residuals: -2.30174, Convergence: 0.011283\n",
      "Epoch: 42, Loss: 183.68949, Residuals: -2.26805, Convergence: 0.011106\n",
      "Epoch: 43, Loss: 181.70702, Residuals: -2.23458, Convergence: 0.010910\n",
      "Epoch: 44, Loss: 179.78434, Residuals: -2.20137, Convergence: 0.010694\n",
      "Epoch: 45, Loss: 177.92381, Residuals: -2.16848, Convergence: 0.010457\n",
      "Epoch: 46, Loss: 176.12765, Residuals: -2.13596, Convergence: 0.010198\n",
      "Epoch: 47, Loss: 174.39752, Residuals: -2.10388, Convergence: 0.009921\n",
      "Epoch: 48, Loss: 172.73441, Residuals: -2.07229, Convergence: 0.009628\n",
      "Epoch: 49, Loss: 171.13837, Residuals: -2.04123, Convergence: 0.009326\n",
      "Epoch: 50, Loss: 169.60874, Residuals: -2.01072, Convergence: 0.009019\n",
      "Epoch: 51, Loss: 168.14414, Residuals: -1.98080, Convergence: 0.008710\n",
      "Epoch: 52, Loss: 166.74284, Residuals: -1.95148, Convergence: 0.008404\n",
      "Epoch: 53, Loss: 165.40280, Residuals: -1.92276, Convergence: 0.008102\n",
      "Epoch: 54, Loss: 164.12186, Residuals: -1.89466, Convergence: 0.007805\n",
      "Epoch: 55, Loss: 162.89784, Residuals: -1.86718, Convergence: 0.007514\n",
      "Epoch: 56, Loss: 161.72846, Residuals: -1.84034, Convergence: 0.007230\n",
      "Epoch: 57, Loss: 160.61145, Residuals: -1.81415, Convergence: 0.006955\n",
      "Epoch: 58, Loss: 159.54440, Residuals: -1.78861, Convergence: 0.006688\n",
      "Epoch: 59, Loss: 158.52489, Residuals: -1.76372, Convergence: 0.006431\n",
      "Epoch: 60, Loss: 157.55050, Residuals: -1.73949, Convergence: 0.006185\n",
      "Epoch: 61, Loss: 156.61882, Residuals: -1.71592, Convergence: 0.005949\n",
      "Epoch: 62, Loss: 155.72755, Residuals: -1.69299, Convergence: 0.005723\n",
      "Epoch: 63, Loss: 154.87460, Residuals: -1.67070, Convergence: 0.005507\n",
      "Epoch: 64, Loss: 154.05809, Residuals: -1.64903, Convergence: 0.005300\n",
      "Epoch: 65, Loss: 153.27636, Residuals: -1.62798, Convergence: 0.005100\n",
      "Epoch: 66, Loss: 152.52798, Residuals: -1.60755, Convergence: 0.004907\n",
      "Epoch: 67, Loss: 151.81173, Residuals: -1.58773, Convergence: 0.004718\n",
      "Epoch: 68, Loss: 151.12656, Residuals: -1.56851, Convergence: 0.004534\n",
      "Epoch: 69, Loss: 150.47150, Residuals: -1.54990, Convergence: 0.004353\n",
      "Epoch: 70, Loss: 149.84568, Residuals: -1.53188, Convergence: 0.004176\n",
      "Epoch: 71, Loss: 149.24824, Residuals: -1.51446, Convergence: 0.004003\n",
      "Epoch: 72, Loss: 148.67835, Residuals: -1.49763, Convergence: 0.003833\n",
      "Epoch: 73, Loss: 148.13519, Residuals: -1.48139, Convergence: 0.003667\n",
      "Epoch: 74, Loss: 147.61789, Residuals: -1.46573, Convergence: 0.003504\n",
      "Epoch: 75, Loss: 147.12556, Residuals: -1.45065, Convergence: 0.003346\n",
      "Epoch: 76, Loss: 146.65731, Residuals: -1.43612, Convergence: 0.003193\n",
      "Epoch: 77, Loss: 146.21222, Residuals: -1.42215, Convergence: 0.003044\n",
      "Epoch: 78, Loss: 145.78934, Residuals: -1.40872, Convergence: 0.002901\n",
      "Epoch: 79, Loss: 145.38774, Residuals: -1.39582, Convergence: 0.002762\n",
      "Epoch: 80, Loss: 145.00649, Residuals: -1.38343, Convergence: 0.002629\n",
      "Epoch: 81, Loss: 144.64469, Residuals: -1.37154, Convergence: 0.002501\n",
      "Epoch: 82, Loss: 144.30144, Residuals: -1.36013, Convergence: 0.002379\n",
      "Epoch: 83, Loss: 143.97587, Residuals: -1.34919, Convergence: 0.002261\n",
      "Epoch: 84, Loss: 143.66718, Residuals: -1.33871, Convergence: 0.002149\n",
      "Epoch: 85, Loss: 143.37458, Residuals: -1.32866, Convergence: 0.002041\n",
      "Epoch: 86, Loss: 143.09733, Residuals: -1.31903, Convergence: 0.001937\n",
      "Epoch: 87, Loss: 142.83475, Residuals: -1.30981, Convergence: 0.001838\n",
      "Epoch: 88, Loss: 142.58618, Residuals: -1.30098, Convergence: 0.001743\n",
      "Epoch: 89, Loss: 142.35104, Residuals: -1.29252, Convergence: 0.001652\n",
      "Epoch: 90, Loss: 142.12875, Residuals: -1.28443, Convergence: 0.001564\n",
      "Epoch: 91, Loss: 141.91882, Residuals: -1.27669, Convergence: 0.001479\n",
      "Epoch: 92, Loss: 141.72076, Residuals: -1.26929, Convergence: 0.001398\n",
      "Epoch: 93, Loss: 141.53414, Residuals: -1.26222, Convergence: 0.001319\n",
      "Epoch: 94, Loss: 141.35853, Residuals: -1.25546, Convergence: 0.001242\n",
      "Epoch: 95, Loss: 141.19355, Residuals: -1.24901, Convergence: 0.001168\n",
      "Epoch: 96, Loss: 141.03882, Residuals: -1.24285, Convergence: 0.001097\n",
      "Epoch: 97, Loss: 140.89394, Residuals: -1.23699, Convergence: 0.001028\n",
      "Epoch: 98, Loss: 140.75853, Residuals: -1.23142, Convergence: 0.000962\n",
      "Evidence -181.735\n",
      "\n",
      "Epoch: 98, Evidence: -181.73540, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.24e-01\n",
      "Epoch: 98, Loss: 1368.92453, Residuals: -1.23142, Convergence:   inf\n",
      "Epoch: 99, Loss: 1309.97036, Residuals: -1.25903, Convergence: 0.045004\n",
      "Epoch: 100, Loss: 1264.80464, Residuals: -1.28057, Convergence: 0.035710\n",
      "Epoch: 101, Loss: 1230.42269, Residuals: -1.29580, Convergence: 0.027943\n",
      "Epoch: 102, Loss: 1203.54521, Residuals: -1.30609, Convergence: 0.022332\n",
      "Epoch: 103, Loss: 1181.73406, Residuals: -1.31324, Convergence: 0.018457\n",
      "Epoch: 104, Loss: 1163.56352, Residuals: -1.31832, Convergence: 0.015616\n",
      "Epoch: 105, Loss: 1148.18222, Residuals: -1.32184, Convergence: 0.013396\n",
      "Epoch: 106, Loss: 1135.01607, Residuals: -1.32408, Convergence: 0.011600\n",
      "Epoch: 107, Loss: 1123.64209, Residuals: -1.32520, Convergence: 0.010122\n",
      "Epoch: 108, Loss: 1113.72977, Residuals: -1.32535, Convergence: 0.008900\n",
      "Epoch: 109, Loss: 1105.01193, Residuals: -1.32464, Convergence: 0.007889\n",
      "Epoch: 110, Loss: 1097.26860, Residuals: -1.32318, Convergence: 0.007057\n",
      "Epoch: 111, Loss: 1090.31325, Residuals: -1.32103, Convergence: 0.006379\n",
      "Epoch: 112, Loss: 1083.98545, Residuals: -1.31826, Convergence: 0.005838\n",
      "Epoch: 113, Loss: 1078.14166, Residuals: -1.31492, Convergence: 0.005420\n",
      "Epoch: 114, Loss: 1072.65005, Residuals: -1.31102, Convergence: 0.005120\n",
      "Epoch: 115, Loss: 1067.38595, Residuals: -1.30657, Convergence: 0.004932\n",
      "Epoch: 116, Loss: 1062.23153, Residuals: -1.30159, Convergence: 0.004852\n",
      "Epoch: 117, Loss: 1057.08286, Residuals: -1.29608, Convergence: 0.004871\n",
      "Epoch: 118, Loss: 1051.86728, Residuals: -1.29006, Convergence: 0.004958\n",
      "Epoch: 119, Loss: 1046.56506, Residuals: -1.28358, Convergence: 0.005066\n",
      "Epoch: 120, Loss: 1041.22145, Residuals: -1.27672, Convergence: 0.005132\n",
      "Epoch: 121, Loss: 1035.93004, Residuals: -1.26955, Convergence: 0.005108\n",
      "Epoch: 122, Loss: 1030.79228, Residuals: -1.26214, Convergence: 0.004984\n",
      "Epoch: 123, Loss: 1025.88355, Residuals: -1.25454, Convergence: 0.004785\n",
      "Epoch: 124, Loss: 1021.24146, Residuals: -1.24683, Convergence: 0.004546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 125, Loss: 1016.87349, Residuals: -1.23908, Convergence: 0.004295\n",
      "Epoch: 126, Loss: 1012.77038, Residuals: -1.23132, Convergence: 0.004051\n",
      "Epoch: 127, Loss: 1008.91496, Residuals: -1.22361, Convergence: 0.003821\n",
      "Epoch: 128, Loss: 1005.28714, Residuals: -1.21600, Convergence: 0.003609\n",
      "Epoch: 129, Loss: 1001.86675, Residuals: -1.20850, Convergence: 0.003414\n",
      "Epoch: 130, Loss: 998.63512, Residuals: -1.20115, Convergence: 0.003236\n",
      "Epoch: 131, Loss: 995.57494, Residuals: -1.19397, Convergence: 0.003074\n",
      "Epoch: 132, Loss: 992.67163, Residuals: -1.18697, Convergence: 0.002925\n",
      "Epoch: 133, Loss: 989.91181, Residuals: -1.18018, Convergence: 0.002788\n",
      "Epoch: 134, Loss: 987.28457, Residuals: -1.17360, Convergence: 0.002661\n",
      "Epoch: 135, Loss: 984.78034, Residuals: -1.16724, Convergence: 0.002543\n",
      "Epoch: 136, Loss: 982.39094, Residuals: -1.16111, Convergence: 0.002432\n",
      "Epoch: 137, Loss: 980.10920, Residuals: -1.15521, Convergence: 0.002328\n",
      "Epoch: 138, Loss: 977.92871, Residuals: -1.14954, Convergence: 0.002230\n",
      "Epoch: 139, Loss: 975.84417, Residuals: -1.14411, Convergence: 0.002136\n",
      "Epoch: 140, Loss: 973.85011, Residuals: -1.13890, Convergence: 0.002048\n",
      "Epoch: 141, Loss: 971.94143, Residuals: -1.13392, Convergence: 0.001964\n",
      "Epoch: 142, Loss: 970.11368, Residuals: -1.12917, Convergence: 0.001884\n",
      "Epoch: 143, Loss: 968.36214, Residuals: -1.12462, Convergence: 0.001809\n",
      "Epoch: 144, Loss: 966.68259, Residuals: -1.12029, Convergence: 0.001737\n",
      "Epoch: 145, Loss: 965.07088, Residuals: -1.11615, Convergence: 0.001670\n",
      "Epoch: 146, Loss: 963.52299, Residuals: -1.11220, Convergence: 0.001606\n",
      "Epoch: 147, Loss: 962.03529, Residuals: -1.10843, Convergence: 0.001546\n",
      "Epoch: 148, Loss: 960.60392, Residuals: -1.10484, Convergence: 0.001490\n",
      "Epoch: 149, Loss: 959.22547, Residuals: -1.10140, Convergence: 0.001437\n",
      "Epoch: 150, Loss: 957.89681, Residuals: -1.09812, Convergence: 0.001387\n",
      "Epoch: 151, Loss: 956.61469, Residuals: -1.09498, Convergence: 0.001340\n",
      "Epoch: 152, Loss: 955.37566, Residuals: -1.09198, Convergence: 0.001297\n",
      "Epoch: 153, Loss: 954.17705, Residuals: -1.08910, Convergence: 0.001256\n",
      "Epoch: 154, Loss: 953.01547, Residuals: -1.08634, Convergence: 0.001219\n",
      "Epoch: 155, Loss: 951.88774, Residuals: -1.08369, Convergence: 0.001185\n",
      "Epoch: 156, Loss: 950.79028, Residuals: -1.08114, Convergence: 0.001154\n",
      "Epoch: 157, Loss: 949.71989, Residuals: -1.07868, Convergence: 0.001127\n",
      "Epoch: 158, Loss: 948.67253, Residuals: -1.07629, Convergence: 0.001104\n",
      "Epoch: 159, Loss: 947.64409, Residuals: -1.07398, Convergence: 0.001085\n",
      "Epoch: 160, Loss: 946.63037, Residuals: -1.07173, Convergence: 0.001071\n",
      "Epoch: 161, Loss: 945.62744, Residuals: -1.06952, Convergence: 0.001061\n",
      "Epoch: 162, Loss: 944.63095, Residuals: -1.06735, Convergence: 0.001055\n",
      "Epoch: 163, Loss: 943.63736, Residuals: -1.06521, Convergence: 0.001053\n",
      "Epoch: 164, Loss: 942.64391, Residuals: -1.06309, Convergence: 0.001054\n",
      "Epoch: 165, Loss: 941.64932, Residuals: -1.06099, Convergence: 0.001056\n",
      "Epoch: 166, Loss: 940.65381, Residuals: -1.05889, Convergence: 0.001058\n",
      "Epoch: 167, Loss: 939.65960, Residuals: -1.05681, Convergence: 0.001058\n",
      "Epoch: 168, Loss: 938.67041, Residuals: -1.05474, Convergence: 0.001054\n",
      "Epoch: 169, Loss: 937.69159, Residuals: -1.05270, Convergence: 0.001044\n",
      "Epoch: 170, Loss: 936.72864, Residuals: -1.05069, Convergence: 0.001028\n",
      "Epoch: 171, Loss: 935.78659, Residuals: -1.04873, Convergence: 0.001007\n",
      "Epoch: 172, Loss: 934.87057, Residuals: -1.04681, Convergence: 0.000980\n",
      "Evidence 11153.398\n",
      "\n",
      "Epoch: 172, Evidence: 11153.39844, Convergence: 1.016294\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.74e-01\n",
      "Epoch: 172, Loss: 2346.06797, Residuals: -1.04681, Convergence:   inf\n",
      "Epoch: 173, Loss: 2304.02808, Residuals: -1.05502, Convergence: 0.018246\n",
      "Epoch: 174, Loss: 2276.40702, Residuals: -1.05339, Convergence: 0.012134\n",
      "Epoch: 175, Loss: 2253.59651, Residuals: -1.05103, Convergence: 0.010122\n",
      "Epoch: 176, Loss: 2234.49364, Residuals: -1.04852, Convergence: 0.008549\n",
      "Epoch: 177, Loss: 2218.36590, Residuals: -1.04595, Convergence: 0.007270\n",
      "Epoch: 178, Loss: 2204.64588, Residuals: -1.04336, Convergence: 0.006223\n",
      "Epoch: 179, Loss: 2192.87123, Residuals: -1.04078, Convergence: 0.005370\n",
      "Epoch: 180, Loss: 2182.65679, Residuals: -1.03819, Convergence: 0.004680\n",
      "Epoch: 181, Loss: 2173.68270, Residuals: -1.03559, Convergence: 0.004129\n",
      "Epoch: 182, Loss: 2165.68202, Residuals: -1.03294, Convergence: 0.003694\n",
      "Epoch: 183, Loss: 2158.44148, Residuals: -1.03022, Convergence: 0.003355\n",
      "Epoch: 184, Loss: 2151.80198, Residuals: -1.02741, Convergence: 0.003086\n",
      "Epoch: 185, Loss: 2145.66144, Residuals: -1.02450, Convergence: 0.002862\n",
      "Epoch: 186, Loss: 2139.96282, Residuals: -1.02152, Convergence: 0.002663\n",
      "Epoch: 187, Loss: 2134.67676, Residuals: -1.01851, Convergence: 0.002476\n",
      "Epoch: 188, Loss: 2129.78134, Residuals: -1.01551, Convergence: 0.002299\n",
      "Epoch: 189, Loss: 2125.25248, Residuals: -1.01255, Convergence: 0.002131\n",
      "Epoch: 190, Loss: 2121.06174, Residuals: -1.00968, Convergence: 0.001976\n",
      "Epoch: 191, Loss: 2117.17615, Residuals: -1.00691, Convergence: 0.001835\n",
      "Epoch: 192, Loss: 2113.56288, Residuals: -1.00423, Convergence: 0.001710\n",
      "Epoch: 193, Loss: 2110.18945, Residuals: -1.00167, Convergence: 0.001599\n",
      "Epoch: 194, Loss: 2107.02770, Residuals: -0.99922, Convergence: 0.001501\n",
      "Epoch: 195, Loss: 2104.05178, Residuals: -0.99687, Convergence: 0.001414\n",
      "Epoch: 196, Loss: 2101.24001, Residuals: -0.99463, Convergence: 0.001338\n",
      "Epoch: 197, Loss: 2098.57729, Residuals: -0.99249, Convergence: 0.001269\n",
      "Epoch: 198, Loss: 2096.04960, Residuals: -0.99044, Convergence: 0.001206\n",
      "Epoch: 199, Loss: 2093.64771, Residuals: -0.98849, Convergence: 0.001147\n",
      "Epoch: 200, Loss: 2091.36397, Residuals: -0.98662, Convergence: 0.001092\n",
      "Epoch: 201, Loss: 2089.19344, Residuals: -0.98484, Convergence: 0.001039\n",
      "Epoch: 202, Loss: 2087.13095, Residuals: -0.98314, Convergence: 0.000988\n",
      "Evidence 14321.014\n",
      "\n",
      "Epoch: 202, Evidence: 14321.01367, Convergence: 0.221187\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.36e-01\n",
      "Epoch: 202, Loss: 2470.70901, Residuals: -0.98314, Convergence:   inf\n",
      "Epoch: 203, Loss: 2456.92746, Residuals: -0.98078, Convergence: 0.005609\n",
      "Epoch: 204, Loss: 2445.58601, Residuals: -0.97780, Convergence: 0.004638\n",
      "Epoch: 205, Loss: 2435.68667, Residuals: -0.97481, Convergence: 0.004064\n",
      "Epoch: 206, Loss: 2426.99604, Residuals: -0.97193, Convergence: 0.003581\n",
      "Epoch: 207, Loss: 2419.34175, Residuals: -0.96921, Convergence: 0.003164\n",
      "Epoch: 208, Loss: 2412.58029, Residuals: -0.96669, Convergence: 0.002803\n",
      "Epoch: 209, Loss: 2406.59373, Residuals: -0.96435, Convergence: 0.002488\n",
      "Epoch: 210, Loss: 2401.27780, Residuals: -0.96218, Convergence: 0.002214\n",
      "Epoch: 211, Loss: 2396.54349, Residuals: -0.96019, Convergence: 0.001975\n",
      "Epoch: 212, Loss: 2392.31132, Residuals: -0.95836, Convergence: 0.001769\n",
      "Epoch: 213, Loss: 2388.51307, Residuals: -0.95667, Convergence: 0.001590\n",
      "Epoch: 214, Loss: 2385.09061, Residuals: -0.95511, Convergence: 0.001435\n",
      "Epoch: 215, Loss: 2381.99172, Residuals: -0.95368, Convergence: 0.001301\n",
      "Epoch: 216, Loss: 2379.17507, Residuals: -0.95237, Convergence: 0.001184\n",
      "Epoch: 217, Loss: 2376.60264, Residuals: -0.95116, Convergence: 0.001082\n",
      "Epoch: 218, Loss: 2374.24265, Residuals: -0.95005, Convergence: 0.000994\n",
      "Evidence 14718.285\n",
      "\n",
      "Epoch: 218, Evidence: 14718.28516, Convergence: 0.026992\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.34e-01\n",
      "Epoch: 218, Loss: 2475.19831, Residuals: -0.95005, Convergence:   inf\n",
      "Epoch: 219, Loss: 2468.30787, Residuals: -0.94712, Convergence: 0.002792\n",
      "Epoch: 220, Loss: 2462.55968, Residuals: -0.94454, Convergence: 0.002334\n",
      "Epoch: 221, Loss: 2457.66430, Residuals: -0.94234, Convergence: 0.001992\n",
      "Epoch: 222, Loss: 2453.44947, Residuals: -0.94047, Convergence: 0.001718\n",
      "Epoch: 223, Loss: 2449.78210, Residuals: -0.93888, Convergence: 0.001497\n",
      "Epoch: 224, Loss: 2446.55892, Residuals: -0.93754, Convergence: 0.001317\n",
      "Epoch: 225, Loss: 2443.69614, Residuals: -0.93640, Convergence: 0.001171\n",
      "Epoch: 226, Loss: 2441.12999, Residuals: -0.93544, Convergence: 0.001051\n",
      "Epoch: 227, Loss: 2438.80796, Residuals: -0.93462, Convergence: 0.000952\n",
      "Evidence 14808.569\n",
      "\n",
      "Epoch: 227, Evidence: 14808.56934, Convergence: 0.006097\n",
      "Updating hyper-parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 182, Updated regularization: 2.61e-01\n",
      "Epoch: 227, Loss: 2476.54779, Residuals: -0.93462, Convergence:   inf\n",
      "Epoch: 228, Loss: 2472.52868, Residuals: -0.93239, Convergence: 0.001626\n",
      "Epoch: 229, Loss: 2469.16660, Residuals: -0.93063, Convergence: 0.001362\n",
      "Epoch: 230, Loss: 2466.29038, Residuals: -0.92924, Convergence: 0.001166\n",
      "Epoch: 231, Loss: 2463.78755, Residuals: -0.92814, Convergence: 0.001016\n",
      "Epoch: 232, Loss: 2461.57620, Residuals: -0.92727, Convergence: 0.000898\n",
      "Evidence 14841.240\n",
      "\n",
      "Epoch: 232, Evidence: 14841.24023, Convergence: 0.002201\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.10e-01\n",
      "Epoch: 232, Loss: 2477.40182, Residuals: -0.92727, Convergence:   inf\n",
      "Epoch: 233, Loss: 2474.57072, Residuals: -0.92560, Convergence: 0.001144\n",
      "Epoch: 234, Loss: 2472.19400, Residuals: -0.92434, Convergence: 0.000961\n",
      "Evidence 14854.142\n",
      "\n",
      "Epoch: 234, Evidence: 14854.14160, Convergence: 0.000869\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.75e-01\n",
      "Epoch: 234, Loss: 2478.07465, Residuals: -0.92434, Convergence:   inf\n",
      "Epoch: 235, Loss: 2473.69763, Residuals: -0.92219, Convergence: 0.001769\n",
      "Epoch: 236, Loss: 2470.31641, Residuals: -0.92068, Convergence: 0.001369\n",
      "Epoch: 237, Loss: 2467.58633, Residuals: -0.91975, Convergence: 0.001106\n",
      "Epoch: 238, Loss: 2465.27436, Residuals: -0.91930, Convergence: 0.000938\n",
      "Evidence 14871.540\n",
      "\n",
      "Epoch: 238, Evidence: 14871.54004, Convergence: 0.002037\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.44e-01\n",
      "Epoch: 238, Loss: 2478.19846, Residuals: -0.91930, Convergence:   inf\n",
      "Epoch: 239, Loss: 2475.32256, Residuals: -0.91714, Convergence: 0.001162\n",
      "Epoch: 240, Loss: 2473.04273, Residuals: -0.91611, Convergence: 0.000922\n",
      "Evidence 14882.270\n",
      "\n",
      "Epoch: 240, Evidence: 14882.26953, Convergence: 0.000721\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.22e-01\n",
      "Epoch: 240, Loss: 2478.35780, Residuals: -0.91611, Convergence:   inf\n",
      "Epoch: 241, Loss: 2474.19538, Residuals: -0.91291, Convergence: 0.001682\n",
      "Epoch: 242, Loss: 2471.20337, Residuals: -0.91414, Convergence: 0.001211\n",
      "Epoch: 243, Loss: 2468.67350, Residuals: -0.91499, Convergence: 0.001025\n",
      "Epoch: 244, Loss: 2466.44062, Residuals: -0.91749, Convergence: 0.000905\n",
      "Evidence 14897.390\n",
      "\n",
      "Epoch: 244, Evidence: 14897.38965, Convergence: 0.001735\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.09e-01\n",
      "Epoch: 244, Loss: 2477.61881, Residuals: -0.91749, Convergence:   inf\n",
      "Epoch: 245, Loss: 2476.10528, Residuals: -0.91522, Convergence: 0.000611\n",
      "Evidence 14903.719\n",
      "\n",
      "Epoch: 245, Evidence: 14903.71875, Convergence: 0.000425\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.90e-02\n",
      "Epoch: 245, Loss: 2478.53960, Residuals: -0.91522, Convergence:   inf\n",
      "Epoch: 246, Loss: 2525.97423, Residuals: -0.95694, Convergence: -0.018779\n",
      "Epoch: 246, Loss: 2476.34209, Residuals: -0.91318, Convergence: 0.000887\n",
      "Evidence 14907.891\n",
      "\n",
      "Epoch: 246, Evidence: 14907.89062, Convergence: 0.000704\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.17e-02\n",
      "Epoch: 246, Loss: 2477.99068, Residuals: -0.91318, Convergence:   inf\n",
      "Epoch: 247, Loss: 2480.97696, Residuals: -0.91482, Convergence: -0.001204\n",
      "Epoch: 247, Loss: 2477.58917, Residuals: -0.91175, Convergence: 0.000162\n",
      "Evidence 14909.940\n",
      "\n",
      "Epoch: 247, Evidence: 14909.94043, Convergence: 0.000842\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 385.41268, Residuals: -4.54349, Convergence:   inf\n",
      "Epoch: 1, Loss: 359.66553, Residuals: -4.42450, Convergence: 0.071586\n",
      "Epoch: 2, Loss: 338.51782, Residuals: -4.26050, Convergence: 0.062472\n",
      "Epoch: 3, Loss: 322.39058, Residuals: -4.09587, Convergence: 0.050024\n",
      "Epoch: 4, Loss: 310.10060, Residuals: -3.95108, Convergence: 0.039632\n",
      "Epoch: 5, Loss: 300.34893, Residuals: -3.82308, Convergence: 0.032468\n",
      "Epoch: 6, Loss: 292.42304, Residuals: -3.71156, Convergence: 0.027104\n",
      "Epoch: 7, Loss: 285.83194, Residuals: -3.61601, Convergence: 0.023059\n",
      "Epoch: 8, Loss: 280.21265, Residuals: -3.53433, Convergence: 0.020054\n",
      "Epoch: 9, Loss: 275.30892, Residuals: -3.46420, Convergence: 0.017812\n",
      "Epoch: 10, Loss: 270.93848, Residuals: -3.40355, Convergence: 0.016131\n",
      "Epoch: 11, Loss: 266.96922, Residuals: -3.35061, Convergence: 0.014868\n",
      "Epoch: 12, Loss: 263.30460, Residuals: -3.30389, Convergence: 0.013918\n",
      "Epoch: 13, Loss: 259.87453, Residuals: -3.26211, Convergence: 0.013199\n",
      "Epoch: 14, Loss: 256.62932, Residuals: -3.22411, Convergence: 0.012645\n",
      "Epoch: 15, Loss: 253.53638, Residuals: -3.18896, Convergence: 0.012199\n",
      "Epoch: 16, Loss: 250.57775, Residuals: -3.15597, Convergence: 0.011807\n",
      "Epoch: 17, Loss: 247.74112, Residuals: -3.12471, Convergence: 0.011450\n",
      "Epoch: 18, Loss: 245.00649, Residuals: -3.09473, Convergence: 0.011161\n",
      "Epoch: 19, Loss: 242.34257, Residuals: -3.06547, Convergence: 0.010992\n",
      "Epoch: 20, Loss: 239.71399, Residuals: -3.03632, Convergence: 0.010966\n",
      "Epoch: 21, Loss: 237.09066, Residuals: -3.00674, Convergence: 0.011065\n",
      "Epoch: 22, Loss: 234.45113, Residuals: -2.97640, Convergence: 0.011258\n",
      "Epoch: 23, Loss: 231.77130, Residuals: -2.94502, Convergence: 0.011562\n",
      "Epoch: 24, Loss: 229.00768, Residuals: -2.91212, Convergence: 0.012068\n",
      "Epoch: 25, Loss: 226.10607, Residuals: -2.87702, Convergence: 0.012833\n",
      "Epoch: 26, Loss: 223.07224, Residuals: -2.83966, Convergence: 0.013600\n",
      "Epoch: 27, Loss: 220.02682, Residuals: -2.80129, Convergence: 0.013841\n",
      "Epoch: 28, Loss: 217.07014, Residuals: -2.76310, Convergence: 0.013621\n",
      "Epoch: 29, Loss: 214.21693, Residuals: -2.72538, Convergence: 0.013319\n",
      "Epoch: 30, Loss: 211.45308, Residuals: -2.68809, Convergence: 0.013071\n",
      "Epoch: 31, Loss: 208.76390, Residuals: -2.65114, Convergence: 0.012881\n",
      "Epoch: 32, Loss: 206.13899, Residuals: -2.61446, Convergence: 0.012734\n",
      "Epoch: 33, Loss: 203.57193, Residuals: -2.57800, Convergence: 0.012610\n",
      "Epoch: 34, Loss: 201.05917, Residuals: -2.54175, Convergence: 0.012498\n",
      "Epoch: 35, Loss: 198.59915, Residuals: -2.50570, Convergence: 0.012387\n",
      "Epoch: 36, Loss: 196.19163, Residuals: -2.46984, Convergence: 0.012271\n",
      "Epoch: 37, Loss: 193.83735, Residuals: -2.43418, Convergence: 0.012146\n",
      "Epoch: 38, Loss: 191.53769, Residuals: -2.39872, Convergence: 0.012006\n",
      "Epoch: 39, Loss: 189.29447, Residuals: -2.36347, Convergence: 0.011850\n",
      "Epoch: 40, Loss: 187.10978, Residuals: -2.32846, Convergence: 0.011676\n",
      "Epoch: 41, Loss: 184.98582, Residuals: -2.29371, Convergence: 0.011482\n",
      "Epoch: 42, Loss: 182.92470, Residuals: -2.25925, Convergence: 0.011268\n",
      "Epoch: 43, Loss: 180.92826, Residuals: -2.22512, Convergence: 0.011034\n",
      "Epoch: 44, Loss: 178.99795, Residuals: -2.19136, Convergence: 0.010784\n",
      "Epoch: 45, Loss: 177.13471, Residuals: -2.15800, Convergence: 0.010519\n",
      "Epoch: 46, Loss: 175.33893, Residuals: -2.12508, Convergence: 0.010242\n",
      "Epoch: 47, Loss: 173.61046, Residuals: -2.09262, Convergence: 0.009956\n",
      "Epoch: 48, Loss: 171.94869, Residuals: -2.06065, Convergence: 0.009664\n",
      "Epoch: 49, Loss: 170.35263, Residuals: -2.02920, Convergence: 0.009369\n",
      "Epoch: 50, Loss: 168.82103, Residuals: -1.99827, Convergence: 0.009072\n",
      "Epoch: 51, Loss: 167.35246, Residuals: -1.96788, Convergence: 0.008775\n",
      "Epoch: 52, Loss: 165.94543, Residuals: -1.93806, Convergence: 0.008479\n",
      "Epoch: 53, Loss: 164.59836, Residuals: -1.90881, Convergence: 0.008184\n",
      "Epoch: 54, Loss: 163.30970, Residuals: -1.88015, Convergence: 0.007891\n",
      "Epoch: 55, Loss: 162.07790, Residuals: -1.85211, Convergence: 0.007600\n",
      "Epoch: 56, Loss: 160.90146, Residuals: -1.82468, Convergence: 0.007312\n",
      "Epoch: 57, Loss: 159.77895, Residuals: -1.79790, Convergence: 0.007025\n",
      "Epoch: 58, Loss: 158.70896, Residuals: -1.77176, Convergence: 0.006742\n",
      "Epoch: 59, Loss: 157.69022, Residuals: -1.74629, Convergence: 0.006460\n",
      "Epoch: 60, Loss: 156.72146, Residuals: -1.72149, Convergence: 0.006181\n",
      "Epoch: 61, Loss: 155.80151, Residuals: -1.69738, Convergence: 0.005905\n",
      "Epoch: 62, Loss: 154.92918, Residuals: -1.67396, Convergence: 0.005631\n",
      "Epoch: 63, Loss: 154.10326, Residuals: -1.65126, Convergence: 0.005360\n",
      "Epoch: 64, Loss: 153.32244, Residuals: -1.62927, Convergence: 0.005093\n",
      "Epoch: 65, Loss: 152.58533, Residuals: -1.60802, Convergence: 0.004831\n",
      "Epoch: 66, Loss: 151.89042, Residuals: -1.58750, Convergence: 0.004575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67, Loss: 151.23601, Residuals: -1.56771, Convergence: 0.004327\n",
      "Epoch: 68, Loss: 150.62028, Residuals: -1.54867, Convergence: 0.004088\n",
      "Epoch: 69, Loss: 150.04123, Residuals: -1.53036, Convergence: 0.003859\n",
      "Epoch: 70, Loss: 149.49671, Residuals: -1.51278, Convergence: 0.003642\n",
      "Epoch: 71, Loss: 148.98442, Residuals: -1.49591, Convergence: 0.003439\n",
      "Epoch: 72, Loss: 148.50190, Residuals: -1.47974, Convergence: 0.003249\n",
      "Epoch: 73, Loss: 148.04663, Residuals: -1.46424, Convergence: 0.003075\n",
      "Epoch: 74, Loss: 147.61605, Residuals: -1.44939, Convergence: 0.002917\n",
      "Epoch: 75, Loss: 147.20772, Residuals: -1.43513, Convergence: 0.002774\n",
      "Epoch: 76, Loss: 146.81934, Residuals: -1.42145, Convergence: 0.002645\n",
      "Epoch: 77, Loss: 146.44888, Residuals: -1.40831, Convergence: 0.002530\n",
      "Epoch: 78, Loss: 146.09462, Residuals: -1.39566, Convergence: 0.002425\n",
      "Epoch: 79, Loss: 145.75516, Residuals: -1.38349, Convergence: 0.002329\n",
      "Epoch: 80, Loss: 145.42935, Residuals: -1.37176, Convergence: 0.002240\n",
      "Epoch: 81, Loss: 145.11634, Residuals: -1.36045, Convergence: 0.002157\n",
      "Epoch: 82, Loss: 144.81546, Residuals: -1.34955, Convergence: 0.002078\n",
      "Epoch: 83, Loss: 144.52619, Residuals: -1.33904, Convergence: 0.002001\n",
      "Epoch: 84, Loss: 144.24814, Residuals: -1.32890, Convergence: 0.001928\n",
      "Epoch: 85, Loss: 143.98098, Residuals: -1.31913, Convergence: 0.001855\n",
      "Epoch: 86, Loss: 143.72444, Residuals: -1.30971, Convergence: 0.001785\n",
      "Epoch: 87, Loss: 143.47830, Residuals: -1.30064, Convergence: 0.001715\n",
      "Epoch: 88, Loss: 143.24237, Residuals: -1.29190, Convergence: 0.001647\n",
      "Epoch: 89, Loss: 143.01646, Residuals: -1.28349, Convergence: 0.001580\n",
      "Epoch: 90, Loss: 142.80040, Residuals: -1.27540, Convergence: 0.001513\n",
      "Epoch: 91, Loss: 142.59404, Residuals: -1.26761, Convergence: 0.001447\n",
      "Epoch: 92, Loss: 142.39723, Residuals: -1.26014, Convergence: 0.001382\n",
      "Epoch: 93, Loss: 142.20980, Residuals: -1.25296, Convergence: 0.001318\n",
      "Epoch: 94, Loss: 142.03158, Residuals: -1.24608, Convergence: 0.001255\n",
      "Epoch: 95, Loss: 141.86239, Residuals: -1.23949, Convergence: 0.001193\n",
      "Epoch: 96, Loss: 141.70202, Residuals: -1.23318, Convergence: 0.001132\n",
      "Epoch: 97, Loss: 141.55019, Residuals: -1.22716, Convergence: 0.001073\n",
      "Epoch: 98, Loss: 141.40658, Residuals: -1.22142, Convergence: 0.001016\n",
      "Epoch: 99, Loss: 141.27079, Residuals: -1.21595, Convergence: 0.000961\n",
      "Evidence -183.253\n",
      "\n",
      "Epoch: 99, Evidence: -183.25339, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.24e-01\n",
      "Epoch: 99, Loss: 1366.01396, Residuals: -1.21595, Convergence:   inf\n",
      "Epoch: 100, Loss: 1303.57317, Residuals: -1.24704, Convergence: 0.047900\n",
      "Epoch: 101, Loss: 1256.03095, Residuals: -1.27140, Convergence: 0.037851\n",
      "Epoch: 102, Loss: 1219.99976, Residuals: -1.28909, Convergence: 0.029534\n",
      "Epoch: 103, Loss: 1191.92653, Residuals: -1.30161, Convergence: 0.023553\n",
      "Epoch: 104, Loss: 1169.20694, Residuals: -1.31080, Convergence: 0.019432\n",
      "Epoch: 105, Loss: 1150.31331, Residuals: -1.31772, Convergence: 0.016425\n",
      "Epoch: 106, Loss: 1134.32854, Residuals: -1.32289, Convergence: 0.014092\n",
      "Epoch: 107, Loss: 1120.63705, Residuals: -1.32657, Convergence: 0.012218\n",
      "Epoch: 108, Loss: 1108.78761, Residuals: -1.32895, Convergence: 0.010687\n",
      "Epoch: 109, Loss: 1098.43134, Residuals: -1.33018, Convergence: 0.009428\n",
      "Epoch: 110, Loss: 1089.28939, Residuals: -1.33037, Convergence: 0.008393\n",
      "Epoch: 111, Loss: 1081.13278, Residuals: -1.32964, Convergence: 0.007545\n",
      "Epoch: 112, Loss: 1073.77007, Residuals: -1.32808, Convergence: 0.006857\n",
      "Epoch: 113, Loss: 1067.03788, Residuals: -1.32576, Convergence: 0.006309\n",
      "Epoch: 114, Loss: 1060.79379, Residuals: -1.32275, Convergence: 0.005886\n",
      "Epoch: 115, Loss: 1054.91121, Residuals: -1.31907, Convergence: 0.005576\n",
      "Epoch: 116, Loss: 1049.27704, Residuals: -1.31475, Convergence: 0.005370\n",
      "Epoch: 117, Loss: 1043.78815, Residuals: -1.30981, Convergence: 0.005259\n",
      "Epoch: 118, Loss: 1038.35434, Residuals: -1.30425, Convergence: 0.005233\n",
      "Epoch: 119, Loss: 1032.90387, Residuals: -1.29808, Convergence: 0.005277\n",
      "Epoch: 120, Loss: 1027.39714, Residuals: -1.29134, Convergence: 0.005360\n",
      "Epoch: 121, Loss: 1021.84224, Residuals: -1.28410, Convergence: 0.005436\n",
      "Epoch: 122, Loss: 1016.29988, Residuals: -1.27645, Convergence: 0.005453\n",
      "Epoch: 123, Loss: 1010.86729, Residuals: -1.26848, Convergence: 0.005374\n",
      "Epoch: 124, Loss: 1005.64237, Residuals: -1.26030, Convergence: 0.005196\n",
      "Epoch: 125, Loss: 1000.69576, Residuals: -1.25198, Convergence: 0.004943\n",
      "Epoch: 126, Loss: 996.05881, Residuals: -1.24360, Convergence: 0.004655\n",
      "Epoch: 127, Loss: 991.73223, Residuals: -1.23523, Convergence: 0.004363\n",
      "Epoch: 128, Loss: 987.69872, Residuals: -1.22693, Convergence: 0.004084\n",
      "Epoch: 129, Loss: 983.93421, Residuals: -1.21873, Convergence: 0.003826\n",
      "Epoch: 130, Loss: 980.41250, Residuals: -1.21069, Convergence: 0.003592\n",
      "Epoch: 131, Loss: 977.11079, Residuals: -1.20283, Convergence: 0.003379\n",
      "Epoch: 132, Loss: 974.00839, Residuals: -1.19517, Convergence: 0.003185\n",
      "Epoch: 133, Loss: 971.08857, Residuals: -1.18775, Convergence: 0.003007\n",
      "Epoch: 134, Loss: 968.33756, Residuals: -1.18057, Convergence: 0.002841\n",
      "Epoch: 135, Loss: 965.74279, Residuals: -1.17366, Convergence: 0.002687\n",
      "Epoch: 136, Loss: 963.29428, Residuals: -1.16702, Convergence: 0.002542\n",
      "Epoch: 137, Loss: 960.98276, Residuals: -1.16065, Convergence: 0.002405\n",
      "Epoch: 138, Loss: 958.79925, Residuals: -1.15457, Convergence: 0.002277\n",
      "Epoch: 139, Loss: 956.73601, Residuals: -1.14876, Convergence: 0.002157\n",
      "Epoch: 140, Loss: 954.78499, Residuals: -1.14322, Convergence: 0.002043\n",
      "Epoch: 141, Loss: 952.93816, Residuals: -1.13796, Convergence: 0.001938\n",
      "Epoch: 142, Loss: 951.18856, Residuals: -1.13295, Convergence: 0.001839\n",
      "Epoch: 143, Loss: 949.52856, Residuals: -1.12820, Convergence: 0.001748\n",
      "Epoch: 144, Loss: 947.95154, Residuals: -1.12369, Convergence: 0.001664\n",
      "Epoch: 145, Loss: 946.45074, Residuals: -1.11940, Convergence: 0.001586\n",
      "Epoch: 146, Loss: 945.01989, Residuals: -1.11534, Convergence: 0.001514\n",
      "Epoch: 147, Loss: 943.65337, Residuals: -1.11148, Convergence: 0.001448\n",
      "Epoch: 148, Loss: 942.34525, Residuals: -1.10781, Convergence: 0.001388\n",
      "Epoch: 149, Loss: 941.09074, Residuals: -1.10432, Convergence: 0.001333\n",
      "Epoch: 150, Loss: 939.88511, Residuals: -1.10101, Convergence: 0.001283\n",
      "Epoch: 151, Loss: 938.72400, Residuals: -1.09785, Convergence: 0.001237\n",
      "Epoch: 152, Loss: 937.60365, Residuals: -1.09484, Convergence: 0.001195\n",
      "Epoch: 153, Loss: 936.51979, Residuals: -1.09197, Convergence: 0.001157\n",
      "Epoch: 154, Loss: 935.46959, Residuals: -1.08923, Convergence: 0.001123\n",
      "Epoch: 155, Loss: 934.45030, Residuals: -1.08660, Convergence: 0.001091\n",
      "Epoch: 156, Loss: 933.45803, Residuals: -1.08409, Convergence: 0.001063\n",
      "Epoch: 157, Loss: 932.49070, Residuals: -1.08167, Convergence: 0.001037\n",
      "Epoch: 158, Loss: 931.54601, Residuals: -1.07935, Convergence: 0.001014\n",
      "Epoch: 159, Loss: 930.62140, Residuals: -1.07712, Convergence: 0.000994\n",
      "Evidence 11154.631\n",
      "\n",
      "Epoch: 159, Evidence: 11154.63086, Convergence: 1.016428\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.78e-01\n",
      "Epoch: 159, Loss: 2342.68064, Residuals: -1.07712, Convergence:   inf\n",
      "Epoch: 160, Loss: 2302.62084, Residuals: -1.08817, Convergence: 0.017397\n",
      "Epoch: 161, Loss: 2274.27304, Residuals: -1.08879, Convergence: 0.012465\n",
      "Epoch: 162, Loss: 2250.33482, Residuals: -1.08810, Convergence: 0.010638\n",
      "Epoch: 163, Loss: 2229.88120, Residuals: -1.08684, Convergence: 0.009173\n",
      "Epoch: 164, Loss: 2212.27039, Residuals: -1.08517, Convergence: 0.007961\n",
      "Epoch: 165, Loss: 2197.00263, Residuals: -1.08318, Convergence: 0.006949\n",
      "Epoch: 166, Loss: 2183.66509, Residuals: -1.08092, Convergence: 0.006108\n",
      "Epoch: 167, Loss: 2171.91258, Residuals: -1.07842, Convergence: 0.005411\n",
      "Epoch: 168, Loss: 2161.45618, Residuals: -1.07571, Convergence: 0.004838\n",
      "Epoch: 169, Loss: 2152.04853, Residuals: -1.07283, Convergence: 0.004371\n",
      "Epoch: 170, Loss: 2143.48653, Residuals: -1.06977, Convergence: 0.003994\n",
      "Epoch: 171, Loss: 2135.60786, Residuals: -1.06656, Convergence: 0.003689\n",
      "Epoch: 172, Loss: 2128.29583, Residuals: -1.06321, Convergence: 0.003436\n",
      "Epoch: 173, Loss: 2121.48310, Residuals: -1.05974, Convergence: 0.003211\n",
      "Epoch: 174, Loss: 2115.13882, Residuals: -1.05622, Convergence: 0.002999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 175, Loss: 2109.25207, Residuals: -1.05268, Convergence: 0.002791\n",
      "Epoch: 176, Loss: 2103.81495, Residuals: -1.04918, Convergence: 0.002584\n",
      "Epoch: 177, Loss: 2098.81195, Residuals: -1.04577, Convergence: 0.002384\n",
      "Epoch: 178, Loss: 2094.21986, Residuals: -1.04248, Convergence: 0.002193\n",
      "Epoch: 179, Loss: 2090.00990, Residuals: -1.03933, Convergence: 0.002014\n",
      "Epoch: 180, Loss: 2086.14813, Residuals: -1.03633, Convergence: 0.001851\n",
      "Epoch: 181, Loss: 2082.60151, Residuals: -1.03349, Convergence: 0.001703\n",
      "Epoch: 182, Loss: 2079.33506, Residuals: -1.03080, Convergence: 0.001571\n",
      "Epoch: 183, Loss: 2076.31660, Residuals: -1.02827, Convergence: 0.001454\n",
      "Epoch: 184, Loss: 2073.51739, Residuals: -1.02588, Convergence: 0.001350\n",
      "Epoch: 185, Loss: 2070.91050, Residuals: -1.02363, Convergence: 0.001259\n",
      "Epoch: 186, Loss: 2068.47127, Residuals: -1.02152, Convergence: 0.001179\n",
      "Epoch: 187, Loss: 2066.18059, Residuals: -1.01954, Convergence: 0.001109\n",
      "Epoch: 188, Loss: 2064.01973, Residuals: -1.01768, Convergence: 0.001047\n",
      "Epoch: 189, Loss: 2061.97456, Residuals: -1.01593, Convergence: 0.000992\n",
      "Evidence 14360.992\n",
      "\n",
      "Epoch: 189, Evidence: 14360.99219, Convergence: 0.223269\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.40e-01\n",
      "Epoch: 189, Loss: 2474.66790, Residuals: -1.01593, Convergence:   inf\n",
      "Epoch: 190, Loss: 2459.76974, Residuals: -1.01311, Convergence: 0.006057\n",
      "Epoch: 191, Loss: 2447.79546, Residuals: -1.00952, Convergence: 0.004892\n",
      "Epoch: 192, Loss: 2437.53268, Residuals: -1.00605, Convergence: 0.004210\n",
      "Epoch: 193, Loss: 2428.64409, Residuals: -1.00281, Convergence: 0.003660\n",
      "Epoch: 194, Loss: 2420.88121, Residuals: -0.99980, Convergence: 0.003207\n",
      "Epoch: 195, Loss: 2414.05366, Residuals: -0.99703, Convergence: 0.002828\n",
      "Epoch: 196, Loss: 2408.01092, Residuals: -0.99448, Convergence: 0.002509\n",
      "Epoch: 197, Loss: 2402.63059, Residuals: -0.99214, Convergence: 0.002239\n",
      "Epoch: 198, Loss: 2397.81458, Residuals: -0.98999, Convergence: 0.002008\n",
      "Epoch: 199, Loss: 2393.48081, Residuals: -0.98802, Convergence: 0.001811\n",
      "Epoch: 200, Loss: 2389.56134, Residuals: -0.98621, Convergence: 0.001640\n",
      "Epoch: 201, Loss: 2385.99989, Residuals: -0.98454, Convergence: 0.001493\n",
      "Epoch: 202, Loss: 2382.74927, Residuals: -0.98301, Convergence: 0.001364\n",
      "Epoch: 203, Loss: 2379.76989, Residuals: -0.98160, Convergence: 0.001252\n",
      "Epoch: 204, Loss: 2377.02837, Residuals: -0.98029, Convergence: 0.001153\n",
      "Epoch: 205, Loss: 2374.49711, Residuals: -0.97909, Convergence: 0.001066\n",
      "Epoch: 206, Loss: 2372.15238, Residuals: -0.97798, Convergence: 0.000988\n",
      "Evidence 14788.721\n",
      "\n",
      "Epoch: 206, Evidence: 14788.72070, Convergence: 0.028923\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.36e-01\n",
      "Epoch: 206, Loss: 2480.14704, Residuals: -0.97798, Convergence:   inf\n",
      "Epoch: 207, Loss: 2473.43700, Residuals: -0.97471, Convergence: 0.002713\n",
      "Epoch: 208, Loss: 2467.89222, Residuals: -0.97177, Convergence: 0.002247\n",
      "Epoch: 209, Loss: 2463.14139, Residuals: -0.96920, Convergence: 0.001929\n",
      "Epoch: 210, Loss: 2459.00493, Residuals: -0.96695, Convergence: 0.001682\n",
      "Epoch: 211, Loss: 2455.35997, Residuals: -0.96497, Convergence: 0.001484\n",
      "Epoch: 212, Loss: 2452.11399, Residuals: -0.96323, Convergence: 0.001324\n",
      "Epoch: 213, Loss: 2449.19547, Residuals: -0.96168, Convergence: 0.001192\n",
      "Epoch: 214, Loss: 2446.54920, Residuals: -0.96030, Convergence: 0.001082\n",
      "Epoch: 215, Loss: 2444.13138, Residuals: -0.95908, Convergence: 0.000989\n",
      "Evidence 14878.225\n",
      "\n",
      "Epoch: 215, Evidence: 14878.22461, Convergence: 0.006016\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.64e-01\n",
      "Epoch: 215, Loss: 2481.79259, Residuals: -0.95908, Convergence:   inf\n",
      "Epoch: 216, Loss: 2477.89802, Residuals: -0.95632, Convergence: 0.001572\n",
      "Epoch: 217, Loss: 2474.60074, Residuals: -0.95404, Convergence: 0.001332\n",
      "Epoch: 218, Loss: 2471.72071, Residuals: -0.95211, Convergence: 0.001165\n",
      "Epoch: 219, Loss: 2469.16361, Residuals: -0.95046, Convergence: 0.001036\n",
      "Epoch: 220, Loss: 2466.86521, Residuals: -0.94904, Convergence: 0.000932\n",
      "Evidence 14909.420\n",
      "\n",
      "Epoch: 220, Evidence: 14909.41992, Convergence: 0.002092\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.13e-01\n",
      "Epoch: 220, Loss: 2482.76113, Residuals: -0.94904, Convergence:   inf\n",
      "Epoch: 221, Loss: 2479.96067, Residuals: -0.94673, Convergence: 0.001129\n",
      "Epoch: 222, Loss: 2477.55150, Residuals: -0.94486, Convergence: 0.000972\n",
      "Evidence 14921.657\n",
      "\n",
      "Epoch: 222, Evidence: 14921.65723, Convergence: 0.000820\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.77e-01\n",
      "Epoch: 222, Loss: 2483.52179, Residuals: -0.94486, Convergence:   inf\n",
      "Epoch: 223, Loss: 2478.99961, Residuals: -0.94134, Convergence: 0.001824\n",
      "Epoch: 224, Loss: 2475.44839, Residuals: -0.93882, Convergence: 0.001435\n",
      "Epoch: 225, Loss: 2472.50920, Residuals: -0.93693, Convergence: 0.001189\n",
      "Epoch: 226, Loss: 2469.98159, Residuals: -0.93568, Convergence: 0.001023\n",
      "Epoch: 227, Loss: 2467.74585, Residuals: -0.93487, Convergence: 0.000906\n",
      "Evidence 14942.299\n",
      "\n",
      "Epoch: 227, Evidence: 14942.29883, Convergence: 0.002200\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.47e-01\n",
      "Epoch: 227, Loss: 2483.63021, Residuals: -0.93487, Convergence:   inf\n",
      "Epoch: 228, Loss: 2480.81954, Residuals: -0.93194, Convergence: 0.001133\n",
      "Epoch: 229, Loss: 2478.52746, Residuals: -0.93028, Convergence: 0.000925\n",
      "Evidence 14953.381\n",
      "\n",
      "Epoch: 229, Evidence: 14953.38086, Convergence: 0.000741\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.25e-01\n",
      "Epoch: 229, Loss: 2483.88047, Residuals: -0.93028, Convergence:   inf\n",
      "Epoch: 230, Loss: 2479.74388, Residuals: -0.92621, Convergence: 0.001668\n",
      "Epoch: 231, Loss: 2476.64369, Residuals: -0.92673, Convergence: 0.001252\n",
      "Epoch: 232, Loss: 2474.02846, Residuals: -0.92745, Convergence: 0.001057\n",
      "Epoch: 233, Loss: 2471.76579, Residuals: -0.93018, Convergence: 0.000915\n",
      "Evidence 14968.734\n",
      "\n",
      "Epoch: 233, Evidence: 14968.73438, Convergence: 0.001766\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.10e-01\n",
      "Epoch: 233, Loss: 2483.41622, Residuals: -0.93018, Convergence:   inf\n",
      "Epoch: 234, Loss: 2482.41749, Residuals: -0.92699, Convergence: 0.000402\n",
      "Evidence 14974.338\n",
      "\n",
      "Epoch: 234, Evidence: 14974.33789, Convergence: 0.000374\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 9.00e-02\n",
      "Epoch: 234, Loss: 2484.29564, Residuals: -0.92699, Convergence:   inf\n",
      "Epoch: 235, Loss: 2537.45878, Residuals: -0.95645, Convergence: -0.020951\n",
      "Epoch: 235, Loss: 2482.33352, Residuals: -0.92398, Convergence: 0.000790\n",
      "Evidence 14978.373\n",
      "\n",
      "Epoch: 235, Evidence: 14978.37305, Convergence: 0.000644\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.25e-02\n",
      "Epoch: 235, Loss: 2483.81680, Residuals: -0.92398, Convergence:   inf\n",
      "Epoch: 236, Loss: 2491.71406, Residuals: -0.92228, Convergence: -0.003169\n",
      "Epoch: 236, Loss: 2484.94038, Residuals: -0.92105, Convergence: -0.000452\n",
      "Evidence 14978.776\n",
      "\n",
      "Epoch: 236, Evidence: 14978.77637, Convergence: 0.000670\n",
      "Total samples: 183, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 384.57462, Residuals: -4.52918, Convergence:   inf\n",
      "Epoch: 1, Loss: 358.78864, Residuals: -4.40894, Convergence: 0.071870\n",
      "Epoch: 2, Loss: 337.61121, Residuals: -4.24367, Convergence: 0.062727\n",
      "Epoch: 3, Loss: 321.43945, Residuals: -4.07796, Convergence: 0.050310\n",
      "Epoch: 4, Loss: 309.09815, Residuals: -3.93192, Convergence: 0.039927\n",
      "Epoch: 5, Loss: 299.30194, Residuals: -3.80256, Convergence: 0.032730\n",
      "Epoch: 6, Loss: 291.34511, Residuals: -3.68972, Convergence: 0.027311\n",
      "Epoch: 7, Loss: 284.73545, Residuals: -3.59302, Convergence: 0.023213\n",
      "Epoch: 8, Loss: 279.10757, Residuals: -3.51039, Convergence: 0.020164\n",
      "Epoch: 9, Loss: 274.20404, Residuals: -3.43950, Convergence: 0.017883\n",
      "Epoch: 10, Loss: 269.84205, Residuals: -3.37822, Convergence: 0.016165\n",
      "Epoch: 11, Loss: 265.88930, Residuals: -3.32476, Convergence: 0.014866\n",
      "Epoch: 12, Loss: 262.24923, Residuals: -3.27754, Convergence: 0.013880\n",
      "Epoch: 13, Loss: 258.85185, Residuals: -3.23523, Convergence: 0.013125\n",
      "Epoch: 14, Loss: 255.64845, Residuals: -3.19666, Convergence: 0.012530\n",
      "Epoch: 15, Loss: 252.60994, Residuals: -3.16093, Convergence: 0.012028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Loss: 249.72373, Residuals: -3.12751, Convergence: 0.011558\n",
      "Epoch: 17, Loss: 246.98041, Residuals: -3.09606, Convergence: 0.011107\n",
      "Epoch: 18, Loss: 244.35934, Residuals: -3.06621, Convergence: 0.010726\n",
      "Epoch: 19, Loss: 241.82828, Residuals: -3.03745, Convergence: 0.010466\n",
      "Epoch: 20, Loss: 239.35077, Residuals: -3.00921, Convergence: 0.010351\n",
      "Epoch: 21, Loss: 236.89215, Residuals: -2.98093, Convergence: 0.010379\n",
      "Epoch: 22, Loss: 234.42210, Residuals: -2.95214, Convergence: 0.010537\n",
      "Epoch: 23, Loss: 231.91239, Residuals: -2.92245, Convergence: 0.010822\n",
      "Epoch: 24, Loss: 229.32719, Residuals: -2.89141, Convergence: 0.011273\n",
      "Epoch: 25, Loss: 226.61496, Residuals: -2.85842, Convergence: 0.011968\n",
      "Epoch: 26, Loss: 223.73641, Residuals: -2.82296, Convergence: 0.012866\n",
      "Epoch: 27, Loss: 220.75504, Residuals: -2.78561, Convergence: 0.013505\n",
      "Epoch: 28, Loss: 217.81363, Residuals: -2.74792, Convergence: 0.013504\n",
      "Epoch: 29, Loss: 214.97604, Residuals: -2.71071, Convergence: 0.013200\n",
      "Epoch: 30, Loss: 212.23875, Residuals: -2.67407, Convergence: 0.012897\n",
      "Epoch: 31, Loss: 209.58560, Residuals: -2.63790, Convergence: 0.012659\n",
      "Epoch: 32, Loss: 207.00355, Residuals: -2.60211, Convergence: 0.012473\n",
      "Epoch: 33, Loss: 204.48441, Residuals: -2.56663, Convergence: 0.012319\n",
      "Epoch: 34, Loss: 202.02373, Residuals: -2.53141, Convergence: 0.012180\n",
      "Epoch: 35, Loss: 199.61963, Residuals: -2.49642, Convergence: 0.012043\n",
      "Epoch: 36, Loss: 197.27175, Residuals: -2.46166, Convergence: 0.011902\n",
      "Epoch: 37, Loss: 194.98054, Residuals: -2.42710, Convergence: 0.011751\n",
      "Epoch: 38, Loss: 192.74677, Residuals: -2.39275, Convergence: 0.011589\n",
      "Epoch: 39, Loss: 190.57119, Residuals: -2.35860, Convergence: 0.011416\n",
      "Epoch: 40, Loss: 188.45439, Residuals: -2.32467, Convergence: 0.011232\n",
      "Epoch: 41, Loss: 186.39672, Residuals: -2.29096, Convergence: 0.011039\n",
      "Epoch: 42, Loss: 184.39830, Residuals: -2.25749, Convergence: 0.010837\n",
      "Epoch: 43, Loss: 182.45910, Residuals: -2.22427, Convergence: 0.010628\n",
      "Epoch: 44, Loss: 180.57908, Residuals: -2.19133, Convergence: 0.010411\n",
      "Epoch: 45, Loss: 178.75828, Residuals: -2.15868, Convergence: 0.010186\n",
      "Epoch: 46, Loss: 176.99693, Residuals: -2.12638, Convergence: 0.009951\n",
      "Epoch: 47, Loss: 175.29549, Residuals: -2.09445, Convergence: 0.009706\n",
      "Epoch: 48, Loss: 173.65459, Residuals: -2.06294, Convergence: 0.009449\n",
      "Epoch: 49, Loss: 172.07479, Residuals: -2.03190, Convergence: 0.009181\n",
      "Epoch: 50, Loss: 170.55638, Residuals: -2.00138, Convergence: 0.008903\n",
      "Epoch: 51, Loss: 169.09918, Residuals: -1.97142, Convergence: 0.008617\n",
      "Epoch: 52, Loss: 167.70244, Residuals: -1.94204, Convergence: 0.008329\n",
      "Epoch: 53, Loss: 166.36492, Residuals: -1.91328, Convergence: 0.008040\n",
      "Epoch: 54, Loss: 165.08498, Residuals: -1.88515, Convergence: 0.007753\n",
      "Epoch: 55, Loss: 163.86072, Residuals: -1.85766, Convergence: 0.007471\n",
      "Epoch: 56, Loss: 162.69011, Residuals: -1.83081, Convergence: 0.007195\n",
      "Epoch: 57, Loss: 161.57101, Residuals: -1.80463, Convergence: 0.006926\n",
      "Epoch: 58, Loss: 160.50124, Residuals: -1.77910, Convergence: 0.006665\n",
      "Epoch: 59, Loss: 159.47856, Residuals: -1.75423, Convergence: 0.006413\n",
      "Epoch: 60, Loss: 158.50075, Residuals: -1.73002, Convergence: 0.006169\n",
      "Epoch: 61, Loss: 157.56560, Residuals: -1.70647, Convergence: 0.005935\n",
      "Epoch: 62, Loss: 156.67100, Residuals: -1.68355, Convergence: 0.005710\n",
      "Epoch: 63, Loss: 155.81500, Residuals: -1.66127, Convergence: 0.005494\n",
      "Epoch: 64, Loss: 154.99581, Residuals: -1.63962, Convergence: 0.005285\n",
      "Epoch: 65, Loss: 154.21188, Residuals: -1.61859, Convergence: 0.005084\n",
      "Epoch: 66, Loss: 153.46182, Residuals: -1.59818, Convergence: 0.004888\n",
      "Epoch: 67, Loss: 152.74445, Residuals: -1.57837, Convergence: 0.004697\n",
      "Epoch: 68, Loss: 152.05868, Residuals: -1.55917, Convergence: 0.004510\n",
      "Epoch: 69, Loss: 151.40354, Residuals: -1.54058, Convergence: 0.004327\n",
      "Epoch: 70, Loss: 150.77811, Residuals: -1.52258, Convergence: 0.004148\n",
      "Epoch: 71, Loss: 150.18147, Residuals: -1.50518, Convergence: 0.003973\n",
      "Epoch: 72, Loss: 149.61274, Residuals: -1.48838, Convergence: 0.003801\n",
      "Epoch: 73, Loss: 149.07100, Residuals: -1.47216, Convergence: 0.003634\n",
      "Epoch: 74, Loss: 148.55532, Residuals: -1.45653, Convergence: 0.003471\n",
      "Epoch: 75, Loss: 148.06474, Residuals: -1.44146, Convergence: 0.003313\n",
      "Epoch: 76, Loss: 147.59829, Residuals: -1.42696, Convergence: 0.003160\n",
      "Epoch: 77, Loss: 147.15496, Residuals: -1.41302, Convergence: 0.003013\n",
      "Epoch: 78, Loss: 146.73378, Residuals: -1.39961, Convergence: 0.002870\n",
      "Epoch: 79, Loss: 146.33375, Residuals: -1.38673, Convergence: 0.002734\n",
      "Epoch: 80, Loss: 145.95390, Residuals: -1.37436, Convergence: 0.002602\n",
      "Epoch: 81, Loss: 145.59327, Residuals: -1.36249, Convergence: 0.002477\n",
      "Epoch: 82, Loss: 145.25095, Residuals: -1.35110, Convergence: 0.002357\n",
      "Epoch: 83, Loss: 144.92602, Residuals: -1.34018, Convergence: 0.002242\n",
      "Epoch: 84, Loss: 144.61766, Residuals: -1.32970, Convergence: 0.002132\n",
      "Epoch: 85, Loss: 144.32504, Residuals: -1.31966, Convergence: 0.002028\n",
      "Epoch: 86, Loss: 144.04741, Residuals: -1.31004, Convergence: 0.001927\n",
      "Epoch: 87, Loss: 143.78404, Residuals: -1.30081, Convergence: 0.001832\n",
      "Epoch: 88, Loss: 143.53427, Residuals: -1.29197, Convergence: 0.001740\n",
      "Epoch: 89, Loss: 143.29747, Residuals: -1.28350, Convergence: 0.001652\n",
      "Epoch: 90, Loss: 143.07306, Residuals: -1.27539, Convergence: 0.001569\n",
      "Epoch: 91, Loss: 142.86049, Residuals: -1.26761, Convergence: 0.001488\n",
      "Epoch: 92, Loss: 142.65928, Residuals: -1.26017, Convergence: 0.001410\n",
      "Epoch: 93, Loss: 142.46895, Residuals: -1.25304, Convergence: 0.001336\n",
      "Epoch: 94, Loss: 142.28909, Residuals: -1.24621, Convergence: 0.001264\n",
      "Epoch: 95, Loss: 142.11931, Residuals: -1.23967, Convergence: 0.001195\n",
      "Epoch: 96, Loss: 141.95924, Residuals: -1.23341, Convergence: 0.001128\n",
      "Epoch: 97, Loss: 141.80855, Residuals: -1.22743, Convergence: 0.001063\n",
      "Epoch: 98, Loss: 141.66691, Residuals: -1.22171, Convergence: 0.001000\n",
      "Evidence -183.702\n",
      "\n",
      "Epoch: 98, Evidence: -183.70219, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.25e-01\n",
      "Epoch: 98, Loss: 1361.01687, Residuals: -1.22171, Convergence:   inf\n",
      "Epoch: 99, Loss: 1297.51686, Residuals: -1.25329, Convergence: 0.048940\n",
      "Epoch: 100, Loss: 1249.71486, Residuals: -1.27767, Convergence: 0.038250\n",
      "Epoch: 101, Loss: 1213.86059, Residuals: -1.29482, Convergence: 0.029537\n",
      "Epoch: 102, Loss: 1186.01228, Residuals: -1.30674, Convergence: 0.023481\n",
      "Epoch: 103, Loss: 1163.49543, Residuals: -1.31553, Convergence: 0.019353\n",
      "Epoch: 104, Loss: 1144.81486, Residuals: -1.32224, Convergence: 0.016318\n",
      "Epoch: 105, Loss: 1129.06783, Residuals: -1.32731, Convergence: 0.013947\n",
      "Epoch: 106, Loss: 1115.63526, Residuals: -1.33099, Convergence: 0.012040\n",
      "Epoch: 107, Loss: 1104.05577, Residuals: -1.33345, Convergence: 0.010488\n",
      "Epoch: 108, Loss: 1093.96836, Residuals: -1.33481, Convergence: 0.009221\n",
      "Epoch: 109, Loss: 1085.08282, Residuals: -1.33520, Convergence: 0.008189\n",
      "Epoch: 110, Loss: 1077.16010, Residuals: -1.33470, Convergence: 0.007355\n",
      "Epoch: 111, Loss: 1070.00018, Residuals: -1.33338, Convergence: 0.006692\n",
      "Epoch: 112, Loss: 1063.43039, Residuals: -1.33130, Convergence: 0.006178\n",
      "Epoch: 113, Loss: 1057.29981, Residuals: -1.32850, Convergence: 0.005798\n",
      "Epoch: 114, Loss: 1051.47304, Residuals: -1.32500, Convergence: 0.005542\n",
      "Epoch: 115, Loss: 1045.82887, Residuals: -1.32079, Convergence: 0.005397\n",
      "Epoch: 116, Loss: 1040.26200, Residuals: -1.31589, Convergence: 0.005351\n",
      "Epoch: 117, Loss: 1034.68983, Residuals: -1.31030, Convergence: 0.005385\n",
      "Epoch: 118, Loss: 1029.06610, Residuals: -1.30408, Convergence: 0.005465\n",
      "Epoch: 119, Loss: 1023.39766, Residuals: -1.29730, Convergence: 0.005539\n",
      "Epoch: 120, Loss: 1017.74905, Residuals: -1.29005, Convergence: 0.005550\n",
      "Epoch: 121, Loss: 1012.22382, Residuals: -1.28243, Convergence: 0.005459\n",
      "Epoch: 122, Loss: 1006.92697, Residuals: -1.27455, Convergence: 0.005260\n",
      "Epoch: 123, Loss: 1001.93016, Residuals: -1.26650, Convergence: 0.004987\n",
      "Epoch: 124, Loss: 997.26378, Residuals: -1.25834, Convergence: 0.004679\n",
      "Epoch: 125, Loss: 992.92416, Residuals: -1.25016, Convergence: 0.004371\n",
      "Epoch: 126, Loss: 988.88926, Residuals: -1.24201, Convergence: 0.004080\n",
      "Epoch: 127, Loss: 985.12962, Residuals: -1.23393, Convergence: 0.003816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 128, Loss: 981.61454, Residuals: -1.22597, Convergence: 0.003581\n",
      "Epoch: 129, Loss: 978.31686, Residuals: -1.21815, Convergence: 0.003371\n",
      "Epoch: 130, Loss: 975.21167, Residuals: -1.21051, Convergence: 0.003184\n",
      "Epoch: 131, Loss: 972.27873, Residuals: -1.20306, Convergence: 0.003017\n",
      "Epoch: 132, Loss: 969.50105, Residuals: -1.19582, Convergence: 0.002865\n",
      "Epoch: 133, Loss: 966.86478, Residuals: -1.18880, Convergence: 0.002727\n",
      "Epoch: 134, Loss: 964.35866, Residuals: -1.18202, Convergence: 0.002599\n",
      "Epoch: 135, Loss: 961.97309, Residuals: -1.17548, Convergence: 0.002480\n",
      "Epoch: 136, Loss: 959.70045, Residuals: -1.16919, Convergence: 0.002368\n",
      "Epoch: 137, Loss: 957.53420, Residuals: -1.16315, Convergence: 0.002262\n",
      "Epoch: 138, Loss: 955.46828, Residuals: -1.15736, Convergence: 0.002162\n",
      "Epoch: 139, Loss: 953.49696, Residuals: -1.15183, Convergence: 0.002067\n",
      "Epoch: 140, Loss: 951.61518, Residuals: -1.14654, Convergence: 0.001977\n",
      "Epoch: 141, Loss: 949.81764, Residuals: -1.14150, Convergence: 0.001893\n",
      "Epoch: 142, Loss: 948.09949, Residuals: -1.13669, Convergence: 0.001812\n",
      "Epoch: 143, Loss: 946.45554, Residuals: -1.13211, Convergence: 0.001737\n",
      "Epoch: 144, Loss: 944.88151, Residuals: -1.12774, Convergence: 0.001666\n",
      "Epoch: 145, Loss: 943.37230, Residuals: -1.12359, Convergence: 0.001600\n",
      "Epoch: 146, Loss: 941.92342, Residuals: -1.11963, Convergence: 0.001538\n",
      "Epoch: 147, Loss: 940.53048, Residuals: -1.11585, Convergence: 0.001481\n",
      "Epoch: 148, Loss: 939.18917, Residuals: -1.11225, Convergence: 0.001428\n",
      "Epoch: 149, Loss: 937.89598, Residuals: -1.10882, Convergence: 0.001379\n",
      "Epoch: 150, Loss: 936.64690, Residuals: -1.10554, Convergence: 0.001334\n",
      "Epoch: 151, Loss: 935.43865, Residuals: -1.10240, Convergence: 0.001292\n",
      "Epoch: 152, Loss: 934.26825, Residuals: -1.09940, Convergence: 0.001253\n",
      "Epoch: 153, Loss: 933.13245, Residuals: -1.09653, Convergence: 0.001217\n",
      "Epoch: 154, Loss: 932.02874, Residuals: -1.09377, Convergence: 0.001184\n",
      "Epoch: 155, Loss: 930.95456, Residuals: -1.09112, Convergence: 0.001154\n",
      "Epoch: 156, Loss: 929.90790, Residuals: -1.08857, Convergence: 0.001126\n",
      "Epoch: 157, Loss: 928.88643, Residuals: -1.08611, Convergence: 0.001100\n",
      "Epoch: 158, Loss: 927.88802, Residuals: -1.08374, Convergence: 0.001076\n",
      "Epoch: 159, Loss: 926.91082, Residuals: -1.08145, Convergence: 0.001054\n",
      "Epoch: 160, Loss: 925.95304, Residuals: -1.07923, Convergence: 0.001034\n",
      "Epoch: 161, Loss: 925.01242, Residuals: -1.07707, Convergence: 0.001017\n",
      "Epoch: 162, Loss: 924.08744, Residuals: -1.07497, Convergence: 0.001001\n",
      "Epoch: 163, Loss: 923.17574, Residuals: -1.07293, Convergence: 0.000988\n",
      "Evidence 11128.489\n",
      "\n",
      "Epoch: 163, Evidence: 11128.48926, Convergence: 1.016507\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 5.76e-01\n",
      "Epoch: 163, Loss: 2345.63234, Residuals: -1.07293, Convergence:   inf\n",
      "Epoch: 164, Loss: 2306.46109, Residuals: -1.08492, Convergence: 0.016983\n",
      "Epoch: 165, Loss: 2278.24104, Residuals: -1.08420, Convergence: 0.012387\n",
      "Epoch: 166, Loss: 2254.50627, Residuals: -1.08220, Convergence: 0.010528\n",
      "Epoch: 167, Loss: 2234.27638, Residuals: -1.07973, Convergence: 0.009054\n",
      "Epoch: 168, Loss: 2216.87735, Residuals: -1.07696, Convergence: 0.007848\n",
      "Epoch: 169, Loss: 2201.79633, Residuals: -1.07395, Convergence: 0.006849\n",
      "Epoch: 170, Loss: 2188.61976, Residuals: -1.07077, Convergence: 0.006020\n",
      "Epoch: 171, Loss: 2177.00697, Residuals: -1.06746, Convergence: 0.005334\n",
      "Epoch: 172, Loss: 2166.66971, Residuals: -1.06403, Convergence: 0.004771\n",
      "Epoch: 173, Loss: 2157.36336, Residuals: -1.06050, Convergence: 0.004314\n",
      "Epoch: 174, Loss: 2148.88288, Residuals: -1.05685, Convergence: 0.003946\n",
      "Epoch: 175, Loss: 2141.06602, Residuals: -1.05307, Convergence: 0.003651\n",
      "Epoch: 176, Loss: 2133.80151, Residuals: -1.04917, Convergence: 0.003404\n",
      "Epoch: 177, Loss: 2127.02354, Residuals: -1.04517, Convergence: 0.003187\n",
      "Epoch: 178, Loss: 2120.69887, Residuals: -1.04112, Convergence: 0.002982\n",
      "Epoch: 179, Loss: 2114.80883, Residuals: -1.03707, Convergence: 0.002785\n",
      "Epoch: 180, Loss: 2109.33708, Residuals: -1.03309, Convergence: 0.002594\n",
      "Epoch: 181, Loss: 2104.26243, Residuals: -1.02921, Convergence: 0.002412\n",
      "Epoch: 182, Loss: 2099.56132, Residuals: -1.02546, Convergence: 0.002239\n",
      "Epoch: 183, Loss: 2095.20879, Residuals: -1.02188, Convergence: 0.002077\n",
      "Epoch: 184, Loss: 2091.18086, Residuals: -1.01847, Convergence: 0.001926\n",
      "Epoch: 185, Loss: 2087.45293, Residuals: -1.01524, Convergence: 0.001786\n",
      "Epoch: 186, Loss: 2084.00299, Residuals: -1.01219, Convergence: 0.001655\n",
      "Epoch: 187, Loss: 2080.80976, Residuals: -1.00931, Convergence: 0.001535\n",
      "Epoch: 188, Loss: 2077.85279, Residuals: -1.00661, Convergence: 0.001423\n",
      "Epoch: 189, Loss: 2075.11270, Residuals: -1.00407, Convergence: 0.001320\n",
      "Epoch: 190, Loss: 2072.57031, Residuals: -1.00168, Convergence: 0.001227\n",
      "Epoch: 191, Loss: 2070.20977, Residuals: -0.99943, Convergence: 0.001140\n",
      "Epoch: 192, Loss: 2068.01348, Residuals: -0.99731, Convergence: 0.001062\n",
      "Epoch: 193, Loss: 2065.96713, Residuals: -0.99531, Convergence: 0.000991\n",
      "Evidence 14394.623\n",
      "\n",
      "Epoch: 193, Evidence: 14394.62305, Convergence: 0.226900\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 4.39e-01\n",
      "Epoch: 193, Loss: 2479.73608, Residuals: -0.99531, Convergence:   inf\n",
      "Epoch: 194, Loss: 2465.52867, Residuals: -0.99286, Convergence: 0.005762\n",
      "Epoch: 195, Loss: 2453.94525, Residuals: -0.98963, Convergence: 0.004720\n",
      "Epoch: 196, Loss: 2444.02931, Residuals: -0.98645, Convergence: 0.004057\n",
      "Epoch: 197, Loss: 2435.47263, Residuals: -0.98342, Convergence: 0.003513\n",
      "Epoch: 198, Loss: 2428.04346, Residuals: -0.98059, Convergence: 0.003060\n",
      "Epoch: 199, Loss: 2421.55546, Residuals: -0.97797, Convergence: 0.002679\n",
      "Epoch: 200, Loss: 2415.85520, Residuals: -0.97555, Convergence: 0.002360\n",
      "Epoch: 201, Loss: 2410.81588, Residuals: -0.97333, Convergence: 0.002090\n",
      "Epoch: 202, Loss: 2406.33081, Residuals: -0.97129, Convergence: 0.001864\n",
      "Epoch: 203, Loss: 2402.31418, Residuals: -0.96941, Convergence: 0.001672\n",
      "Epoch: 204, Loss: 2398.69297, Residuals: -0.96769, Convergence: 0.001510\n",
      "Epoch: 205, Loss: 2395.40734, Residuals: -0.96610, Convergence: 0.001372\n",
      "Epoch: 206, Loss: 2392.40928, Residuals: -0.96464, Convergence: 0.001253\n",
      "Epoch: 207, Loss: 2389.65740, Residuals: -0.96329, Convergence: 0.001152\n",
      "Epoch: 208, Loss: 2387.11919, Residuals: -0.96204, Convergence: 0.001063\n",
      "Epoch: 209, Loss: 2384.76699, Residuals: -0.96089, Convergence: 0.000986\n",
      "Evidence 14815.918\n",
      "\n",
      "Epoch: 209, Evidence: 14815.91797, Convergence: 0.028435\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 3.35e-01\n",
      "Epoch: 209, Loss: 2486.03019, Residuals: -0.96089, Convergence:   inf\n",
      "Epoch: 210, Loss: 2479.46315, Residuals: -0.95803, Convergence: 0.002649\n",
      "Epoch: 211, Loss: 2474.02285, Residuals: -0.95542, Convergence: 0.002199\n",
      "Epoch: 212, Loss: 2469.39623, Residuals: -0.95315, Convergence: 0.001874\n",
      "Epoch: 213, Loss: 2465.40647, Residuals: -0.95118, Convergence: 0.001618\n",
      "Epoch: 214, Loss: 2461.91858, Residuals: -0.94947, Convergence: 0.001417\n",
      "Epoch: 215, Loss: 2458.82962, Residuals: -0.94800, Convergence: 0.001256\n",
      "Epoch: 216, Loss: 2456.06028, Residuals: -0.94672, Convergence: 0.001128\n",
      "Epoch: 217, Loss: 2453.55073, Residuals: -0.94561, Convergence: 0.001023\n",
      "Epoch: 218, Loss: 2451.25501, Residuals: -0.94464, Convergence: 0.000937\n",
      "Evidence 14901.772\n",
      "\n",
      "Epoch: 218, Evidence: 14901.77246, Convergence: 0.005761\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.62e-01\n",
      "Epoch: 218, Loss: 2487.65148, Residuals: -0.94464, Convergence:   inf\n",
      "Epoch: 219, Loss: 2483.78354, Residuals: -0.94245, Convergence: 0.001557\n",
      "Epoch: 220, Loss: 2480.54189, Residuals: -0.94063, Convergence: 0.001307\n",
      "Epoch: 221, Loss: 2477.74476, Residuals: -0.93913, Convergence: 0.001129\n",
      "Epoch: 222, Loss: 2475.28292, Residuals: -0.93791, Convergence: 0.000995\n",
      "Evidence 14930.754\n",
      "\n",
      "Epoch: 222, Evidence: 14930.75391, Convergence: 0.001941\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.12e-01\n",
      "Epoch: 222, Loss: 2488.76784, Residuals: -0.93791, Convergence:   inf\n",
      "Epoch: 223, Loss: 2485.83479, Residuals: -0.93615, Convergence: 0.001180\n",
      "Epoch: 224, Loss: 2483.35234, Residuals: -0.93475, Convergence: 0.001000\n",
      "Evidence 14943.009\n",
      "\n",
      "Epoch: 224, Evidence: 14943.00879, Convergence: 0.000820\n",
      "Updating hyper-parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 183, Updated regularization: 1.76e-01\n",
      "Epoch: 224, Loss: 2489.60427, Residuals: -0.93475, Convergence:   inf\n",
      "Epoch: 225, Loss: 2484.87934, Residuals: -0.93243, Convergence: 0.001901\n",
      "Epoch: 226, Loss: 2481.27267, Residuals: -0.93083, Convergence: 0.001454\n",
      "Epoch: 227, Loss: 2478.31488, Residuals: -0.92976, Convergence: 0.001193\n",
      "Epoch: 228, Loss: 2475.78669, Residuals: -0.92907, Convergence: 0.001021\n",
      "Epoch: 229, Loss: 2473.55932, Residuals: -0.92861, Convergence: 0.000900\n",
      "Evidence 14963.839\n",
      "\n",
      "Epoch: 229, Evidence: 14963.83887, Convergence: 0.002211\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.46e-01\n",
      "Epoch: 229, Loss: 2489.67063, Residuals: -0.92861, Convergence:   inf\n",
      "Epoch: 230, Loss: 2486.67262, Residuals: -0.92668, Convergence: 0.001206\n",
      "Epoch: 231, Loss: 2484.26787, Residuals: -0.92574, Convergence: 0.000968\n",
      "Evidence 14975.555\n",
      "\n",
      "Epoch: 231, Evidence: 14975.55469, Convergence: 0.000782\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.24e-01\n",
      "Epoch: 231, Loss: 2489.97476, Residuals: -0.92574, Convergence:   inf\n",
      "Epoch: 232, Loss: 2485.48435, Residuals: -0.92341, Convergence: 0.001807\n",
      "Epoch: 233, Loss: 2482.22345, Residuals: -0.92460, Convergence: 0.001314\n",
      "Epoch: 234, Loss: 2479.55280, Residuals: -0.92509, Convergence: 0.001077\n",
      "Epoch: 235, Loss: 2477.15109, Residuals: -0.92737, Convergence: 0.000970\n",
      "Evidence 14991.565\n",
      "\n",
      "Epoch: 235, Evidence: 14991.56543, Convergence: 0.001849\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.10e-01\n",
      "Epoch: 235, Loss: 2489.44354, Residuals: -0.92737, Convergence:   inf\n",
      "Epoch: 236, Loss: 2487.49852, Residuals: -0.92765, Convergence: 0.000782\n",
      "Evidence 14998.240\n",
      "\n",
      "Epoch: 236, Evidence: 14998.24023, Convergence: 0.000445\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 9.02e-02\n",
      "Epoch: 236, Loss: 2490.41820, Residuals: -0.92765, Convergence:   inf\n",
      "Epoch: 237, Loss: 2535.10836, Residuals: -0.97408, Convergence: -0.017628\n",
      "Epoch: 237, Loss: 2487.92285, Residuals: -0.92658, Convergence: 0.001003\n",
      "Epoch: 238, Loss: 2487.79091, Residuals: -0.92964, Convergence: 0.000053\n",
      "Evidence 15003.127\n",
      "\n",
      "Epoch: 238, Evidence: 15003.12695, Convergence: 0.000771\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.37e-02\n",
      "Epoch: 238, Loss: 2490.42628, Residuals: -0.92964, Convergence:   inf\n",
      "Epoch: 239, Loss: 2550.07842, Residuals: -0.98688, Convergence: -0.023392\n",
      "Epoch: 239, Loss: 2487.82276, Residuals: -0.92695, Convergence: 0.001047\n",
      "Epoch: 240, Loss: 2487.61399, Residuals: -0.92837, Convergence: 0.000084\n",
      "Evidence 15008.272\n",
      "\n",
      "Epoch: 240, Evidence: 15008.27246, Convergence: 0.001113\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 6.35e-02\n",
      "Epoch: 240, Loss: 2490.24467, Residuals: -0.92837, Convergence:   inf\n",
      "Epoch: 241, Loss: 2487.92096, Residuals: -0.92735, Convergence: 0.000934\n",
      "Evidence 15012.165\n",
      "\n",
      "Epoch: 241, Evidence: 15012.16504, Convergence: 0.000259\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 5.94e-02\n",
      "Epoch: 241, Loss: 2489.99267, Residuals: -0.92735, Convergence:   inf\n",
      "Epoch: 242, Loss: 2495.30092, Residuals: -0.94306, Convergence: -0.002127\n",
      "Epoch: 242, Loss: 2489.50128, Residuals: -0.92691, Convergence: 0.000197\n",
      "Evidence 15013.856\n",
      "\n",
      "Epoch: 242, Evidence: 15013.85645, Convergence: 0.000372\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 5.25e-02\n",
      "Epoch: 242, Loss: 2490.37975, Residuals: -0.92691, Convergence:   inf\n",
      "Epoch: 243, Loss: 2547.05885, Residuals: -0.98595, Convergence: -0.022253\n",
      "Epoch: 243, Loss: 2488.63965, Residuals: -0.92732, Convergence: 0.000699\n",
      "Evidence 15016.491\n",
      "\n",
      "Epoch: 243, Evidence: 15016.49121, Convergence: 0.000547\n",
      "Total samples: 183, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 381.52373, Residuals: -4.51421, Convergence:   inf\n",
      "Epoch: 1, Loss: 355.75990, Residuals: -4.39415, Convergence: 0.072419\n",
      "Epoch: 2, Loss: 334.64282, Residuals: -4.22928, Convergence: 0.063103\n",
      "Epoch: 3, Loss: 318.56965, Residuals: -4.06464, Convergence: 0.050454\n",
      "Epoch: 4, Loss: 306.30235, Residuals: -3.91946, Convergence: 0.040050\n",
      "Epoch: 5, Loss: 296.56601, Residuals: -3.79106, Convergence: 0.032830\n",
      "Epoch: 6, Loss: 288.65635, Residuals: -3.67924, Convergence: 0.027402\n",
      "Epoch: 7, Loss: 282.08548, Residuals: -3.58355, Convergence: 0.023294\n",
      "Epoch: 8, Loss: 276.49206, Residuals: -3.50192, Convergence: 0.020230\n",
      "Epoch: 9, Loss: 271.62130, Residuals: -3.43203, Convergence: 0.017932\n",
      "Epoch: 10, Loss: 267.29230, Residuals: -3.37177, Convergence: 0.016196\n",
      "Epoch: 11, Loss: 263.37421, Residuals: -3.31933, Convergence: 0.014877\n",
      "Epoch: 12, Loss: 259.77160, Residuals: -3.27318, Convergence: 0.013868\n",
      "Epoch: 13, Loss: 256.41487, Residuals: -3.23198, Convergence: 0.013091\n",
      "Epoch: 14, Loss: 253.25350, Residuals: -3.19458, Convergence: 0.012483\n",
      "Epoch: 15, Loss: 250.25189, Residuals: -3.15999, Convergence: 0.011994\n",
      "Epoch: 16, Loss: 247.38742, Residuals: -3.12752, Convergence: 0.011579\n",
      "Epoch: 17, Loss: 244.64497, Residuals: -3.09670, Convergence: 0.011210\n",
      "Epoch: 18, Loss: 242.00555, Residuals: -3.06719, Convergence: 0.010906\n",
      "Epoch: 19, Loss: 239.43991, Residuals: -3.03851, Convergence: 0.010715\n",
      "Epoch: 20, Loss: 236.91272, Residuals: -3.01009, Convergence: 0.010667\n",
      "Epoch: 21, Loss: 234.39078, Residuals: -2.98139, Convergence: 0.010760\n",
      "Epoch: 22, Loss: 231.84847, Residuals: -2.95198, Convergence: 0.010965\n",
      "Epoch: 23, Loss: 229.26294, Residuals: -2.92156, Convergence: 0.011278\n",
      "Epoch: 24, Loss: 226.59829, Residuals: -2.88969, Convergence: 0.011759\n",
      "Epoch: 25, Loss: 223.80056, Residuals: -2.85572, Convergence: 0.012501\n",
      "Epoch: 26, Loss: 220.84551, Residuals: -2.81928, Convergence: 0.013381\n",
      "Epoch: 27, Loss: 217.82815, Residuals: -2.78125, Convergence: 0.013852\n",
      "Epoch: 28, Loss: 214.88195, Residuals: -2.74313, Convergence: 0.013711\n",
      "Epoch: 29, Loss: 212.04851, Residuals: -2.70553, Convergence: 0.013362\n",
      "Epoch: 30, Loss: 209.31940, Residuals: -2.66848, Convergence: 0.013038\n",
      "Epoch: 31, Loss: 206.67934, Residuals: -2.63189, Convergence: 0.012774\n",
      "Epoch: 32, Loss: 204.11618, Residuals: -2.59571, Convergence: 0.012557\n",
      "Epoch: 33, Loss: 201.62147, Residuals: -2.55989, Convergence: 0.012373\n",
      "Epoch: 34, Loss: 199.18947, Residuals: -2.52441, Convergence: 0.012209\n",
      "Epoch: 35, Loss: 196.81644, Residuals: -2.48926, Convergence: 0.012057\n",
      "Epoch: 36, Loss: 194.49993, Residuals: -2.45444, Convergence: 0.011910\n",
      "Epoch: 37, Loss: 192.23850, Residuals: -2.41992, Convergence: 0.011764\n",
      "Epoch: 38, Loss: 190.03142, Residuals: -2.38569, Convergence: 0.011614\n",
      "Epoch: 39, Loss: 187.87847, Residuals: -2.35176, Convergence: 0.011459\n",
      "Epoch: 40, Loss: 185.77967, Residuals: -2.31812, Convergence: 0.011297\n",
      "Epoch: 41, Loss: 183.73513, Residuals: -2.28477, Convergence: 0.011128\n",
      "Epoch: 42, Loss: 181.74483, Residuals: -2.25171, Convergence: 0.010951\n",
      "Epoch: 43, Loss: 179.80855, Residuals: -2.21893, Convergence: 0.010769\n",
      "Epoch: 44, Loss: 177.92590, Residuals: -2.18645, Convergence: 0.010581\n",
      "Epoch: 45, Loss: 176.09644, Residuals: -2.15427, Convergence: 0.010389\n",
      "Epoch: 46, Loss: 174.31998, Residuals: -2.12237, Convergence: 0.010191\n",
      "Epoch: 47, Loss: 172.59687, Residuals: -2.09077, Convergence: 0.009983\n",
      "Epoch: 48, Loss: 170.92813, Residuals: -2.05948, Convergence: 0.009763\n",
      "Epoch: 49, Loss: 169.31541, Residuals: -2.02853, Convergence: 0.009525\n",
      "Epoch: 50, Loss: 167.76068, Residuals: -1.99795, Convergence: 0.009268\n",
      "Epoch: 51, Loss: 166.26582, Residuals: -1.96779, Convergence: 0.008991\n",
      "Epoch: 52, Loss: 164.83222, Residuals: -1.93811, Convergence: 0.008697\n",
      "Epoch: 53, Loss: 163.46061, Residuals: -1.90894, Convergence: 0.008391\n",
      "Epoch: 54, Loss: 162.15100, Residuals: -1.88035, Convergence: 0.008076\n",
      "Epoch: 55, Loss: 160.90273, Residuals: -1.85236, Convergence: 0.007758\n",
      "Epoch: 56, Loss: 159.71454, Residuals: -1.82501, Convergence: 0.007439\n",
      "Epoch: 57, Loss: 158.58479, Residuals: -1.79834, Convergence: 0.007124\n",
      "Epoch: 58, Loss: 157.51145, Residuals: -1.77236, Convergence: 0.006814\n",
      "Epoch: 59, Loss: 156.49223, Residuals: -1.74709, Convergence: 0.006513\n",
      "Epoch: 60, Loss: 155.52466, Residuals: -1.72254, Convergence: 0.006221\n",
      "Epoch: 61, Loss: 154.60613, Residuals: -1.69872, Convergence: 0.005941\n",
      "Epoch: 62, Loss: 153.73397, Residuals: -1.67562, Convergence: 0.005673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 63, Loss: 152.90555, Residuals: -1.65323, Convergence: 0.005418\n",
      "Epoch: 64, Loss: 152.11835, Residuals: -1.63154, Convergence: 0.005175\n",
      "Epoch: 65, Loss: 151.37001, Residuals: -1.61055, Convergence: 0.004944\n",
      "Epoch: 66, Loss: 150.65841, Residuals: -1.59024, Convergence: 0.004723\n",
      "Epoch: 67, Loss: 149.98161, Residuals: -1.57060, Convergence: 0.004513\n",
      "Epoch: 68, Loss: 149.33793, Residuals: -1.55162, Convergence: 0.004310\n",
      "Epoch: 69, Loss: 148.72585, Residuals: -1.53329, Convergence: 0.004116\n",
      "Epoch: 70, Loss: 148.14400, Residuals: -1.51560, Convergence: 0.003928\n",
      "Epoch: 71, Loss: 147.59114, Residuals: -1.49855, Convergence: 0.003746\n",
      "Epoch: 72, Loss: 147.06609, Residuals: -1.48212, Convergence: 0.003570\n",
      "Epoch: 73, Loss: 146.56772, Residuals: -1.46631, Convergence: 0.003400\n",
      "Epoch: 74, Loss: 146.09496, Residuals: -1.45111, Convergence: 0.003236\n",
      "Epoch: 75, Loss: 145.64673, Residuals: -1.43649, Convergence: 0.003077\n",
      "Epoch: 76, Loss: 145.22199, Residuals: -1.42246, Convergence: 0.002925\n",
      "Epoch: 77, Loss: 144.81969, Residuals: -1.40900, Convergence: 0.002778\n",
      "Epoch: 78, Loss: 144.43880, Residuals: -1.39609, Convergence: 0.002637\n",
      "Epoch: 79, Loss: 144.07830, Residuals: -1.38371, Convergence: 0.002502\n",
      "Epoch: 80, Loss: 143.73720, Residuals: -1.37185, Convergence: 0.002373\n",
      "Epoch: 81, Loss: 143.41451, Residuals: -1.36050, Convergence: 0.002250\n",
      "Epoch: 82, Loss: 143.10927, Residuals: -1.34962, Convergence: 0.002133\n",
      "Epoch: 83, Loss: 142.82056, Residuals: -1.33921, Convergence: 0.002021\n",
      "Epoch: 84, Loss: 142.54750, Residuals: -1.32924, Convergence: 0.001916\n",
      "Epoch: 85, Loss: 142.28922, Residuals: -1.31970, Convergence: 0.001815\n",
      "Epoch: 86, Loss: 142.04493, Residuals: -1.31056, Convergence: 0.001720\n",
      "Epoch: 87, Loss: 141.81383, Residuals: -1.30182, Convergence: 0.001630\n",
      "Epoch: 88, Loss: 141.59523, Residuals: -1.29345, Convergence: 0.001544\n",
      "Epoch: 89, Loss: 141.38844, Residuals: -1.28543, Convergence: 0.001463\n",
      "Epoch: 90, Loss: 141.19282, Residuals: -1.27776, Convergence: 0.001385\n",
      "Epoch: 91, Loss: 141.00779, Residuals: -1.27040, Convergence: 0.001312\n",
      "Epoch: 92, Loss: 140.83281, Residuals: -1.26336, Convergence: 0.001242\n",
      "Epoch: 93, Loss: 140.66739, Residuals: -1.25661, Convergence: 0.001176\n",
      "Epoch: 94, Loss: 140.51109, Residuals: -1.25014, Convergence: 0.001112\n",
      "Epoch: 95, Loss: 140.36349, Residuals: -1.24395, Convergence: 0.001052\n",
      "Epoch: 96, Loss: 140.22421, Residuals: -1.23801, Convergence: 0.000993\n",
      "Evidence -181.832\n",
      "\n",
      "Epoch: 96, Evidence: -181.83176, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.24e-01\n",
      "Epoch: 96, Loss: 1365.89948, Residuals: -1.23801, Convergence:   inf\n",
      "Epoch: 97, Loss: 1304.11816, Residuals: -1.26850, Convergence: 0.047374\n",
      "Epoch: 98, Loss: 1256.86125, Residuals: -1.29179, Convergence: 0.037599\n",
      "Epoch: 99, Loss: 1221.00093, Residuals: -1.30812, Convergence: 0.029370\n",
      "Epoch: 100, Loss: 1193.11309, Residuals: -1.31929, Convergence: 0.023374\n",
      "Epoch: 101, Loss: 1170.63104, Residuals: -1.32727, Convergence: 0.019205\n",
      "Epoch: 102, Loss: 1152.02942, Residuals: -1.33318, Convergence: 0.016147\n",
      "Epoch: 103, Loss: 1136.38042, Residuals: -1.33748, Convergence: 0.013771\n",
      "Epoch: 104, Loss: 1123.05481, Residuals: -1.34043, Convergence: 0.011866\n",
      "Epoch: 105, Loss: 1111.58670, Residuals: -1.34219, Convergence: 0.010317\n",
      "Epoch: 106, Loss: 1101.61316, Residuals: -1.34289, Convergence: 0.009054\n",
      "Epoch: 107, Loss: 1092.84082, Residuals: -1.34263, Convergence: 0.008027\n",
      "Epoch: 108, Loss: 1085.02682, Residuals: -1.34150, Convergence: 0.007202\n",
      "Epoch: 109, Loss: 1077.96414, Residuals: -1.33956, Convergence: 0.006552\n",
      "Epoch: 110, Loss: 1071.47239, Residuals: -1.33688, Convergence: 0.006059\n",
      "Epoch: 111, Loss: 1065.38998, Residuals: -1.33347, Convergence: 0.005709\n",
      "Epoch: 112, Loss: 1059.57134, Residuals: -1.32937, Convergence: 0.005492\n",
      "Epoch: 113, Loss: 1053.88711, Residuals: -1.32457, Convergence: 0.005394\n",
      "Epoch: 114, Loss: 1048.22830, Residuals: -1.31908, Convergence: 0.005398\n",
      "Epoch: 115, Loss: 1042.51406, Residuals: -1.31293, Convergence: 0.005481\n",
      "Epoch: 116, Loss: 1036.70237, Residuals: -1.30616, Convergence: 0.005606\n",
      "Epoch: 117, Loss: 1030.80445, Residuals: -1.29885, Convergence: 0.005722\n",
      "Epoch: 118, Loss: 1024.88909, Residuals: -1.29109, Convergence: 0.005772\n",
      "Epoch: 119, Loss: 1019.06716, Residuals: -1.28299, Convergence: 0.005713\n",
      "Epoch: 120, Loss: 1013.45659, Residuals: -1.27463, Convergence: 0.005536\n",
      "Epoch: 121, Loss: 1008.14437, Residuals: -1.26611, Convergence: 0.005269\n",
      "Epoch: 122, Loss: 1003.17331, Residuals: -1.25750, Convergence: 0.004955\n",
      "Epoch: 123, Loss: 998.54772, Residuals: -1.24887, Convergence: 0.004632\n",
      "Epoch: 124, Loss: 994.24926, Residuals: -1.24028, Convergence: 0.004323\n",
      "Epoch: 125, Loss: 990.25001, Residuals: -1.23177, Convergence: 0.004039\n",
      "Epoch: 126, Loss: 986.52044, Residuals: -1.22340, Convergence: 0.003781\n",
      "Epoch: 127, Loss: 983.03234, Residuals: -1.21519, Convergence: 0.003548\n",
      "Epoch: 128, Loss: 979.76209, Residuals: -1.20717, Convergence: 0.003338\n",
      "Epoch: 129, Loss: 976.68950, Residuals: -1.19938, Convergence: 0.003146\n",
      "Epoch: 130, Loss: 973.79720, Residuals: -1.19182, Convergence: 0.002970\n",
      "Epoch: 131, Loss: 971.07161, Residuals: -1.18451, Convergence: 0.002807\n",
      "Epoch: 132, Loss: 968.50049, Residuals: -1.17747, Convergence: 0.002655\n",
      "Epoch: 133, Loss: 966.07284, Residuals: -1.17070, Convergence: 0.002513\n",
      "Epoch: 134, Loss: 963.77954, Residuals: -1.16420, Convergence: 0.002379\n",
      "Epoch: 135, Loss: 961.61182, Residuals: -1.15797, Convergence: 0.002254\n",
      "Epoch: 136, Loss: 959.56172, Residuals: -1.15202, Convergence: 0.002136\n",
      "Epoch: 137, Loss: 957.62183, Residuals: -1.14634, Convergence: 0.002026\n",
      "Epoch: 138, Loss: 955.78426, Residuals: -1.14092, Convergence: 0.001923\n",
      "Epoch: 139, Loss: 954.04284, Residuals: -1.13576, Convergence: 0.001825\n",
      "Epoch: 140, Loss: 952.39050, Residuals: -1.13084, Convergence: 0.001735\n",
      "Epoch: 141, Loss: 950.82073, Residuals: -1.12617, Convergence: 0.001651\n",
      "Epoch: 142, Loss: 949.32770, Residuals: -1.12172, Convergence: 0.001573\n",
      "Epoch: 143, Loss: 947.90557, Residuals: -1.11749, Convergence: 0.001500\n",
      "Epoch: 144, Loss: 946.54865, Residuals: -1.11347, Convergence: 0.001434\n",
      "Epoch: 145, Loss: 945.25166, Residuals: -1.10965, Convergence: 0.001372\n",
      "Epoch: 146, Loss: 944.01006, Residuals: -1.10601, Convergence: 0.001315\n",
      "Epoch: 147, Loss: 942.81897, Residuals: -1.10255, Convergence: 0.001263\n",
      "Epoch: 148, Loss: 941.67405, Residuals: -1.09926, Convergence: 0.001216\n",
      "Epoch: 149, Loss: 940.57124, Residuals: -1.09613, Convergence: 0.001172\n",
      "Epoch: 150, Loss: 939.50651, Residuals: -1.09314, Convergence: 0.001133\n",
      "Epoch: 151, Loss: 938.47667, Residuals: -1.09029, Convergence: 0.001097\n",
      "Epoch: 152, Loss: 937.47807, Residuals: -1.08757, Convergence: 0.001065\n",
      "Epoch: 153, Loss: 936.50752, Residuals: -1.08497, Convergence: 0.001036\n",
      "Epoch: 154, Loss: 935.56199, Residuals: -1.08248, Convergence: 0.001011\n",
      "Epoch: 155, Loss: 934.63871, Residuals: -1.08009, Convergence: 0.000988\n",
      "Evidence 11084.808\n",
      "\n",
      "Epoch: 155, Evidence: 11084.80762, Convergence: 1.016404\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 5.78e-01\n",
      "Epoch: 155, Loss: 2347.51103, Residuals: -1.08009, Convergence:   inf\n",
      "Epoch: 156, Loss: 2305.53473, Residuals: -1.09064, Convergence: 0.018207\n",
      "Epoch: 157, Loss: 2276.63421, Residuals: -1.09149, Convergence: 0.012694\n",
      "Epoch: 158, Loss: 2252.12571, Residuals: -1.09084, Convergence: 0.010882\n",
      "Epoch: 159, Loss: 2230.97760, Residuals: -1.08951, Convergence: 0.009479\n",
      "Epoch: 160, Loss: 2212.60384, Residuals: -1.08767, Convergence: 0.008304\n",
      "Epoch: 161, Loss: 2196.55021, Residuals: -1.08543, Convergence: 0.007309\n",
      "Epoch: 162, Loss: 2182.43787, Residuals: -1.08283, Convergence: 0.006466\n",
      "Epoch: 163, Loss: 2169.94473, Residuals: -1.07994, Convergence: 0.005757\n",
      "Epoch: 164, Loss: 2158.79516, Residuals: -1.07680, Convergence: 0.005165\n",
      "Epoch: 165, Loss: 2148.75209, Residuals: -1.07344, Convergence: 0.004674\n",
      "Epoch: 166, Loss: 2139.61755, Residuals: -1.06989, Convergence: 0.004269\n",
      "Epoch: 167, Loss: 2131.22891, Residuals: -1.06616, Convergence: 0.003936\n",
      "Epoch: 168, Loss: 2123.46799, Residuals: -1.06227, Convergence: 0.003655\n",
      "Epoch: 169, Loss: 2116.26288, Residuals: -1.05825, Convergence: 0.003405\n",
      "Epoch: 170, Loss: 2109.57705, Residuals: -1.05416, Convergence: 0.003169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 171, Loss: 2103.39074, Residuals: -1.05005, Convergence: 0.002941\n",
      "Epoch: 172, Loss: 2097.68669, Residuals: -1.04598, Convergence: 0.002719\n",
      "Epoch: 173, Loss: 2092.44011, Residuals: -1.04198, Convergence: 0.002507\n",
      "Epoch: 174, Loss: 2087.61904, Residuals: -1.03811, Convergence: 0.002309\n",
      "Epoch: 175, Loss: 2083.18561, Residuals: -1.03437, Convergence: 0.002128\n",
      "Epoch: 176, Loss: 2079.10174, Residuals: -1.03078, Convergence: 0.001964\n",
      "Epoch: 177, Loss: 2075.32939, Residuals: -1.02735, Convergence: 0.001818\n",
      "Epoch: 178, Loss: 2071.83384, Residuals: -1.02408, Convergence: 0.001687\n",
      "Epoch: 179, Loss: 2068.58270, Residuals: -1.02097, Convergence: 0.001572\n",
      "Epoch: 180, Loss: 2065.54818, Residuals: -1.01802, Convergence: 0.001469\n",
      "Epoch: 181, Loss: 2062.70533, Residuals: -1.01522, Convergence: 0.001378\n",
      "Epoch: 182, Loss: 2060.03189, Residuals: -1.01258, Convergence: 0.001298\n",
      "Epoch: 183, Loss: 2057.51135, Residuals: -1.01007, Convergence: 0.001225\n",
      "Epoch: 184, Loss: 2055.12603, Residuals: -1.00771, Convergence: 0.001161\n",
      "Epoch: 185, Loss: 2052.86469, Residuals: -1.00547, Convergence: 0.001102\n",
      "Epoch: 186, Loss: 2050.71592, Residuals: -1.00336, Convergence: 0.001048\n",
      "Epoch: 187, Loss: 2048.67056, Residuals: -1.00137, Convergence: 0.000998\n",
      "Evidence 14331.666\n",
      "\n",
      "Epoch: 187, Evidence: 14331.66602, Convergence: 0.226551\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 4.40e-01\n",
      "Epoch: 187, Loss: 2477.48101, Residuals: -1.00137, Convergence:   inf\n",
      "Epoch: 188, Loss: 2461.67758, Residuals: -0.99875, Convergence: 0.006420\n",
      "Epoch: 189, Loss: 2448.86169, Residuals: -0.99532, Convergence: 0.005233\n",
      "Epoch: 190, Loss: 2437.82953, Residuals: -0.99183, Convergence: 0.004525\n",
      "Epoch: 191, Loss: 2428.25787, Residuals: -0.98840, Convergence: 0.003942\n",
      "Epoch: 192, Loss: 2419.90236, Residuals: -0.98512, Convergence: 0.003453\n",
      "Epoch: 193, Loss: 2412.57107, Residuals: -0.98202, Convergence: 0.003039\n",
      "Epoch: 194, Loss: 2406.10491, Residuals: -0.97912, Convergence: 0.002687\n",
      "Epoch: 195, Loss: 2400.37230, Residuals: -0.97641, Convergence: 0.002388\n",
      "Epoch: 196, Loss: 2395.26444, Residuals: -0.97389, Convergence: 0.002132\n",
      "Epoch: 197, Loss: 2390.68963, Residuals: -0.97155, Convergence: 0.001914\n",
      "Epoch: 198, Loss: 2386.56955, Residuals: -0.96938, Convergence: 0.001726\n",
      "Epoch: 199, Loss: 2382.84006, Residuals: -0.96735, Convergence: 0.001565\n",
      "Epoch: 200, Loss: 2379.44736, Residuals: -0.96546, Convergence: 0.001426\n",
      "Epoch: 201, Loss: 2376.34502, Residuals: -0.96370, Convergence: 0.001306\n",
      "Epoch: 202, Loss: 2373.49545, Residuals: -0.96206, Convergence: 0.001201\n",
      "Epoch: 203, Loss: 2370.86736, Residuals: -0.96052, Convergence: 0.001108\n",
      "Epoch: 204, Loss: 2368.43318, Residuals: -0.95908, Convergence: 0.001028\n",
      "Epoch: 205, Loss: 2366.16996, Residuals: -0.95773, Convergence: 0.000956\n",
      "Evidence 14793.564\n",
      "\n",
      "Epoch: 205, Evidence: 14793.56445, Convergence: 0.031223\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 3.36e-01\n",
      "Epoch: 205, Loss: 2482.47580, Residuals: -0.95773, Convergence:   inf\n",
      "Epoch: 206, Loss: 2475.43469, Residuals: -0.95442, Convergence: 0.002844\n",
      "Epoch: 207, Loss: 2469.62848, Residuals: -0.95129, Convergence: 0.002351\n",
      "Epoch: 208, Loss: 2464.68892, Residuals: -0.94845, Convergence: 0.002004\n",
      "Epoch: 209, Loss: 2460.41795, Residuals: -0.94590, Convergence: 0.001736\n",
      "Epoch: 210, Loss: 2456.67395, Residuals: -0.94362, Convergence: 0.001524\n",
      "Epoch: 211, Loss: 2453.35492, Residuals: -0.94159, Convergence: 0.001353\n",
      "Epoch: 212, Loss: 2450.38269, Residuals: -0.93978, Convergence: 0.001213\n",
      "Epoch: 213, Loss: 2447.69644, Residuals: -0.93816, Convergence: 0.001097\n",
      "Epoch: 214, Loss: 2445.24968, Residuals: -0.93671, Convergence: 0.001001\n",
      "Epoch: 215, Loss: 2443.00445, Residuals: -0.93542, Convergence: 0.000919\n",
      "Evidence 14893.080\n",
      "\n",
      "Epoch: 215, Evidence: 14893.08008, Convergence: 0.006682\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.64e-01\n",
      "Epoch: 215, Loss: 2484.02137, Residuals: -0.93542, Convergence:   inf\n",
      "Epoch: 216, Loss: 2480.17351, Residuals: -0.93255, Convergence: 0.001551\n",
      "Epoch: 217, Loss: 2476.93612, Residuals: -0.93009, Convergence: 0.001307\n",
      "Epoch: 218, Loss: 2474.12122, Residuals: -0.92796, Convergence: 0.001138\n",
      "Epoch: 219, Loss: 2471.62843, Residuals: -0.92614, Convergence: 0.001009\n",
      "Epoch: 220, Loss: 2469.39143, Residuals: -0.92457, Convergence: 0.000906\n",
      "Evidence 14925.507\n",
      "\n",
      "Epoch: 220, Evidence: 14925.50684, Convergence: 0.002173\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.14e-01\n",
      "Epoch: 220, Loss: 2485.07652, Residuals: -0.92457, Convergence:   inf\n",
      "Epoch: 221, Loss: 2482.35868, Residuals: -0.92216, Convergence: 0.001095\n",
      "Epoch: 222, Loss: 2480.02665, Residuals: -0.92014, Convergence: 0.000940\n",
      "Evidence 14937.593\n",
      "\n",
      "Epoch: 222, Evidence: 14937.59277, Convergence: 0.000809\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.79e-01\n",
      "Epoch: 222, Loss: 2485.86436, Residuals: -0.92014, Convergence:   inf\n",
      "Epoch: 223, Loss: 2481.52095, Residuals: -0.91636, Convergence: 0.001750\n",
      "Epoch: 224, Loss: 2478.08058, Residuals: -0.91346, Convergence: 0.001388\n",
      "Epoch: 225, Loss: 2475.23274, Residuals: -0.91144, Convergence: 0.001151\n",
      "Epoch: 226, Loss: 2472.78458, Residuals: -0.91023, Convergence: 0.000990\n",
      "Evidence 14955.174\n",
      "\n",
      "Epoch: 226, Evidence: 14955.17383, Convergence: 0.001984\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.49e-01\n",
      "Epoch: 226, Loss: 2486.18897, Residuals: -0.91023, Convergence:   inf\n",
      "Epoch: 227, Loss: 2483.29988, Residuals: -0.90704, Convergence: 0.001163\n",
      "Epoch: 228, Loss: 2480.95411, Residuals: -0.90520, Convergence: 0.000946\n",
      "Evidence 14965.552\n",
      "\n",
      "Epoch: 228, Evidence: 14965.55176, Convergence: 0.000693\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.27e-01\n",
      "Epoch: 228, Loss: 2486.46319, Residuals: -0.90520, Convergence:   inf\n",
      "Epoch: 229, Loss: 2482.23081, Residuals: -0.90048, Convergence: 0.001705\n",
      "Epoch: 230, Loss: 2479.11399, Residuals: -0.90111, Convergence: 0.001257\n",
      "Epoch: 231, Loss: 2476.43178, Residuals: -0.90289, Convergence: 0.001083\n",
      "Epoch: 232, Loss: 2474.10761, Residuals: -0.90613, Convergence: 0.000939\n",
      "Evidence 14981.320\n",
      "\n",
      "Epoch: 232, Evidence: 14981.32031, Convergence: 0.001745\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.12e-01\n",
      "Epoch: 232, Loss: 2486.07200, Residuals: -0.90613, Convergence:   inf\n",
      "Epoch: 233, Loss: 2484.49167, Residuals: -0.90320, Convergence: 0.000636\n",
      "Evidence 14987.711\n",
      "\n",
      "Epoch: 233, Evidence: 14987.71094, Convergence: 0.000426\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 9.27e-02\n",
      "Epoch: 233, Loss: 2486.92444, Residuals: -0.90320, Convergence:   inf\n",
      "Epoch: 234, Loss: 2524.17880, Residuals: -0.93456, Convergence: -0.014759\n",
      "Epoch: 234, Loss: 2485.17194, Residuals: -0.89985, Convergence: 0.000705\n",
      "Evidence 14991.400\n",
      "\n",
      "Epoch: 234, Evidence: 14991.40039, Convergence: 0.000672\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 8.54e-02\n",
      "Epoch: 234, Loss: 2486.44184, Residuals: -0.89985, Convergence:   inf\n",
      "Epoch: 235, Loss: 2491.95467, Residuals: -0.90144, Convergence: -0.002212\n",
      "Epoch: 235, Loss: 2486.67670, Residuals: -0.89857, Convergence: -0.000094\n",
      "Evidence 14992.749\n",
      "\n",
      "Epoch: 235, Evidence: 14992.74902, Convergence: 0.000762\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 381.31457, Residuals: -4.50814, Convergence:   inf\n",
      "Epoch: 1, Loss: 355.61436, Residuals: -4.38838, Convergence: 0.072270\n",
      "Epoch: 2, Loss: 334.64545, Residuals: -4.22554, Convergence: 0.062660\n",
      "Epoch: 3, Loss: 318.62065, Residuals: -4.06187, Convergence: 0.050294\n",
      "Epoch: 4, Loss: 306.39380, Residuals: -3.91738, Convergence: 0.039906\n",
      "Epoch: 5, Loss: 296.69554, Residuals: -3.78954, Convergence: 0.032688\n",
      "Epoch: 6, Loss: 288.82346, Residuals: -3.67824, Convergence: 0.027256\n",
      "Epoch: 7, Loss: 282.28939, Residuals: -3.58306, Convergence: 0.023147\n",
      "Epoch: 8, Loss: 276.73185, Residuals: -3.50188, Convergence: 0.020083\n",
      "Epoch: 9, Loss: 271.89596, Residuals: -3.43235, Convergence: 0.017786\n",
      "Epoch: 10, Loss: 267.60052, Residuals: -3.37235, Convergence: 0.016052\n",
      "Epoch: 11, Loss: 263.71422, Residuals: -3.32006, Convergence: 0.014737\n",
      "Epoch: 12, Loss: 260.14134, Residuals: -3.27396, Convergence: 0.013734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Loss: 256.81250, Residuals: -3.23272, Convergence: 0.012962\n",
      "Epoch: 14, Loss: 253.67816, Residuals: -3.19518, Convergence: 0.012356\n",
      "Epoch: 15, Loss: 250.70488, Residuals: -3.16038, Convergence: 0.011860\n",
      "Epoch: 16, Loss: 247.87354, Residuals: -3.12765, Convergence: 0.011423\n",
      "Epoch: 17, Loss: 245.17244, Residuals: -3.09662, Convergence: 0.011017\n",
      "Epoch: 18, Loss: 242.58395, Residuals: -3.06697, Convergence: 0.010670\n",
      "Epoch: 19, Loss: 240.07825, Residuals: -3.03828, Convergence: 0.010437\n",
      "Epoch: 20, Loss: 237.61908, Residuals: -3.00999, Convergence: 0.010349\n",
      "Epoch: 21, Loss: 235.17279, Residuals: -2.98158, Convergence: 0.010402\n",
      "Epoch: 22, Loss: 232.71413, Residuals: -2.95268, Convergence: 0.010565\n",
      "Epoch: 23, Loss: 230.22010, Residuals: -2.92298, Convergence: 0.010833\n",
      "Epoch: 24, Loss: 227.65306, Residuals: -2.89205, Convergence: 0.011276\n",
      "Epoch: 25, Loss: 224.95512, Residuals: -2.85919, Convergence: 0.011993\n",
      "Epoch: 26, Loss: 222.09082, Residuals: -2.82387, Convergence: 0.012897\n",
      "Epoch: 27, Loss: 219.14265, Residuals: -2.78685, Convergence: 0.013453\n",
      "Epoch: 28, Loss: 216.25058, Residuals: -2.74965, Convergence: 0.013374\n",
      "Epoch: 29, Loss: 213.46386, Residuals: -2.71296, Convergence: 0.013055\n",
      "Epoch: 30, Loss: 210.77454, Residuals: -2.67679, Convergence: 0.012759\n",
      "Epoch: 31, Loss: 208.16625, Residuals: -2.64105, Convergence: 0.012530\n",
      "Epoch: 32, Loss: 205.62633, Residuals: -2.60563, Convergence: 0.012352\n",
      "Epoch: 33, Loss: 203.14667, Residuals: -2.57048, Convergence: 0.012206\n",
      "Epoch: 34, Loss: 200.72282, Residuals: -2.53554, Convergence: 0.012076\n",
      "Epoch: 35, Loss: 198.35277, Residuals: -2.50080, Convergence: 0.011949\n",
      "Epoch: 36, Loss: 196.03621, Residuals: -2.46625, Convergence: 0.011817\n",
      "Epoch: 37, Loss: 193.77383, Residuals: -2.43188, Convergence: 0.011675\n",
      "Epoch: 38, Loss: 191.56684, Residuals: -2.39771, Convergence: 0.011521\n",
      "Epoch: 39, Loss: 189.41663, Residuals: -2.36373, Convergence: 0.011352\n",
      "Epoch: 40, Loss: 187.32460, Residuals: -2.32998, Convergence: 0.011168\n",
      "Epoch: 41, Loss: 185.29194, Residuals: -2.29648, Convergence: 0.010970\n",
      "Epoch: 42, Loss: 183.31963, Residuals: -2.26325, Convergence: 0.010759\n",
      "Epoch: 43, Loss: 181.40842, Residuals: -2.23033, Convergence: 0.010535\n",
      "Epoch: 44, Loss: 179.55885, Residuals: -2.19775, Convergence: 0.010301\n",
      "Epoch: 45, Loss: 177.77144, Residuals: -2.16555, Convergence: 0.010055\n",
      "Epoch: 46, Loss: 176.04669, Residuals: -2.13376, Convergence: 0.009797\n",
      "Epoch: 47, Loss: 174.38515, Residuals: -2.10242, Convergence: 0.009528\n",
      "Epoch: 48, Loss: 172.78743, Residuals: -2.07157, Convergence: 0.009247\n",
      "Epoch: 49, Loss: 171.25405, Residuals: -2.04123, Convergence: 0.008954\n",
      "Epoch: 50, Loss: 169.78537, Residuals: -2.01143, Convergence: 0.008650\n",
      "Epoch: 51, Loss: 168.38145, Residuals: -1.98219, Convergence: 0.008338\n",
      "Epoch: 52, Loss: 167.04196, Residuals: -1.95356, Convergence: 0.008019\n",
      "Epoch: 53, Loss: 165.76614, Residuals: -1.92554, Convergence: 0.007697\n",
      "Epoch: 54, Loss: 164.55274, Residuals: -1.89816, Convergence: 0.007374\n",
      "Epoch: 55, Loss: 163.40012, Residuals: -1.87146, Convergence: 0.007054\n",
      "Epoch: 56, Loss: 162.30619, Residuals: -1.84544, Convergence: 0.006740\n",
      "Epoch: 57, Loss: 161.26851, Residuals: -1.82012, Convergence: 0.006434\n",
      "Epoch: 58, Loss: 160.28433, Residuals: -1.79552, Convergence: 0.006140\n",
      "Epoch: 59, Loss: 159.35061, Residuals: -1.77163, Convergence: 0.005860\n",
      "Epoch: 60, Loss: 158.46412, Residuals: -1.74845, Convergence: 0.005594\n",
      "Epoch: 61, Loss: 157.62148, Residuals: -1.72597, Convergence: 0.005346\n",
      "Epoch: 62, Loss: 156.81933, Residuals: -1.70417, Convergence: 0.005115\n",
      "Epoch: 63, Loss: 156.05432, Residuals: -1.68304, Convergence: 0.004902\n",
      "Epoch: 64, Loss: 155.32333, Residuals: -1.66254, Convergence: 0.004706\n",
      "Epoch: 65, Loss: 154.62350, Residuals: -1.64266, Convergence: 0.004526\n",
      "Epoch: 66, Loss: 153.95231, Residuals: -1.62336, Convergence: 0.004360\n",
      "Epoch: 67, Loss: 153.30759, Residuals: -1.60461, Convergence: 0.004205\n",
      "Epoch: 68, Loss: 152.68754, Residuals: -1.58642, Convergence: 0.004061\n",
      "Epoch: 69, Loss: 152.09068, Residuals: -1.56874, Convergence: 0.003924\n",
      "Epoch: 70, Loss: 151.51582, Residuals: -1.55158, Convergence: 0.003794\n",
      "Epoch: 71, Loss: 150.96198, Residuals: -1.53492, Convergence: 0.003669\n",
      "Epoch: 72, Loss: 150.42838, Residuals: -1.51874, Convergence: 0.003547\n",
      "Epoch: 73, Loss: 149.91438, Residuals: -1.50306, Convergence: 0.003429\n",
      "Epoch: 74, Loss: 149.41943, Residuals: -1.48784, Convergence: 0.003313\n",
      "Epoch: 75, Loss: 148.94305, Residuals: -1.47310, Convergence: 0.003198\n",
      "Epoch: 76, Loss: 148.48482, Residuals: -1.45883, Convergence: 0.003086\n",
      "Epoch: 77, Loss: 148.04433, Residuals: -1.44501, Convergence: 0.002975\n",
      "Epoch: 78, Loss: 147.62119, Residuals: -1.43165, Convergence: 0.002866\n",
      "Epoch: 79, Loss: 147.21501, Residuals: -1.41874, Convergence: 0.002759\n",
      "Epoch: 80, Loss: 146.82538, Residuals: -1.40628, Convergence: 0.002654\n",
      "Epoch: 81, Loss: 146.45189, Residuals: -1.39425, Convergence: 0.002550\n",
      "Epoch: 82, Loss: 146.09412, Residuals: -1.38264, Convergence: 0.002449\n",
      "Epoch: 83, Loss: 145.75159, Residuals: -1.37146, Convergence: 0.002350\n",
      "Epoch: 84, Loss: 145.42384, Residuals: -1.36069, Convergence: 0.002254\n",
      "Epoch: 85, Loss: 145.11038, Residuals: -1.35033, Convergence: 0.002160\n",
      "Epoch: 86, Loss: 144.81071, Residuals: -1.34036, Convergence: 0.002069\n",
      "Epoch: 87, Loss: 144.52431, Residuals: -1.33077, Convergence: 0.001982\n",
      "Epoch: 88, Loss: 144.25065, Residuals: -1.32155, Convergence: 0.001897\n",
      "Epoch: 89, Loss: 143.98923, Residuals: -1.31269, Convergence: 0.001816\n",
      "Epoch: 90, Loss: 143.73951, Residuals: -1.30419, Convergence: 0.001737\n",
      "Epoch: 91, Loss: 143.50099, Residuals: -1.29602, Convergence: 0.001662\n",
      "Epoch: 92, Loss: 143.27317, Residuals: -1.28818, Convergence: 0.001590\n",
      "Epoch: 93, Loss: 143.05557, Residuals: -1.28066, Convergence: 0.001521\n",
      "Epoch: 94, Loss: 142.84772, Residuals: -1.27344, Convergence: 0.001455\n",
      "Epoch: 95, Loss: 142.64919, Residuals: -1.26651, Convergence: 0.001392\n",
      "Epoch: 96, Loss: 142.45957, Residuals: -1.25987, Convergence: 0.001331\n",
      "Epoch: 97, Loss: 142.27848, Residuals: -1.25349, Convergence: 0.001273\n",
      "Epoch: 98, Loss: 142.10556, Residuals: -1.24738, Convergence: 0.001217\n",
      "Epoch: 99, Loss: 141.94048, Residuals: -1.24152, Convergence: 0.001163\n",
      "Epoch: 100, Loss: 141.78296, Residuals: -1.23589, Convergence: 0.001111\n",
      "Epoch: 101, Loss: 141.63272, Residuals: -1.23050, Convergence: 0.001061\n",
      "Epoch: 102, Loss: 141.48954, Residuals: -1.22534, Convergence: 0.001012\n",
      "Epoch: 103, Loss: 141.35321, Residuals: -1.22038, Convergence: 0.000965\n",
      "Evidence -182.860\n",
      "\n",
      "Epoch: 103, Evidence: -182.85974, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.25e-01\n",
      "Epoch: 103, Loss: 1382.14892, Residuals: -1.22038, Convergence:   inf\n",
      "Epoch: 104, Loss: 1320.55439, Residuals: -1.25056, Convergence: 0.046643\n",
      "Epoch: 105, Loss: 1272.86380, Residuals: -1.27472, Convergence: 0.037467\n",
      "Epoch: 106, Loss: 1236.34444, Residuals: -1.29289, Convergence: 0.029538\n",
      "Epoch: 107, Loss: 1207.90263, Residuals: -1.30630, Convergence: 0.023546\n",
      "Epoch: 108, Loss: 1185.02901, Residuals: -1.31654, Convergence: 0.019302\n",
      "Epoch: 109, Loss: 1166.13315, Residuals: -1.32460, Convergence: 0.016204\n",
      "Epoch: 110, Loss: 1150.23294, Residuals: -1.33095, Convergence: 0.013823\n",
      "Epoch: 111, Loss: 1136.66747, Residuals: -1.33581, Convergence: 0.011934\n",
      "Epoch: 112, Loss: 1124.95244, Residuals: -1.33933, Convergence: 0.010414\n",
      "Epoch: 113, Loss: 1114.71223, Residuals: -1.34163, Convergence: 0.009186\n",
      "Epoch: 114, Loss: 1105.64551, Residuals: -1.34279, Convergence: 0.008200\n",
      "Epoch: 115, Loss: 1097.50422, Residuals: -1.34290, Convergence: 0.007418\n",
      "Epoch: 116, Loss: 1090.07980, Residuals: -1.34201, Convergence: 0.006811\n",
      "Epoch: 117, Loss: 1083.19495, Residuals: -1.34018, Convergence: 0.006356\n",
      "Epoch: 118, Loss: 1076.69774, Residuals: -1.33744, Convergence: 0.006034\n",
      "Epoch: 119, Loss: 1070.45846, Residuals: -1.33383, Convergence: 0.005829\n",
      "Epoch: 120, Loss: 1064.37192, Residuals: -1.32939, Convergence: 0.005718\n",
      "Epoch: 121, Loss: 1058.36118, Residuals: -1.32415, Convergence: 0.005679\n",
      "Epoch: 122, Loss: 1052.38726, Residuals: -1.31821, Convergence: 0.005677\n",
      "Epoch: 123, Loss: 1046.45633, Residuals: -1.31164, Convergence: 0.005668\n",
      "Epoch: 124, Loss: 1040.61923, Residuals: -1.30456, Convergence: 0.005609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 125, Loss: 1034.95272, Residuals: -1.29710, Convergence: 0.005475\n",
      "Epoch: 126, Loss: 1029.53024, Residuals: -1.28935, Convergence: 0.005267\n",
      "Epoch: 127, Loss: 1024.40058, Residuals: -1.28140, Convergence: 0.005007\n",
      "Epoch: 128, Loss: 1019.57940, Residuals: -1.27334, Convergence: 0.004729\n",
      "Epoch: 129, Loss: 1015.05879, Residuals: -1.26522, Convergence: 0.004454\n",
      "Epoch: 130, Loss: 1010.81684, Residuals: -1.25711, Convergence: 0.004197\n",
      "Epoch: 131, Loss: 1006.82549, Residuals: -1.24905, Convergence: 0.003964\n",
      "Epoch: 132, Loss: 1003.05755, Residuals: -1.24107, Convergence: 0.003756\n",
      "Epoch: 133, Loss: 999.48815, Residuals: -1.23321, Convergence: 0.003571\n",
      "Epoch: 134, Loss: 996.09609, Residuals: -1.22549, Convergence: 0.003405\n",
      "Epoch: 135, Loss: 992.86365, Residuals: -1.21794, Convergence: 0.003256\n",
      "Epoch: 136, Loss: 989.77688, Residuals: -1.21056, Convergence: 0.003119\n",
      "Epoch: 137, Loss: 986.82435, Residuals: -1.20338, Convergence: 0.002992\n",
      "Epoch: 138, Loss: 983.99704, Residuals: -1.19641, Convergence: 0.002873\n",
      "Epoch: 139, Loss: 981.28831, Residuals: -1.18965, Convergence: 0.002760\n",
      "Epoch: 140, Loss: 978.69184, Residuals: -1.18312, Convergence: 0.002653\n",
      "Epoch: 141, Loss: 976.20293, Residuals: -1.17682, Convergence: 0.002550\n",
      "Epoch: 142, Loss: 973.81711, Residuals: -1.17074, Convergence: 0.002450\n",
      "Epoch: 143, Loss: 971.53017, Residuals: -1.16490, Convergence: 0.002354\n",
      "Epoch: 144, Loss: 969.33824, Residuals: -1.15928, Convergence: 0.002261\n",
      "Epoch: 145, Loss: 967.23671, Residuals: -1.15389, Convergence: 0.002173\n",
      "Epoch: 146, Loss: 965.22133, Residuals: -1.14872, Convergence: 0.002088\n",
      "Epoch: 147, Loss: 963.28715, Residuals: -1.14376, Convergence: 0.002008\n",
      "Epoch: 148, Loss: 961.42902, Residuals: -1.13900, Convergence: 0.001933\n",
      "Epoch: 149, Loss: 959.64087, Residuals: -1.13444, Convergence: 0.001863\n",
      "Epoch: 150, Loss: 957.91702, Residuals: -1.13006, Convergence: 0.001800\n",
      "Epoch: 151, Loss: 956.25085, Residuals: -1.12584, Convergence: 0.001742\n",
      "Epoch: 152, Loss: 954.63548, Residuals: -1.12179, Convergence: 0.001692\n",
      "Epoch: 153, Loss: 953.06390, Residuals: -1.11787, Convergence: 0.001649\n",
      "Epoch: 154, Loss: 951.52876, Residuals: -1.11409, Convergence: 0.001613\n",
      "Epoch: 155, Loss: 950.02351, Residuals: -1.11041, Convergence: 0.001584\n",
      "Epoch: 156, Loss: 948.54234, Residuals: -1.10683, Convergence: 0.001562\n",
      "Epoch: 157, Loss: 947.08057, Residuals: -1.10333, Convergence: 0.001543\n",
      "Epoch: 158, Loss: 945.63675, Residuals: -1.09992, Convergence: 0.001527\n",
      "Epoch: 159, Loss: 944.21057, Residuals: -1.09657, Convergence: 0.001510\n",
      "Epoch: 160, Loss: 942.80534, Residuals: -1.09330, Convergence: 0.001490\n",
      "Epoch: 161, Loss: 941.42559, Residuals: -1.09012, Convergence: 0.001466\n",
      "Epoch: 162, Loss: 940.07738, Residuals: -1.08702, Convergence: 0.001434\n",
      "Epoch: 163, Loss: 938.76646, Residuals: -1.08402, Convergence: 0.001396\n",
      "Epoch: 164, Loss: 937.49777, Residuals: -1.08113, Convergence: 0.001353\n",
      "Epoch: 165, Loss: 936.27512, Residuals: -1.07834, Convergence: 0.001306\n",
      "Epoch: 166, Loss: 935.10067, Residuals: -1.07567, Convergence: 0.001256\n",
      "Epoch: 167, Loss: 933.97587, Residuals: -1.07310, Convergence: 0.001204\n",
      "Epoch: 168, Loss: 932.90077, Residuals: -1.07065, Convergence: 0.001152\n",
      "Epoch: 169, Loss: 931.87487, Residuals: -1.06830, Convergence: 0.001101\n",
      "Epoch: 170, Loss: 930.89750, Residuals: -1.06606, Convergence: 0.001050\n",
      "Epoch: 171, Loss: 929.96717, Residuals: -1.06392, Convergence: 0.001000\n",
      "Epoch: 172, Loss: 929.08252, Residuals: -1.06188, Convergence: 0.000952\n",
      "Evidence 11283.580\n",
      "\n",
      "Epoch: 172, Evidence: 11283.58008, Convergence: 1.016206\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.75e-01\n",
      "Epoch: 172, Loss: 2354.01379, Residuals: -1.06188, Convergence:   inf\n",
      "Epoch: 173, Loss: 2315.81326, Residuals: -1.06886, Convergence: 0.016496\n",
      "Epoch: 174, Loss: 2289.76483, Residuals: -1.06770, Convergence: 0.011376\n",
      "Epoch: 175, Loss: 2268.20833, Residuals: -1.06567, Convergence: 0.009504\n",
      "Epoch: 176, Loss: 2250.07384, Residuals: -1.06340, Convergence: 0.008060\n",
      "Epoch: 177, Loss: 2234.65972, Residuals: -1.06099, Convergence: 0.006898\n",
      "Epoch: 178, Loss: 2221.42644, Residuals: -1.05847, Convergence: 0.005957\n",
      "Epoch: 179, Loss: 2209.93314, Residuals: -1.05583, Convergence: 0.005201\n",
      "Epoch: 180, Loss: 2199.82192, Residuals: -1.05308, Convergence: 0.004596\n",
      "Epoch: 181, Loss: 2190.80725, Residuals: -1.05021, Convergence: 0.004115\n",
      "Epoch: 182, Loss: 2182.67755, Residuals: -1.04721, Convergence: 0.003725\n",
      "Epoch: 183, Loss: 2175.28694, Residuals: -1.04410, Convergence: 0.003398\n",
      "Epoch: 184, Loss: 2168.54048, Residuals: -1.04092, Convergence: 0.003111\n",
      "Epoch: 185, Loss: 2162.37407, Residuals: -1.03770, Convergence: 0.002852\n",
      "Epoch: 186, Loss: 2156.73515, Residuals: -1.03450, Convergence: 0.002615\n",
      "Epoch: 187, Loss: 2151.57360, Residuals: -1.03134, Convergence: 0.002399\n",
      "Epoch: 188, Loss: 2146.84213, Residuals: -1.02826, Convergence: 0.002204\n",
      "Epoch: 189, Loss: 2142.49394, Residuals: -1.02527, Convergence: 0.002030\n",
      "Epoch: 190, Loss: 2138.48459, Residuals: -1.02237, Convergence: 0.001875\n",
      "Epoch: 191, Loss: 2134.77454, Residuals: -1.01958, Convergence: 0.001738\n",
      "Epoch: 192, Loss: 2131.32930, Residuals: -1.01690, Convergence: 0.001616\n",
      "Epoch: 193, Loss: 2128.11792, Residuals: -1.01433, Convergence: 0.001509\n",
      "Epoch: 194, Loss: 2125.11346, Residuals: -1.01187, Convergence: 0.001414\n",
      "Epoch: 195, Loss: 2122.29314, Residuals: -1.00951, Convergence: 0.001329\n",
      "Epoch: 196, Loss: 2119.63726, Residuals: -1.00725, Convergence: 0.001253\n",
      "Epoch: 197, Loss: 2117.12904, Residuals: -1.00510, Convergence: 0.001185\n",
      "Epoch: 198, Loss: 2114.75355, Residuals: -1.00304, Convergence: 0.001123\n",
      "Epoch: 199, Loss: 2112.49952, Residuals: -1.00108, Convergence: 0.001067\n",
      "Epoch: 200, Loss: 2110.35693, Residuals: -0.99921, Convergence: 0.001015\n",
      "Epoch: 201, Loss: 2108.31694, Residuals: -0.99743, Convergence: 0.000968\n",
      "Evidence 14384.805\n",
      "\n",
      "Epoch: 201, Evidence: 14384.80469, Convergence: 0.215590\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.38e-01\n",
      "Epoch: 201, Loss: 2470.98575, Residuals: -0.99743, Convergence:   inf\n",
      "Epoch: 202, Loss: 2457.95440, Residuals: -0.99373, Convergence: 0.005302\n",
      "Epoch: 203, Loss: 2447.27946, Residuals: -0.98992, Convergence: 0.004362\n",
      "Epoch: 204, Loss: 2438.04948, Residuals: -0.98635, Convergence: 0.003786\n",
      "Epoch: 205, Loss: 2430.03308, Residuals: -0.98307, Convergence: 0.003299\n",
      "Epoch: 206, Loss: 2423.03975, Residuals: -0.98010, Convergence: 0.002886\n",
      "Epoch: 207, Loss: 2416.91106, Residuals: -0.97741, Convergence: 0.002536\n",
      "Epoch: 208, Loss: 2411.51132, Residuals: -0.97500, Convergence: 0.002239\n",
      "Epoch: 209, Loss: 2406.72899, Residuals: -0.97283, Convergence: 0.001987\n",
      "Epoch: 210, Loss: 2402.46919, Residuals: -0.97088, Convergence: 0.001773\n",
      "Epoch: 211, Loss: 2398.65384, Residuals: -0.96913, Convergence: 0.001591\n",
      "Epoch: 212, Loss: 2395.21741, Residuals: -0.96755, Convergence: 0.001435\n",
      "Epoch: 213, Loss: 2392.10563, Residuals: -0.96614, Convergence: 0.001301\n",
      "Epoch: 214, Loss: 2389.27403, Residuals: -0.96486, Convergence: 0.001185\n",
      "Epoch: 215, Loss: 2386.68466, Residuals: -0.96371, Convergence: 0.001085\n",
      "Epoch: 216, Loss: 2384.30717, Residuals: -0.96267, Convergence: 0.000997\n",
      "Evidence 14738.094\n",
      "\n",
      "Epoch: 216, Evidence: 14738.09375, Convergence: 0.023971\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.34e-01\n",
      "Epoch: 216, Loss: 2476.06451, Residuals: -0.96267, Convergence:   inf\n",
      "Epoch: 217, Loss: 2469.45347, Residuals: -0.95950, Convergence: 0.002677\n",
      "Epoch: 218, Loss: 2463.98658, Residuals: -0.95686, Convergence: 0.002219\n",
      "Epoch: 219, Loss: 2459.35987, Residuals: -0.95469, Convergence: 0.001881\n",
      "Epoch: 220, Loss: 2455.38985, Residuals: -0.95289, Convergence: 0.001617\n",
      "Epoch: 221, Loss: 2451.93674, Residuals: -0.95141, Convergence: 0.001408\n",
      "Epoch: 222, Loss: 2448.89600, Residuals: -0.95019, Convergence: 0.001242\n",
      "Epoch: 223, Loss: 2446.18872, Residuals: -0.94917, Convergence: 0.001107\n",
      "Epoch: 224, Loss: 2443.75197, Residuals: -0.94832, Convergence: 0.000997\n",
      "Evidence 14818.557\n",
      "\n",
      "Epoch: 224, Evidence: 14818.55664, Convergence: 0.005430\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.61e-01\n",
      "Epoch: 224, Loss: 2477.76972, Residuals: -0.94832, Convergence:   inf\n",
      "Epoch: 225, Loss: 2473.75367, Residuals: -0.94615, Convergence: 0.001623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 226, Loss: 2470.43163, Residuals: -0.94449, Convergence: 0.001345\n",
      "Epoch: 227, Loss: 2467.59739, Residuals: -0.94319, Convergence: 0.001149\n",
      "Epoch: 228, Loss: 2465.12988, Residuals: -0.94219, Convergence: 0.001001\n",
      "Epoch: 229, Loss: 2462.94475, Residuals: -0.94139, Convergence: 0.000887\n",
      "Evidence 14849.482\n",
      "\n",
      "Epoch: 229, Evidence: 14849.48242, Convergence: 0.002083\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.08e-01\n",
      "Epoch: 229, Loss: 2478.70790, Residuals: -0.94139, Convergence:   inf\n",
      "Epoch: 230, Loss: 2475.88849, Residuals: -0.93983, Convergence: 0.001139\n",
      "Epoch: 231, Loss: 2473.53257, Residuals: -0.93867, Convergence: 0.000952\n",
      "Evidence 14862.329\n",
      "\n",
      "Epoch: 231, Evidence: 14862.32910, Convergence: 0.000864\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.72e-01\n",
      "Epoch: 231, Loss: 2479.43472, Residuals: -0.93867, Convergence:   inf\n",
      "Epoch: 232, Loss: 2475.03538, Residuals: -0.93682, Convergence: 0.001777\n",
      "Epoch: 233, Loss: 2471.67445, Residuals: -0.93546, Convergence: 0.001360\n",
      "Epoch: 234, Loss: 2468.94958, Residuals: -0.93453, Convergence: 0.001104\n",
      "Epoch: 235, Loss: 2466.63740, Residuals: -0.93400, Convergence: 0.000937\n",
      "Evidence 14879.628\n",
      "\n",
      "Epoch: 235, Evidence: 14879.62793, Convergence: 0.002026\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.42e-01\n",
      "Epoch: 235, Loss: 2479.55639, Residuals: -0.93400, Convergence:   inf\n",
      "Epoch: 236, Loss: 2476.64377, Residuals: -0.93195, Convergence: 0.001176\n",
      "Epoch: 237, Loss: 2474.33848, Residuals: -0.93090, Convergence: 0.000932\n",
      "Evidence 14890.334\n",
      "\n",
      "Epoch: 237, Evidence: 14890.33398, Convergence: 0.000719\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.20e-01\n",
      "Epoch: 237, Loss: 2479.75551, Residuals: -0.93090, Convergence:   inf\n",
      "Epoch: 238, Loss: 2475.51833, Residuals: -0.92803, Convergence: 0.001712\n",
      "Epoch: 239, Loss: 2472.47959, Residuals: -0.92880, Convergence: 0.001229\n",
      "Epoch: 240, Loss: 2469.93783, Residuals: -0.92875, Convergence: 0.001029\n",
      "Epoch: 241, Loss: 2467.71904, Residuals: -0.93124, Convergence: 0.000899\n",
      "Evidence 14905.488\n",
      "\n",
      "Epoch: 241, Evidence: 14905.48828, Convergence: 0.001735\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.07e-01\n",
      "Epoch: 241, Loss: 2479.00529, Residuals: -0.93124, Convergence:   inf\n",
      "Epoch: 242, Loss: 2477.14068, Residuals: -0.92848, Convergence: 0.000753\n",
      "Evidence 14912.195\n",
      "\n",
      "Epoch: 242, Evidence: 14912.19531, Convergence: 0.000450\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.76e-02\n",
      "Epoch: 242, Loss: 2479.98760, Residuals: -0.92848, Convergence:   inf\n",
      "Epoch: 243, Loss: 2519.33931, Residuals: -0.96534, Convergence: -0.015620\n",
      "Epoch: 243, Loss: 2477.95526, Residuals: -0.92743, Convergence: 0.000820\n",
      "Evidence 14916.139\n",
      "\n",
      "Epoch: 243, Evidence: 14916.13867, Convergence: 0.000714\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.09e-02\n",
      "Epoch: 243, Loss: 2479.40175, Residuals: -0.92743, Convergence:   inf\n",
      "Epoch: 244, Loss: 2483.82710, Residuals: -0.92771, Convergence: -0.001782\n",
      "Epoch: 244, Loss: 2479.27276, Residuals: -0.92524, Convergence: 0.000052\n",
      "Evidence 14917.931\n",
      "\n",
      "Epoch: 244, Evidence: 14917.93066, Convergence: 0.000834\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 382.17191, Residuals: -4.53140, Convergence:   inf\n",
      "Epoch: 1, Loss: 356.48461, Residuals: -4.41042, Convergence: 0.072057\n",
      "Epoch: 2, Loss: 335.47345, Residuals: -4.24644, Convergence: 0.062631\n",
      "Epoch: 3, Loss: 319.33239, Residuals: -4.08101, Convergence: 0.050546\n",
      "Epoch: 4, Loss: 307.04982, Residuals: -3.93599, Convergence: 0.040002\n",
      "Epoch: 5, Loss: 297.31568, Residuals: -3.80813, Convergence: 0.032740\n",
      "Epoch: 6, Loss: 289.41707, Residuals: -3.69683, Convergence: 0.027291\n",
      "Epoch: 7, Loss: 282.86572, Residuals: -3.60155, Convergence: 0.023161\n",
      "Epoch: 8, Loss: 277.30110, Residuals: -3.52025, Convergence: 0.020067\n",
      "Epoch: 9, Loss: 272.46705, Residuals: -3.45055, Convergence: 0.017742\n",
      "Epoch: 10, Loss: 268.18013, Residuals: -3.39030, Convergence: 0.015985\n",
      "Epoch: 11, Loss: 264.30638, Residuals: -3.33766, Convergence: 0.014656\n",
      "Epoch: 12, Loss: 260.74719, Residuals: -3.29108, Convergence: 0.013650\n",
      "Epoch: 13, Loss: 257.43049, Residuals: -3.24923, Convergence: 0.012884\n",
      "Epoch: 14, Loss: 254.30498, Residuals: -3.21099, Convergence: 0.012290\n",
      "Epoch: 15, Loss: 251.33728, Residuals: -3.17545, Convergence: 0.011808\n",
      "Epoch: 16, Loss: 248.50983, Residuals: -3.14201, Convergence: 0.011378\n",
      "Epoch: 17, Loss: 245.81227, Residuals: -3.11032, Convergence: 0.010974\n",
      "Epoch: 18, Loss: 243.22705, Residuals: -3.08005, Convergence: 0.010629\n",
      "Epoch: 19, Loss: 240.72431, Residuals: -3.05075, Convergence: 0.010397\n",
      "Epoch: 20, Loss: 238.26799, Residuals: -3.02183, Convergence: 0.010309\n",
      "Epoch: 21, Loss: 235.82472, Residuals: -2.99275, Convergence: 0.010361\n",
      "Epoch: 22, Loss: 233.36941, Residuals: -2.96309, Convergence: 0.010521\n",
      "Epoch: 23, Loss: 230.88024, Residuals: -2.93257, Convergence: 0.010781\n",
      "Epoch: 24, Loss: 228.32239, Residuals: -2.90076, Convergence: 0.011203\n",
      "Epoch: 25, Loss: 225.64242, Residuals: -2.86702, Convergence: 0.011877\n",
      "Epoch: 26, Loss: 222.81150, Residuals: -2.83092, Convergence: 0.012705\n",
      "Epoch: 27, Loss: 219.91319, Residuals: -2.79326, Convergence: 0.013179\n",
      "Epoch: 28, Loss: 217.07820, Residuals: -2.75553, Convergence: 0.013060\n",
      "Epoch: 29, Loss: 214.35076, Residuals: -2.71837, Convergence: 0.012724\n",
      "Epoch: 30, Loss: 211.72236, Residuals: -2.68184, Convergence: 0.012414\n",
      "Epoch: 31, Loss: 209.17644, Residuals: -2.64586, Convergence: 0.012171\n",
      "Epoch: 32, Loss: 206.69959, Residuals: -2.61035, Convergence: 0.011983\n",
      "Epoch: 33, Loss: 204.28250, Residuals: -2.57523, Convergence: 0.011832\n",
      "Epoch: 34, Loss: 201.91919, Residuals: -2.54045, Convergence: 0.011704\n",
      "Epoch: 35, Loss: 199.60625, Residuals: -2.50596, Convergence: 0.011588\n",
      "Epoch: 36, Loss: 197.34202, Residuals: -2.47171, Convergence: 0.011474\n",
      "Epoch: 37, Loss: 195.12612, Residuals: -2.43767, Convergence: 0.011356\n",
      "Epoch: 38, Loss: 192.95889, Residuals: -2.40381, Convergence: 0.011232\n",
      "Epoch: 39, Loss: 190.84109, Residuals: -2.37013, Convergence: 0.011097\n",
      "Epoch: 40, Loss: 188.77359, Residuals: -2.33662, Convergence: 0.010952\n",
      "Epoch: 41, Loss: 186.75715, Residuals: -2.30329, Convergence: 0.010797\n",
      "Epoch: 42, Loss: 184.79235, Residuals: -2.27013, Convergence: 0.010632\n",
      "Epoch: 43, Loss: 182.87958, Residuals: -2.23716, Convergence: 0.010459\n",
      "Epoch: 44, Loss: 181.01914, Residuals: -2.20440, Convergence: 0.010278\n",
      "Epoch: 45, Loss: 179.21140, Residuals: -2.17187, Convergence: 0.010087\n",
      "Epoch: 46, Loss: 177.45698, Residuals: -2.13958, Convergence: 0.009886\n",
      "Epoch: 47, Loss: 175.75680, Residuals: -2.10757, Convergence: 0.009673\n",
      "Epoch: 48, Loss: 174.11211, Residuals: -2.07588, Convergence: 0.009446\n",
      "Epoch: 49, Loss: 172.52424, Residuals: -2.04456, Convergence: 0.009204\n",
      "Epoch: 50, Loss: 170.99447, Residuals: -2.01366, Convergence: 0.008946\n",
      "Epoch: 51, Loss: 169.52368, Residuals: -1.98323, Convergence: 0.008676\n",
      "Epoch: 52, Loss: 168.11231, Residuals: -1.95333, Convergence: 0.008395\n",
      "Epoch: 53, Loss: 166.76026, Residuals: -1.92399, Convergence: 0.008108\n",
      "Epoch: 54, Loss: 165.46690, Residuals: -1.89526, Convergence: 0.007816\n",
      "Epoch: 55, Loss: 164.23123, Residuals: -1.86717, Convergence: 0.007524\n",
      "Epoch: 56, Loss: 163.05189, Residuals: -1.83975, Convergence: 0.007233\n",
      "Epoch: 57, Loss: 161.92729, Residuals: -1.81304, Convergence: 0.006945\n",
      "Epoch: 58, Loss: 160.85571, Residuals: -1.78704, Convergence: 0.006662\n",
      "Epoch: 59, Loss: 159.83527, Residuals: -1.76178, Convergence: 0.006384\n",
      "Epoch: 60, Loss: 158.86401, Residuals: -1.73727, Convergence: 0.006114\n",
      "Epoch: 61, Loss: 157.93993, Residuals: -1.71352, Convergence: 0.005851\n",
      "Epoch: 62, Loss: 157.06087, Residuals: -1.69054, Convergence: 0.005597\n",
      "Epoch: 63, Loss: 156.22462, Residuals: -1.66831, Convergence: 0.005353\n",
      "Epoch: 64, Loss: 155.42885, Residuals: -1.64685, Convergence: 0.005120\n",
      "Epoch: 65, Loss: 154.67118, Residuals: -1.62613, Convergence: 0.004899\n",
      "Epoch: 66, Loss: 153.94914, Residuals: -1.60613, Convergence: 0.004690\n",
      "Epoch: 67, Loss: 153.26031, Residuals: -1.58684, Convergence: 0.004494\n",
      "Epoch: 68, Loss: 152.60240, Residuals: -1.56822, Convergence: 0.004311\n",
      "Epoch: 69, Loss: 151.97328, Residuals: -1.55025, Convergence: 0.004140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 70, Loss: 151.37112, Residuals: -1.53289, Convergence: 0.003978\n",
      "Epoch: 71, Loss: 150.79433, Residuals: -1.51612, Convergence: 0.003825\n",
      "Epoch: 72, Loss: 150.24161, Residuals: -1.49993, Convergence: 0.003679\n",
      "Epoch: 73, Loss: 149.71187, Residuals: -1.48428, Convergence: 0.003538\n",
      "Epoch: 74, Loss: 149.20424, Residuals: -1.46918, Convergence: 0.003402\n",
      "Epoch: 75, Loss: 148.71794, Residuals: -1.45459, Convergence: 0.003270\n",
      "Epoch: 76, Loss: 148.25230, Residuals: -1.44051, Convergence: 0.003141\n",
      "Epoch: 77, Loss: 147.80667, Residuals: -1.42693, Convergence: 0.003015\n",
      "Epoch: 78, Loss: 147.38044, Residuals: -1.41384, Convergence: 0.002892\n",
      "Epoch: 79, Loss: 146.97299, Residuals: -1.40123, Convergence: 0.002772\n",
      "Epoch: 80, Loss: 146.58371, Residuals: -1.38908, Convergence: 0.002656\n",
      "Epoch: 81, Loss: 146.21197, Residuals: -1.37739, Convergence: 0.002542\n",
      "Epoch: 82, Loss: 145.85712, Residuals: -1.36614, Convergence: 0.002433\n",
      "Epoch: 83, Loss: 145.51852, Residuals: -1.35532, Convergence: 0.002327\n",
      "Epoch: 84, Loss: 145.19549, Residuals: -1.34493, Convergence: 0.002225\n",
      "Epoch: 85, Loss: 144.88739, Residuals: -1.33493, Convergence: 0.002126\n",
      "Epoch: 86, Loss: 144.59356, Residuals: -1.32534, Convergence: 0.002032\n",
      "Epoch: 87, Loss: 144.31336, Residuals: -1.31612, Convergence: 0.001942\n",
      "Epoch: 88, Loss: 144.04617, Residuals: -1.30727, Convergence: 0.001855\n",
      "Epoch: 89, Loss: 143.79138, Residuals: -1.29877, Convergence: 0.001772\n",
      "Epoch: 90, Loss: 143.54843, Residuals: -1.29062, Convergence: 0.001692\n",
      "Epoch: 91, Loss: 143.31677, Residuals: -1.28279, Convergence: 0.001616\n",
      "Epoch: 92, Loss: 143.09588, Residuals: -1.27528, Convergence: 0.001544\n",
      "Epoch: 93, Loss: 142.88529, Residuals: -1.26807, Convergence: 0.001474\n",
      "Epoch: 94, Loss: 142.68454, Residuals: -1.26116, Convergence: 0.001407\n",
      "Epoch: 95, Loss: 142.49324, Residuals: -1.25452, Convergence: 0.001343\n",
      "Epoch: 96, Loss: 142.31100, Residuals: -1.24815, Convergence: 0.001281\n",
      "Epoch: 97, Loss: 142.13749, Residuals: -1.24204, Convergence: 0.001221\n",
      "Epoch: 98, Loss: 141.97240, Residuals: -1.23618, Convergence: 0.001163\n",
      "Epoch: 99, Loss: 141.81546, Residuals: -1.23056, Convergence: 0.001107\n",
      "Epoch: 100, Loss: 141.66643, Residuals: -1.22517, Convergence: 0.001052\n",
      "Epoch: 101, Loss: 141.52509, Residuals: -1.21999, Convergence: 0.000999\n",
      "Evidence -182.995\n",
      "\n",
      "Epoch: 101, Evidence: -182.99477, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.24e-01\n",
      "Epoch: 101, Loss: 1367.73120, Residuals: -1.21999, Convergence:   inf\n",
      "Epoch: 102, Loss: 1306.02532, Residuals: -1.25017, Convergence: 0.047247\n",
      "Epoch: 103, Loss: 1259.53767, Residuals: -1.27374, Convergence: 0.036908\n",
      "Epoch: 104, Loss: 1224.62263, Residuals: -1.29040, Convergence: 0.028511\n",
      "Epoch: 105, Loss: 1197.48288, Residuals: -1.30187, Convergence: 0.022664\n",
      "Epoch: 106, Loss: 1175.50528, Residuals: -1.31018, Convergence: 0.018696\n",
      "Epoch: 107, Loss: 1157.22257, Residuals: -1.31640, Convergence: 0.015799\n",
      "Epoch: 108, Loss: 1141.76086, Residuals: -1.32098, Convergence: 0.013542\n",
      "Epoch: 109, Loss: 1128.52746, Residuals: -1.32418, Convergence: 0.011726\n",
      "Epoch: 110, Loss: 1117.08248, Residuals: -1.32616, Convergence: 0.010245\n",
      "Epoch: 111, Loss: 1107.08241, Residuals: -1.32705, Convergence: 0.009033\n",
      "Epoch: 112, Loss: 1098.25059, Residuals: -1.32698, Convergence: 0.008042\n",
      "Epoch: 113, Loss: 1090.36048, Residuals: -1.32603, Convergence: 0.007236\n",
      "Epoch: 114, Loss: 1083.22129, Residuals: -1.32431, Convergence: 0.006591\n",
      "Epoch: 115, Loss: 1076.67235, Residuals: -1.32186, Convergence: 0.006083\n",
      "Epoch: 116, Loss: 1070.57630, Residuals: -1.31876, Convergence: 0.005694\n",
      "Epoch: 117, Loss: 1064.81533, Residuals: -1.31503, Convergence: 0.005410\n",
      "Epoch: 118, Loss: 1059.29028, Residuals: -1.31071, Convergence: 0.005216\n",
      "Epoch: 119, Loss: 1053.91965, Residuals: -1.30582, Convergence: 0.005096\n",
      "Epoch: 120, Loss: 1048.63633, Residuals: -1.30037, Convergence: 0.005038\n",
      "Epoch: 121, Loss: 1043.38748, Residuals: -1.29438, Convergence: 0.005031\n",
      "Epoch: 122, Loss: 1038.13498, Residuals: -1.28789, Convergence: 0.005060\n",
      "Epoch: 123, Loss: 1032.86223, Residuals: -1.28093, Convergence: 0.005105\n",
      "Epoch: 124, Loss: 1027.58408, Residuals: -1.27360, Convergence: 0.005136\n",
      "Epoch: 125, Loss: 1022.35359, Residuals: -1.26596, Convergence: 0.005116\n",
      "Epoch: 126, Loss: 1017.25163, Residuals: -1.25812, Convergence: 0.005015\n",
      "Epoch: 127, Loss: 1012.36161, Residuals: -1.25016, Convergence: 0.004830\n",
      "Epoch: 128, Loss: 1007.74394, Residuals: -1.24215, Convergence: 0.004582\n",
      "Epoch: 129, Loss: 1003.42543, Residuals: -1.23417, Convergence: 0.004304\n",
      "Epoch: 130, Loss: 999.40378, Residuals: -1.22626, Convergence: 0.004024\n",
      "Epoch: 131, Loss: 995.66034, Residuals: -1.21848, Convergence: 0.003760\n",
      "Epoch: 132, Loss: 992.16959, Residuals: -1.21085, Convergence: 0.003518\n",
      "Epoch: 133, Loss: 988.90545, Residuals: -1.20342, Convergence: 0.003301\n",
      "Epoch: 134, Loss: 985.84379, Residuals: -1.19620, Convergence: 0.003106\n",
      "Epoch: 135, Loss: 982.96344, Residuals: -1.18920, Convergence: 0.002930\n",
      "Epoch: 136, Loss: 980.24713, Residuals: -1.18245, Convergence: 0.002771\n",
      "Epoch: 137, Loss: 977.67900, Residuals: -1.17594, Convergence: 0.002627\n",
      "Epoch: 138, Loss: 975.24731, Residuals: -1.16969, Convergence: 0.002493\n",
      "Epoch: 139, Loss: 972.94043, Residuals: -1.16370, Convergence: 0.002371\n",
      "Epoch: 140, Loss: 970.74921, Residuals: -1.15796, Convergence: 0.002257\n",
      "Epoch: 141, Loss: 968.66518, Residuals: -1.15248, Convergence: 0.002151\n",
      "Epoch: 142, Loss: 966.68052, Residuals: -1.14724, Convergence: 0.002053\n",
      "Epoch: 143, Loss: 964.78848, Residuals: -1.14224, Convergence: 0.001961\n",
      "Epoch: 144, Loss: 962.98273, Residuals: -1.13748, Convergence: 0.001875\n",
      "Epoch: 145, Loss: 961.25730, Residuals: -1.13295, Convergence: 0.001795\n",
      "Epoch: 146, Loss: 959.60613, Residuals: -1.12863, Convergence: 0.001721\n",
      "Epoch: 147, Loss: 958.02462, Residuals: -1.12452, Convergence: 0.001651\n",
      "Epoch: 148, Loss: 956.50756, Residuals: -1.12060, Convergence: 0.001586\n",
      "Epoch: 149, Loss: 955.05035, Residuals: -1.11687, Convergence: 0.001526\n",
      "Epoch: 150, Loss: 953.64859, Residuals: -1.11332, Convergence: 0.001470\n",
      "Epoch: 151, Loss: 952.29812, Residuals: -1.10993, Convergence: 0.001418\n",
      "Epoch: 152, Loss: 950.99540, Residuals: -1.10670, Convergence: 0.001370\n",
      "Epoch: 153, Loss: 949.73699, Residuals: -1.10362, Convergence: 0.001325\n",
      "Epoch: 154, Loss: 948.51919, Residuals: -1.10067, Convergence: 0.001284\n",
      "Epoch: 155, Loss: 947.33916, Residuals: -1.09784, Convergence: 0.001246\n",
      "Epoch: 156, Loss: 946.19359, Residuals: -1.09514, Convergence: 0.001211\n",
      "Epoch: 157, Loss: 945.07998, Residuals: -1.09254, Convergence: 0.001178\n",
      "Epoch: 158, Loss: 943.99548, Residuals: -1.09004, Convergence: 0.001149\n",
      "Epoch: 159, Loss: 942.93740, Residuals: -1.08763, Convergence: 0.001122\n",
      "Epoch: 160, Loss: 941.90305, Residuals: -1.08530, Convergence: 0.001098\n",
      "Epoch: 161, Loss: 940.88979, Residuals: -1.08305, Convergence: 0.001077\n",
      "Epoch: 162, Loss: 939.89517, Residuals: -1.08087, Convergence: 0.001058\n",
      "Epoch: 163, Loss: 938.91623, Residuals: -1.07874, Convergence: 0.001043\n",
      "Epoch: 164, Loss: 937.95082, Residuals: -1.07667, Convergence: 0.001029\n",
      "Epoch: 165, Loss: 936.99657, Residuals: -1.07464, Convergence: 0.001018\n",
      "Epoch: 166, Loss: 936.05121, Residuals: -1.07264, Convergence: 0.001010\n",
      "Epoch: 167, Loss: 935.11286, Residuals: -1.07068, Convergence: 0.001003\n",
      "Epoch: 168, Loss: 934.18011, Residuals: -1.06874, Convergence: 0.000998\n",
      "Evidence 11187.051\n",
      "\n",
      "Epoch: 168, Evidence: 11187.05078, Convergence: 1.016358\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.75e-01\n",
      "Epoch: 168, Loss: 2346.44766, Residuals: -1.06874, Convergence:   inf\n",
      "Epoch: 169, Loss: 2307.30537, Residuals: -1.07781, Convergence: 0.016965\n",
      "Epoch: 170, Loss: 2279.91936, Residuals: -1.07663, Convergence: 0.012012\n",
      "Epoch: 171, Loss: 2257.06730, Residuals: -1.07427, Convergence: 0.010125\n",
      "Epoch: 172, Loss: 2237.71107, Residuals: -1.07160, Convergence: 0.008650\n",
      "Epoch: 173, Loss: 2221.18849, Residuals: -1.06877, Convergence: 0.007439\n",
      "Epoch: 174, Loss: 2206.99483, Residuals: -1.06586, Convergence: 0.006431\n",
      "Epoch: 175, Loss: 2194.72086, Residuals: -1.06289, Convergence: 0.005592\n",
      "Epoch: 176, Loss: 2184.02293, Residuals: -1.05991, Convergence: 0.004898\n",
      "Epoch: 177, Loss: 2174.60928, Residuals: -1.05691, Convergence: 0.004329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 178, Loss: 2166.23107, Residuals: -1.05389, Convergence: 0.003868\n",
      "Epoch: 179, Loss: 2158.67955, Residuals: -1.05084, Convergence: 0.003498\n",
      "Epoch: 180, Loss: 2151.78175, Residuals: -1.04772, Convergence: 0.003206\n",
      "Epoch: 181, Loss: 2145.40872, Residuals: -1.04454, Convergence: 0.002971\n",
      "Epoch: 182, Loss: 2139.47482, Residuals: -1.04129, Convergence: 0.002774\n",
      "Epoch: 183, Loss: 2133.92931, Residuals: -1.03798, Convergence: 0.002599\n",
      "Epoch: 184, Loss: 2128.74543, Residuals: -1.03467, Convergence: 0.002435\n",
      "Epoch: 185, Loss: 2123.90787, Residuals: -1.03138, Convergence: 0.002278\n",
      "Epoch: 186, Loss: 2119.39993, Residuals: -1.02816, Convergence: 0.002127\n",
      "Epoch: 187, Loss: 2115.20592, Residuals: -1.02504, Convergence: 0.001983\n",
      "Epoch: 188, Loss: 2111.30689, Residuals: -1.02202, Convergence: 0.001847\n",
      "Epoch: 189, Loss: 2107.68324, Residuals: -1.01914, Convergence: 0.001719\n",
      "Epoch: 190, Loss: 2104.31608, Residuals: -1.01639, Convergence: 0.001600\n",
      "Epoch: 191, Loss: 2101.18679, Residuals: -1.01377, Convergence: 0.001489\n",
      "Epoch: 192, Loss: 2098.27769, Residuals: -1.01129, Convergence: 0.001386\n",
      "Epoch: 193, Loss: 2095.57233, Residuals: -1.00895, Convergence: 0.001291\n",
      "Epoch: 194, Loss: 2093.05423, Residuals: -1.00673, Convergence: 0.001203\n",
      "Epoch: 195, Loss: 2090.70844, Residuals: -1.00463, Convergence: 0.001122\n",
      "Epoch: 196, Loss: 2088.52116, Residuals: -1.00264, Convergence: 0.001047\n",
      "Epoch: 197, Loss: 2086.47874, Residuals: -1.00076, Convergence: 0.000979\n",
      "Evidence 14311.717\n",
      "\n",
      "Epoch: 197, Evidence: 14311.71680, Convergence: 0.218329\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.38e-01\n",
      "Epoch: 197, Loss: 2471.47932, Residuals: -1.00076, Convergence:   inf\n",
      "Epoch: 198, Loss: 2457.62232, Residuals: -0.99786, Convergence: 0.005638\n",
      "Epoch: 199, Loss: 2446.30279, Residuals: -0.99418, Convergence: 0.004627\n",
      "Epoch: 200, Loss: 2436.55826, Residuals: -0.99061, Convergence: 0.003999\n",
      "Epoch: 201, Loss: 2428.10810, Residuals: -0.98725, Convergence: 0.003480\n",
      "Epoch: 202, Loss: 2420.74051, Residuals: -0.98414, Convergence: 0.003044\n",
      "Epoch: 203, Loss: 2414.28421, Residuals: -0.98126, Convergence: 0.002674\n",
      "Epoch: 204, Loss: 2408.59879, Residuals: -0.97862, Convergence: 0.002360\n",
      "Epoch: 205, Loss: 2403.56618, Residuals: -0.97617, Convergence: 0.002094\n",
      "Epoch: 206, Loss: 2399.08776, Residuals: -0.97392, Convergence: 0.001867\n",
      "Epoch: 207, Loss: 2395.08213, Residuals: -0.97184, Convergence: 0.001672\n",
      "Epoch: 208, Loss: 2391.47865, Residuals: -0.96992, Convergence: 0.001507\n",
      "Epoch: 209, Loss: 2388.21960, Residuals: -0.96813, Convergence: 0.001365\n",
      "Epoch: 210, Loss: 2385.25650, Residuals: -0.96647, Convergence: 0.001242\n",
      "Epoch: 211, Loss: 2382.54886, Residuals: -0.96493, Convergence: 0.001136\n",
      "Epoch: 212, Loss: 2380.06307, Residuals: -0.96349, Convergence: 0.001044\n",
      "Epoch: 213, Loss: 2377.77066, Residuals: -0.96215, Convergence: 0.000964\n",
      "Evidence 14699.413\n",
      "\n",
      "Epoch: 213, Evidence: 14699.41309, Convergence: 0.026375\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.35e-01\n",
      "Epoch: 213, Loss: 2477.08526, Residuals: -0.96215, Convergence:   inf\n",
      "Epoch: 214, Loss: 2470.29694, Residuals: -0.95866, Convergence: 0.002748\n",
      "Epoch: 215, Loss: 2464.70888, Residuals: -0.95549, Convergence: 0.002267\n",
      "Epoch: 216, Loss: 2459.97226, Residuals: -0.95271, Convergence: 0.001925\n",
      "Epoch: 217, Loss: 2455.89475, Residuals: -0.95028, Convergence: 0.001660\n",
      "Epoch: 218, Loss: 2452.33828, Residuals: -0.94814, Convergence: 0.001450\n",
      "Epoch: 219, Loss: 2449.19957, Residuals: -0.94625, Convergence: 0.001282\n",
      "Epoch: 220, Loss: 2446.40017, Residuals: -0.94458, Convergence: 0.001144\n",
      "Epoch: 221, Loss: 2443.87735, Residuals: -0.94309, Convergence: 0.001032\n",
      "Epoch: 222, Loss: 2441.58525, Residuals: -0.94176, Convergence: 0.000939\n",
      "Evidence 14786.272\n",
      "\n",
      "Epoch: 222, Evidence: 14786.27246, Convergence: 0.005874\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.62e-01\n",
      "Epoch: 222, Loss: 2478.56513, Residuals: -0.94176, Convergence:   inf\n",
      "Epoch: 223, Loss: 2474.56524, Residuals: -0.93899, Convergence: 0.001616\n",
      "Epoch: 224, Loss: 2471.23906, Residuals: -0.93668, Convergence: 0.001346\n",
      "Epoch: 225, Loss: 2468.38309, Residuals: -0.93473, Convergence: 0.001157\n",
      "Epoch: 226, Loss: 2465.88244, Residuals: -0.93306, Convergence: 0.001014\n",
      "Epoch: 227, Loss: 2463.65875, Residuals: -0.93163, Convergence: 0.000903\n",
      "Evidence 14818.328\n",
      "\n",
      "Epoch: 227, Evidence: 14818.32812, Convergence: 0.002163\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.11e-01\n",
      "Epoch: 227, Loss: 2479.44509, Residuals: -0.93163, Convergence:   inf\n",
      "Epoch: 228, Loss: 2476.58598, Residuals: -0.92943, Convergence: 0.001154\n",
      "Epoch: 229, Loss: 2474.17565, Residuals: -0.92764, Convergence: 0.000974\n",
      "Evidence 14831.131\n",
      "\n",
      "Epoch: 229, Evidence: 14831.13086, Convergence: 0.000863\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.75e-01\n",
      "Epoch: 229, Loss: 2480.14020, Residuals: -0.92764, Convergence:   inf\n",
      "Epoch: 230, Loss: 2475.59785, Residuals: -0.92448, Convergence: 0.001835\n",
      "Epoch: 231, Loss: 2472.10508, Residuals: -0.92213, Convergence: 0.001413\n",
      "Epoch: 232, Loss: 2469.26867, Residuals: -0.92044, Convergence: 0.001149\n",
      "Epoch: 233, Loss: 2466.86522, Residuals: -0.91937, Convergence: 0.000974\n",
      "Evidence 14849.125\n",
      "\n",
      "Epoch: 233, Evidence: 14849.12500, Convergence: 0.002074\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.45e-01\n",
      "Epoch: 233, Loss: 2480.28648, Residuals: -0.91937, Convergence:   inf\n",
      "Epoch: 234, Loss: 2477.24772, Residuals: -0.91669, Convergence: 0.001227\n",
      "Epoch: 235, Loss: 2474.84052, Residuals: -0.91522, Convergence: 0.000973\n",
      "Evidence 14860.237\n",
      "\n",
      "Epoch: 235, Evidence: 14860.23730, Convergence: 0.000748\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.23e-01\n",
      "Epoch: 235, Loss: 2480.48081, Residuals: -0.91522, Convergence:   inf\n",
      "Epoch: 236, Loss: 2476.06799, Residuals: -0.91155, Convergence: 0.001782\n",
      "Epoch: 237, Loss: 2472.90221, Residuals: -0.91225, Convergence: 0.001280\n",
      "Epoch: 238, Loss: 2470.26942, Residuals: -0.91283, Convergence: 0.001066\n",
      "Epoch: 239, Loss: 2467.96145, Residuals: -0.91530, Convergence: 0.000935\n",
      "Evidence 14876.113\n",
      "\n",
      "Epoch: 239, Evidence: 14876.11328, Convergence: 0.001814\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.09e-01\n",
      "Epoch: 239, Loss: 2479.81676, Residuals: -0.91530, Convergence:   inf\n",
      "Epoch: 240, Loss: 2478.04676, Residuals: -0.91232, Convergence: 0.000714\n",
      "Evidence 14882.953\n",
      "\n",
      "Epoch: 240, Evidence: 14882.95312, Convergence: 0.000460\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.94e-02\n",
      "Epoch: 240, Loss: 2480.72824, Residuals: -0.91232, Convergence:   inf\n",
      "Epoch: 241, Loss: 2518.29564, Residuals: -0.94939, Convergence: -0.014918\n",
      "Epoch: 241, Loss: 2478.54584, Residuals: -0.91006, Convergence: 0.000881\n",
      "Evidence 14887.181\n",
      "\n",
      "Epoch: 241, Evidence: 14887.18066, Convergence: 0.000743\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.27e-02\n",
      "Epoch: 241, Loss: 2480.15319, Residuals: -0.91006, Convergence:   inf\n",
      "Epoch: 242, Loss: 2485.38938, Residuals: -0.91191, Convergence: -0.002107\n",
      "Epoch: 242, Loss: 2480.13365, Residuals: -0.90864, Convergence: 0.000008\n",
      "Evidence 14888.856\n",
      "\n",
      "Epoch: 242, Evidence: 14888.85645, Convergence: 0.000856\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 380.43997, Residuals: -4.51151, Convergence:   inf\n",
      "Epoch: 1, Loss: 354.78955, Residuals: -4.39158, Convergence: 0.072298\n",
      "Epoch: 2, Loss: 333.78284, Residuals: -4.22782, Convergence: 0.062935\n",
      "Epoch: 3, Loss: 317.73718, Residuals: -4.06329, Convergence: 0.050500\n",
      "Epoch: 4, Loss: 305.50989, Residuals: -3.91836, Convergence: 0.040023\n",
      "Epoch: 5, Loss: 295.81960, Residuals: -3.79035, Convergence: 0.032757\n",
      "Epoch: 6, Loss: 287.95366, Residuals: -3.67878, Convergence: 0.027317\n",
      "Epoch: 7, Loss: 281.42207, Residuals: -3.58311, Convergence: 0.023209\n",
      "Epoch: 8, Loss: 275.86346, Residuals: -3.50130, Convergence: 0.020150\n",
      "Epoch: 9, Loss: 271.02246, Residuals: -3.43107, Convergence: 0.017862\n",
      "Epoch: 10, Loss: 266.71769, Residuals: -3.37033, Convergence: 0.016140\n",
      "Epoch: 11, Loss: 262.81803, Residuals: -3.31733, Convergence: 0.014838\n",
      "Epoch: 12, Loss: 259.22801, Residuals: -3.27056, Convergence: 0.013849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Loss: 255.87870, Residuals: -3.22872, Convergence: 0.013089\n",
      "Epoch: 14, Loss: 252.72155, Residuals: -3.19067, Convergence: 0.012493\n",
      "Epoch: 15, Loss: 249.72533, Residuals: -3.15546, Convergence: 0.011998\n",
      "Epoch: 16, Loss: 246.87392, Residuals: -3.12244, Convergence: 0.011550\n",
      "Epoch: 17, Loss: 244.15709, Residuals: -3.09122, Convergence: 0.011127\n",
      "Epoch: 18, Loss: 241.55659, Residuals: -3.06141, Convergence: 0.010766\n",
      "Epoch: 19, Loss: 239.04230, Residuals: -3.03252, Convergence: 0.010518\n",
      "Epoch: 20, Loss: 236.57924, Residuals: -3.00397, Convergence: 0.010411\n",
      "Epoch: 21, Loss: 234.13675, Residuals: -2.97526, Convergence: 0.010432\n",
      "Epoch: 22, Loss: 231.69351, Residuals: -2.94603, Convergence: 0.010545\n",
      "Epoch: 23, Loss: 229.23064, Residuals: -2.91605, Convergence: 0.010744\n",
      "Epoch: 24, Loss: 226.71477, Residuals: -2.88497, Convergence: 0.011097\n",
      "Epoch: 25, Loss: 224.09209, Residuals: -2.85215, Convergence: 0.011704\n",
      "Epoch: 26, Loss: 221.32586, Residuals: -2.81712, Convergence: 0.012498\n",
      "Epoch: 27, Loss: 218.48566, Residuals: -2.78054, Convergence: 0.013000\n",
      "Epoch: 28, Loss: 215.70438, Residuals: -2.74391, Convergence: 0.012894\n",
      "Epoch: 29, Loss: 213.03313, Residuals: -2.70796, Convergence: 0.012539\n",
      "Epoch: 30, Loss: 210.46304, Residuals: -2.67272, Convergence: 0.012212\n",
      "Epoch: 31, Loss: 207.97457, Residuals: -2.63805, Convergence: 0.011965\n",
      "Epoch: 32, Loss: 205.55147, Residuals: -2.60381, Convergence: 0.011788\n",
      "Epoch: 33, Loss: 203.18241, Residuals: -2.56988, Convergence: 0.011660\n",
      "Epoch: 34, Loss: 200.86026, Residuals: -2.53617, Convergence: 0.011561\n",
      "Epoch: 35, Loss: 198.58099, Residuals: -2.50263, Convergence: 0.011478\n",
      "Epoch: 36, Loss: 196.34273, Residuals: -2.46919, Convergence: 0.011400\n",
      "Epoch: 37, Loss: 194.14488, Residuals: -2.43582, Convergence: 0.011321\n",
      "Epoch: 38, Loss: 191.98760, Residuals: -2.40251, Convergence: 0.011237\n",
      "Epoch: 39, Loss: 189.87140, Residuals: -2.36924, Convergence: 0.011145\n",
      "Epoch: 40, Loss: 187.79698, Residuals: -2.33601, Convergence: 0.011046\n",
      "Epoch: 41, Loss: 185.76525, Residuals: -2.30281, Convergence: 0.010937\n",
      "Epoch: 42, Loss: 183.77733, Residuals: -2.26964, Convergence: 0.010817\n",
      "Epoch: 43, Loss: 181.83482, Residuals: -2.23652, Convergence: 0.010683\n",
      "Epoch: 44, Loss: 179.93974, Residuals: -2.20349, Convergence: 0.010532\n",
      "Epoch: 45, Loss: 178.09451, Residuals: -2.17056, Convergence: 0.010361\n",
      "Epoch: 46, Loss: 176.30176, Residuals: -2.13780, Convergence: 0.010169\n",
      "Epoch: 47, Loss: 174.56403, Residuals: -2.10526, Convergence: 0.009955\n",
      "Epoch: 48, Loss: 172.88349, Residuals: -2.07299, Convergence: 0.009721\n",
      "Epoch: 49, Loss: 171.26186, Residuals: -2.04107, Convergence: 0.009469\n",
      "Epoch: 50, Loss: 169.70024, Residuals: -2.00955, Convergence: 0.009202\n",
      "Epoch: 51, Loss: 168.19912, Residuals: -1.97848, Convergence: 0.008925\n",
      "Epoch: 52, Loss: 166.75850, Residuals: -1.94792, Convergence: 0.008639\n",
      "Epoch: 53, Loss: 165.37788, Residuals: -1.91791, Convergence: 0.008348\n",
      "Epoch: 54, Loss: 164.05634, Residuals: -1.88851, Convergence: 0.008055\n",
      "Epoch: 55, Loss: 162.79262, Residuals: -1.85974, Convergence: 0.007763\n",
      "Epoch: 56, Loss: 161.58514, Residuals: -1.83164, Convergence: 0.007473\n",
      "Epoch: 57, Loss: 160.43201, Residuals: -1.80423, Convergence: 0.007188\n",
      "Epoch: 58, Loss: 159.33114, Residuals: -1.77753, Convergence: 0.006909\n",
      "Epoch: 59, Loss: 158.28022, Residuals: -1.75156, Convergence: 0.006640\n",
      "Epoch: 60, Loss: 157.27687, Residuals: -1.72630, Convergence: 0.006380\n",
      "Epoch: 61, Loss: 156.31873, Residuals: -1.70176, Convergence: 0.006129\n",
      "Epoch: 62, Loss: 155.40355, Residuals: -1.67794, Convergence: 0.005889\n",
      "Epoch: 63, Loss: 154.52927, Residuals: -1.65481, Convergence: 0.005658\n",
      "Epoch: 64, Loss: 153.69409, Residuals: -1.63239, Convergence: 0.005434\n",
      "Epoch: 65, Loss: 152.89644, Residuals: -1.61065, Convergence: 0.005217\n",
      "Epoch: 66, Loss: 152.13495, Residuals: -1.58959, Convergence: 0.005005\n",
      "Epoch: 67, Loss: 151.40843, Residuals: -1.56921, Convergence: 0.004798\n",
      "Epoch: 68, Loss: 150.71579, Residuals: -1.54951, Convergence: 0.004596\n",
      "Epoch: 69, Loss: 150.05597, Residuals: -1.53049, Convergence: 0.004397\n",
      "Epoch: 70, Loss: 149.42794, Residuals: -1.51213, Convergence: 0.004203\n",
      "Epoch: 71, Loss: 148.83065, Residuals: -1.49444, Convergence: 0.004013\n",
      "Epoch: 72, Loss: 148.26306, Residuals: -1.47741, Convergence: 0.003828\n",
      "Epoch: 73, Loss: 147.72404, Residuals: -1.46102, Convergence: 0.003649\n",
      "Epoch: 74, Loss: 147.21249, Residuals: -1.44528, Convergence: 0.003475\n",
      "Epoch: 75, Loss: 146.72726, Residuals: -1.43016, Convergence: 0.003307\n",
      "Epoch: 76, Loss: 146.26719, Residuals: -1.41565, Convergence: 0.003145\n",
      "Epoch: 77, Loss: 145.83113, Residuals: -1.40175, Convergence: 0.002990\n",
      "Epoch: 78, Loss: 145.41791, Residuals: -1.38842, Convergence: 0.002842\n",
      "Epoch: 79, Loss: 145.02643, Residuals: -1.37566, Convergence: 0.002699\n",
      "Epoch: 80, Loss: 144.65555, Residuals: -1.36344, Convergence: 0.002564\n",
      "Epoch: 81, Loss: 144.30424, Residuals: -1.35175, Convergence: 0.002435\n",
      "Epoch: 82, Loss: 143.97145, Residuals: -1.34057, Convergence: 0.002312\n",
      "Epoch: 83, Loss: 143.65621, Residuals: -1.32988, Convergence: 0.002194\n",
      "Epoch: 84, Loss: 143.35758, Residuals: -1.31966, Convergence: 0.002083\n",
      "Epoch: 85, Loss: 143.07469, Residuals: -1.30990, Convergence: 0.001977\n",
      "Epoch: 86, Loss: 142.80672, Residuals: -1.30056, Convergence: 0.001876\n",
      "Epoch: 87, Loss: 142.55287, Residuals: -1.29164, Convergence: 0.001781\n",
      "Epoch: 88, Loss: 142.31244, Residuals: -1.28312, Convergence: 0.001689\n",
      "Epoch: 89, Loss: 142.08475, Residuals: -1.27498, Convergence: 0.001602\n",
      "Epoch: 90, Loss: 141.86918, Residuals: -1.26720, Convergence: 0.001519\n",
      "Epoch: 91, Loss: 141.66516, Residuals: -1.25978, Convergence: 0.001440\n",
      "Epoch: 92, Loss: 141.47215, Residuals: -1.25268, Convergence: 0.001364\n",
      "Epoch: 93, Loss: 141.28969, Residuals: -1.24591, Convergence: 0.001291\n",
      "Epoch: 94, Loss: 141.11733, Residuals: -1.23945, Convergence: 0.001221\n",
      "Epoch: 95, Loss: 140.95465, Residuals: -1.23328, Convergence: 0.001154\n",
      "Epoch: 96, Loss: 140.80129, Residuals: -1.22740, Convergence: 0.001089\n",
      "Epoch: 97, Loss: 140.65691, Residuals: -1.22179, Convergence: 0.001027\n",
      "Epoch: 98, Loss: 140.52115, Residuals: -1.21645, Convergence: 0.000966\n",
      "Evidence -181.803\n",
      "\n",
      "Epoch: 98, Evidence: -181.80298, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.24e-01\n",
      "Epoch: 98, Loss: 1360.17007, Residuals: -1.21645, Convergence:   inf\n",
      "Epoch: 99, Loss: 1298.10517, Residuals: -1.24579, Convergence: 0.047812\n",
      "Epoch: 100, Loss: 1251.14934, Residuals: -1.26882, Convergence: 0.037530\n",
      "Epoch: 101, Loss: 1215.88690, Residuals: -1.28511, Convergence: 0.029001\n",
      "Epoch: 102, Loss: 1188.55787, Residuals: -1.29627, Convergence: 0.022993\n",
      "Epoch: 103, Loss: 1166.49842, Residuals: -1.30430, Convergence: 0.018911\n",
      "Epoch: 104, Loss: 1148.20358, Residuals: -1.31029, Convergence: 0.015933\n",
      "Epoch: 105, Loss: 1132.77634, Residuals: -1.31472, Convergence: 0.013619\n",
      "Epoch: 106, Loss: 1119.61088, Residuals: -1.31785, Convergence: 0.011759\n",
      "Epoch: 107, Loss: 1108.25945, Residuals: -1.31983, Convergence: 0.010243\n",
      "Epoch: 108, Loss: 1098.37337, Residuals: -1.32080, Convergence: 0.009001\n",
      "Epoch: 109, Loss: 1089.67453, Residuals: -1.32086, Convergence: 0.007983\n",
      "Epoch: 110, Loss: 1081.93482, Residuals: -1.32011, Convergence: 0.007154\n",
      "Epoch: 111, Loss: 1074.96652, Residuals: -1.31862, Convergence: 0.006482\n",
      "Epoch: 112, Loss: 1068.61352, Residuals: -1.31646, Convergence: 0.005945\n",
      "Epoch: 113, Loss: 1062.74713, Residuals: -1.31370, Convergence: 0.005520\n",
      "Epoch: 114, Loss: 1057.26338, Residuals: -1.31038, Convergence: 0.005187\n",
      "Epoch: 115, Loss: 1052.07918, Residuals: -1.30654, Convergence: 0.004928\n",
      "Epoch: 116, Loss: 1047.13227, Residuals: -1.30223, Convergence: 0.004724\n",
      "Epoch: 117, Loss: 1042.37410, Residuals: -1.29748, Convergence: 0.004565\n",
      "Epoch: 118, Loss: 1037.76746, Residuals: -1.29232, Convergence: 0.004439\n",
      "Epoch: 119, Loss: 1033.28021, Residuals: -1.28677, Convergence: 0.004343\n",
      "Epoch: 120, Loss: 1028.88079, Residuals: -1.28085, Convergence: 0.004276\n",
      "Epoch: 121, Loss: 1024.53787, Residuals: -1.27459, Convergence: 0.004239\n",
      "Epoch: 122, Loss: 1020.22404, Residuals: -1.26802, Convergence: 0.004228\n",
      "Epoch: 123, Loss: 1015.92362, Residuals: -1.26117, Convergence: 0.004233\n",
      "Epoch: 124, Loss: 1011.64412, Residuals: -1.25410, Convergence: 0.004230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 125, Loss: 1007.41825, Residuals: -1.24688, Convergence: 0.004195\n",
      "Epoch: 126, Loss: 1003.29869, Residuals: -1.23959, Convergence: 0.004106\n",
      "Epoch: 127, Loss: 999.34048, Residuals: -1.23228, Convergence: 0.003961\n",
      "Epoch: 128, Loss: 995.58374, Residuals: -1.22500, Convergence: 0.003773\n",
      "Epoch: 129, Loss: 992.04827, Residuals: -1.21780, Convergence: 0.003564\n",
      "Epoch: 130, Loss: 988.73449, Residuals: -1.21070, Convergence: 0.003352\n",
      "Epoch: 131, Loss: 985.63201, Residuals: -1.20374, Convergence: 0.003148\n",
      "Epoch: 132, Loss: 982.72436, Residuals: -1.19693, Convergence: 0.002959\n",
      "Epoch: 133, Loss: 979.99453, Residuals: -1.19030, Convergence: 0.002786\n",
      "Epoch: 134, Loss: 977.42615, Residuals: -1.18386, Convergence: 0.002628\n",
      "Epoch: 135, Loss: 975.00455, Residuals: -1.17763, Convergence: 0.002484\n",
      "Epoch: 136, Loss: 972.71730, Residuals: -1.17161, Convergence: 0.002351\n",
      "Epoch: 137, Loss: 970.55279, Residuals: -1.16581, Convergence: 0.002230\n",
      "Epoch: 138, Loss: 968.50106, Residuals: -1.16024, Convergence: 0.002118\n",
      "Epoch: 139, Loss: 966.55402, Residuals: -1.15488, Convergence: 0.002014\n",
      "Epoch: 140, Loss: 964.70298, Residuals: -1.14976, Convergence: 0.001919\n",
      "Epoch: 141, Loss: 962.94099, Residuals: -1.14485, Convergence: 0.001830\n",
      "Epoch: 142, Loss: 961.26166, Residuals: -1.14015, Convergence: 0.001747\n",
      "Epoch: 143, Loss: 959.65876, Residuals: -1.13566, Convergence: 0.001670\n",
      "Epoch: 144, Loss: 958.12628, Residuals: -1.13138, Convergence: 0.001599\n",
      "Epoch: 145, Loss: 956.65981, Residuals: -1.12729, Convergence: 0.001533\n",
      "Epoch: 146, Loss: 955.25401, Residuals: -1.12338, Convergence: 0.001472\n",
      "Epoch: 147, Loss: 953.90468, Residuals: -1.11966, Convergence: 0.001415\n",
      "Epoch: 148, Loss: 952.60768, Residuals: -1.11610, Convergence: 0.001362\n",
      "Epoch: 149, Loss: 951.35914, Residuals: -1.11270, Convergence: 0.001312\n",
      "Epoch: 150, Loss: 950.15625, Residuals: -1.10946, Convergence: 0.001266\n",
      "Epoch: 151, Loss: 948.99502, Residuals: -1.10636, Convergence: 0.001224\n",
      "Epoch: 152, Loss: 947.87327, Residuals: -1.10340, Convergence: 0.001183\n",
      "Epoch: 153, Loss: 946.78805, Residuals: -1.10056, Convergence: 0.001146\n",
      "Epoch: 154, Loss: 945.73723, Residuals: -1.09785, Convergence: 0.001111\n",
      "Epoch: 155, Loss: 944.71825, Residuals: -1.09525, Convergence: 0.001079\n",
      "Epoch: 156, Loss: 943.72910, Residuals: -1.09276, Convergence: 0.001048\n",
      "Epoch: 157, Loss: 942.76806, Residuals: -1.09037, Convergence: 0.001019\n",
      "Epoch: 158, Loss: 941.83327, Residuals: -1.08806, Convergence: 0.000993\n",
      "Evidence 11107.441\n",
      "\n",
      "Epoch: 158, Evidence: 11107.44141, Convergence: 1.016368\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.76e-01\n",
      "Epoch: 158, Loss: 2332.42000, Residuals: -1.08806, Convergence:   inf\n",
      "Epoch: 159, Loss: 2289.18552, Residuals: -1.09817, Convergence: 0.018886\n",
      "Epoch: 160, Loss: 2259.26235, Residuals: -1.09762, Convergence: 0.013245\n",
      "Epoch: 161, Loss: 2234.17079, Residuals: -1.09596, Convergence: 0.011231\n",
      "Epoch: 162, Loss: 2212.83026, Residuals: -1.09385, Convergence: 0.009644\n",
      "Epoch: 163, Loss: 2194.53789, Residuals: -1.09142, Convergence: 0.008335\n",
      "Epoch: 164, Loss: 2178.75019, Residuals: -1.08875, Convergence: 0.007246\n",
      "Epoch: 165, Loss: 2165.02454, Residuals: -1.08587, Convergence: 0.006340\n",
      "Epoch: 166, Loss: 2152.99739, Residuals: -1.08282, Convergence: 0.005586\n",
      "Epoch: 167, Loss: 2142.36889, Residuals: -1.07963, Convergence: 0.004961\n",
      "Epoch: 168, Loss: 2132.89142, Residuals: -1.07632, Convergence: 0.004443\n",
      "Epoch: 169, Loss: 2124.35931, Residuals: -1.07293, Convergence: 0.004016\n",
      "Epoch: 170, Loss: 2116.60074, Residuals: -1.06946, Convergence: 0.003666\n",
      "Epoch: 171, Loss: 2109.47408, Residuals: -1.06592, Convergence: 0.003378\n",
      "Epoch: 172, Loss: 2102.86311, Residuals: -1.06232, Convergence: 0.003144\n",
      "Epoch: 173, Loss: 2096.68365, Residuals: -1.05866, Convergence: 0.002947\n",
      "Epoch: 174, Loss: 2090.88239, Residuals: -1.05496, Convergence: 0.002775\n",
      "Epoch: 175, Loss: 2085.43585, Residuals: -1.05123, Convergence: 0.002612\n",
      "Epoch: 176, Loss: 2080.33460, Residuals: -1.04753, Convergence: 0.002452\n",
      "Epoch: 177, Loss: 2075.57660, Residuals: -1.04389, Convergence: 0.002292\n",
      "Epoch: 178, Loss: 2071.15328, Residuals: -1.04034, Convergence: 0.002136\n",
      "Epoch: 179, Loss: 2067.05206, Residuals: -1.03692, Convergence: 0.001984\n",
      "Epoch: 180, Loss: 2063.25288, Residuals: -1.03363, Convergence: 0.001841\n",
      "Epoch: 181, Loss: 2059.73451, Residuals: -1.03050, Convergence: 0.001708\n",
      "Epoch: 182, Loss: 2056.47304, Residuals: -1.02752, Convergence: 0.001586\n",
      "Epoch: 183, Loss: 2053.44229, Residuals: -1.02469, Convergence: 0.001476\n",
      "Epoch: 184, Loss: 2050.62049, Residuals: -1.02201, Convergence: 0.001376\n",
      "Epoch: 185, Loss: 2047.98428, Residuals: -1.01948, Convergence: 0.001287\n",
      "Epoch: 186, Loss: 2045.51379, Residuals: -1.01708, Convergence: 0.001208\n",
      "Epoch: 187, Loss: 2043.18990, Residuals: -1.01482, Convergence: 0.001137\n",
      "Epoch: 188, Loss: 2040.99609, Residuals: -1.01267, Convergence: 0.001075\n",
      "Epoch: 189, Loss: 2038.91786, Residuals: -1.01064, Convergence: 0.001019\n",
      "Epoch: 190, Loss: 2036.94280, Residuals: -1.00872, Convergence: 0.000970\n",
      "Evidence 14310.444\n",
      "\n",
      "Epoch: 190, Evidence: 14310.44434, Convergence: 0.223823\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.38e-01\n",
      "Epoch: 190, Loss: 2441.01070, Residuals: -1.00872, Convergence:   inf\n",
      "Epoch: 191, Loss: 2425.60174, Residuals: -1.00514, Convergence: 0.006353\n",
      "Epoch: 192, Loss: 2413.06102, Residuals: -1.00095, Convergence: 0.005197\n",
      "Epoch: 193, Loss: 2402.27802, Residuals: -0.99691, Convergence: 0.004489\n",
      "Epoch: 194, Loss: 2392.95383, Residuals: -0.99309, Convergence: 0.003897\n",
      "Epoch: 195, Loss: 2384.85006, Residuals: -0.98953, Convergence: 0.003398\n",
      "Epoch: 196, Loss: 2377.76738, Residuals: -0.98623, Convergence: 0.002979\n",
      "Epoch: 197, Loss: 2371.53998, Residuals: -0.98320, Convergence: 0.002626\n",
      "Epoch: 198, Loss: 2366.03202, Residuals: -0.98042, Convergence: 0.002328\n",
      "Epoch: 199, Loss: 2361.12727, Residuals: -0.97788, Convergence: 0.002077\n",
      "Epoch: 200, Loss: 2356.73268, Residuals: -0.97554, Convergence: 0.001865\n",
      "Epoch: 201, Loss: 2352.76963, Residuals: -0.97341, Convergence: 0.001684\n",
      "Epoch: 202, Loss: 2349.17621, Residuals: -0.97144, Convergence: 0.001530\n",
      "Epoch: 203, Loss: 2345.89945, Residuals: -0.96964, Convergence: 0.001397\n",
      "Epoch: 204, Loss: 2342.89654, Residuals: -0.96797, Convergence: 0.001282\n",
      "Epoch: 205, Loss: 2340.13271, Residuals: -0.96642, Convergence: 0.001181\n",
      "Epoch: 206, Loss: 2337.57854, Residuals: -0.96499, Convergence: 0.001093\n",
      "Epoch: 207, Loss: 2335.20925, Residuals: -0.96367, Convergence: 0.001015\n",
      "Epoch: 208, Loss: 2333.00463, Residuals: -0.96243, Convergence: 0.000945\n",
      "Evidence 14795.411\n",
      "\n",
      "Epoch: 208, Evidence: 14795.41113, Convergence: 0.032778\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.34e-01\n",
      "Epoch: 208, Loss: 2445.25234, Residuals: -0.96243, Convergence:   inf\n",
      "Epoch: 209, Loss: 2438.44500, Residuals: -0.95869, Convergence: 0.002792\n",
      "Epoch: 210, Loss: 2432.75697, Residuals: -0.95539, Convergence: 0.002338\n",
      "Epoch: 211, Loss: 2427.91216, Residuals: -0.95255, Convergence: 0.001995\n",
      "Epoch: 212, Loss: 2423.73435, Residuals: -0.95010, Convergence: 0.001724\n",
      "Epoch: 213, Loss: 2420.08347, Residuals: -0.94800, Convergence: 0.001509\n",
      "Epoch: 214, Loss: 2416.85176, Residuals: -0.94620, Convergence: 0.001337\n",
      "Epoch: 215, Loss: 2413.95751, Residuals: -0.94466, Convergence: 0.001199\n",
      "Epoch: 216, Loss: 2411.34078, Residuals: -0.94333, Convergence: 0.001085\n",
      "Epoch: 217, Loss: 2408.95403, Residuals: -0.94218, Convergence: 0.000991\n",
      "Evidence 14889.509\n",
      "\n",
      "Epoch: 217, Evidence: 14889.50879, Convergence: 0.006320\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.61e-01\n",
      "Epoch: 217, Loss: 2447.04780, Residuals: -0.94218, Convergence:   inf\n",
      "Epoch: 218, Loss: 2443.05135, Residuals: -0.93924, Convergence: 0.001636\n",
      "Epoch: 219, Loss: 2439.68650, Residuals: -0.93689, Convergence: 0.001379\n",
      "Epoch: 220, Loss: 2436.78110, Residuals: -0.93500, Convergence: 0.001192\n",
      "Epoch: 221, Loss: 2434.22297, Residuals: -0.93346, Convergence: 0.001051\n",
      "Epoch: 222, Loss: 2431.93421, Residuals: -0.93222, Convergence: 0.000941\n",
      "Evidence 14921.212\n",
      "\n",
      "Epoch: 222, Evidence: 14921.21191, Convergence: 0.002125\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.10e-01\n",
      "Epoch: 222, Loss: 2448.09555, Residuals: -0.93222, Convergence:   inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 223, Loss: 2445.21888, Residuals: -0.92992, Convergence: 0.001176\n",
      "Epoch: 224, Loss: 2442.77394, Residuals: -0.92815, Convergence: 0.001001\n",
      "Epoch: 225, Loss: 2440.62962, Residuals: -0.92678, Convergence: 0.000879\n",
      "Evidence 14936.110\n",
      "\n",
      "Epoch: 225, Evidence: 14936.11035, Convergence: 0.000997\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.73e-01\n",
      "Epoch: 225, Loss: 2448.82288, Residuals: -0.92678, Convergence:   inf\n",
      "Epoch: 226, Loss: 2444.41545, Residuals: -0.92352, Convergence: 0.001803\n",
      "Epoch: 227, Loss: 2441.02831, Residuals: -0.92166, Convergence: 0.001388\n",
      "Epoch: 228, Loss: 2438.22630, Residuals: -0.92054, Convergence: 0.001149\n",
      "Epoch: 229, Loss: 2435.81420, Residuals: -0.91996, Convergence: 0.000990\n",
      "Evidence 14953.889\n",
      "\n",
      "Epoch: 229, Evidence: 14953.88867, Convergence: 0.002185\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.43e-01\n",
      "Epoch: 229, Loss: 2449.03931, Residuals: -0.91996, Convergence:   inf\n",
      "Epoch: 230, Loss: 2446.11612, Residuals: -0.91733, Convergence: 0.001195\n",
      "Epoch: 231, Loss: 2443.77675, Residuals: -0.91613, Convergence: 0.000957\n",
      "Evidence 14964.285\n",
      "\n",
      "Epoch: 231, Evidence: 14964.28516, Convergence: 0.000695\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.21e-01\n",
      "Epoch: 231, Loss: 2449.31942, Residuals: -0.91613, Convergence:   inf\n",
      "Epoch: 232, Loss: 2445.11298, Residuals: -0.91244, Convergence: 0.001720\n",
      "Epoch: 233, Loss: 2441.97297, Residuals: -0.91378, Convergence: 0.001286\n",
      "Epoch: 234, Loss: 2439.28365, Residuals: -0.91472, Convergence: 0.001103\n",
      "Epoch: 235, Loss: 2436.98696, Residuals: -0.91755, Convergence: 0.000942\n",
      "Evidence 14979.608\n",
      "\n",
      "Epoch: 235, Evidence: 14979.60840, Convergence: 0.001717\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.07e-01\n",
      "Epoch: 235, Loss: 2448.76052, Residuals: -0.91755, Convergence:   inf\n",
      "Epoch: 236, Loss: 2447.36161, Residuals: -0.91489, Convergence: 0.000572\n",
      "Evidence 14985.725\n",
      "\n",
      "Epoch: 236, Evidence: 14985.72461, Convergence: 0.000408\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.75e-02\n",
      "Epoch: 236, Loss: 2449.74188, Residuals: -0.91489, Convergence:   inf\n",
      "Epoch: 237, Loss: 2495.49623, Residuals: -0.95137, Convergence: -0.018335\n",
      "Epoch: 237, Loss: 2447.96591, Residuals: -0.91210, Convergence: 0.000725\n",
      "Evidence 14989.448\n",
      "\n",
      "Epoch: 237, Evidence: 14989.44824, Convergence: 0.000656\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.99e-02\n",
      "Epoch: 237, Loss: 2449.26051, Residuals: -0.91210, Convergence:   inf\n",
      "Epoch: 238, Loss: 2456.29649, Residuals: -0.91369, Convergence: -0.002864\n",
      "Epoch: 238, Loss: 2450.08746, Residuals: -0.91021, Convergence: -0.000338\n",
      "Evidence 14990.166\n",
      "\n",
      "Epoch: 238, Evidence: 14990.16602, Convergence: 0.000704\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 381.75229, Residuals: -4.52619, Convergence:   inf\n",
      "Epoch: 1, Loss: 355.92075, Residuals: -4.40507, Convergence: 0.072577\n",
      "Epoch: 2, Loss: 334.81646, Residuals: -4.23944, Convergence: 0.063032\n",
      "Epoch: 3, Loss: 318.72652, Residuals: -4.07451, Convergence: 0.050482\n",
      "Epoch: 4, Loss: 306.47045, Residuals: -3.93008, Convergence: 0.039991\n",
      "Epoch: 5, Loss: 296.75982, Residuals: -3.80304, Convergence: 0.032722\n",
      "Epoch: 6, Loss: 288.88089, Residuals: -3.69254, Convergence: 0.027274\n",
      "Epoch: 7, Loss: 282.34484, Residuals: -3.59785, Convergence: 0.023149\n",
      "Epoch: 8, Loss: 276.78914, Residuals: -3.51678, Convergence: 0.020072\n",
      "Epoch: 9, Loss: 271.95625, Residuals: -3.44694, Convergence: 0.017771\n",
      "Epoch: 10, Loss: 267.66227, Residuals: -3.38626, Convergence: 0.016043\n",
      "Epoch: 11, Loss: 263.77361, Residuals: -3.33301, Convergence: 0.014742\n",
      "Epoch: 12, Loss: 260.19264, Residuals: -3.28577, Convergence: 0.013763\n",
      "Epoch: 13, Loss: 256.84879, Residuals: -3.24331, Convergence: 0.013019\n",
      "Epoch: 14, Loss: 253.69262, Residuals: -3.20460, Convergence: 0.012441\n",
      "Epoch: 15, Loss: 250.69197, Residuals: -3.16875, Convergence: 0.011969\n",
      "Epoch: 16, Loss: 247.82947, Residuals: -3.13509, Convergence: 0.011550\n",
      "Epoch: 17, Loss: 245.09561, Residuals: -3.10324, Convergence: 0.011154\n",
      "Epoch: 18, Loss: 242.47603, Residuals: -3.07286, Convergence: 0.010803\n",
      "Epoch: 19, Loss: 239.94480, Residuals: -3.04352, Convergence: 0.010549\n",
      "Epoch: 20, Loss: 237.46923, Residuals: -3.01469, Convergence: 0.010425\n",
      "Epoch: 21, Loss: 235.01843, Residuals: -2.98582, Convergence: 0.010428\n",
      "Epoch: 22, Loss: 232.56922, Residuals: -2.95653, Convergence: 0.010531\n",
      "Epoch: 23, Loss: 230.10232, Residuals: -2.92653, Convergence: 0.010721\n",
      "Epoch: 24, Loss: 227.58811, Residuals: -2.89544, Convergence: 0.011047\n",
      "Epoch: 25, Loss: 224.97932, Residuals: -2.86271, Convergence: 0.011596\n",
      "Epoch: 26, Loss: 222.24056, Residuals: -2.82784, Convergence: 0.012323\n",
      "Epoch: 27, Loss: 219.42677, Residuals: -2.79137, Convergence: 0.012823\n",
      "Epoch: 28, Loss: 216.66072, Residuals: -2.75469, Convergence: 0.012767\n",
      "Epoch: 29, Loss: 213.99799, Residuals: -2.71858, Convergence: 0.012443\n",
      "Epoch: 30, Loss: 211.43306, Residuals: -2.68312, Convergence: 0.012131\n",
      "Epoch: 31, Loss: 208.94694, Residuals: -2.64819, Convergence: 0.011898\n",
      "Epoch: 32, Loss: 206.52284, Residuals: -2.61365, Convergence: 0.011738\n",
      "Epoch: 33, Loss: 204.14868, Residuals: -2.57939, Convergence: 0.011630\n",
      "Epoch: 34, Loss: 201.81689, Residuals: -2.54529, Convergence: 0.011554\n",
      "Epoch: 35, Loss: 199.52350, Residuals: -2.51131, Convergence: 0.011494\n",
      "Epoch: 36, Loss: 197.26732, Residuals: -2.47739, Convergence: 0.011437\n",
      "Epoch: 37, Loss: 195.04905, Residuals: -2.44350, Convergence: 0.011373\n",
      "Epoch: 38, Loss: 192.87053, Residuals: -2.40964, Convergence: 0.011295\n",
      "Epoch: 39, Loss: 190.73415, Residuals: -2.37581, Convergence: 0.011201\n",
      "Epoch: 40, Loss: 188.64240, Residuals: -2.34203, Convergence: 0.011088\n",
      "Epoch: 41, Loss: 186.59762, Residuals: -2.30831, Convergence: 0.010958\n",
      "Epoch: 42, Loss: 184.60186, Residuals: -2.27469, Convergence: 0.010811\n",
      "Epoch: 43, Loss: 182.65683, Residuals: -2.24119, Convergence: 0.010649\n",
      "Epoch: 44, Loss: 180.76404, Residuals: -2.20784, Convergence: 0.010471\n",
      "Epoch: 45, Loss: 178.92489, Residuals: -2.17467, Convergence: 0.010279\n",
      "Epoch: 46, Loss: 177.14079, Residuals: -2.14174, Convergence: 0.010072\n",
      "Epoch: 47, Loss: 175.41325, Residuals: -2.10908, Convergence: 0.009848\n",
      "Epoch: 48, Loss: 173.74382, Residuals: -2.07674, Convergence: 0.009609\n",
      "Epoch: 49, Loss: 172.13393, Residuals: -2.04478, Convergence: 0.009353\n",
      "Epoch: 50, Loss: 170.58469, Residuals: -2.01325, Convergence: 0.009082\n",
      "Epoch: 51, Loss: 169.09663, Residuals: -1.98219, Convergence: 0.008800\n",
      "Epoch: 52, Loss: 167.66964, Residuals: -1.95166, Convergence: 0.008511\n",
      "Epoch: 53, Loss: 166.30291, Residuals: -1.92169, Convergence: 0.008218\n",
      "Epoch: 54, Loss: 164.99504, Residuals: -1.89230, Convergence: 0.007927\n",
      "Epoch: 55, Loss: 163.74420, Residuals: -1.86351, Convergence: 0.007639\n",
      "Epoch: 56, Loss: 162.54834, Residuals: -1.83534, Convergence: 0.007357\n",
      "Epoch: 57, Loss: 161.40531, Residuals: -1.80780, Convergence: 0.007082\n",
      "Epoch: 58, Loss: 160.31305, Residuals: -1.78090, Convergence: 0.006813\n",
      "Epoch: 59, Loss: 159.26964, Residuals: -1.75464, Convergence: 0.006551\n",
      "Epoch: 60, Loss: 158.27338, Residuals: -1.72902, Convergence: 0.006295\n",
      "Epoch: 61, Loss: 157.32276, Residuals: -1.70408, Convergence: 0.006042\n",
      "Epoch: 62, Loss: 156.41645, Residuals: -1.67981, Convergence: 0.005794\n",
      "Epoch: 63, Loss: 155.55329, Residuals: -1.65623, Convergence: 0.005549\n",
      "Epoch: 64, Loss: 154.73219, Residuals: -1.63336, Convergence: 0.005307\n",
      "Epoch: 65, Loss: 153.95209, Residuals: -1.61121, Convergence: 0.005067\n",
      "Epoch: 66, Loss: 153.21194, Residuals: -1.58980, Convergence: 0.004831\n",
      "Epoch: 67, Loss: 152.51056, Residuals: -1.56913, Convergence: 0.004599\n",
      "Epoch: 68, Loss: 151.84671, Residuals: -1.54922, Convergence: 0.004372\n",
      "Epoch: 69, Loss: 151.21891, Residuals: -1.53007, Convergence: 0.004152\n",
      "Epoch: 70, Loss: 150.62558, Residuals: -1.51168, Convergence: 0.003939\n",
      "Epoch: 71, Loss: 150.06495, Residuals: -1.49404, Convergence: 0.003736\n",
      "Epoch: 72, Loss: 149.53512, Residuals: -1.47713, Convergence: 0.003543\n",
      "Epoch: 73, Loss: 149.03420, Residuals: -1.46095, Convergence: 0.003361\n",
      "Epoch: 74, Loss: 148.56031, Residuals: -1.44546, Convergence: 0.003190\n",
      "Epoch: 75, Loss: 148.11168, Residuals: -1.43064, Convergence: 0.003029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 76, Loss: 147.68670, Residuals: -1.41647, Convergence: 0.002878\n",
      "Epoch: 77, Loss: 147.28394, Residuals: -1.40292, Convergence: 0.002735\n",
      "Epoch: 78, Loss: 146.90207, Residuals: -1.38999, Convergence: 0.002599\n",
      "Epoch: 79, Loss: 146.53985, Residuals: -1.37765, Convergence: 0.002472\n",
      "Epoch: 80, Loss: 146.19606, Residuals: -1.36587, Convergence: 0.002352\n",
      "Epoch: 81, Loss: 145.86944, Residuals: -1.35466, Convergence: 0.002239\n",
      "Epoch: 82, Loss: 145.55866, Residuals: -1.34397, Convergence: 0.002135\n",
      "Epoch: 83, Loss: 145.26234, Residuals: -1.33379, Convergence: 0.002040\n",
      "Epoch: 84, Loss: 144.97900, Residuals: -1.32410, Convergence: 0.001954\n",
      "Epoch: 85, Loss: 144.70717, Residuals: -1.31484, Convergence: 0.001878\n",
      "Epoch: 86, Loss: 144.44548, Residuals: -1.30600, Convergence: 0.001812\n",
      "Epoch: 87, Loss: 144.19261, Residuals: -1.29754, Convergence: 0.001754\n",
      "Epoch: 88, Loss: 143.94748, Residuals: -1.28942, Convergence: 0.001703\n",
      "Epoch: 89, Loss: 143.70918, Residuals: -1.28161, Convergence: 0.001658\n",
      "Epoch: 90, Loss: 143.47704, Residuals: -1.27409, Convergence: 0.001618\n",
      "Epoch: 91, Loss: 143.25055, Residuals: -1.26683, Convergence: 0.001581\n",
      "Epoch: 92, Loss: 143.02940, Residuals: -1.25981, Convergence: 0.001546\n",
      "Epoch: 93, Loss: 142.81341, Residuals: -1.25303, Convergence: 0.001512\n",
      "Epoch: 94, Loss: 142.60247, Residuals: -1.24647, Convergence: 0.001479\n",
      "Epoch: 95, Loss: 142.39655, Residuals: -1.24012, Convergence: 0.001446\n",
      "Epoch: 96, Loss: 142.19564, Residuals: -1.23397, Convergence: 0.001413\n",
      "Epoch: 97, Loss: 141.99975, Residuals: -1.22802, Convergence: 0.001380\n",
      "Epoch: 98, Loss: 141.80888, Residuals: -1.22225, Convergence: 0.001346\n",
      "Epoch: 99, Loss: 141.62303, Residuals: -1.21668, Convergence: 0.001312\n",
      "Epoch: 100, Loss: 141.44220, Residuals: -1.21128, Convergence: 0.001279\n",
      "Epoch: 101, Loss: 141.26634, Residuals: -1.20606, Convergence: 0.001245\n",
      "Epoch: 102, Loss: 141.09543, Residuals: -1.20102, Convergence: 0.001211\n",
      "Epoch: 103, Loss: 140.92939, Residuals: -1.19613, Convergence: 0.001178\n",
      "Epoch: 104, Loss: 140.76816, Residuals: -1.19141, Convergence: 0.001145\n",
      "Epoch: 105, Loss: 140.61166, Residuals: -1.18684, Convergence: 0.001113\n",
      "Epoch: 106, Loss: 140.45979, Residuals: -1.18242, Convergence: 0.001081\n",
      "Epoch: 107, Loss: 140.31246, Residuals: -1.17815, Convergence: 0.001050\n",
      "Epoch: 108, Loss: 140.16955, Residuals: -1.17403, Convergence: 0.001020\n",
      "Epoch: 109, Loss: 140.03097, Residuals: -1.17003, Convergence: 0.000990\n",
      "Evidence -180.764\n",
      "\n",
      "Epoch: 109, Evidence: -180.76369, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.24e-01\n",
      "Epoch: 109, Loss: 1365.83847, Residuals: -1.17003, Convergence:   inf\n",
      "Epoch: 110, Loss: 1309.36309, Residuals: -1.19567, Convergence: 0.043132\n",
      "Epoch: 111, Loss: 1266.16672, Residuals: -1.21729, Convergence: 0.034116\n",
      "Epoch: 112, Loss: 1233.28345, Residuals: -1.23420, Convergence: 0.026663\n",
      "Epoch: 113, Loss: 1207.40522, Residuals: -1.24732, Convergence: 0.021433\n",
      "Epoch: 114, Loss: 1186.31709, Residuals: -1.25788, Convergence: 0.017776\n",
      "Epoch: 115, Loss: 1168.78172, Residuals: -1.26648, Convergence: 0.015003\n",
      "Epoch: 116, Loss: 1154.01660, Residuals: -1.27341, Convergence: 0.012795\n",
      "Epoch: 117, Loss: 1141.45718, Residuals: -1.27886, Convergence: 0.011003\n",
      "Epoch: 118, Loss: 1130.66724, Residuals: -1.28297, Convergence: 0.009543\n",
      "Epoch: 119, Loss: 1121.29802, Residuals: -1.28589, Convergence: 0.008356\n",
      "Epoch: 120, Loss: 1113.06586, Residuals: -1.28775, Convergence: 0.007396\n",
      "Epoch: 121, Loss: 1105.73433, Residuals: -1.28865, Convergence: 0.006630\n",
      "Epoch: 122, Loss: 1099.10426, Residuals: -1.28868, Convergence: 0.006032\n",
      "Epoch: 123, Loss: 1093.00175, Residuals: -1.28789, Convergence: 0.005583\n",
      "Epoch: 124, Loss: 1087.27483, Residuals: -1.28634, Convergence: 0.005267\n",
      "Epoch: 125, Loss: 1081.78633, Residuals: -1.28406, Convergence: 0.005074\n",
      "Epoch: 126, Loss: 1076.41365, Residuals: -1.28105, Convergence: 0.004991\n",
      "Epoch: 127, Loss: 1071.05064, Residuals: -1.27733, Convergence: 0.005007\n",
      "Epoch: 128, Loss: 1065.61321, Residuals: -1.27291, Convergence: 0.005103\n",
      "Epoch: 129, Loss: 1060.05117, Residuals: -1.26782, Convergence: 0.005247\n",
      "Epoch: 130, Loss: 1054.36662, Residuals: -1.26213, Convergence: 0.005391\n",
      "Epoch: 131, Loss: 1048.62469, Residuals: -1.25594, Convergence: 0.005476\n",
      "Epoch: 132, Loss: 1042.94368, Residuals: -1.24933, Convergence: 0.005447\n",
      "Epoch: 133, Loss: 1037.45280, Residuals: -1.24241, Convergence: 0.005293\n",
      "Epoch: 134, Loss: 1032.24832, Residuals: -1.23527, Convergence: 0.005042\n",
      "Epoch: 135, Loss: 1027.37503, Residuals: -1.22800, Convergence: 0.004743\n",
      "Epoch: 136, Loss: 1022.83625, Residuals: -1.22067, Convergence: 0.004437\n",
      "Epoch: 137, Loss: 1018.61070, Residuals: -1.21335, Convergence: 0.004148\n",
      "Epoch: 138, Loss: 1014.66700, Residuals: -1.20607, Convergence: 0.003887\n",
      "Epoch: 139, Loss: 1010.97298, Residuals: -1.19890, Convergence: 0.003654\n",
      "Epoch: 140, Loss: 1007.49926, Residuals: -1.19185, Convergence: 0.003448\n",
      "Epoch: 141, Loss: 1004.22148, Residuals: -1.18496, Convergence: 0.003264\n",
      "Epoch: 142, Loss: 1001.11941, Residuals: -1.17825, Convergence: 0.003099\n",
      "Epoch: 143, Loss: 998.17723, Residuals: -1.17174, Convergence: 0.002948\n",
      "Epoch: 144, Loss: 995.38205, Residuals: -1.16544, Convergence: 0.002808\n",
      "Epoch: 145, Loss: 992.72404, Residuals: -1.15937, Convergence: 0.002677\n",
      "Epoch: 146, Loss: 990.19428, Residuals: -1.15354, Convergence: 0.002555\n",
      "Epoch: 147, Loss: 987.78643, Residuals: -1.14795, Convergence: 0.002438\n",
      "Epoch: 148, Loss: 985.49357, Residuals: -1.14260, Convergence: 0.002327\n",
      "Epoch: 149, Loss: 983.31047, Residuals: -1.13749, Convergence: 0.002220\n",
      "Epoch: 150, Loss: 981.23189, Residuals: -1.13262, Convergence: 0.002118\n",
      "Epoch: 151, Loss: 979.25298, Residuals: -1.12798, Convergence: 0.002021\n",
      "Epoch: 152, Loss: 977.36875, Residuals: -1.12357, Convergence: 0.001928\n",
      "Epoch: 153, Loss: 975.57468, Residuals: -1.11937, Convergence: 0.001839\n",
      "Epoch: 154, Loss: 973.86604, Residuals: -1.11539, Convergence: 0.001754\n",
      "Epoch: 155, Loss: 972.23820, Residuals: -1.11160, Convergence: 0.001674\n",
      "Epoch: 156, Loss: 970.68662, Residuals: -1.10801, Convergence: 0.001598\n",
      "Epoch: 157, Loss: 969.20603, Residuals: -1.10459, Convergence: 0.001528\n",
      "Epoch: 158, Loss: 967.79164, Residuals: -1.10135, Convergence: 0.001461\n",
      "Epoch: 159, Loss: 966.43858, Residuals: -1.09825, Convergence: 0.001400\n",
      "Epoch: 160, Loss: 965.14140, Residuals: -1.09531, Convergence: 0.001344\n",
      "Epoch: 161, Loss: 963.89496, Residuals: -1.09250, Convergence: 0.001293\n",
      "Epoch: 162, Loss: 962.69370, Residuals: -1.08981, Convergence: 0.001248\n",
      "Epoch: 163, Loss: 961.53202, Residuals: -1.08724, Convergence: 0.001208\n",
      "Epoch: 164, Loss: 960.40410, Residuals: -1.08476, Convergence: 0.001174\n",
      "Epoch: 165, Loss: 959.30439, Residuals: -1.08237, Convergence: 0.001146\n",
      "Epoch: 166, Loss: 958.22688, Residuals: -1.08006, Convergence: 0.001124\n",
      "Epoch: 167, Loss: 957.16586, Residuals: -1.07780, Convergence: 0.001109\n",
      "Epoch: 168, Loss: 956.11631, Residuals: -1.07559, Convergence: 0.001098\n",
      "Epoch: 169, Loss: 955.07321, Residuals: -1.07342, Convergence: 0.001092\n",
      "Epoch: 170, Loss: 954.03309, Residuals: -1.07128, Convergence: 0.001090\n",
      "Epoch: 171, Loss: 952.99419, Residuals: -1.06915, Convergence: 0.001090\n",
      "Epoch: 172, Loss: 951.95617, Residuals: -1.06704, Convergence: 0.001090\n",
      "Epoch: 173, Loss: 950.92125, Residuals: -1.06495, Convergence: 0.001088\n",
      "Epoch: 174, Loss: 949.89285, Residuals: -1.06287, Convergence: 0.001083\n",
      "Epoch: 175, Loss: 948.87653, Residuals: -1.06083, Convergence: 0.001071\n",
      "Epoch: 176, Loss: 947.87768, Residuals: -1.05883, Convergence: 0.001054\n",
      "Epoch: 177, Loss: 946.90144, Residuals: -1.05688, Convergence: 0.001031\n",
      "Epoch: 178, Loss: 945.95292, Residuals: -1.05498, Convergence: 0.001003\n",
      "Epoch: 179, Loss: 945.03529, Residuals: -1.05315, Convergence: 0.000971\n",
      "Evidence 11348.794\n",
      "\n",
      "Epoch: 179, Evidence: 11348.79395, Convergence: 1.015928\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.69e-01\n",
      "Epoch: 179, Loss: 2349.53915, Residuals: -1.05315, Convergence:   inf\n",
      "Epoch: 180, Loss: 2315.38008, Residuals: -1.05900, Convergence: 0.014753\n",
      "Epoch: 181, Loss: 2291.20154, Residuals: -1.05695, Convergence: 0.010553\n",
      "Epoch: 182, Loss: 2270.86089, Residuals: -1.05465, Convergence: 0.008957\n",
      "Epoch: 183, Loss: 2253.66237, Residuals: -1.05236, Convergence: 0.007631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 184, Loss: 2239.02239, Residuals: -1.05011, Convergence: 0.006539\n",
      "Epoch: 185, Loss: 2226.44872, Residuals: -1.04788, Convergence: 0.005647\n",
      "Epoch: 186, Loss: 2215.52903, Residuals: -1.04568, Convergence: 0.004929\n",
      "Epoch: 187, Loss: 2205.91652, Residuals: -1.04348, Convergence: 0.004358\n",
      "Epoch: 188, Loss: 2197.32613, Residuals: -1.04124, Convergence: 0.003909\n",
      "Epoch: 189, Loss: 2189.53419, Residuals: -1.03893, Convergence: 0.003559\n",
      "Epoch: 190, Loss: 2182.37993, Residuals: -1.03653, Convergence: 0.003278\n",
      "Epoch: 191, Loss: 2175.76307, Residuals: -1.03403, Convergence: 0.003041\n",
      "Epoch: 192, Loss: 2169.62941, Residuals: -1.03145, Convergence: 0.002827\n",
      "Epoch: 193, Loss: 2163.94752, Residuals: -1.02885, Convergence: 0.002626\n",
      "Epoch: 194, Loss: 2158.69568, Residuals: -1.02626, Convergence: 0.002433\n",
      "Epoch: 195, Loss: 2153.85019, Residuals: -1.02372, Convergence: 0.002250\n",
      "Epoch: 196, Loss: 2149.38418, Residuals: -1.02125, Convergence: 0.002078\n",
      "Epoch: 197, Loss: 2145.27035, Residuals: -1.01887, Convergence: 0.001918\n",
      "Epoch: 198, Loss: 2141.47879, Residuals: -1.01660, Convergence: 0.001771\n",
      "Epoch: 199, Loss: 2137.97949, Residuals: -1.01444, Convergence: 0.001637\n",
      "Epoch: 200, Loss: 2134.74455, Residuals: -1.01239, Convergence: 0.001515\n",
      "Epoch: 201, Loss: 2131.74641, Residuals: -1.01044, Convergence: 0.001406\n",
      "Epoch: 202, Loss: 2128.95977, Residuals: -1.00859, Convergence: 0.001309\n",
      "Epoch: 203, Loss: 2126.36218, Residuals: -1.00684, Convergence: 0.001222\n",
      "Epoch: 204, Loss: 2123.93225, Residuals: -1.00517, Convergence: 0.001144\n",
      "Epoch: 205, Loss: 2121.65224, Residuals: -1.00360, Convergence: 0.001075\n",
      "Epoch: 206, Loss: 2119.50662, Residuals: -1.00209, Convergence: 0.001012\n",
      "Epoch: 207, Loss: 2117.48140, Residuals: -1.00066, Convergence: 0.000956\n",
      "Evidence 14338.817\n",
      "\n",
      "Epoch: 207, Evidence: 14338.81738, Convergence: 0.208527\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.31e-01\n",
      "Epoch: 207, Loss: 2467.39328, Residuals: -1.00066, Convergence:   inf\n",
      "Epoch: 208, Loss: 2454.26301, Residuals: -0.99696, Convergence: 0.005350\n",
      "Epoch: 209, Loss: 2443.54339, Residuals: -0.99324, Convergence: 0.004387\n",
      "Epoch: 210, Loss: 2434.38079, Residuals: -0.98987, Convergence: 0.003764\n",
      "Epoch: 211, Loss: 2426.49079, Residuals: -0.98687, Convergence: 0.003252\n",
      "Epoch: 212, Loss: 2419.64806, Residuals: -0.98423, Convergence: 0.002828\n",
      "Epoch: 213, Loss: 2413.67357, Residuals: -0.98191, Convergence: 0.002475\n",
      "Epoch: 214, Loss: 2408.42208, Residuals: -0.97987, Convergence: 0.002180\n",
      "Epoch: 215, Loss: 2403.77498, Residuals: -0.97807, Convergence: 0.001933\n",
      "Epoch: 216, Loss: 2399.63570, Residuals: -0.97649, Convergence: 0.001725\n",
      "Epoch: 217, Loss: 2395.92544, Residuals: -0.97509, Convergence: 0.001549\n",
      "Epoch: 218, Loss: 2392.57953, Residuals: -0.97385, Convergence: 0.001398\n",
      "Epoch: 219, Loss: 2389.54425, Residuals: -0.97274, Convergence: 0.001270\n",
      "Epoch: 220, Loss: 2386.77712, Residuals: -0.97175, Convergence: 0.001159\n",
      "Epoch: 221, Loss: 2384.24120, Residuals: -0.97086, Convergence: 0.001064\n",
      "Epoch: 222, Loss: 2381.90692, Residuals: -0.97006, Convergence: 0.000980\n",
      "Evidence 14672.747\n",
      "\n",
      "Epoch: 222, Evidence: 14672.74707, Convergence: 0.022758\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.28e-01\n",
      "Epoch: 222, Loss: 2472.26288, Residuals: -0.97006, Convergence:   inf\n",
      "Epoch: 223, Loss: 2465.83942, Residuals: -0.96714, Convergence: 0.002605\n",
      "Epoch: 224, Loss: 2460.57619, Residuals: -0.96474, Convergence: 0.002139\n",
      "Epoch: 225, Loss: 2456.13600, Residuals: -0.96280, Convergence: 0.001808\n",
      "Epoch: 226, Loss: 2452.32609, Residuals: -0.96124, Convergence: 0.001554\n",
      "Epoch: 227, Loss: 2449.00577, Residuals: -0.95997, Convergence: 0.001356\n",
      "Epoch: 228, Loss: 2446.07326, Residuals: -0.95894, Convergence: 0.001199\n",
      "Epoch: 229, Loss: 2443.45212, Residuals: -0.95810, Convergence: 0.001073\n",
      "Epoch: 230, Loss: 2441.08450, Residuals: -0.95741, Convergence: 0.000970\n",
      "Evidence 14750.064\n",
      "\n",
      "Epoch: 230, Evidence: 14750.06445, Convergence: 0.005242\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.55e-01\n",
      "Epoch: 230, Loss: 2473.82692, Residuals: -0.95741, Convergence:   inf\n",
      "Epoch: 231, Loss: 2469.96884, Residuals: -0.95544, Convergence: 0.001562\n",
      "Epoch: 232, Loss: 2466.78455, Residuals: -0.95395, Convergence: 0.001291\n",
      "Epoch: 233, Loss: 2464.05788, Residuals: -0.95283, Convergence: 0.001107\n",
      "Epoch: 234, Loss: 2461.67046, Residuals: -0.95197, Convergence: 0.000970\n",
      "Evidence 14777.514\n",
      "\n",
      "Epoch: 234, Evidence: 14777.51367, Convergence: 0.001857\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.04e-01\n",
      "Epoch: 234, Loss: 2474.78541, Residuals: -0.95197, Convergence:   inf\n",
      "Epoch: 235, Loss: 2471.92533, Residuals: -0.95045, Convergence: 0.001157\n",
      "Epoch: 236, Loss: 2469.53106, Residuals: -0.94934, Convergence: 0.000970\n",
      "Evidence 14789.494\n",
      "\n",
      "Epoch: 236, Evidence: 14789.49414, Convergence: 0.000810\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.68e-01\n",
      "Epoch: 236, Loss: 2475.48441, Residuals: -0.94934, Convergence:   inf\n",
      "Epoch: 237, Loss: 2470.96871, Residuals: -0.94742, Convergence: 0.001827\n",
      "Epoch: 238, Loss: 2467.52509, Residuals: -0.94625, Convergence: 0.001396\n",
      "Epoch: 239, Loss: 2464.71316, Residuals: -0.94550, Convergence: 0.001141\n",
      "Epoch: 240, Loss: 2462.31746, Residuals: -0.94510, Convergence: 0.000973\n",
      "Evidence 14807.058\n",
      "\n",
      "Epoch: 240, Evidence: 14807.05762, Convergence: 0.001995\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.39e-01\n",
      "Epoch: 240, Loss: 2475.56145, Residuals: -0.94510, Convergence:   inf\n",
      "Epoch: 241, Loss: 2472.55869, Residuals: -0.94328, Convergence: 0.001214\n",
      "Epoch: 242, Loss: 2470.16439, Residuals: -0.94249, Convergence: 0.000969\n",
      "Evidence 14817.900\n",
      "\n",
      "Epoch: 242, Evidence: 14817.90039, Convergence: 0.000732\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.17e-01\n",
      "Epoch: 242, Loss: 2475.73486, Residuals: -0.94249, Convergence:   inf\n",
      "Epoch: 243, Loss: 2471.36094, Residuals: -0.94040, Convergence: 0.001770\n",
      "Epoch: 244, Loss: 2468.14380, Residuals: -0.94149, Convergence: 0.001303\n",
      "Epoch: 245, Loss: 2465.50980, Residuals: -0.94183, Convergence: 0.001068\n",
      "Epoch: 246, Loss: 2463.19865, Residuals: -0.94447, Convergence: 0.000938\n",
      "Evidence 14833.533\n",
      "\n",
      "Epoch: 246, Evidence: 14833.53320, Convergence: 0.001785\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.05e-01\n",
      "Epoch: 246, Loss: 2474.89786, Residuals: -0.94447, Convergence:   inf\n",
      "Epoch: 247, Loss: 2473.23292, Residuals: -0.94337, Convergence: 0.000673\n",
      "Evidence 14840.014\n",
      "\n",
      "Epoch: 247, Evidence: 14840.01367, Convergence: 0.000437\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.52e-02\n",
      "Epoch: 247, Loss: 2475.90671, Residuals: -0.94337, Convergence:   inf\n",
      "Epoch: 248, Loss: 2523.21598, Residuals: -0.99112, Convergence: -0.018750\n",
      "Epoch: 248, Loss: 2473.62612, Residuals: -0.94303, Convergence: 0.000922\n",
      "Evidence 14844.433\n",
      "\n",
      "Epoch: 248, Evidence: 14844.43262, Convergence: 0.000734\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.84e-02\n",
      "Epoch: 248, Loss: 2475.28811, Residuals: -0.94303, Convergence:   inf\n",
      "Epoch: 249, Loss: 2479.46314, Residuals: -0.94381, Convergence: -0.001684\n",
      "Epoch: 249, Loss: 2475.01430, Residuals: -0.94124, Convergence: 0.000111\n",
      "Evidence 14846.370\n",
      "\n",
      "Epoch: 249, Evidence: 14846.37012, Convergence: 0.000865\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 382.11888, Residuals: -4.51658, Convergence:   inf\n",
      "Epoch: 1, Loss: 356.39522, Residuals: -4.39449, Convergence: 0.072177\n",
      "Epoch: 2, Loss: 335.28928, Residuals: -4.22849, Convergence: 0.062948\n",
      "Epoch: 3, Loss: 319.12515, Residuals: -4.06165, Convergence: 0.050651\n",
      "Epoch: 4, Loss: 306.79426, Residuals: -3.91464, Convergence: 0.040193\n",
      "Epoch: 5, Loss: 297.01231, Residuals: -3.78464, Convergence: 0.032934\n",
      "Epoch: 6, Loss: 289.07224, Residuals: -3.67142, Convergence: 0.027467\n",
      "Epoch: 7, Loss: 282.48176, Residuals: -3.57454, Convergence: 0.023331\n",
      "Epoch: 8, Loss: 276.87571, Residuals: -3.49192, Convergence: 0.020248\n",
      "Epoch: 9, Loss: 271.99695, Residuals: -3.42119, Convergence: 0.017937\n",
      "Epoch: 10, Loss: 267.66326, Residuals: -3.36024, Convergence: 0.016191\n",
      "Epoch: 11, Loss: 263.74336, Residuals: -3.30726, Convergence: 0.014863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Loss: 260.14232, Residuals: -3.26074, Convergence: 0.013843\n",
      "Epoch: 13, Loss: 256.79196, Residuals: -3.21935, Convergence: 0.013047\n",
      "Epoch: 14, Loss: 253.64358, Residuals: -3.18193, Convergence: 0.012413\n",
      "Epoch: 15, Loss: 250.66249, Residuals: -3.14743, Convergence: 0.011893\n",
      "Epoch: 16, Loss: 247.82491, Residuals: -3.11504, Convergence: 0.011450\n",
      "Epoch: 17, Loss: 245.11384, Residuals: -3.08416, Convergence: 0.011060\n",
      "Epoch: 18, Loss: 242.51074, Residuals: -3.05440, Convergence: 0.010734\n",
      "Epoch: 19, Loss: 239.98935, Residuals: -3.02531, Convergence: 0.010506\n",
      "Epoch: 20, Loss: 237.51790, Residuals: -2.99644, Convergence: 0.010405\n",
      "Epoch: 21, Loss: 235.06592, Residuals: -2.96735, Convergence: 0.010431\n",
      "Epoch: 22, Loss: 232.60918, Residuals: -2.93770, Convergence: 0.010562\n",
      "Epoch: 23, Loss: 230.12571, Residuals: -2.90726, Convergence: 0.010792\n",
      "Epoch: 24, Loss: 227.58135, Residuals: -2.87568, Convergence: 0.011180\n",
      "Epoch: 25, Loss: 224.92277, Residuals: -2.84236, Convergence: 0.011820\n",
      "Epoch: 26, Loss: 222.11461, Residuals: -2.80686, Convergence: 0.012643\n",
      "Epoch: 27, Loss: 219.23047, Residuals: -2.76989, Convergence: 0.013156\n",
      "Epoch: 28, Loss: 216.40434, Residuals: -2.73295, Convergence: 0.013059\n",
      "Epoch: 29, Loss: 213.68388, Residuals: -2.69670, Convergence: 0.012731\n",
      "Epoch: 30, Loss: 211.05810, Residuals: -2.66112, Convergence: 0.012441\n",
      "Epoch: 31, Loss: 208.50746, Residuals: -2.62604, Convergence: 0.012233\n",
      "Epoch: 32, Loss: 206.01711, Residuals: -2.59128, Convergence: 0.012088\n",
      "Epoch: 33, Loss: 203.57815, Residuals: -2.55674, Convergence: 0.011980\n",
      "Epoch: 34, Loss: 201.18636, Residuals: -2.52232, Convergence: 0.011888\n",
      "Epoch: 35, Loss: 198.84087, Residuals: -2.48800, Convergence: 0.011796\n",
      "Epoch: 36, Loss: 196.54286, Residuals: -2.45376, Convergence: 0.011692\n",
      "Epoch: 37, Loss: 194.29461, Residuals: -2.41960, Convergence: 0.011571\n",
      "Epoch: 38, Loss: 192.09876, Residuals: -2.38556, Convergence: 0.011431\n",
      "Epoch: 39, Loss: 189.95789, Residuals: -2.35166, Convergence: 0.011270\n",
      "Epoch: 40, Loss: 187.87419, Residuals: -2.31795, Convergence: 0.011091\n",
      "Epoch: 41, Loss: 185.84930, Residuals: -2.28445, Convergence: 0.010895\n",
      "Epoch: 42, Loss: 183.88425, Residuals: -2.25120, Convergence: 0.010686\n",
      "Epoch: 43, Loss: 181.97945, Residuals: -2.21823, Convergence: 0.010467\n",
      "Epoch: 44, Loss: 180.13473, Residuals: -2.18558, Convergence: 0.010241\n",
      "Epoch: 45, Loss: 178.34945, Residuals: -2.15325, Convergence: 0.010010\n",
      "Epoch: 46, Loss: 176.62266, Residuals: -2.12127, Convergence: 0.009777\n",
      "Epoch: 47, Loss: 174.95319, Residuals: -2.08965, Convergence: 0.009542\n",
      "Epoch: 48, Loss: 173.33986, Residuals: -2.05841, Convergence: 0.009307\n",
      "Epoch: 49, Loss: 171.78167, Residuals: -2.02755, Convergence: 0.009071\n",
      "Epoch: 50, Loss: 170.27786, Residuals: -1.99710, Convergence: 0.008832\n",
      "Epoch: 51, Loss: 168.82804, Residuals: -1.96708, Convergence: 0.008588\n",
      "Epoch: 52, Loss: 167.43216, Residuals: -1.93751, Convergence: 0.008337\n",
      "Epoch: 53, Loss: 166.09045, Residuals: -1.90842, Convergence: 0.008078\n",
      "Epoch: 54, Loss: 164.80318, Residuals: -1.87985, Convergence: 0.007811\n",
      "Epoch: 55, Loss: 163.57048, Residuals: -1.85184, Convergence: 0.007536\n",
      "Epoch: 56, Loss: 162.39223, Residuals: -1.82441, Convergence: 0.007256\n",
      "Epoch: 57, Loss: 161.26790, Residuals: -1.79760, Convergence: 0.006972\n",
      "Epoch: 58, Loss: 160.19657, Residuals: -1.77144, Convergence: 0.006688\n",
      "Epoch: 59, Loss: 159.17698, Residuals: -1.74595, Convergence: 0.006405\n",
      "Epoch: 60, Loss: 158.20752, Residuals: -1.72116, Convergence: 0.006128\n",
      "Epoch: 61, Loss: 157.28626, Residuals: -1.69708, Convergence: 0.005857\n",
      "Epoch: 62, Loss: 156.41108, Residuals: -1.67371, Convergence: 0.005595\n",
      "Epoch: 63, Loss: 155.57962, Residuals: -1.65107, Convergence: 0.005344\n",
      "Epoch: 64, Loss: 154.78937, Residuals: -1.62915, Convergence: 0.005105\n",
      "Epoch: 65, Loss: 154.03771, Residuals: -1.60794, Convergence: 0.004880\n",
      "Epoch: 66, Loss: 153.32201, Residuals: -1.58742, Convergence: 0.004668\n",
      "Epoch: 67, Loss: 152.63975, Residuals: -1.56758, Convergence: 0.004470\n",
      "Epoch: 68, Loss: 151.98856, Residuals: -1.54841, Convergence: 0.004284\n",
      "Epoch: 69, Loss: 151.36637, Residuals: -1.52986, Convergence: 0.004110\n",
      "Epoch: 70, Loss: 150.77138, Residuals: -1.51194, Convergence: 0.003946\n",
      "Epoch: 71, Loss: 150.20207, Residuals: -1.49462, Convergence: 0.003790\n",
      "Epoch: 72, Loss: 149.65719, Residuals: -1.47788, Convergence: 0.003641\n",
      "Epoch: 73, Loss: 149.13569, Residuals: -1.46173, Convergence: 0.003497\n",
      "Epoch: 74, Loss: 148.63666, Residuals: -1.44614, Convergence: 0.003357\n",
      "Epoch: 75, Loss: 148.15929, Residuals: -1.43111, Convergence: 0.003222\n",
      "Epoch: 76, Loss: 147.70283, Residuals: -1.41663, Convergence: 0.003090\n",
      "Epoch: 77, Loss: 147.26657, Residuals: -1.40269, Convergence: 0.002962\n",
      "Epoch: 78, Loss: 146.84983, Residuals: -1.38928, Convergence: 0.002838\n",
      "Epoch: 79, Loss: 146.45191, Residuals: -1.37639, Convergence: 0.002717\n",
      "Epoch: 80, Loss: 146.07215, Residuals: -1.36401, Convergence: 0.002600\n",
      "Epoch: 81, Loss: 145.70985, Residuals: -1.35212, Convergence: 0.002486\n",
      "Epoch: 82, Loss: 145.36435, Residuals: -1.34072, Convergence: 0.002377\n",
      "Epoch: 83, Loss: 145.03500, Residuals: -1.32978, Convergence: 0.002271\n",
      "Epoch: 84, Loss: 144.72113, Residuals: -1.31930, Convergence: 0.002169\n",
      "Epoch: 85, Loss: 144.42214, Residuals: -1.30926, Convergence: 0.002070\n",
      "Epoch: 86, Loss: 144.13740, Residuals: -1.29965, Convergence: 0.001975\n",
      "Epoch: 87, Loss: 143.86636, Residuals: -1.29044, Convergence: 0.001884\n",
      "Epoch: 88, Loss: 143.60846, Residuals: -1.28163, Convergence: 0.001796\n",
      "Epoch: 89, Loss: 143.36318, Residuals: -1.27319, Convergence: 0.001711\n",
      "Epoch: 90, Loss: 143.13004, Residuals: -1.26513, Convergence: 0.001629\n",
      "Epoch: 91, Loss: 142.90859, Residuals: -1.25742, Convergence: 0.001550\n",
      "Epoch: 92, Loss: 142.69840, Residuals: -1.25005, Convergence: 0.001473\n",
      "Epoch: 93, Loss: 142.49909, Residuals: -1.24301, Convergence: 0.001399\n",
      "Epoch: 94, Loss: 142.31029, Residuals: -1.23629, Convergence: 0.001327\n",
      "Epoch: 95, Loss: 142.13168, Residuals: -1.22987, Convergence: 0.001257\n",
      "Epoch: 96, Loss: 141.96291, Residuals: -1.22376, Convergence: 0.001189\n",
      "Epoch: 97, Loss: 141.80372, Residuals: -1.21793, Convergence: 0.001123\n",
      "Epoch: 98, Loss: 141.65376, Residuals: -1.21239, Convergence: 0.001059\n",
      "Epoch: 99, Loss: 141.51276, Residuals: -1.20712, Convergence: 0.000996\n",
      "Evidence -183.374\n",
      "\n",
      "Epoch: 99, Evidence: -183.37402, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.25e-01\n",
      "Epoch: 99, Loss: 1365.48705, Residuals: -1.20712, Convergence:   inf\n",
      "Epoch: 100, Loss: 1301.72936, Residuals: -1.23696, Convergence: 0.048979\n",
      "Epoch: 101, Loss: 1253.61027, Residuals: -1.26042, Convergence: 0.038384\n",
      "Epoch: 102, Loss: 1217.57195, Residuals: -1.27720, Convergence: 0.029599\n",
      "Epoch: 103, Loss: 1189.66748, Residuals: -1.28901, Convergence: 0.023456\n",
      "Epoch: 104, Loss: 1167.16955, Residuals: -1.29785, Convergence: 0.019276\n",
      "Epoch: 105, Loss: 1148.55407, Residuals: -1.30470, Convergence: 0.016208\n",
      "Epoch: 106, Loss: 1132.90221, Residuals: -1.30999, Convergence: 0.013816\n",
      "Epoch: 107, Loss: 1119.58365, Residuals: -1.31394, Convergence: 0.011896\n",
      "Epoch: 108, Loss: 1108.12755, Residuals: -1.31668, Convergence: 0.010338\n",
      "Epoch: 109, Loss: 1098.16629, Residuals: -1.31835, Convergence: 0.009071\n",
      "Epoch: 110, Loss: 1089.40362, Residuals: -1.31903, Convergence: 0.008044\n",
      "Epoch: 111, Loss: 1081.59683, Residuals: -1.31881, Convergence: 0.007218\n",
      "Epoch: 112, Loss: 1074.54159, Residuals: -1.31775, Convergence: 0.006566\n",
      "Epoch: 113, Loss: 1068.06228, Residuals: -1.31591, Convergence: 0.006066\n",
      "Epoch: 114, Loss: 1062.00424, Residuals: -1.31331, Convergence: 0.005704\n",
      "Epoch: 115, Loss: 1056.22852, Residuals: -1.30998, Convergence: 0.005468\n",
      "Epoch: 116, Loss: 1050.61120, Residuals: -1.30591, Convergence: 0.005347\n",
      "Epoch: 117, Loss: 1045.04576, Residuals: -1.30112, Convergence: 0.005326\n",
      "Epoch: 118, Loss: 1039.45740, Residuals: -1.29564, Convergence: 0.005376\n",
      "Epoch: 119, Loss: 1033.81649, Residuals: -1.28954, Convergence: 0.005456\n",
      "Epoch: 120, Loss: 1028.15272, Residuals: -1.28289, Convergence: 0.005509\n",
      "Epoch: 121, Loss: 1022.54694, Residuals: -1.27580, Convergence: 0.005482\n",
      "Epoch: 122, Loss: 1017.10193, Residuals: -1.26839, Convergence: 0.005353\n",
      "Epoch: 123, Loss: 1011.90436, Residuals: -1.26075, Convergence: 0.005136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 124, Loss: 1007.00606, Residuals: -1.25296, Convergence: 0.004864\n",
      "Epoch: 125, Loss: 1002.42193, Residuals: -1.24510, Convergence: 0.004573\n",
      "Epoch: 126, Loss: 998.14152, Residuals: -1.23722, Convergence: 0.004288\n",
      "Epoch: 127, Loss: 994.14271, Residuals: -1.22939, Convergence: 0.004022\n",
      "Epoch: 128, Loss: 990.39908, Residuals: -1.22165, Convergence: 0.003780\n",
      "Epoch: 129, Loss: 986.88465, Residuals: -1.21403, Convergence: 0.003561\n",
      "Epoch: 130, Loss: 983.57655, Residuals: -1.20657, Convergence: 0.003363\n",
      "Epoch: 131, Loss: 980.45492, Residuals: -1.19928, Convergence: 0.003184\n",
      "Epoch: 132, Loss: 977.50341, Residuals: -1.19219, Convergence: 0.003019\n",
      "Epoch: 133, Loss: 974.70860, Residuals: -1.18532, Convergence: 0.002867\n",
      "Epoch: 134, Loss: 972.05938, Residuals: -1.17868, Convergence: 0.002725\n",
      "Epoch: 135, Loss: 969.54607, Residuals: -1.17227, Convergence: 0.002592\n",
      "Epoch: 136, Loss: 967.16030, Residuals: -1.16611, Convergence: 0.002467\n",
      "Epoch: 137, Loss: 964.89495, Residuals: -1.16019, Convergence: 0.002348\n",
      "Epoch: 138, Loss: 962.74273, Residuals: -1.15453, Convergence: 0.002236\n",
      "Epoch: 139, Loss: 960.69732, Residuals: -1.14911, Convergence: 0.002129\n",
      "Epoch: 140, Loss: 958.75205, Residuals: -1.14393, Convergence: 0.002029\n",
      "Epoch: 141, Loss: 956.90091, Residuals: -1.13899, Convergence: 0.001935\n",
      "Epoch: 142, Loss: 955.13801, Residuals: -1.13429, Convergence: 0.001846\n",
      "Epoch: 143, Loss: 953.45775, Residuals: -1.12981, Convergence: 0.001762\n",
      "Epoch: 144, Loss: 951.85436, Residuals: -1.12554, Convergence: 0.001684\n",
      "Epoch: 145, Loss: 950.32222, Residuals: -1.12148, Convergence: 0.001612\n",
      "Epoch: 146, Loss: 948.85700, Residuals: -1.11762, Convergence: 0.001544\n",
      "Epoch: 147, Loss: 947.45381, Residuals: -1.11395, Convergence: 0.001481\n",
      "Epoch: 148, Loss: 946.10835, Residuals: -1.11045, Convergence: 0.001422\n",
      "Epoch: 149, Loss: 944.81632, Residuals: -1.10712, Convergence: 0.001367\n",
      "Epoch: 150, Loss: 943.57438, Residuals: -1.10394, Convergence: 0.001316\n",
      "Epoch: 151, Loss: 942.37863, Residuals: -1.10092, Convergence: 0.001269\n",
      "Epoch: 152, Loss: 941.22620, Residuals: -1.09803, Convergence: 0.001224\n",
      "Epoch: 153, Loss: 940.11385, Residuals: -1.09527, Convergence: 0.001183\n",
      "Epoch: 154, Loss: 939.03909, Residuals: -1.09264, Convergence: 0.001145\n",
      "Epoch: 155, Loss: 937.99883, Residuals: -1.09012, Convergence: 0.001109\n",
      "Epoch: 156, Loss: 936.99062, Residuals: -1.08770, Convergence: 0.001076\n",
      "Epoch: 157, Loss: 936.01212, Residuals: -1.08538, Convergence: 0.001045\n",
      "Epoch: 158, Loss: 935.06063, Residuals: -1.08315, Convergence: 0.001018\n",
      "Epoch: 159, Loss: 934.13340, Residuals: -1.08101, Convergence: 0.000993\n",
      "Evidence 11132.607\n",
      "\n",
      "Epoch: 159, Evidence: 11132.60742, Convergence: 1.016472\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.78e-01\n",
      "Epoch: 159, Loss: 2342.88344, Residuals: -1.08101, Convergence:   inf\n",
      "Epoch: 160, Loss: 2305.06911, Residuals: -1.09068, Convergence: 0.016405\n",
      "Epoch: 161, Loss: 2278.35311, Residuals: -1.08999, Convergence: 0.011726\n",
      "Epoch: 162, Loss: 2255.79867, Residuals: -1.08828, Convergence: 0.009998\n",
      "Epoch: 163, Loss: 2236.49029, Residuals: -1.08611, Convergence: 0.008633\n",
      "Epoch: 164, Loss: 2219.79383, Residuals: -1.08360, Convergence: 0.007522\n",
      "Epoch: 165, Loss: 2205.22723, Residuals: -1.08080, Convergence: 0.006605\n",
      "Epoch: 166, Loss: 2192.40966, Residuals: -1.07777, Convergence: 0.005846\n",
      "Epoch: 167, Loss: 2181.03355, Residuals: -1.07455, Convergence: 0.005216\n",
      "Epoch: 168, Loss: 2170.84656, Residuals: -1.07118, Convergence: 0.004693\n",
      "Epoch: 169, Loss: 2161.63770, Residuals: -1.06768, Convergence: 0.004260\n",
      "Epoch: 170, Loss: 2153.23518, Residuals: -1.06405, Convergence: 0.003902\n",
      "Epoch: 171, Loss: 2145.50532, Residuals: -1.06032, Convergence: 0.003603\n",
      "Epoch: 172, Loss: 2138.35724, Residuals: -1.05650, Convergence: 0.003343\n",
      "Epoch: 173, Loss: 2131.73587, Residuals: -1.05262, Convergence: 0.003106\n",
      "Epoch: 174, Loss: 2125.61217, Residuals: -1.04874, Convergence: 0.002881\n",
      "Epoch: 175, Loss: 2119.96367, Residuals: -1.04490, Convergence: 0.002664\n",
      "Epoch: 176, Loss: 2114.76763, Residuals: -1.04115, Convergence: 0.002457\n",
      "Epoch: 177, Loss: 2109.99755, Residuals: -1.03752, Convergence: 0.002261\n",
      "Epoch: 178, Loss: 2105.62231, Residuals: -1.03403, Convergence: 0.002078\n",
      "Epoch: 179, Loss: 2101.60725, Residuals: -1.03069, Convergence: 0.001910\n",
      "Epoch: 180, Loss: 2097.91827, Residuals: -1.02752, Convergence: 0.001758\n",
      "Epoch: 181, Loss: 2094.52102, Residuals: -1.02451, Convergence: 0.001622\n",
      "Epoch: 182, Loss: 2091.38312, Residuals: -1.02166, Convergence: 0.001500\n",
      "Epoch: 183, Loss: 2088.47586, Residuals: -1.01896, Convergence: 0.001392\n",
      "Epoch: 184, Loss: 2085.77189, Residuals: -1.01641, Convergence: 0.001296\n",
      "Epoch: 185, Loss: 2083.24733, Residuals: -1.01400, Convergence: 0.001212\n",
      "Epoch: 186, Loss: 2080.88146, Residuals: -1.01172, Convergence: 0.001137\n",
      "Epoch: 187, Loss: 2078.65537, Residuals: -1.00957, Convergence: 0.001071\n",
      "Epoch: 188, Loss: 2076.55439, Residuals: -1.00753, Convergence: 0.001012\n",
      "Epoch: 189, Loss: 2074.56442, Residuals: -1.00560, Convergence: 0.000959\n",
      "Evidence 14297.759\n",
      "\n",
      "Epoch: 189, Evidence: 14297.75879, Convergence: 0.221374\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.42e-01\n",
      "Epoch: 189, Loss: 2467.93410, Residuals: -1.00560, Convergence:   inf\n",
      "Epoch: 190, Loss: 2453.97067, Residuals: -1.00194, Convergence: 0.005690\n",
      "Epoch: 191, Loss: 2442.58163, Residuals: -0.99798, Convergence: 0.004663\n",
      "Epoch: 192, Loss: 2432.83248, Residuals: -0.99420, Convergence: 0.004007\n",
      "Epoch: 193, Loss: 2424.42630, Residuals: -0.99069, Convergence: 0.003467\n",
      "Epoch: 194, Loss: 2417.12917, Residuals: -0.98744, Convergence: 0.003019\n",
      "Epoch: 195, Loss: 2410.75425, Residuals: -0.98446, Convergence: 0.002644\n",
      "Epoch: 196, Loss: 2405.14740, Residuals: -0.98173, Convergence: 0.002331\n",
      "Epoch: 197, Loss: 2400.18348, Residuals: -0.97922, Convergence: 0.002068\n",
      "Epoch: 198, Loss: 2395.75935, Residuals: -0.97692, Convergence: 0.001847\n",
      "Epoch: 199, Loss: 2391.78985, Residuals: -0.97480, Convergence: 0.001660\n",
      "Epoch: 200, Loss: 2388.20532, Residuals: -0.97285, Convergence: 0.001501\n",
      "Epoch: 201, Loss: 2384.94863, Residuals: -0.97105, Convergence: 0.001366\n",
      "Epoch: 202, Loss: 2381.97360, Residuals: -0.96939, Convergence: 0.001249\n",
      "Epoch: 203, Loss: 2379.24065, Residuals: -0.96785, Convergence: 0.001149\n",
      "Epoch: 204, Loss: 2376.71860, Residuals: -0.96643, Convergence: 0.001061\n",
      "Epoch: 205, Loss: 2374.38078, Residuals: -0.96512, Convergence: 0.000985\n",
      "Evidence 14695.191\n",
      "\n",
      "Epoch: 205, Evidence: 14695.19141, Convergence: 0.027045\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.39e-01\n",
      "Epoch: 205, Loss: 2473.57719, Residuals: -0.96512, Convergence:   inf\n",
      "Epoch: 206, Loss: 2466.98105, Residuals: -0.96173, Convergence: 0.002674\n",
      "Epoch: 207, Loss: 2461.54696, Residuals: -0.95877, Convergence: 0.002208\n",
      "Epoch: 208, Loss: 2456.94598, Residuals: -0.95624, Convergence: 0.001873\n",
      "Epoch: 209, Loss: 2452.98688, Residuals: -0.95407, Convergence: 0.001614\n",
      "Epoch: 210, Loss: 2449.53244, Residuals: -0.95221, Convergence: 0.001410\n",
      "Epoch: 211, Loss: 2446.47946, Residuals: -0.95061, Convergence: 0.001248\n",
      "Epoch: 212, Loss: 2443.75025, Residuals: -0.94923, Convergence: 0.001117\n",
      "Epoch: 213, Loss: 2441.28475, Residuals: -0.94804, Convergence: 0.001010\n",
      "Epoch: 214, Loss: 2439.03621, Residuals: -0.94700, Convergence: 0.000922\n",
      "Evidence 14779.360\n",
      "\n",
      "Epoch: 214, Evidence: 14779.36035, Convergence: 0.005695\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.65e-01\n",
      "Epoch: 214, Loss: 2475.15054, Residuals: -0.94700, Convergence:   inf\n",
      "Epoch: 215, Loss: 2471.24734, Residuals: -0.94464, Convergence: 0.001579\n",
      "Epoch: 216, Loss: 2468.01845, Residuals: -0.94275, Convergence: 0.001308\n",
      "Epoch: 217, Loss: 2465.25511, Residuals: -0.94123, Convergence: 0.001121\n",
      "Epoch: 218, Loss: 2462.83856, Residuals: -0.94000, Convergence: 0.000981\n",
      "Evidence 14808.173\n",
      "\n",
      "Epoch: 218, Evidence: 14808.17285, Convergence: 0.001946\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.13e-01\n",
      "Epoch: 218, Loss: 2476.18593, Residuals: -0.94000, Convergence:   inf\n",
      "Epoch: 219, Loss: 2473.24909, Residuals: -0.93817, Convergence: 0.001187\n",
      "Epoch: 220, Loss: 2470.79572, Residuals: -0.93675, Convergence: 0.000993\n",
      "Evidence 14820.390\n",
      "\n",
      "Epoch: 220, Evidence: 14820.38965, Convergence: 0.000824\n",
      "Updating hyper-parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 182, Updated regularization: 1.77e-01\n",
      "Epoch: 220, Loss: 2476.92823, Residuals: -0.93675, Convergence:   inf\n",
      "Epoch: 221, Loss: 2472.26409, Residuals: -0.93449, Convergence: 0.001887\n",
      "Epoch: 222, Loss: 2468.74963, Residuals: -0.93288, Convergence: 0.001424\n",
      "Epoch: 223, Loss: 2465.90018, Residuals: -0.93181, Convergence: 0.001156\n",
      "Epoch: 224, Loss: 2463.48406, Residuals: -0.93121, Convergence: 0.000981\n",
      "Evidence 14838.520\n",
      "\n",
      "Epoch: 224, Evidence: 14838.51953, Convergence: 0.002045\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.46e-01\n",
      "Epoch: 224, Loss: 2477.18018, Residuals: -0.93121, Convergence:   inf\n",
      "Epoch: 225, Loss: 2474.08660, Residuals: -0.92919, Convergence: 0.001250\n",
      "Epoch: 226, Loss: 2471.65162, Residuals: -0.92822, Convergence: 0.000985\n",
      "Evidence 14849.665\n",
      "\n",
      "Epoch: 226, Evidence: 14849.66504, Convergence: 0.000751\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.24e-01\n",
      "Epoch: 226, Loss: 2477.44031, Residuals: -0.92822, Convergence:   inf\n",
      "Epoch: 227, Loss: 2472.93574, Residuals: -0.92579, Convergence: 0.001822\n",
      "Epoch: 228, Loss: 2469.70354, Residuals: -0.92696, Convergence: 0.001309\n",
      "Epoch: 229, Loss: 2467.06529, Residuals: -0.92726, Convergence: 0.001069\n",
      "Epoch: 230, Loss: 2464.79159, Residuals: -0.92983, Convergence: 0.000922\n",
      "Evidence 14865.523\n",
      "\n",
      "Epoch: 230, Evidence: 14865.52344, Convergence: 0.001817\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.10e-01\n",
      "Epoch: 230, Loss: 2476.83770, Residuals: -0.92983, Convergence:   inf\n",
      "Epoch: 231, Loss: 2475.14904, Residuals: -0.92833, Convergence: 0.000682\n",
      "Evidence 14872.119\n",
      "\n",
      "Epoch: 231, Evidence: 14872.11914, Convergence: 0.000443\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.94e-02\n",
      "Epoch: 231, Loss: 2477.87237, Residuals: -0.92833, Convergence:   inf\n",
      "Epoch: 232, Loss: 2521.82247, Residuals: -0.96952, Convergence: -0.017428\n",
      "Epoch: 232, Loss: 2475.53330, Residuals: -0.92627, Convergence: 0.000945\n",
      "Evidence 14876.564\n",
      "\n",
      "Epoch: 232, Evidence: 14876.56445, Convergence: 0.000742\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.23e-02\n",
      "Epoch: 232, Loss: 2477.32862, Residuals: -0.92627, Convergence:   inf\n",
      "Epoch: 233, Loss: 2482.90664, Residuals: -0.92878, Convergence: -0.002247\n",
      "Epoch: 233, Loss: 2477.44588, Residuals: -0.92498, Convergence: -0.000047\n",
      "Evidence 14877.980\n",
      "\n",
      "Epoch: 233, Evidence: 14877.98047, Convergence: 0.000837\n",
      "Total samples: 184, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 385.07991, Residuals: -4.54916, Convergence:   inf\n",
      "Epoch: 1, Loss: 359.21272, Residuals: -4.42842, Convergence: 0.072011\n",
      "Epoch: 2, Loss: 338.07962, Residuals: -4.26449, Convergence: 0.062509\n",
      "Epoch: 3, Loss: 321.90780, Residuals: -4.09991, Convergence: 0.050237\n",
      "Epoch: 4, Loss: 309.56989, Residuals: -3.95502, Convergence: 0.039855\n",
      "Epoch: 5, Loss: 299.77944, Residuals: -3.82712, Convergence: 0.032659\n",
      "Epoch: 6, Loss: 291.82614, Residuals: -3.71588, Convergence: 0.027254\n",
      "Epoch: 7, Loss: 285.21776, Residuals: -3.62070, Convergence: 0.023170\n",
      "Epoch: 8, Loss: 279.59037, Residuals: -3.53943, Convergence: 0.020127\n",
      "Epoch: 9, Loss: 274.68691, Residuals: -3.46967, Convergence: 0.017851\n",
      "Epoch: 10, Loss: 270.32409, Residuals: -3.40930, Convergence: 0.016139\n",
      "Epoch: 11, Loss: 266.36859, Residuals: -3.35648, Convergence: 0.014850\n",
      "Epoch: 12, Loss: 262.72270, Residuals: -3.30968, Convergence: 0.013877\n",
      "Epoch: 13, Loss: 259.31544, Residuals: -3.26755, Convergence: 0.013139\n",
      "Epoch: 14, Loss: 256.09718, Residuals: -3.22895, Convergence: 0.012567\n",
      "Epoch: 15, Loss: 253.03757, Residuals: -3.19299, Convergence: 0.012092\n",
      "Epoch: 16, Loss: 250.12221, Residuals: -3.15910, Convergence: 0.011656\n",
      "Epoch: 17, Loss: 247.33987, Residuals: -3.12695, Convergence: 0.011249\n",
      "Epoch: 18, Loss: 244.66781, Residuals: -3.09615, Convergence: 0.010921\n",
      "Epoch: 19, Loss: 242.07135, Residuals: -3.06616, Convergence: 0.010726\n",
      "Epoch: 20, Loss: 239.51335, Residuals: -3.03638, Convergence: 0.010680\n",
      "Epoch: 21, Loss: 236.96383, Residuals: -3.00634, Convergence: 0.010759\n",
      "Epoch: 22, Loss: 234.40088, Residuals: -2.97572, Convergence: 0.010934\n",
      "Epoch: 23, Loss: 231.79586, Residuals: -2.94421, Convergence: 0.011238\n",
      "Epoch: 24, Loss: 229.09788, Residuals: -2.91121, Convergence: 0.011777\n",
      "Epoch: 25, Loss: 226.25061, Residuals: -2.87601, Convergence: 0.012585\n",
      "Epoch: 26, Loss: 223.27730, Residuals: -2.83872, Convergence: 0.013317\n",
      "Epoch: 27, Loss: 220.31573, Residuals: -2.80078, Convergence: 0.013442\n",
      "Epoch: 28, Loss: 217.45519, Residuals: -2.76328, Convergence: 0.013155\n",
      "Epoch: 29, Loss: 214.69964, Residuals: -2.72641, Convergence: 0.012834\n",
      "Epoch: 30, Loss: 212.03084, Residuals: -2.69005, Convergence: 0.012587\n",
      "Epoch: 31, Loss: 209.43237, Residuals: -2.65408, Convergence: 0.012407\n",
      "Epoch: 32, Loss: 206.89319, Residuals: -2.61839, Convergence: 0.012273\n",
      "Epoch: 33, Loss: 204.40691, Residuals: -2.58294, Convergence: 0.012163\n",
      "Epoch: 34, Loss: 201.97061, Residuals: -2.54766, Convergence: 0.012063\n",
      "Epoch: 35, Loss: 199.58375, Residuals: -2.51255, Convergence: 0.011959\n",
      "Epoch: 36, Loss: 197.24737, Residuals: -2.47759, Convergence: 0.011845\n",
      "Epoch: 37, Loss: 194.96339, Residuals: -2.44279, Convergence: 0.011715\n",
      "Epoch: 38, Loss: 192.73414, Residuals: -2.40816, Convergence: 0.011566\n",
      "Epoch: 39, Loss: 190.56201, Residuals: -2.37373, Convergence: 0.011399\n",
      "Epoch: 40, Loss: 188.44923, Residuals: -2.33951, Convergence: 0.011211\n",
      "Epoch: 41, Loss: 186.39779, Residuals: -2.30555, Convergence: 0.011006\n",
      "Epoch: 42, Loss: 184.40939, Residuals: -2.27186, Convergence: 0.010783\n",
      "Epoch: 43, Loss: 182.48548, Residuals: -2.23850, Convergence: 0.010543\n",
      "Epoch: 44, Loss: 180.62740, Residuals: -2.20550, Convergence: 0.010287\n",
      "Epoch: 45, Loss: 178.83646, Residuals: -2.17292, Convergence: 0.010014\n",
      "Epoch: 46, Loss: 177.11389, Residuals: -2.14081, Convergence: 0.009726\n",
      "Epoch: 47, Loss: 175.46073, Residuals: -2.10922, Convergence: 0.009422\n",
      "Epoch: 48, Loss: 173.87759, Residuals: -2.07821, Convergence: 0.009105\n",
      "Epoch: 49, Loss: 172.36446, Residuals: -2.04782, Convergence: 0.008779\n",
      "Epoch: 50, Loss: 170.92054, Residuals: -2.01809, Convergence: 0.008448\n",
      "Epoch: 51, Loss: 169.54433, Residuals: -1.98905, Convergence: 0.008117\n",
      "Epoch: 52, Loss: 168.23370, Residuals: -1.96070, Convergence: 0.007791\n",
      "Epoch: 53, Loss: 166.98605, Residuals: -1.93304, Convergence: 0.007472\n",
      "Epoch: 54, Loss: 165.79840, Residuals: -1.90609, Convergence: 0.007163\n",
      "Epoch: 55, Loss: 164.66756, Residuals: -1.87983, Convergence: 0.006867\n",
      "Epoch: 56, Loss: 163.59018, Residuals: -1.85425, Convergence: 0.006586\n",
      "Epoch: 57, Loss: 162.56287, Residuals: -1.82936, Convergence: 0.006319\n",
      "Epoch: 58, Loss: 161.58230, Residuals: -1.80512, Convergence: 0.006069\n",
      "Epoch: 59, Loss: 160.64530, Residuals: -1.78154, Convergence: 0.005833\n",
      "Epoch: 60, Loss: 159.74896, Residuals: -1.75859, Convergence: 0.005611\n",
      "Epoch: 61, Loss: 158.89063, Residuals: -1.73625, Convergence: 0.005402\n",
      "Epoch: 62, Loss: 158.06800, Residuals: -1.71453, Convergence: 0.005204\n",
      "Epoch: 63, Loss: 157.27900, Residuals: -1.69340, Convergence: 0.005017\n",
      "Epoch: 64, Loss: 156.52184, Residuals: -1.67286, Convergence: 0.004837\n",
      "Epoch: 65, Loss: 155.79497, Residuals: -1.65290, Convergence: 0.004666\n",
      "Epoch: 66, Loss: 155.09700, Residuals: -1.63351, Convergence: 0.004500\n",
      "Epoch: 67, Loss: 154.42669, Residuals: -1.61469, Convergence: 0.004341\n",
      "Epoch: 68, Loss: 153.78293, Residuals: -1.59642, Convergence: 0.004186\n",
      "Epoch: 69, Loss: 153.16472, Residuals: -1.57870, Convergence: 0.004036\n",
      "Epoch: 70, Loss: 152.57112, Residuals: -1.56153, Convergence: 0.003891\n",
      "Epoch: 71, Loss: 152.00126, Residuals: -1.54489, Convergence: 0.003749\n",
      "Epoch: 72, Loss: 151.45432, Residuals: -1.52879, Convergence: 0.003611\n",
      "Epoch: 73, Loss: 150.92953, Residuals: -1.51320, Convergence: 0.003477\n",
      "Epoch: 74, Loss: 150.42615, Residuals: -1.49812, Convergence: 0.003346\n",
      "Epoch: 75, Loss: 149.94345, Residuals: -1.48354, Convergence: 0.003219\n",
      "Epoch: 76, Loss: 149.48079, Residuals: -1.46946, Convergence: 0.003095\n",
      "Epoch: 77, Loss: 149.03746, Residuals: -1.45585, Convergence: 0.002975\n",
      "Epoch: 78, Loss: 148.61284, Residuals: -1.44272, Convergence: 0.002857\n",
      "Epoch: 79, Loss: 148.20629, Residuals: -1.43005, Convergence: 0.002743\n",
      "Epoch: 80, Loss: 147.81719, Residuals: -1.41783, Convergence: 0.002632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81, Loss: 147.44494, Residuals: -1.40605, Convergence: 0.002525\n",
      "Epoch: 82, Loss: 147.08893, Residuals: -1.39469, Convergence: 0.002420\n",
      "Epoch: 83, Loss: 146.74858, Residuals: -1.38375, Convergence: 0.002319\n",
      "Epoch: 84, Loss: 146.42332, Residuals: -1.37322, Convergence: 0.002221\n",
      "Epoch: 85, Loss: 146.11255, Residuals: -1.36308, Convergence: 0.002127\n",
      "Epoch: 86, Loss: 145.81573, Residuals: -1.35332, Convergence: 0.002036\n",
      "Epoch: 87, Loss: 145.53229, Residuals: -1.34393, Convergence: 0.001948\n",
      "Epoch: 88, Loss: 145.26170, Residuals: -1.33490, Convergence: 0.001863\n",
      "Epoch: 89, Loss: 145.00341, Residuals: -1.32622, Convergence: 0.001781\n",
      "Epoch: 90, Loss: 144.75694, Residuals: -1.31788, Convergence: 0.001703\n",
      "Epoch: 91, Loss: 144.52176, Residuals: -1.30986, Convergence: 0.001627\n",
      "Epoch: 92, Loss: 144.29742, Residuals: -1.30215, Convergence: 0.001555\n",
      "Epoch: 93, Loss: 144.08347, Residuals: -1.29474, Convergence: 0.001485\n",
      "Epoch: 94, Loss: 143.87947, Residuals: -1.28762, Convergence: 0.001418\n",
      "Epoch: 95, Loss: 143.68503, Residuals: -1.28079, Convergence: 0.001353\n",
      "Epoch: 96, Loss: 143.49976, Residuals: -1.27423, Convergence: 0.001291\n",
      "Epoch: 97, Loss: 143.32336, Residuals: -1.26793, Convergence: 0.001231\n",
      "Epoch: 98, Loss: 143.15548, Residuals: -1.26189, Convergence: 0.001173\n",
      "Epoch: 99, Loss: 142.99585, Residuals: -1.25609, Convergence: 0.001116\n",
      "Epoch: 100, Loss: 142.84421, Residuals: -1.25054, Convergence: 0.001062\n",
      "Epoch: 101, Loss: 142.70031, Residuals: -1.24521, Convergence: 0.001008\n",
      "Epoch: 102, Loss: 142.56396, Residuals: -1.24011, Convergence: 0.000956\n",
      "Evidence -184.352\n",
      "\n",
      "Epoch: 102, Evidence: -184.35229, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 7.25e-01\n",
      "Epoch: 102, Loss: 1395.51192, Residuals: -1.24011, Convergence:   inf\n",
      "Epoch: 103, Loss: 1332.15238, Residuals: -1.27081, Convergence: 0.047562\n",
      "Epoch: 104, Loss: 1283.78418, Residuals: -1.29491, Convergence: 0.037676\n",
      "Epoch: 105, Loss: 1247.18199, Residuals: -1.31221, Convergence: 0.029348\n",
      "Epoch: 106, Loss: 1218.72686, Residuals: -1.32432, Convergence: 0.023348\n",
      "Epoch: 107, Loss: 1195.75461, Residuals: -1.33325, Convergence: 0.019212\n",
      "Epoch: 108, Loss: 1176.71528, Residuals: -1.34008, Convergence: 0.016180\n",
      "Epoch: 109, Loss: 1160.66805, Residuals: -1.34528, Convergence: 0.013826\n",
      "Epoch: 110, Loss: 1146.97182, Residuals: -1.34909, Convergence: 0.011941\n",
      "Epoch: 111, Loss: 1135.14911, Residuals: -1.35165, Convergence: 0.010415\n",
      "Epoch: 112, Loss: 1124.82701, Residuals: -1.35307, Convergence: 0.009177\n",
      "Epoch: 113, Loss: 1115.70411, Residuals: -1.35347, Convergence: 0.008177\n",
      "Epoch: 114, Loss: 1107.53260, Residuals: -1.35290, Convergence: 0.007378\n",
      "Epoch: 115, Loss: 1100.10274, Residuals: -1.35145, Convergence: 0.006754\n",
      "Epoch: 116, Loss: 1093.23648, Residuals: -1.34916, Convergence: 0.006281\n",
      "Epoch: 117, Loss: 1086.77934, Residuals: -1.34607, Convergence: 0.005942\n",
      "Epoch: 118, Loss: 1080.59823, Residuals: -1.34220, Convergence: 0.005720\n",
      "Epoch: 119, Loss: 1074.57880, Residuals: -1.33757, Convergence: 0.005602\n",
      "Epoch: 120, Loss: 1068.62952, Residuals: -1.33223, Convergence: 0.005567\n",
      "Epoch: 121, Loss: 1062.68874, Residuals: -1.32622, Convergence: 0.005590\n",
      "Epoch: 122, Loss: 1056.74027, Residuals: -1.31962, Convergence: 0.005629\n",
      "Epoch: 123, Loss: 1050.82303, Residuals: -1.31253, Convergence: 0.005631\n",
      "Epoch: 124, Loss: 1045.02340, Residuals: -1.30506, Convergence: 0.005550\n",
      "Epoch: 125, Loss: 1039.44225, Residuals: -1.29731, Convergence: 0.005369\n",
      "Epoch: 126, Loss: 1034.15847, Residuals: -1.28937, Convergence: 0.005109\n",
      "Epoch: 127, Loss: 1029.21057, Residuals: -1.28132, Convergence: 0.004807\n",
      "Epoch: 128, Loss: 1024.59973, Residuals: -1.27322, Convergence: 0.004500\n",
      "Epoch: 129, Loss: 1020.30458, Residuals: -1.26515, Convergence: 0.004210\n",
      "Epoch: 130, Loss: 1016.29462, Residuals: -1.25713, Convergence: 0.003946\n",
      "Epoch: 131, Loss: 1012.53731, Residuals: -1.24922, Convergence: 0.003711\n",
      "Epoch: 132, Loss: 1009.00211, Residuals: -1.24145, Convergence: 0.003504\n",
      "Epoch: 133, Loss: 1005.66298, Residuals: -1.23383, Convergence: 0.003320\n",
      "Epoch: 134, Loss: 1002.49835, Residuals: -1.22640, Convergence: 0.003157\n",
      "Epoch: 135, Loss: 999.48976, Residuals: -1.21917, Convergence: 0.003010\n",
      "Epoch: 136, Loss: 996.62252, Residuals: -1.21214, Convergence: 0.002877\n",
      "Epoch: 137, Loss: 993.88505, Residuals: -1.20534, Convergence: 0.002754\n",
      "Epoch: 138, Loss: 991.26774, Residuals: -1.19876, Convergence: 0.002640\n",
      "Epoch: 139, Loss: 988.76291, Residuals: -1.19242, Convergence: 0.002533\n",
      "Epoch: 140, Loss: 986.36409, Residuals: -1.18631, Convergence: 0.002432\n",
      "Epoch: 141, Loss: 984.06588, Residuals: -1.18044, Convergence: 0.002335\n",
      "Epoch: 142, Loss: 981.86384, Residuals: -1.17479, Convergence: 0.002243\n",
      "Epoch: 143, Loss: 979.75340, Residuals: -1.16938, Convergence: 0.002154\n",
      "Epoch: 144, Loss: 977.73094, Residuals: -1.16420, Convergence: 0.002069\n",
      "Epoch: 145, Loss: 975.79298, Residuals: -1.15923, Convergence: 0.001986\n",
      "Epoch: 146, Loss: 973.93551, Residuals: -1.15448, Convergence: 0.001907\n",
      "Epoch: 147, Loss: 972.15492, Residuals: -1.14993, Convergence: 0.001832\n",
      "Epoch: 148, Loss: 970.44739, Residuals: -1.14558, Convergence: 0.001760\n",
      "Epoch: 149, Loss: 968.80917, Residuals: -1.14142, Convergence: 0.001691\n",
      "Epoch: 150, Loss: 967.23583, Residuals: -1.13743, Convergence: 0.001627\n",
      "Epoch: 151, Loss: 965.72292, Residuals: -1.13362, Convergence: 0.001567\n",
      "Epoch: 152, Loss: 964.26578, Residuals: -1.12996, Convergence: 0.001511\n",
      "Epoch: 153, Loss: 962.85969, Residuals: -1.12645, Convergence: 0.001460\n",
      "Epoch: 154, Loss: 961.49931, Residuals: -1.12307, Convergence: 0.001415\n",
      "Epoch: 155, Loss: 960.17913, Residuals: -1.11982, Convergence: 0.001375\n",
      "Epoch: 156, Loss: 958.89371, Residuals: -1.11667, Convergence: 0.001341\n",
      "Epoch: 157, Loss: 957.63790, Residuals: -1.11362, Convergence: 0.001311\n",
      "Epoch: 158, Loss: 956.40640, Residuals: -1.11066, Convergence: 0.001288\n",
      "Epoch: 159, Loss: 955.19442, Residuals: -1.10776, Convergence: 0.001269\n",
      "Epoch: 160, Loss: 953.99861, Residuals: -1.10493, Convergence: 0.001253\n",
      "Epoch: 161, Loss: 952.81656, Residuals: -1.10215, Convergence: 0.001241\n",
      "Epoch: 162, Loss: 951.64826, Residuals: -1.09942, Convergence: 0.001228\n",
      "Epoch: 163, Loss: 950.49438, Residuals: -1.09673, Convergence: 0.001214\n",
      "Epoch: 164, Loss: 949.35837, Residuals: -1.09410, Convergence: 0.001197\n",
      "Epoch: 165, Loss: 948.24386, Residuals: -1.09153, Convergence: 0.001175\n",
      "Epoch: 166, Loss: 947.15538, Residuals: -1.08902, Convergence: 0.001149\n",
      "Epoch: 167, Loss: 946.09730, Residuals: -1.08659, Convergence: 0.001118\n",
      "Epoch: 168, Loss: 945.07328, Residuals: -1.08423, Convergence: 0.001084\n",
      "Epoch: 169, Loss: 944.08542, Residuals: -1.08195, Convergence: 0.001046\n",
      "Epoch: 170, Loss: 943.13576, Residuals: -1.07976, Convergence: 0.001007\n",
      "Epoch: 171, Loss: 942.22503, Residuals: -1.07766, Convergence: 0.000967\n",
      "Evidence 11320.278\n",
      "\n",
      "Epoch: 171, Evidence: 11320.27832, Convergence: 1.016285\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 5.74e-01\n",
      "Epoch: 171, Loss: 2377.07053, Residuals: -1.07766, Convergence:   inf\n",
      "Epoch: 172, Loss: 2340.10773, Residuals: -1.08633, Convergence: 0.015795\n",
      "Epoch: 173, Loss: 2313.84497, Residuals: -1.08458, Convergence: 0.011350\n",
      "Epoch: 174, Loss: 2291.98152, Residuals: -1.08211, Convergence: 0.009539\n",
      "Epoch: 175, Loss: 2273.56742, Residuals: -1.07945, Convergence: 0.008099\n",
      "Epoch: 176, Loss: 2257.91407, Residuals: -1.07671, Convergence: 0.006933\n",
      "Epoch: 177, Loss: 2244.48432, Residuals: -1.07392, Convergence: 0.005983\n",
      "Epoch: 178, Loss: 2232.83932, Residuals: -1.07108, Convergence: 0.005215\n",
      "Epoch: 179, Loss: 2222.61565, Residuals: -1.06819, Convergence: 0.004600\n",
      "Epoch: 180, Loss: 2213.51574, Residuals: -1.06522, Convergence: 0.004111\n",
      "Epoch: 181, Loss: 2205.30470, Residuals: -1.06217, Convergence: 0.003723\n",
      "Epoch: 182, Loss: 2197.81437, Residuals: -1.05901, Convergence: 0.003408\n",
      "Epoch: 183, Loss: 2190.93400, Residuals: -1.05577, Convergence: 0.003140\n",
      "Epoch: 184, Loss: 2184.59761, Residuals: -1.05247, Convergence: 0.002900\n",
      "Epoch: 185, Loss: 2178.76208, Residuals: -1.04917, Convergence: 0.002678\n",
      "Epoch: 186, Loss: 2173.39006, Residuals: -1.04589, Convergence: 0.002472\n",
      "Epoch: 187, Loss: 2168.44544, Residuals: -1.04269, Convergence: 0.002280\n",
      "Epoch: 188, Loss: 2163.89002, Residuals: -1.03957, Convergence: 0.002105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 189, Loss: 2159.68699, Residuals: -1.03655, Convergence: 0.001946\n",
      "Epoch: 190, Loss: 2155.80004, Residuals: -1.03365, Convergence: 0.001803\n",
      "Epoch: 191, Loss: 2152.19635, Residuals: -1.03088, Convergence: 0.001674\n",
      "Epoch: 192, Loss: 2148.84542, Residuals: -1.02822, Convergence: 0.001559\n",
      "Epoch: 193, Loss: 2145.71977, Residuals: -1.02568, Convergence: 0.001457\n",
      "Epoch: 194, Loss: 2142.79584, Residuals: -1.02327, Convergence: 0.001365\n",
      "Epoch: 195, Loss: 2140.05193, Residuals: -1.02096, Convergence: 0.001282\n",
      "Epoch: 196, Loss: 2137.47004, Residuals: -1.01878, Convergence: 0.001208\n",
      "Epoch: 197, Loss: 2135.03437, Residuals: -1.01669, Convergence: 0.001141\n",
      "Epoch: 198, Loss: 2132.73039, Residuals: -1.01471, Convergence: 0.001080\n",
      "Epoch: 199, Loss: 2130.54721, Residuals: -1.01283, Convergence: 0.001025\n",
      "Epoch: 200, Loss: 2128.47400, Residuals: -1.01104, Convergence: 0.000974\n",
      "Evidence 14448.420\n",
      "\n",
      "Epoch: 200, Evidence: 14448.41992, Convergence: 0.216504\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 4.35e-01\n",
      "Epoch: 200, Loss: 2497.96130, Residuals: -1.01104, Convergence:   inf\n",
      "Epoch: 201, Loss: 2484.57258, Residuals: -1.00724, Convergence: 0.005389\n",
      "Epoch: 202, Loss: 2473.55358, Residuals: -1.00321, Convergence: 0.004455\n",
      "Epoch: 203, Loss: 2464.05613, Residuals: -0.99948, Convergence: 0.003854\n",
      "Epoch: 204, Loss: 2455.83147, Residuals: -0.99608, Convergence: 0.003349\n",
      "Epoch: 205, Loss: 2448.67706, Residuals: -0.99301, Convergence: 0.002922\n",
      "Epoch: 206, Loss: 2442.41983, Residuals: -0.99025, Convergence: 0.002562\n",
      "Epoch: 207, Loss: 2436.91819, Residuals: -0.98779, Convergence: 0.002258\n",
      "Epoch: 208, Loss: 2432.05162, Residuals: -0.98559, Convergence: 0.002001\n",
      "Epoch: 209, Loss: 2427.72074, Residuals: -0.98363, Convergence: 0.001784\n",
      "Epoch: 210, Loss: 2423.84310, Residuals: -0.98189, Convergence: 0.001600\n",
      "Epoch: 211, Loss: 2420.34954, Residuals: -0.98033, Convergence: 0.001443\n",
      "Epoch: 212, Loss: 2417.18424, Residuals: -0.97894, Convergence: 0.001310\n",
      "Epoch: 213, Loss: 2414.30088, Residuals: -0.97769, Convergence: 0.001194\n",
      "Epoch: 214, Loss: 2411.66062, Residuals: -0.97657, Convergence: 0.001095\n",
      "Epoch: 215, Loss: 2409.23196, Residuals: -0.97557, Convergence: 0.001008\n",
      "Epoch: 216, Loss: 2406.98838, Residuals: -0.97466, Convergence: 0.000932\n",
      "Evidence 14810.584\n",
      "\n",
      "Epoch: 216, Evidence: 14810.58398, Convergence: 0.024453\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 3.30e-01\n",
      "Epoch: 216, Loss: 2502.73909, Residuals: -0.97466, Convergence:   inf\n",
      "Epoch: 217, Loss: 2496.26473, Residuals: -0.97129, Convergence: 0.002594\n",
      "Epoch: 218, Loss: 2490.91702, Residuals: -0.96852, Convergence: 0.002147\n",
      "Epoch: 219, Loss: 2486.39865, Residuals: -0.96624, Convergence: 0.001817\n",
      "Epoch: 220, Loss: 2482.52489, Residuals: -0.96438, Convergence: 0.001560\n",
      "Epoch: 221, Loss: 2479.15621, Residuals: -0.96285, Convergence: 0.001359\n",
      "Epoch: 222, Loss: 2476.18855, Residuals: -0.96158, Convergence: 0.001198\n",
      "Epoch: 223, Loss: 2473.54165, Residuals: -0.96054, Convergence: 0.001070\n",
      "Epoch: 224, Loss: 2471.15677, Residuals: -0.95966, Convergence: 0.000965\n",
      "Evidence 14891.949\n",
      "\n",
      "Epoch: 224, Evidence: 14891.94922, Convergence: 0.005464\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 2.57e-01\n",
      "Epoch: 224, Loss: 2504.52987, Residuals: -0.95966, Convergence:   inf\n",
      "Epoch: 225, Loss: 2500.61808, Residuals: -0.95727, Convergence: 0.001564\n",
      "Epoch: 226, Loss: 2497.38453, Residuals: -0.95543, Convergence: 0.001295\n",
      "Epoch: 227, Loss: 2494.62604, Residuals: -0.95398, Convergence: 0.001106\n",
      "Epoch: 228, Loss: 2492.22115, Residuals: -0.95285, Convergence: 0.000965\n",
      "Evidence 14919.812\n",
      "\n",
      "Epoch: 228, Evidence: 14919.81250, Convergence: 0.001868\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 2.06e-01\n",
      "Epoch: 228, Loss: 2505.62753, Residuals: -0.95285, Convergence:   inf\n",
      "Epoch: 229, Loss: 2502.73122, Residuals: -0.95096, Convergence: 0.001157\n",
      "Epoch: 230, Loss: 2500.31615, Residuals: -0.94953, Convergence: 0.000966\n",
      "Evidence 14931.943\n",
      "\n",
      "Epoch: 230, Evidence: 14931.94336, Convergence: 0.000812\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.70e-01\n",
      "Epoch: 230, Loss: 2506.41245, Residuals: -0.94953, Convergence:   inf\n",
      "Epoch: 231, Loss: 2501.89216, Residuals: -0.94698, Convergence: 0.001807\n",
      "Epoch: 232, Loss: 2498.45720, Residuals: -0.94518, Convergence: 0.001375\n",
      "Epoch: 233, Loss: 2495.67678, Residuals: -0.94394, Convergence: 0.001114\n",
      "Epoch: 234, Loss: 2493.31590, Residuals: -0.94319, Convergence: 0.000947\n",
      "Evidence 14949.508\n",
      "\n",
      "Epoch: 234, Evidence: 14949.50781, Convergence: 0.001986\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.40e-01\n",
      "Epoch: 234, Loss: 2506.62671, Residuals: -0.94319, Convergence:   inf\n",
      "Epoch: 235, Loss: 2503.63906, Residuals: -0.94056, Convergence: 0.001193\n",
      "Epoch: 236, Loss: 2501.27859, Residuals: -0.93915, Convergence: 0.000944\n",
      "Evidence 14960.331\n",
      "\n",
      "Epoch: 236, Evidence: 14960.33105, Convergence: 0.000723\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.19e-01\n",
      "Epoch: 236, Loss: 2506.85959, Residuals: -0.93915, Convergence:   inf\n",
      "Epoch: 237, Loss: 2502.50528, Residuals: -0.93516, Convergence: 0.001740\n",
      "Epoch: 238, Loss: 2499.39019, Residuals: -0.93574, Convergence: 0.001246\n",
      "Epoch: 239, Loss: 2496.75858, Residuals: -0.93611, Convergence: 0.001054\n",
      "Epoch: 240, Loss: 2494.43869, Residuals: -0.93861, Convergence: 0.000930\n",
      "Evidence 14975.882\n",
      "\n",
      "Epoch: 240, Evidence: 14975.88184, Convergence: 0.001761\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.06e-01\n",
      "Epoch: 240, Loss: 2506.21177, Residuals: -0.93861, Convergence:   inf\n",
      "Epoch: 241, Loss: 2504.58524, Residuals: -0.93534, Convergence: 0.000649\n",
      "Evidence 14982.366\n",
      "\n",
      "Epoch: 241, Evidence: 14982.36621, Convergence: 0.000433\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 8.64e-02\n",
      "Epoch: 241, Loss: 2507.20881, Residuals: -0.93534, Convergence:   inf\n",
      "Epoch: 242, Loss: 2550.53246, Residuals: -0.97547, Convergence: -0.016986\n",
      "Epoch: 242, Loss: 2505.00578, Residuals: -0.93375, Convergence: 0.000879\n",
      "Evidence 14986.551\n",
      "\n",
      "Epoch: 242, Evidence: 14986.55078, Convergence: 0.000712\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 8.00e-02\n",
      "Epoch: 242, Loss: 2506.62095, Residuals: -0.93375, Convergence:   inf\n",
      "Epoch: 243, Loss: 2512.38629, Residuals: -0.93570, Convergence: -0.002295\n",
      "Epoch: 243, Loss: 2506.82354, Residuals: -0.93236, Convergence: -0.000081\n",
      "Evidence 14988.021\n",
      "\n",
      "Epoch: 243, Evidence: 14988.02051, Convergence: 0.000810\n",
      "Total samples: 185, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 384.36901, Residuals: -4.51796, Convergence:   inf\n",
      "Epoch: 1, Loss: 358.58402, Residuals: -4.39854, Convergence: 0.071908\n",
      "Epoch: 2, Loss: 337.44245, Residuals: -4.23501, Convergence: 0.062652\n",
      "Epoch: 3, Loss: 321.29165, Residuals: -4.07105, Convergence: 0.050268\n",
      "Epoch: 4, Loss: 308.97530, Residuals: -3.92693, Convergence: 0.039862\n",
      "Epoch: 5, Loss: 299.20089, Residuals: -3.79969, Convergence: 0.032668\n",
      "Epoch: 6, Loss: 291.25604, Residuals: -3.68886, Convergence: 0.027278\n",
      "Epoch: 7, Loss: 284.65031, Residuals: -3.59387, Convergence: 0.023206\n",
      "Epoch: 8, Loss: 279.02150, Residuals: -3.51267, Convergence: 0.020173\n",
      "Epoch: 9, Loss: 274.11328, Residuals: -3.44295, Convergence: 0.017906\n",
      "Epoch: 10, Loss: 269.74321, Residuals: -3.38261, Convergence: 0.016201\n",
      "Epoch: 11, Loss: 265.77893, Residuals: -3.32985, Convergence: 0.014916\n",
      "Epoch: 12, Loss: 262.12375, Residuals: -3.28313, Convergence: 0.013944\n",
      "Epoch: 13, Loss: 258.70752, Residuals: -3.24113, Convergence: 0.013205\n",
      "Epoch: 14, Loss: 255.48093, Residuals: -3.20269, Convergence: 0.012629\n",
      "Epoch: 15, Loss: 252.41326, Residuals: -3.16688, Convergence: 0.012153\n",
      "Epoch: 16, Loss: 249.48972, Residuals: -3.13311, Convergence: 0.011718\n",
      "Epoch: 17, Loss: 246.69970, Residuals: -3.10103, Convergence: 0.011309\n",
      "Epoch: 18, Loss: 244.02207, Residuals: -3.07024, Convergence: 0.010973\n",
      "Epoch: 19, Loss: 241.42377, Residuals: -3.04023, Convergence: 0.010762\n",
      "Epoch: 20, Loss: 238.86896, Residuals: -3.01043, Convergence: 0.010695\n",
      "Epoch: 21, Loss: 236.32867, Residuals: -2.98039, Convergence: 0.010749\n",
      "Epoch: 22, Loss: 233.78323, Residuals: -2.94982, Convergence: 0.010888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Loss: 231.20903, Residuals: -2.91850, Convergence: 0.011134\n",
      "Epoch: 24, Loss: 228.56069, Residuals: -2.88594, Convergence: 0.011587\n",
      "Epoch: 25, Loss: 225.77692, Residuals: -2.85141, Convergence: 0.012330\n",
      "Epoch: 26, Loss: 222.84767, Residuals: -2.81469, Convergence: 0.013145\n",
      "Epoch: 27, Loss: 219.89134, Residuals: -2.77698, Convergence: 0.013444\n",
      "Epoch: 28, Loss: 217.02458, Residuals: -2.73963, Convergence: 0.013209\n",
      "Epoch: 29, Loss: 214.26664, Residuals: -2.70299, Convergence: 0.012872\n",
      "Epoch: 30, Loss: 211.60022, Residuals: -2.66698, Convergence: 0.012601\n",
      "Epoch: 31, Loss: 209.00699, Residuals: -2.63144, Convergence: 0.012407\n",
      "Epoch: 32, Loss: 206.47427, Residuals: -2.59625, Convergence: 0.012267\n",
      "Epoch: 33, Loss: 203.99470, Residuals: -2.56131, Convergence: 0.012155\n",
      "Epoch: 34, Loss: 201.56492, Residuals: -2.52655, Convergence: 0.012055\n",
      "Epoch: 35, Loss: 199.18424, Residuals: -2.49194, Convergence: 0.011952\n",
      "Epoch: 36, Loss: 196.85355, Residuals: -2.45746, Convergence: 0.011840\n",
      "Epoch: 37, Loss: 194.57463, Residuals: -2.42311, Convergence: 0.011712\n",
      "Epoch: 38, Loss: 192.34961, Residuals: -2.38889, Convergence: 0.011568\n",
      "Epoch: 39, Loss: 190.18061, Residuals: -2.35483, Convergence: 0.011405\n",
      "Epoch: 40, Loss: 188.06960, Residuals: -2.32095, Convergence: 0.011225\n",
      "Epoch: 41, Loss: 186.01833, Residuals: -2.28727, Convergence: 0.011027\n",
      "Epoch: 42, Loss: 184.02831, Residuals: -2.25384, Convergence: 0.010814\n",
      "Epoch: 43, Loss: 182.10086, Residuals: -2.22069, Convergence: 0.010585\n",
      "Epoch: 44, Loss: 180.23718, Residuals: -2.18786, Convergence: 0.010340\n",
      "Epoch: 45, Loss: 178.43843, Residuals: -2.15540, Convergence: 0.010080\n",
      "Epoch: 46, Loss: 176.70567, Residuals: -2.12338, Convergence: 0.009806\n",
      "Epoch: 47, Loss: 175.03973, Residuals: -2.09183, Convergence: 0.009517\n",
      "Epoch: 48, Loss: 173.44102, Residuals: -2.06081, Convergence: 0.009218\n",
      "Epoch: 49, Loss: 171.90929, Residuals: -2.03038, Convergence: 0.008910\n",
      "Epoch: 50, Loss: 170.44354, Residuals: -2.00056, Convergence: 0.008600\n",
      "Epoch: 51, Loss: 169.04201, Residuals: -1.97138, Convergence: 0.008291\n",
      "Epoch: 52, Loss: 167.70239, Residuals: -1.94284, Convergence: 0.007988\n",
      "Epoch: 53, Loss: 166.42194, Residuals: -1.91496, Convergence: 0.007694\n",
      "Epoch: 54, Loss: 165.19774, Residuals: -1.88772, Convergence: 0.007411\n",
      "Epoch: 55, Loss: 164.02686, Residuals: -1.86112, Convergence: 0.007138\n",
      "Epoch: 56, Loss: 162.90644, Residuals: -1.83515, Convergence: 0.006878\n",
      "Epoch: 57, Loss: 161.83381, Residuals: -1.80980, Convergence: 0.006628\n",
      "Epoch: 58, Loss: 160.80652, Residuals: -1.78507, Convergence: 0.006388\n",
      "Epoch: 59, Loss: 159.82232, Residuals: -1.76094, Convergence: 0.006158\n",
      "Epoch: 60, Loss: 158.87923, Residuals: -1.73742, Convergence: 0.005936\n",
      "Epoch: 61, Loss: 157.97544, Residuals: -1.71450, Convergence: 0.005721\n",
      "Epoch: 62, Loss: 157.10934, Residuals: -1.69219, Convergence: 0.005513\n",
      "Epoch: 63, Loss: 156.27945, Residuals: -1.67047, Convergence: 0.005310\n",
      "Epoch: 64, Loss: 155.48440, Residuals: -1.64936, Convergence: 0.005113\n",
      "Epoch: 65, Loss: 154.72289, Residuals: -1.62886, Convergence: 0.004922\n",
      "Epoch: 66, Loss: 153.99373, Residuals: -1.60895, Convergence: 0.004735\n",
      "Epoch: 67, Loss: 153.29573, Residuals: -1.58965, Convergence: 0.004553\n",
      "Epoch: 68, Loss: 152.62779, Residuals: -1.57093, Convergence: 0.004376\n",
      "Epoch: 69, Loss: 151.98883, Residuals: -1.55281, Convergence: 0.004204\n",
      "Epoch: 70, Loss: 151.37779, Residuals: -1.53528, Convergence: 0.004036\n",
      "Epoch: 71, Loss: 150.79368, Residuals: -1.51832, Convergence: 0.003874\n",
      "Epoch: 72, Loss: 150.23551, Residuals: -1.50193, Convergence: 0.003715\n",
      "Epoch: 73, Loss: 149.70231, Residuals: -1.48611, Convergence: 0.003562\n",
      "Epoch: 74, Loss: 149.19317, Residuals: -1.47084, Convergence: 0.003413\n",
      "Epoch: 75, Loss: 148.70717, Residuals: -1.45610, Convergence: 0.003268\n",
      "Epoch: 76, Loss: 148.24342, Residuals: -1.44190, Convergence: 0.003128\n",
      "Epoch: 77, Loss: 147.80105, Residuals: -1.42822, Convergence: 0.002993\n",
      "Epoch: 78, Loss: 147.37919, Residuals: -1.41505, Convergence: 0.002862\n",
      "Epoch: 79, Loss: 146.97700, Residuals: -1.40237, Convergence: 0.002736\n",
      "Epoch: 80, Loss: 146.59365, Residuals: -1.39017, Convergence: 0.002615\n",
      "Epoch: 81, Loss: 146.22831, Residuals: -1.37844, Convergence: 0.002498\n",
      "Epoch: 82, Loss: 145.88017, Residuals: -1.36717, Convergence: 0.002386\n",
      "Epoch: 83, Loss: 145.54846, Residuals: -1.35633, Convergence: 0.002279\n",
      "Epoch: 84, Loss: 145.23241, Residuals: -1.34593, Convergence: 0.002176\n",
      "Epoch: 85, Loss: 144.93125, Residuals: -1.33593, Convergence: 0.002078\n",
      "Epoch: 86, Loss: 144.64429, Residuals: -1.32633, Convergence: 0.001984\n",
      "Epoch: 87, Loss: 144.37081, Residuals: -1.31712, Convergence: 0.001894\n",
      "Epoch: 88, Loss: 144.11018, Residuals: -1.30828, Convergence: 0.001809\n",
      "Epoch: 89, Loss: 143.86174, Residuals: -1.29979, Convergence: 0.001727\n",
      "Epoch: 90, Loss: 143.62492, Residuals: -1.29165, Convergence: 0.001649\n",
      "Epoch: 91, Loss: 143.39916, Residuals: -1.28384, Convergence: 0.001574\n",
      "Epoch: 92, Loss: 143.18394, Residuals: -1.27634, Convergence: 0.001503\n",
      "Epoch: 93, Loss: 142.97879, Residuals: -1.26915, Convergence: 0.001435\n",
      "Epoch: 94, Loss: 142.78327, Residuals: -1.26225, Convergence: 0.001369\n",
      "Epoch: 95, Loss: 142.59698, Residuals: -1.25563, Convergence: 0.001306\n",
      "Epoch: 96, Loss: 142.41955, Residuals: -1.24929, Convergence: 0.001246\n",
      "Epoch: 97, Loss: 142.25068, Residuals: -1.24320, Convergence: 0.001187\n",
      "Epoch: 98, Loss: 142.09005, Residuals: -1.23737, Convergence: 0.001130\n",
      "Epoch: 99, Loss: 141.93743, Residuals: -1.23178, Convergence: 0.001075\n",
      "Epoch: 100, Loss: 141.79257, Residuals: -1.22642, Convergence: 0.001022\n",
      "Epoch: 101, Loss: 141.65528, Residuals: -1.22129, Convergence: 0.000969\n",
      "Evidence -183.602\n",
      "\n",
      "Epoch: 101, Evidence: -183.60191, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 7.25e-01\n",
      "Epoch: 101, Loss: 1393.49951, Residuals: -1.22129, Convergence:   inf\n",
      "Epoch: 102, Loss: 1330.01106, Residuals: -1.25113, Convergence: 0.047735\n",
      "Epoch: 103, Loss: 1281.45702, Residuals: -1.27463, Convergence: 0.037890\n",
      "Epoch: 104, Loss: 1244.67154, Residuals: -1.29162, Convergence: 0.029554\n",
      "Epoch: 105, Loss: 1216.08766, Residuals: -1.30363, Convergence: 0.023505\n",
      "Epoch: 106, Loss: 1193.03865, Residuals: -1.31256, Convergence: 0.019320\n",
      "Epoch: 107, Loss: 1173.95688, Residuals: -1.31946, Convergence: 0.016254\n",
      "Epoch: 108, Loss: 1157.89445, Residuals: -1.32477, Convergence: 0.013872\n",
      "Epoch: 109, Loss: 1144.20921, Residuals: -1.32872, Convergence: 0.011960\n",
      "Epoch: 110, Loss: 1132.42656, Residuals: -1.33146, Convergence: 0.010405\n",
      "Epoch: 111, Loss: 1122.17730, Residuals: -1.33311, Convergence: 0.009133\n",
      "Epoch: 112, Loss: 1113.16613, Residuals: -1.33377, Convergence: 0.008095\n",
      "Epoch: 113, Loss: 1105.15293, Residuals: -1.33354, Convergence: 0.007251\n",
      "Epoch: 114, Loss: 1097.93713, Residuals: -1.33249, Convergence: 0.006572\n",
      "Epoch: 115, Loss: 1091.35033, Residuals: -1.33068, Convergence: 0.006035\n",
      "Epoch: 116, Loss: 1085.24687, Residuals: -1.32817, Convergence: 0.005624\n",
      "Epoch: 117, Loss: 1079.49972, Residuals: -1.32498, Convergence: 0.005324\n",
      "Epoch: 118, Loss: 1073.99591, Residuals: -1.32114, Convergence: 0.005125\n",
      "Epoch: 119, Loss: 1068.63466, Residuals: -1.31666, Convergence: 0.005017\n",
      "Epoch: 120, Loss: 1063.32864, Residuals: -1.31156, Convergence: 0.004990\n",
      "Epoch: 121, Loss: 1058.00942, Residuals: -1.30585, Convergence: 0.005028\n",
      "Epoch: 122, Loss: 1052.63877, Residuals: -1.29959, Convergence: 0.005102\n",
      "Epoch: 123, Loss: 1047.22248, Residuals: -1.29285, Convergence: 0.005172\n",
      "Epoch: 124, Loss: 1041.81526, Residuals: -1.28571, Convergence: 0.005190\n",
      "Epoch: 125, Loss: 1036.51005, Residuals: -1.27828, Convergence: 0.005118\n",
      "Epoch: 126, Loss: 1031.40284, Residuals: -1.27064, Convergence: 0.004952\n",
      "Epoch: 127, Loss: 1026.56201, Residuals: -1.26287, Convergence: 0.004716\n",
      "Epoch: 128, Loss: 1022.01804, Residuals: -1.25505, Convergence: 0.004446\n",
      "Epoch: 129, Loss: 1017.77070, Residuals: -1.24722, Convergence: 0.004173\n",
      "Epoch: 130, Loss: 1013.80057, Residuals: -1.23945, Convergence: 0.003916\n",
      "Epoch: 131, Loss: 1010.08174, Residuals: -1.23177, Convergence: 0.003682\n",
      "Epoch: 132, Loss: 1006.58746, Residuals: -1.22421, Convergence: 0.003471\n",
      "Epoch: 133, Loss: 1003.29277, Residuals: -1.21680, Convergence: 0.003284\n",
      "Epoch: 134, Loss: 1000.17632, Residuals: -1.20956, Convergence: 0.003116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 135, Loss: 997.21996, Residuals: -1.20252, Convergence: 0.002965\n",
      "Epoch: 136, Loss: 994.40909, Residuals: -1.19567, Convergence: 0.002827\n",
      "Epoch: 137, Loss: 991.73082, Residuals: -1.18903, Convergence: 0.002701\n",
      "Epoch: 138, Loss: 989.17543, Residuals: -1.18262, Convergence: 0.002583\n",
      "Epoch: 139, Loss: 986.73422, Residuals: -1.17642, Convergence: 0.002474\n",
      "Epoch: 140, Loss: 984.40000, Residuals: -1.17045, Convergence: 0.002371\n",
      "Epoch: 141, Loss: 982.16694, Residuals: -1.16471, Convergence: 0.002274\n",
      "Epoch: 142, Loss: 980.02969, Residuals: -1.15919, Convergence: 0.002181\n",
      "Epoch: 143, Loss: 977.98388, Residuals: -1.15389, Convergence: 0.002092\n",
      "Epoch: 144, Loss: 976.02533, Residuals: -1.14881, Convergence: 0.002007\n",
      "Epoch: 145, Loss: 974.15037, Residuals: -1.14394, Convergence: 0.001925\n",
      "Epoch: 146, Loss: 972.35517, Residuals: -1.13929, Convergence: 0.001846\n",
      "Epoch: 147, Loss: 970.63717, Residuals: -1.13483, Convergence: 0.001770\n",
      "Epoch: 148, Loss: 968.99205, Residuals: -1.13057, Convergence: 0.001698\n",
      "Epoch: 149, Loss: 967.41726, Residuals: -1.12650, Convergence: 0.001628\n",
      "Epoch: 150, Loss: 965.90935, Residuals: -1.12261, Convergence: 0.001561\n",
      "Epoch: 151, Loss: 964.46490, Residuals: -1.11890, Convergence: 0.001498\n",
      "Epoch: 152, Loss: 963.08023, Residuals: -1.11535, Convergence: 0.001438\n",
      "Epoch: 153, Loss: 961.75202, Residuals: -1.11196, Convergence: 0.001381\n",
      "Epoch: 154, Loss: 960.47634, Residuals: -1.10872, Convergence: 0.001328\n",
      "Epoch: 155, Loss: 959.24958, Residuals: -1.10562, Convergence: 0.001279\n",
      "Epoch: 156, Loss: 958.06773, Residuals: -1.10265, Convergence: 0.001234\n",
      "Epoch: 157, Loss: 956.92673, Residuals: -1.09980, Convergence: 0.001192\n",
      "Epoch: 158, Loss: 955.82241, Residuals: -1.09707, Convergence: 0.001155\n",
      "Epoch: 159, Loss: 954.75041, Residuals: -1.09444, Convergence: 0.001123\n",
      "Epoch: 160, Loss: 953.70642, Residuals: -1.09191, Convergence: 0.001095\n",
      "Epoch: 161, Loss: 952.68568, Residuals: -1.08946, Convergence: 0.001071\n",
      "Epoch: 162, Loss: 951.68433, Residuals: -1.08709, Convergence: 0.001052\n",
      "Epoch: 163, Loss: 950.69792, Residuals: -1.08478, Convergence: 0.001038\n",
      "Epoch: 164, Loss: 949.72248, Residuals: -1.08252, Convergence: 0.001027\n",
      "Epoch: 165, Loss: 948.75499, Residuals: -1.08031, Convergence: 0.001020\n",
      "Epoch: 166, Loss: 947.79290, Residuals: -1.07813, Convergence: 0.001015\n",
      "Epoch: 167, Loss: 946.83575, Residuals: -1.07599, Convergence: 0.001011\n",
      "Epoch: 168, Loss: 945.88366, Residuals: -1.07388, Convergence: 0.001007\n",
      "Epoch: 169, Loss: 944.93829, Residuals: -1.07179, Convergence: 0.001000\n",
      "Epoch: 170, Loss: 944.00336, Residuals: -1.06974, Convergence: 0.000990\n",
      "Evidence 11355.174\n",
      "\n",
      "Epoch: 170, Evidence: 11355.17383, Convergence: 1.016169\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 5.75e-01\n",
      "Epoch: 170, Loss: 2384.10443, Residuals: -1.06974, Convergence:   inf\n",
      "Epoch: 171, Loss: 2345.31472, Residuals: -1.07825, Convergence: 0.016539\n",
      "Epoch: 172, Loss: 2318.70245, Residuals: -1.07720, Convergence: 0.011477\n",
      "Epoch: 173, Loss: 2296.52832, Residuals: -1.07518, Convergence: 0.009656\n",
      "Epoch: 174, Loss: 2277.76204, Residuals: -1.07283, Convergence: 0.008239\n",
      "Epoch: 175, Loss: 2261.73170, Residuals: -1.07029, Convergence: 0.007088\n",
      "Epoch: 176, Loss: 2247.92226, Residuals: -1.06763, Convergence: 0.006143\n",
      "Epoch: 177, Loss: 2235.91402, Residuals: -1.06487, Convergence: 0.005371\n",
      "Epoch: 178, Loss: 2225.35617, Residuals: -1.06201, Convergence: 0.004744\n",
      "Epoch: 179, Loss: 2215.95969, Residuals: -1.05907, Convergence: 0.004240\n",
      "Epoch: 180, Loss: 2207.49213, Residuals: -1.05602, Convergence: 0.003836\n",
      "Epoch: 181, Loss: 2199.77904, Residuals: -1.05288, Convergence: 0.003506\n",
      "Epoch: 182, Loss: 2192.70123, Residuals: -1.04964, Convergence: 0.003228\n",
      "Epoch: 183, Loss: 2186.18516, Residuals: -1.04634, Convergence: 0.002981\n",
      "Epoch: 184, Loss: 2180.18031, Residuals: -1.04302, Convergence: 0.002754\n",
      "Epoch: 185, Loss: 2174.64705, Residuals: -1.03972, Convergence: 0.002544\n",
      "Epoch: 186, Loss: 2169.54793, Residuals: -1.03649, Convergence: 0.002350\n",
      "Epoch: 187, Loss: 2164.84610, Residuals: -1.03334, Convergence: 0.002172\n",
      "Epoch: 188, Loss: 2160.50436, Residuals: -1.03029, Convergence: 0.002010\n",
      "Epoch: 189, Loss: 2156.48839, Residuals: -1.02736, Convergence: 0.001862\n",
      "Epoch: 190, Loss: 2152.76437, Residuals: -1.02455, Convergence: 0.001730\n",
      "Epoch: 191, Loss: 2149.30329, Residuals: -1.02188, Convergence: 0.001610\n",
      "Epoch: 192, Loss: 2146.07849, Residuals: -1.01933, Convergence: 0.001503\n",
      "Epoch: 193, Loss: 2143.06533, Residuals: -1.01691, Convergence: 0.001406\n",
      "Epoch: 194, Loss: 2140.24376, Residuals: -1.01462, Convergence: 0.001318\n",
      "Epoch: 195, Loss: 2137.59527, Residuals: -1.01245, Convergence: 0.001239\n",
      "Epoch: 196, Loss: 2135.10385, Residuals: -1.01040, Convergence: 0.001167\n",
      "Epoch: 197, Loss: 2132.75664, Residuals: -1.00846, Convergence: 0.001101\n",
      "Epoch: 198, Loss: 2130.54124, Residuals: -1.00662, Convergence: 0.001040\n",
      "Epoch: 199, Loss: 2128.44845, Residuals: -1.00488, Convergence: 0.000983\n",
      "Evidence 14533.411\n",
      "\n",
      "Epoch: 199, Evidence: 14533.41113, Convergence: 0.218685\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 4.38e-01\n",
      "Epoch: 199, Loss: 2507.81628, Residuals: -1.00488, Convergence:   inf\n",
      "Epoch: 200, Loss: 2494.20393, Residuals: -1.00182, Convergence: 0.005458\n",
      "Epoch: 201, Loss: 2483.10866, Residuals: -0.99827, Convergence: 0.004468\n",
      "Epoch: 202, Loss: 2473.53474, Residuals: -0.99483, Convergence: 0.003871\n",
      "Epoch: 203, Loss: 2465.21882, Residuals: -0.99159, Convergence: 0.003373\n",
      "Epoch: 204, Loss: 2457.95997, Residuals: -0.98861, Convergence: 0.002953\n",
      "Epoch: 205, Loss: 2451.59543, Residuals: -0.98587, Convergence: 0.002596\n",
      "Epoch: 206, Loss: 2445.98736, Residuals: -0.98336, Convergence: 0.002293\n",
      "Epoch: 207, Loss: 2441.02025, Residuals: -0.98106, Convergence: 0.002035\n",
      "Epoch: 208, Loss: 2436.59712, Residuals: -0.97895, Convergence: 0.001815\n",
      "Epoch: 209, Loss: 2432.63670, Residuals: -0.97702, Convergence: 0.001628\n",
      "Epoch: 210, Loss: 2429.07113, Residuals: -0.97525, Convergence: 0.001468\n",
      "Epoch: 211, Loss: 2425.84274, Residuals: -0.97362, Convergence: 0.001331\n",
      "Epoch: 212, Loss: 2422.90474, Residuals: -0.97213, Convergence: 0.001213\n",
      "Epoch: 213, Loss: 2420.21902, Residuals: -0.97076, Convergence: 0.001110\n",
      "Epoch: 214, Loss: 2417.75145, Residuals: -0.96950, Convergence: 0.001021\n",
      "Epoch: 215, Loss: 2415.47565, Residuals: -0.96835, Convergence: 0.000942\n",
      "Evidence 14908.798\n",
      "\n",
      "Epoch: 215, Evidence: 14908.79785, Convergence: 0.025179\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 3.36e-01\n",
      "Epoch: 215, Loss: 2512.63791, Residuals: -0.96835, Convergence:   inf\n",
      "Epoch: 216, Loss: 2506.09725, Residuals: -0.96519, Convergence: 0.002610\n",
      "Epoch: 217, Loss: 2500.67399, Residuals: -0.96242, Convergence: 0.002169\n",
      "Epoch: 218, Loss: 2496.05671, Residuals: -0.96007, Convergence: 0.001850\n",
      "Epoch: 219, Loss: 2492.07190, Residuals: -0.95806, Convergence: 0.001599\n",
      "Epoch: 220, Loss: 2488.59168, Residuals: -0.95635, Convergence: 0.001398\n",
      "Epoch: 221, Loss: 2485.51861, Residuals: -0.95487, Convergence: 0.001236\n",
      "Epoch: 222, Loss: 2482.77564, Residuals: -0.95361, Convergence: 0.001105\n",
      "Epoch: 223, Loss: 2480.30328, Residuals: -0.95253, Convergence: 0.000997\n",
      "Evidence 14991.414\n",
      "\n",
      "Epoch: 223, Evidence: 14991.41406, Convergence: 0.005511\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 2.64e-01\n",
      "Epoch: 223, Loss: 2514.35874, Residuals: -0.95253, Convergence:   inf\n",
      "Epoch: 224, Loss: 2510.33556, Residuals: -0.95024, Convergence: 0.001603\n",
      "Epoch: 225, Loss: 2506.97403, Residuals: -0.94844, Convergence: 0.001341\n",
      "Epoch: 226, Loss: 2504.08485, Residuals: -0.94699, Convergence: 0.001154\n",
      "Epoch: 227, Loss: 2501.55759, Residuals: -0.94582, Convergence: 0.001010\n",
      "Epoch: 228, Loss: 2499.31406, Residuals: -0.94487, Convergence: 0.000898\n",
      "Evidence 15022.250\n",
      "\n",
      "Epoch: 228, Evidence: 15022.25000, Convergence: 0.002053\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 2.12e-01\n",
      "Epoch: 228, Loss: 2515.32988, Residuals: -0.94487, Convergence:   inf\n",
      "Epoch: 229, Loss: 2512.44600, Residuals: -0.94324, Convergence: 0.001148\n",
      "Epoch: 230, Loss: 2510.01150, Residuals: -0.94202, Convergence: 0.000970\n",
      "Evidence 15035.076\n",
      "\n",
      "Epoch: 230, Evidence: 15035.07617, Convergence: 0.000853\n",
      "Updating hyper-parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 185, Updated regularization: 1.76e-01\n",
      "Epoch: 230, Loss: 2516.07689, Residuals: -0.94202, Convergence:   inf\n",
      "Epoch: 231, Loss: 2511.48139, Residuals: -0.94006, Convergence: 0.001830\n",
      "Epoch: 232, Loss: 2507.96258, Residuals: -0.93878, Convergence: 0.001403\n",
      "Epoch: 233, Loss: 2505.10389, Residuals: -0.93791, Convergence: 0.001141\n",
      "Epoch: 234, Loss: 2502.67995, Residuals: -0.93745, Convergence: 0.000969\n",
      "Evidence 15053.141\n",
      "\n",
      "Epoch: 234, Evidence: 15053.14062, Convergence: 0.002052\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 1.46e-01\n",
      "Epoch: 234, Loss: 2516.33539, Residuals: -0.93745, Convergence:   inf\n",
      "Epoch: 235, Loss: 2513.26929, Residuals: -0.93574, Convergence: 0.001220\n",
      "Epoch: 236, Loss: 2510.82103, Residuals: -0.93495, Convergence: 0.000975\n",
      "Evidence 15064.208\n",
      "\n",
      "Epoch: 236, Evidence: 15064.20801, Convergence: 0.000735\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 1.23e-01\n",
      "Epoch: 236, Loss: 2516.57604, Residuals: -0.93495, Convergence:   inf\n",
      "Epoch: 237, Loss: 2512.04015, Residuals: -0.93296, Convergence: 0.001806\n",
      "Epoch: 238, Loss: 2508.74803, Residuals: -0.93400, Convergence: 0.001312\n",
      "Epoch: 239, Loss: 2506.06875, Residuals: -0.93472, Convergence: 0.001069\n",
      "Epoch: 240, Loss: 2503.72388, Residuals: -0.93728, Convergence: 0.000937\n",
      "Evidence 15080.269\n",
      "\n",
      "Epoch: 240, Evidence: 15080.26855, Convergence: 0.001799\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 1.10e-01\n",
      "Epoch: 240, Loss: 2515.97741, Residuals: -0.93728, Convergence:   inf\n",
      "Epoch: 241, Loss: 2514.24510, Residuals: -0.93651, Convergence: 0.000689\n",
      "Evidence 15086.763\n",
      "\n",
      "Epoch: 241, Evidence: 15086.76270, Convergence: 0.000430\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 8.98e-02\n",
      "Epoch: 241, Loss: 2516.92052, Residuals: -0.93651, Convergence:   inf\n",
      "Epoch: 242, Loss: 2566.58030, Residuals: -0.97974, Convergence: -0.019349\n",
      "Epoch: 242, Loss: 2514.35038, Residuals: -0.93517, Convergence: 0.001022\n",
      "Epoch: 243, Loss: 2514.34517, Residuals: -0.93676, Convergence: 0.000002\n",
      "Evidence 15091.627\n",
      "\n",
      "Epoch: 243, Evidence: 15091.62695, Convergence: 0.000753\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 7.40e-02\n",
      "Epoch: 243, Loss: 2516.84446, Residuals: -0.93676, Convergence:   inf\n",
      "Epoch: 244, Loss: 2583.43382, Residuals: -0.98570, Convergence: -0.025776\n",
      "Epoch: 244, Loss: 2514.19337, Residuals: -0.93497, Convergence: 0.001054\n",
      "Epoch: 245, Loss: 2514.00308, Residuals: -0.93556, Convergence: 0.000076\n",
      "Evidence 15096.843\n",
      "\n",
      "Epoch: 245, Evidence: 15096.84277, Convergence: 0.001098\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 6.35e-02\n",
      "Epoch: 245, Loss: 2516.72134, Residuals: -0.93556, Convergence:   inf\n",
      "Epoch: 246, Loss: 2514.35145, Residuals: -0.93474, Convergence: 0.000943\n",
      "Evidence 15100.943\n",
      "\n",
      "Epoch: 246, Evidence: 15100.94336, Convergence: 0.000272\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 5.98e-02\n",
      "Epoch: 246, Loss: 2516.55166, Residuals: -0.93474, Convergence:   inf\n",
      "Epoch: 247, Loss: 2521.15852, Residuals: -0.94600, Convergence: -0.001827\n",
      "Epoch: 247, Loss: 2516.49120, Residuals: -0.93684, Convergence: 0.000024\n",
      "Evidence 15102.126\n",
      "\n",
      "Epoch: 247, Evidence: 15102.12598, Convergence: 0.000350\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 5.04e-02\n",
      "Epoch: 247, Loss: 2516.97887, Residuals: -0.93684, Convergence:   inf\n",
      "Epoch: 248, Loss: 2574.42821, Residuals: -0.98853, Convergence: -0.022315\n",
      "Epoch: 248, Loss: 2514.71682, Residuals: -0.93435, Convergence: 0.000900\n",
      "Evidence 15105.943\n",
      "\n",
      "Epoch: 248, Evidence: 15105.94336, Convergence: 0.000602\n",
      "Total samples: 184, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 384.36510, Residuals: -4.53794, Convergence:   inf\n",
      "Epoch: 1, Loss: 358.56900, Residuals: -4.41704, Convergence: 0.071942\n",
      "Epoch: 2, Loss: 337.52840, Residuals: -4.25393, Convergence: 0.062337\n",
      "Epoch: 3, Loss: 321.40361, Residuals: -4.08997, Convergence: 0.050170\n",
      "Epoch: 4, Loss: 309.09984, Residuals: -3.94569, Convergence: 0.039805\n",
      "Epoch: 5, Loss: 299.33713, Residuals: -3.81835, Convergence: 0.032614\n",
      "Epoch: 6, Loss: 291.40746, Residuals: -3.70750, Convergence: 0.027212\n",
      "Epoch: 7, Loss: 284.82198, Residuals: -3.61258, Convergence: 0.023121\n",
      "Epoch: 8, Loss: 279.21810, Residuals: -3.53147, Convergence: 0.020070\n",
      "Epoch: 9, Loss: 274.33886, Residuals: -3.46181, Convergence: 0.017785\n",
      "Epoch: 10, Loss: 270.00077, Residuals: -3.40147, Convergence: 0.016067\n",
      "Epoch: 11, Loss: 266.07014, Residuals: -3.34867, Convergence: 0.014773\n",
      "Epoch: 12, Loss: 262.44872, Residuals: -3.30186, Convergence: 0.013799\n",
      "Epoch: 13, Loss: 259.06506, Residuals: -3.25973, Convergence: 0.013061\n",
      "Epoch: 14, Loss: 255.86956, Residuals: -3.22116, Convergence: 0.012489\n",
      "Epoch: 15, Loss: 252.83286, Residuals: -3.18528, Convergence: 0.012011\n",
      "Epoch: 16, Loss: 249.94206, Residuals: -3.15154, Convergence: 0.011566\n",
      "Epoch: 17, Loss: 247.18671, Residuals: -3.11960, Convergence: 0.011147\n",
      "Epoch: 18, Loss: 244.54411, Residuals: -3.08903, Convergence: 0.010806\n",
      "Epoch: 19, Loss: 241.97937, Residuals: -3.05927, Convergence: 0.010599\n",
      "Epoch: 20, Loss: 239.45474, Residuals: -3.02970, Convergence: 0.010543\n",
      "Epoch: 21, Loss: 236.93863, Residuals: -2.99980, Convergence: 0.010619\n",
      "Epoch: 22, Loss: 234.40762, Residuals: -2.96924, Convergence: 0.010797\n",
      "Epoch: 23, Loss: 231.83385, Residuals: -2.93769, Convergence: 0.011102\n",
      "Epoch: 24, Loss: 229.16873, Residuals: -2.90460, Convergence: 0.011629\n",
      "Epoch: 25, Loss: 226.35618, Residuals: -2.86927, Convergence: 0.012425\n",
      "Epoch: 26, Loss: 223.41480, Residuals: -2.83179, Convergence: 0.013166\n",
      "Epoch: 27, Loss: 220.48128, Residuals: -2.79357, Convergence: 0.013305\n",
      "Epoch: 28, Loss: 217.65055, Residuals: -2.75578, Convergence: 0.013006\n",
      "Epoch: 29, Loss: 214.92952, Residuals: -2.71866, Convergence: 0.012660\n",
      "Epoch: 30, Loss: 212.30042, Residuals: -2.68213, Convergence: 0.012384\n",
      "Epoch: 31, Loss: 209.74640, Residuals: -2.64607, Convergence: 0.012177\n",
      "Epoch: 32, Loss: 207.25550, Residuals: -2.61039, Convergence: 0.012019\n",
      "Epoch: 33, Loss: 204.82009, Residuals: -2.57503, Convergence: 0.011890\n",
      "Epoch: 34, Loss: 202.43587, Residuals: -2.53993, Convergence: 0.011778\n",
      "Epoch: 35, Loss: 200.10082, Residuals: -2.50506, Convergence: 0.011669\n",
      "Epoch: 36, Loss: 197.81458, Residuals: -2.47038, Convergence: 0.011558\n",
      "Epoch: 37, Loss: 195.57774, Residuals: -2.43589, Convergence: 0.011437\n",
      "Epoch: 38, Loss: 193.39152, Residuals: -2.40157, Convergence: 0.011305\n",
      "Epoch: 39, Loss: 191.25740, Residuals: -2.36744, Convergence: 0.011158\n",
      "Epoch: 40, Loss: 189.17693, Residuals: -2.33349, Convergence: 0.010998\n",
      "Epoch: 41, Loss: 187.15160, Residuals: -2.29975, Convergence: 0.010822\n",
      "Epoch: 42, Loss: 185.18288, Residuals: -2.26625, Convergence: 0.010631\n",
      "Epoch: 43, Loss: 183.27219, Residuals: -2.23301, Convergence: 0.010425\n",
      "Epoch: 44, Loss: 181.42097, Residuals: -2.20008, Convergence: 0.010204\n",
      "Epoch: 45, Loss: 179.63065, Residuals: -2.16751, Convergence: 0.009967\n",
      "Epoch: 46, Loss: 177.90248, Residuals: -2.13534, Convergence: 0.009714\n",
      "Epoch: 47, Loss: 176.23734, Residuals: -2.10363, Convergence: 0.009448\n",
      "Epoch: 48, Loss: 174.63556, Residuals: -2.07244, Convergence: 0.009172\n",
      "Epoch: 49, Loss: 173.09683, Residuals: -2.04180, Convergence: 0.008889\n",
      "Epoch: 50, Loss: 171.62012, Residuals: -2.01175, Convergence: 0.008605\n",
      "Epoch: 51, Loss: 170.20387, Residuals: -1.98230, Convergence: 0.008321\n",
      "Epoch: 52, Loss: 168.84616, Residuals: -1.95348, Convergence: 0.008041\n",
      "Epoch: 53, Loss: 167.54490, Residuals: -1.92528, Convergence: 0.007767\n",
      "Epoch: 54, Loss: 166.29800, Residuals: -1.89771, Convergence: 0.007498\n",
      "Epoch: 55, Loss: 165.10348, Residuals: -1.87076, Convergence: 0.007235\n",
      "Epoch: 56, Loss: 163.95954, Residuals: -1.84445, Convergence: 0.006977\n",
      "Epoch: 57, Loss: 162.86442, Residuals: -1.81877, Convergence: 0.006724\n",
      "Epoch: 58, Loss: 161.81648, Residuals: -1.79373, Convergence: 0.006476\n",
      "Epoch: 59, Loss: 160.81413, Residuals: -1.76933, Convergence: 0.006233\n",
      "Epoch: 60, Loss: 159.85574, Residuals: -1.74560, Convergence: 0.005995\n",
      "Epoch: 61, Loss: 158.93972, Residuals: -1.72252, Convergence: 0.005763\n",
      "Epoch: 62, Loss: 158.06447, Residuals: -1.70010, Convergence: 0.005537\n",
      "Epoch: 63, Loss: 157.22836, Residuals: -1.67836, Convergence: 0.005318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 64, Loss: 156.42980, Residuals: -1.65727, Convergence: 0.005105\n",
      "Epoch: 65, Loss: 155.66721, Residuals: -1.63685, Convergence: 0.004899\n",
      "Epoch: 66, Loss: 154.93905, Residuals: -1.61708, Convergence: 0.004700\n",
      "Epoch: 67, Loss: 154.24380, Residuals: -1.59796, Convergence: 0.004507\n",
      "Epoch: 68, Loss: 153.58003, Residuals: -1.57949, Convergence: 0.004322\n",
      "Epoch: 69, Loss: 152.94631, Residuals: -1.56164, Convergence: 0.004143\n",
      "Epoch: 70, Loss: 152.34133, Residuals: -1.54441, Convergence: 0.003971\n",
      "Epoch: 71, Loss: 151.76376, Residuals: -1.52778, Convergence: 0.003806\n",
      "Epoch: 72, Loss: 151.21241, Residuals: -1.51175, Convergence: 0.003646\n",
      "Epoch: 73, Loss: 150.68609, Residuals: -1.49630, Convergence: 0.003493\n",
      "Epoch: 74, Loss: 150.18369, Residuals: -1.48140, Convergence: 0.003345\n",
      "Epoch: 75, Loss: 149.70414, Residuals: -1.46706, Convergence: 0.003203\n",
      "Epoch: 76, Loss: 149.24645, Residuals: -1.45325, Convergence: 0.003067\n",
      "Epoch: 77, Loss: 148.80964, Residuals: -1.43996, Convergence: 0.002935\n",
      "Epoch: 78, Loss: 148.39280, Residuals: -1.42717, Convergence: 0.002809\n",
      "Epoch: 79, Loss: 147.99506, Residuals: -1.41486, Convergence: 0.002688\n",
      "Epoch: 80, Loss: 147.61558, Residuals: -1.40303, Convergence: 0.002571\n",
      "Epoch: 81, Loss: 147.25356, Residuals: -1.39165, Convergence: 0.002458\n",
      "Epoch: 82, Loss: 146.90824, Residuals: -1.38072, Convergence: 0.002351\n",
      "Epoch: 83, Loss: 146.57888, Residuals: -1.37021, Convergence: 0.002247\n",
      "Epoch: 84, Loss: 146.26475, Residuals: -1.36011, Convergence: 0.002148\n",
      "Epoch: 85, Loss: 145.96519, Residuals: -1.35041, Convergence: 0.002052\n",
      "Epoch: 86, Loss: 145.67953, Residuals: -1.34109, Convergence: 0.001961\n",
      "Epoch: 87, Loss: 145.40711, Residuals: -1.33214, Convergence: 0.001873\n",
      "Epoch: 88, Loss: 145.14734, Residuals: -1.32355, Convergence: 0.001790\n",
      "Epoch: 89, Loss: 144.89960, Residuals: -1.31530, Convergence: 0.001710\n",
      "Epoch: 90, Loss: 144.66334, Residuals: -1.30737, Convergence: 0.001633\n",
      "Epoch: 91, Loss: 144.43799, Residuals: -1.29977, Convergence: 0.001560\n",
      "Epoch: 92, Loss: 144.22304, Residuals: -1.29247, Convergence: 0.001490\n",
      "Epoch: 93, Loss: 144.01797, Residuals: -1.28546, Convergence: 0.001424\n",
      "Epoch: 94, Loss: 143.82233, Residuals: -1.27873, Convergence: 0.001360\n",
      "Epoch: 95, Loss: 143.63566, Residuals: -1.27227, Convergence: 0.001300\n",
      "Epoch: 96, Loss: 143.45754, Residuals: -1.26607, Convergence: 0.001242\n",
      "Epoch: 97, Loss: 143.28759, Residuals: -1.26011, Convergence: 0.001186\n",
      "Epoch: 98, Loss: 143.12544, Residuals: -1.25439, Convergence: 0.001133\n",
      "Epoch: 99, Loss: 142.97076, Residuals: -1.24889, Convergence: 0.001082\n",
      "Epoch: 100, Loss: 142.82324, Residuals: -1.24361, Convergence: 0.001033\n",
      "Epoch: 101, Loss: 142.68260, Residuals: -1.23854, Convergence: 0.000986\n",
      "Evidence -184.594\n",
      "\n",
      "Epoch: 101, Evidence: -184.59433, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 7.25e-01\n",
      "Epoch: 101, Loss: 1394.11414, Residuals: -1.23854, Convergence:   inf\n",
      "Epoch: 102, Loss: 1330.98487, Residuals: -1.26802, Convergence: 0.047430\n",
      "Epoch: 103, Loss: 1282.67062, Residuals: -1.29089, Convergence: 0.037667\n",
      "Epoch: 104, Loss: 1246.02513, Residuals: -1.30701, Convergence: 0.029410\n",
      "Epoch: 105, Loss: 1217.50484, Residuals: -1.31805, Convergence: 0.023425\n",
      "Epoch: 106, Loss: 1194.46582, Residuals: -1.32606, Convergence: 0.019288\n",
      "Epoch: 107, Loss: 1175.35784, Residuals: -1.33210, Convergence: 0.016257\n",
      "Epoch: 108, Loss: 1159.24452, Residuals: -1.33662, Convergence: 0.013900\n",
      "Epoch: 109, Loss: 1145.48945, Residuals: -1.33983, Convergence: 0.012008\n",
      "Epoch: 110, Loss: 1133.62115, Residuals: -1.34188, Convergence: 0.010469\n",
      "Epoch: 111, Loss: 1123.27122, Residuals: -1.34288, Convergence: 0.009214\n",
      "Epoch: 112, Loss: 1114.14268, Residuals: -1.34292, Convergence: 0.008193\n",
      "Epoch: 113, Loss: 1105.99154, Residuals: -1.34208, Convergence: 0.007370\n",
      "Epoch: 114, Loss: 1098.61232, Residuals: -1.34043, Convergence: 0.006717\n",
      "Epoch: 115, Loss: 1091.82920, Residuals: -1.33801, Convergence: 0.006213\n",
      "Epoch: 116, Loss: 1085.49005, Residuals: -1.33486, Convergence: 0.005840\n",
      "Epoch: 117, Loss: 1079.46069, Residuals: -1.33101, Convergence: 0.005586\n",
      "Epoch: 118, Loss: 1073.62384, Residuals: -1.32647, Convergence: 0.005437\n",
      "Epoch: 119, Loss: 1067.88004, Residuals: -1.32127, Convergence: 0.005379\n",
      "Epoch: 120, Loss: 1062.15339, Residuals: -1.31544, Convergence: 0.005392\n",
      "Epoch: 121, Loss: 1056.40329, Residuals: -1.30903, Convergence: 0.005443\n",
      "Epoch: 122, Loss: 1050.63847, Residuals: -1.30214, Convergence: 0.005487\n",
      "Epoch: 123, Loss: 1044.92020, Residuals: -1.29485, Convergence: 0.005472\n",
      "Epoch: 124, Loss: 1039.34415, Residuals: -1.28726, Convergence: 0.005365\n",
      "Epoch: 125, Loss: 1034.00387, Residuals: -1.27946, Convergence: 0.005165\n",
      "Epoch: 126, Loss: 1028.96093, Residuals: -1.27152, Convergence: 0.004901\n",
      "Epoch: 127, Loss: 1024.23771, Residuals: -1.26351, Convergence: 0.004611\n",
      "Epoch: 128, Loss: 1019.82609, Residuals: -1.25548, Convergence: 0.004326\n",
      "Epoch: 129, Loss: 1015.70123, Residuals: -1.24750, Convergence: 0.004061\n",
      "Epoch: 130, Loss: 1011.83336, Residuals: -1.23959, Convergence: 0.003823\n",
      "Epoch: 131, Loss: 1008.19281, Residuals: -1.23179, Convergence: 0.003611\n",
      "Epoch: 132, Loss: 1004.75320, Residuals: -1.22414, Convergence: 0.003423\n",
      "Epoch: 133, Loss: 1001.49270, Residuals: -1.21664, Convergence: 0.003256\n",
      "Epoch: 134, Loss: 998.39324, Residuals: -1.20934, Convergence: 0.003104\n",
      "Epoch: 135, Loss: 995.44099, Residuals: -1.20223, Convergence: 0.002966\n",
      "Epoch: 136, Loss: 992.62463, Residuals: -1.19534, Convergence: 0.002837\n",
      "Epoch: 137, Loss: 989.93531, Residuals: -1.18868, Convergence: 0.002717\n",
      "Epoch: 138, Loss: 987.36624, Residuals: -1.18225, Convergence: 0.002602\n",
      "Epoch: 139, Loss: 984.91128, Residuals: -1.17606, Convergence: 0.002493\n",
      "Epoch: 140, Loss: 982.56488, Residuals: -1.17012, Convergence: 0.002388\n",
      "Epoch: 141, Loss: 980.32197, Residuals: -1.16442, Convergence: 0.002288\n",
      "Epoch: 142, Loss: 978.17811, Residuals: -1.15897, Convergence: 0.002192\n",
      "Epoch: 143, Loss: 976.12800, Residuals: -1.15375, Convergence: 0.002100\n",
      "Epoch: 144, Loss: 974.16683, Residuals: -1.14876, Convergence: 0.002013\n",
      "Epoch: 145, Loss: 972.28988, Residuals: -1.14400, Convergence: 0.001930\n",
      "Epoch: 146, Loss: 970.49219, Residuals: -1.13945, Convergence: 0.001852\n",
      "Epoch: 147, Loss: 968.76887, Residuals: -1.13510, Convergence: 0.001779\n",
      "Epoch: 148, Loss: 967.11531, Residuals: -1.13095, Convergence: 0.001710\n",
      "Epoch: 149, Loss: 965.52618, Residuals: -1.12697, Convergence: 0.001646\n",
      "Epoch: 150, Loss: 963.99645, Residuals: -1.12317, Convergence: 0.001587\n",
      "Epoch: 151, Loss: 962.52126, Residuals: -1.11952, Convergence: 0.001533\n",
      "Epoch: 152, Loss: 961.09507, Residuals: -1.11601, Convergence: 0.001484\n",
      "Epoch: 153, Loss: 959.71242, Residuals: -1.11264, Convergence: 0.001441\n",
      "Epoch: 154, Loss: 958.36813, Residuals: -1.10939, Convergence: 0.001403\n",
      "Epoch: 155, Loss: 957.05636, Residuals: -1.10624, Convergence: 0.001371\n",
      "Epoch: 156, Loss: 955.77153, Residuals: -1.10318, Convergence: 0.001344\n",
      "Epoch: 157, Loss: 954.50902, Residuals: -1.10021, Convergence: 0.001323\n",
      "Epoch: 158, Loss: 953.26429, Residuals: -1.09730, Convergence: 0.001306\n",
      "Epoch: 159, Loss: 952.03391, Residuals: -1.09444, Convergence: 0.001292\n",
      "Epoch: 160, Loss: 950.81671, Residuals: -1.09164, Convergence: 0.001280\n",
      "Epoch: 161, Loss: 949.61243, Residuals: -1.08888, Convergence: 0.001268\n",
      "Epoch: 162, Loss: 948.42265, Residuals: -1.08617, Convergence: 0.001254\n",
      "Epoch: 163, Loss: 947.25110, Residuals: -1.08352, Convergence: 0.001237\n",
      "Epoch: 164, Loss: 946.10132, Residuals: -1.08092, Convergence: 0.001215\n",
      "Epoch: 165, Loss: 944.97788, Residuals: -1.07838, Convergence: 0.001189\n",
      "Epoch: 166, Loss: 943.88463, Residuals: -1.07592, Convergence: 0.001158\n",
      "Epoch: 167, Loss: 942.82534, Residuals: -1.07353, Convergence: 0.001124\n",
      "Epoch: 168, Loss: 941.80182, Residuals: -1.07123, Convergence: 0.001087\n",
      "Epoch: 169, Loss: 940.81606, Residuals: -1.06900, Convergence: 0.001048\n",
      "Epoch: 170, Loss: 939.86868, Residuals: -1.06686, Convergence: 0.001008\n",
      "Epoch: 171, Loss: 938.96021, Residuals: -1.06481, Convergence: 0.000968\n",
      "Evidence 11275.021\n",
      "\n",
      "Epoch: 171, Evidence: 11275.02148, Convergence: 1.016372\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 5.74e-01\n",
      "Epoch: 171, Loss: 2373.91761, Residuals: -1.06481, Convergence:   inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 172, Loss: 2334.34327, Residuals: -1.07309, Convergence: 0.016953\n",
      "Epoch: 173, Loss: 2307.45485, Residuals: -1.07163, Convergence: 0.011653\n",
      "Epoch: 174, Loss: 2285.18281, Residuals: -1.06932, Convergence: 0.009746\n",
      "Epoch: 175, Loss: 2266.41969, Residuals: -1.06682, Convergence: 0.008279\n",
      "Epoch: 176, Loss: 2250.46747, Residuals: -1.06423, Convergence: 0.007088\n",
      "Epoch: 177, Loss: 2236.78356, Residuals: -1.06156, Convergence: 0.006118\n",
      "Epoch: 178, Loss: 2224.92383, Residuals: -1.05883, Convergence: 0.005330\n",
      "Epoch: 179, Loss: 2214.51709, Residuals: -1.05603, Convergence: 0.004699\n",
      "Epoch: 180, Loss: 2205.25637, Residuals: -1.05314, Convergence: 0.004199\n",
      "Epoch: 181, Loss: 2196.89897, Residuals: -1.05015, Convergence: 0.003804\n",
      "Epoch: 182, Loss: 2189.26468, Residuals: -1.04704, Convergence: 0.003487\n",
      "Epoch: 183, Loss: 2182.23414, Residuals: -1.04382, Convergence: 0.003222\n",
      "Epoch: 184, Loss: 2175.73377, Residuals: -1.04053, Convergence: 0.002988\n",
      "Epoch: 185, Loss: 2169.71792, Residuals: -1.03722, Convergence: 0.002773\n",
      "Epoch: 186, Loss: 2164.15143, Residuals: -1.03392, Convergence: 0.002572\n",
      "Epoch: 187, Loss: 2159.00130, Residuals: -1.03067, Convergence: 0.002385\n",
      "Epoch: 188, Loss: 2154.23554, Residuals: -1.02752, Convergence: 0.002212\n",
      "Epoch: 189, Loss: 2149.82337, Residuals: -1.02448, Convergence: 0.002052\n",
      "Epoch: 190, Loss: 2145.73554, Residuals: -1.02157, Convergence: 0.001905\n",
      "Epoch: 191, Loss: 2141.94640, Residuals: -1.01879, Convergence: 0.001769\n",
      "Epoch: 192, Loss: 2138.43283, Residuals: -1.01615, Convergence: 0.001643\n",
      "Epoch: 193, Loss: 2135.17261, Residuals: -1.01364, Convergence: 0.001527\n",
      "Epoch: 194, Loss: 2132.14642, Residuals: -1.01127, Convergence: 0.001419\n",
      "Epoch: 195, Loss: 2129.33568, Residuals: -1.00903, Convergence: 0.001320\n",
      "Epoch: 196, Loss: 2126.72343, Residuals: -1.00690, Convergence: 0.001228\n",
      "Epoch: 197, Loss: 2124.29291, Residuals: -1.00489, Convergence: 0.001144\n",
      "Epoch: 198, Loss: 2122.02793, Residuals: -1.00298, Convergence: 0.001067\n",
      "Epoch: 199, Loss: 2119.91556, Residuals: -1.00116, Convergence: 0.000996\n",
      "Evidence 14427.682\n",
      "\n",
      "Epoch: 199, Evidence: 14427.68164, Convergence: 0.218515\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 4.35e-01\n",
      "Epoch: 199, Loss: 2497.16334, Residuals: -1.00116, Convergence:   inf\n",
      "Epoch: 200, Loss: 2483.65253, Residuals: -0.99832, Convergence: 0.005440\n",
      "Epoch: 201, Loss: 2472.59779, Residuals: -0.99493, Convergence: 0.004471\n",
      "Epoch: 202, Loss: 2463.07267, Residuals: -0.99165, Convergence: 0.003867\n",
      "Epoch: 203, Loss: 2454.82193, Residuals: -0.98859, Convergence: 0.003361\n",
      "Epoch: 204, Loss: 2447.64426, Residuals: -0.98575, Convergence: 0.002932\n",
      "Epoch: 205, Loss: 2441.37185, Residuals: -0.98315, Convergence: 0.002569\n",
      "Epoch: 206, Loss: 2435.86326, Residuals: -0.98077, Convergence: 0.002261\n",
      "Epoch: 207, Loss: 2431.00001, Residuals: -0.97859, Convergence: 0.002001\n",
      "Epoch: 208, Loss: 2426.68175, Residuals: -0.97660, Convergence: 0.001779\n",
      "Epoch: 209, Loss: 2422.82508, Residuals: -0.97477, Convergence: 0.001592\n",
      "Epoch: 210, Loss: 2419.36078, Residuals: -0.97309, Convergence: 0.001432\n",
      "Epoch: 211, Loss: 2416.23130, Residuals: -0.97153, Convergence: 0.001295\n",
      "Epoch: 212, Loss: 2413.38788, Residuals: -0.97009, Convergence: 0.001178\n",
      "Epoch: 213, Loss: 2410.79136, Residuals: -0.96876, Convergence: 0.001077\n",
      "Epoch: 214, Loss: 2408.40780, Residuals: -0.96751, Convergence: 0.000990\n",
      "Evidence 14795.014\n",
      "\n",
      "Epoch: 214, Evidence: 14795.01367, Convergence: 0.024828\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 3.32e-01\n",
      "Epoch: 214, Loss: 2502.51718, Residuals: -0.96751, Convergence:   inf\n",
      "Epoch: 215, Loss: 2495.89874, Residuals: -0.96435, Convergence: 0.002652\n",
      "Epoch: 216, Loss: 2490.43082, Residuals: -0.96158, Convergence: 0.002196\n",
      "Epoch: 217, Loss: 2485.80314, Residuals: -0.95919, Convergence: 0.001862\n",
      "Epoch: 218, Loss: 2481.83068, Residuals: -0.95715, Convergence: 0.001601\n",
      "Epoch: 219, Loss: 2478.37496, Residuals: -0.95539, Convergence: 0.001394\n",
      "Epoch: 220, Loss: 2475.33094, Residuals: -0.95386, Convergence: 0.001230\n",
      "Epoch: 221, Loss: 2472.61977, Residuals: -0.95253, Convergence: 0.001096\n",
      "Epoch: 222, Loss: 2470.17839, Residuals: -0.95137, Convergence: 0.000988\n",
      "Evidence 14875.646\n",
      "\n",
      "Epoch: 222, Evidence: 14875.64551, Convergence: 0.005420\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 2.60e-01\n",
      "Epoch: 222, Loss: 2504.15700, Residuals: -0.95137, Convergence:   inf\n",
      "Epoch: 223, Loss: 2500.13279, Residuals: -0.94894, Convergence: 0.001610\n",
      "Epoch: 224, Loss: 2496.79661, Residuals: -0.94698, Convergence: 0.001336\n",
      "Epoch: 225, Loss: 2493.94731, Residuals: -0.94537, Convergence: 0.001142\n",
      "Epoch: 226, Loss: 2491.46249, Residuals: -0.94404, Convergence: 0.000997\n",
      "Evidence 14904.146\n",
      "\n",
      "Epoch: 226, Evidence: 14904.14648, Convergence: 0.001912\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 2.09e-01\n",
      "Epoch: 226, Loss: 2505.16117, Residuals: -0.94404, Convergence:   inf\n",
      "Epoch: 227, Loss: 2502.16432, Residuals: -0.94211, Convergence: 0.001198\n",
      "Epoch: 228, Loss: 2499.65631, Residuals: -0.94057, Convergence: 0.001003\n",
      "Epoch: 229, Loss: 2497.48469, Residuals: -0.93934, Convergence: 0.000870\n",
      "Evidence 14918.941\n",
      "\n",
      "Epoch: 229, Evidence: 14918.94141, Convergence: 0.000992\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.72e-01\n",
      "Epoch: 229, Loss: 2505.82125, Residuals: -0.93934, Convergence:   inf\n",
      "Epoch: 230, Loss: 2501.35172, Residuals: -0.93680, Convergence: 0.001787\n",
      "Epoch: 231, Loss: 2497.92977, Residuals: -0.93512, Convergence: 0.001370\n",
      "Epoch: 232, Loss: 2495.14651, Residuals: -0.93403, Convergence: 0.001115\n",
      "Epoch: 233, Loss: 2492.78252, Residuals: -0.93343, Convergence: 0.000948\n",
      "Evidence 14937.108\n",
      "\n",
      "Epoch: 233, Evidence: 14937.10840, Convergence: 0.002207\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.42e-01\n",
      "Epoch: 233, Loss: 2505.92079, Residuals: -0.93343, Convergence:   inf\n",
      "Epoch: 234, Loss: 2502.90434, Residuals: -0.93120, Convergence: 0.001205\n",
      "Epoch: 235, Loss: 2500.52063, Residuals: -0.93013, Convergence: 0.000953\n",
      "Evidence 14948.048\n",
      "\n",
      "Epoch: 235, Evidence: 14948.04785, Convergence: 0.000732\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.20e-01\n",
      "Epoch: 235, Loss: 2506.11562, Residuals: -0.93013, Convergence:   inf\n",
      "Epoch: 236, Loss: 2501.68643, Residuals: -0.92732, Convergence: 0.001770\n",
      "Epoch: 237, Loss: 2498.53956, Residuals: -0.92849, Convergence: 0.001259\n",
      "Epoch: 238, Loss: 2495.90353, Residuals: -0.92906, Convergence: 0.001056\n",
      "Epoch: 239, Loss: 2493.62656, Residuals: -0.93173, Convergence: 0.000913\n",
      "Evidence 14963.816\n",
      "\n",
      "Epoch: 239, Evidence: 14963.81641, Convergence: 0.001785\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.07e-01\n",
      "Epoch: 239, Loss: 2505.41016, Residuals: -0.93173, Convergence:   inf\n",
      "Epoch: 240, Loss: 2503.43626, Residuals: -0.92915, Convergence: 0.000788\n",
      "Evidence 14970.823\n",
      "\n",
      "Epoch: 240, Evidence: 14970.82324, Convergence: 0.000468\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 8.75e-02\n",
      "Epoch: 240, Loss: 2506.31048, Residuals: -0.92915, Convergence:   inf\n",
      "Epoch: 241, Loss: 2544.53267, Residuals: -0.96846, Convergence: -0.015021\n",
      "Epoch: 241, Loss: 2504.11891, Residuals: -0.92832, Convergence: 0.000875\n",
      "Evidence 14975.041\n",
      "\n",
      "Epoch: 241, Evidence: 14975.04102, Convergence: 0.000750\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 8.08e-02\n",
      "Epoch: 241, Loss: 2505.70865, Residuals: -0.92832, Convergence:   inf\n",
      "Epoch: 242, Loss: 2510.31762, Residuals: -0.92879, Convergence: -0.001836\n",
      "Epoch: 242, Loss: 2505.55248, Residuals: -0.92643, Convergence: 0.000062\n",
      "Evidence 14976.898\n",
      "\n",
      "Epoch: 242, Evidence: 14976.89844, Convergence: 0.000873\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 385.61382, Residuals: -4.58227, Convergence:   inf\n",
      "Epoch: 1, Loss: 359.73717, Residuals: -4.46035, Convergence: 0.071932\n",
      "Epoch: 2, Loss: 338.49225, Residuals: -4.29388, Convergence: 0.062763\n",
      "Epoch: 3, Loss: 322.27731, Residuals: -4.12734, Convergence: 0.050314\n",
      "Epoch: 4, Loss: 309.90306, Residuals: -3.98063, Convergence: 0.039929\n",
      "Epoch: 5, Loss: 300.08123, Residuals: -3.85094, Convergence: 0.032731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Loss: 292.10200, Residuals: -3.73807, Convergence: 0.027317\n",
      "Epoch: 7, Loss: 285.47286, Residuals: -3.64149, Convergence: 0.023222\n",
      "Epoch: 8, Loss: 279.82943, Residuals: -3.55908, Convergence: 0.020167\n",
      "Epoch: 9, Loss: 274.91463, Residuals: -3.48842, Convergence: 0.017878\n",
      "Epoch: 10, Loss: 270.54557, Residuals: -3.42735, Convergence: 0.016149\n",
      "Epoch: 11, Loss: 266.58961, Residuals: -3.37405, Convergence: 0.014839\n",
      "Epoch: 12, Loss: 262.94977, Residuals: -3.32694, Convergence: 0.013842\n",
      "Epoch: 13, Loss: 259.55544, Residuals: -3.28469, Convergence: 0.013077\n",
      "Epoch: 14, Loss: 256.35630, Residuals: -3.24612, Convergence: 0.012479\n",
      "Epoch: 15, Loss: 253.31911, Residuals: -3.21029, Convergence: 0.011990\n",
      "Epoch: 16, Loss: 250.42556, Residuals: -3.17655, Convergence: 0.011555\n",
      "Epoch: 17, Loss: 247.66331, Residuals: -3.14450, Convergence: 0.011153\n",
      "Epoch: 18, Loss: 245.01250, Residuals: -3.11377, Convergence: 0.010819\n",
      "Epoch: 19, Loss: 242.44176, Residuals: -3.08385, Convergence: 0.010604\n",
      "Epoch: 20, Loss: 239.91493, Residuals: -3.05415, Convergence: 0.010532\n",
      "Epoch: 21, Loss: 237.39996, Residuals: -3.02415, Convergence: 0.010594\n",
      "Epoch: 22, Loss: 234.87384, Residuals: -2.99347, Convergence: 0.010755\n",
      "Epoch: 23, Loss: 232.31535, Residuals: -2.96184, Convergence: 0.011013\n",
      "Epoch: 24, Loss: 229.68727, Residuals: -2.92886, Convergence: 0.011442\n",
      "Epoch: 25, Loss: 226.93354, Residuals: -2.89387, Convergence: 0.012135\n",
      "Epoch: 26, Loss: 224.03161, Residuals: -2.85653, Convergence: 0.012953\n",
      "Epoch: 27, Loss: 221.08102, Residuals: -2.81787, Convergence: 0.013346\n",
      "Epoch: 28, Loss: 218.21156, Residuals: -2.77936, Convergence: 0.013150\n",
      "Epoch: 29, Loss: 215.45837, Residuals: -2.74158, Convergence: 0.012778\n",
      "Epoch: 30, Loss: 212.80855, Residuals: -2.70453, Convergence: 0.012452\n",
      "Epoch: 31, Loss: 210.24340, Residuals: -2.66811, Convergence: 0.012201\n",
      "Epoch: 32, Loss: 207.74798, Residuals: -2.63220, Convergence: 0.012012\n",
      "Epoch: 33, Loss: 205.31202, Residuals: -2.59670, Convergence: 0.011865\n",
      "Epoch: 34, Loss: 202.92905, Residuals: -2.56152, Convergence: 0.011743\n",
      "Epoch: 35, Loss: 200.59560, Residuals: -2.52661, Convergence: 0.011633\n",
      "Epoch: 36, Loss: 198.31028, Residuals: -2.49192, Convergence: 0.011524\n",
      "Epoch: 37, Loss: 196.07322, Residuals: -2.45741, Convergence: 0.011409\n",
      "Epoch: 38, Loss: 193.88543, Residuals: -2.42307, Convergence: 0.011284\n",
      "Epoch: 39, Loss: 191.74840, Residuals: -2.38889, Convergence: 0.011145\n",
      "Epoch: 40, Loss: 189.66381, Residuals: -2.35489, Convergence: 0.010991\n",
      "Epoch: 41, Loss: 187.63325, Residuals: -2.32109, Convergence: 0.010822\n",
      "Epoch: 42, Loss: 185.65816, Residuals: -2.28751, Convergence: 0.010638\n",
      "Epoch: 43, Loss: 183.73975, Residuals: -2.25417, Convergence: 0.010441\n",
      "Epoch: 44, Loss: 181.87901, Residuals: -2.22113, Convergence: 0.010231\n",
      "Epoch: 45, Loss: 180.07676, Residuals: -2.18842, Convergence: 0.010008\n",
      "Epoch: 46, Loss: 178.33368, Residuals: -2.15607, Convergence: 0.009774\n",
      "Epoch: 47, Loss: 176.65031, Residuals: -2.12415, Convergence: 0.009529\n",
      "Epoch: 48, Loss: 175.02690, Residuals: -2.09270, Convergence: 0.009275\n",
      "Epoch: 49, Loss: 173.46330, Residuals: -2.06175, Convergence: 0.009014\n",
      "Epoch: 50, Loss: 171.95882, Residuals: -2.03133, Convergence: 0.008749\n",
      "Epoch: 51, Loss: 170.51215, Residuals: -2.00147, Convergence: 0.008484\n",
      "Epoch: 52, Loss: 169.12155, Residuals: -1.97218, Convergence: 0.008223\n",
      "Epoch: 53, Loss: 167.78496, Residuals: -1.94344, Convergence: 0.007966\n",
      "Epoch: 54, Loss: 166.50028, Residuals: -1.91527, Convergence: 0.007716\n",
      "Epoch: 55, Loss: 165.26551, Residuals: -1.88765, Convergence: 0.007471\n",
      "Epoch: 56, Loss: 164.07883, Residuals: -1.86060, Convergence: 0.007232\n",
      "Epoch: 57, Loss: 162.93864, Residuals: -1.83410, Convergence: 0.006998\n",
      "Epoch: 58, Loss: 161.84353, Residuals: -1.80818, Convergence: 0.006767\n",
      "Epoch: 59, Loss: 160.79222, Residuals: -1.78284, Convergence: 0.006538\n",
      "Epoch: 60, Loss: 159.78358, Residuals: -1.75809, Convergence: 0.006313\n",
      "Epoch: 61, Loss: 158.81649, Residuals: -1.73396, Convergence: 0.006089\n",
      "Epoch: 62, Loss: 157.88992, Residuals: -1.71044, Convergence: 0.005868\n",
      "Epoch: 63, Loss: 157.00284, Residuals: -1.68756, Convergence: 0.005650\n",
      "Epoch: 64, Loss: 156.15423, Residuals: -1.66532, Convergence: 0.005434\n",
      "Epoch: 65, Loss: 155.34308, Residuals: -1.64373, Convergence: 0.005222\n",
      "Epoch: 66, Loss: 154.56837, Residuals: -1.62280, Convergence: 0.005012\n",
      "Epoch: 67, Loss: 153.82905, Residuals: -1.60252, Convergence: 0.004806\n",
      "Epoch: 68, Loss: 153.12410, Residuals: -1.58290, Convergence: 0.004604\n",
      "Epoch: 69, Loss: 152.45243, Residuals: -1.56395, Convergence: 0.004406\n",
      "Epoch: 70, Loss: 151.81297, Residuals: -1.54565, Convergence: 0.004212\n",
      "Epoch: 71, Loss: 151.20460, Residuals: -1.52800, Convergence: 0.004023\n",
      "Epoch: 72, Loss: 150.62620, Residuals: -1.51101, Convergence: 0.003840\n",
      "Epoch: 73, Loss: 150.07658, Residuals: -1.49464, Convergence: 0.003662\n",
      "Epoch: 74, Loss: 149.55461, Residuals: -1.47891, Convergence: 0.003490\n",
      "Epoch: 75, Loss: 149.05908, Residuals: -1.46379, Convergence: 0.003324\n",
      "Epoch: 76, Loss: 148.58882, Residuals: -1.44928, Convergence: 0.003165\n",
      "Epoch: 77, Loss: 148.14266, Residuals: -1.43535, Convergence: 0.003012\n",
      "Epoch: 78, Loss: 147.71943, Residuals: -1.42200, Convergence: 0.002865\n",
      "Epoch: 79, Loss: 147.31801, Residuals: -1.40920, Convergence: 0.002725\n",
      "Epoch: 80, Loss: 146.93729, Residuals: -1.39693, Convergence: 0.002591\n",
      "Epoch: 81, Loss: 146.57620, Residuals: -1.38519, Convergence: 0.002463\n",
      "Epoch: 82, Loss: 146.23373, Residuals: -1.37395, Convergence: 0.002342\n",
      "Epoch: 83, Loss: 145.90889, Residuals: -1.36319, Convergence: 0.002226\n",
      "Epoch: 84, Loss: 145.60077, Residuals: -1.35290, Convergence: 0.002116\n",
      "Epoch: 85, Loss: 145.30848, Residuals: -1.34305, Convergence: 0.002012\n",
      "Epoch: 86, Loss: 145.03119, Residuals: -1.33363, Convergence: 0.001912\n",
      "Epoch: 87, Loss: 144.76813, Residuals: -1.32461, Convergence: 0.001817\n",
      "Epoch: 88, Loss: 144.51857, Residuals: -1.31599, Convergence: 0.001727\n",
      "Epoch: 89, Loss: 144.28184, Residuals: -1.30775, Convergence: 0.001641\n",
      "Epoch: 90, Loss: 144.05729, Residuals: -1.29986, Convergence: 0.001559\n",
      "Epoch: 91, Loss: 143.84434, Residuals: -1.29232, Convergence: 0.001480\n",
      "Epoch: 92, Loss: 143.64245, Residuals: -1.28511, Convergence: 0.001406\n",
      "Epoch: 93, Loss: 143.45109, Residuals: -1.27820, Convergence: 0.001334\n",
      "Epoch: 94, Loss: 143.26982, Residuals: -1.27160, Convergence: 0.001265\n",
      "Epoch: 95, Loss: 143.09820, Residuals: -1.26528, Convergence: 0.001199\n",
      "Epoch: 96, Loss: 142.93584, Residuals: -1.25924, Convergence: 0.001136\n",
      "Epoch: 97, Loss: 142.78239, Residuals: -1.25346, Convergence: 0.001075\n",
      "Epoch: 98, Loss: 142.63751, Residuals: -1.24793, Convergence: 0.001016\n",
      "Epoch: 99, Loss: 142.50090, Residuals: -1.24264, Convergence: 0.000959\n",
      "Evidence -184.688\n",
      "\n",
      "Epoch: 99, Evidence: -184.68787, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.25e-01\n",
      "Epoch: 99, Loss: 1369.00342, Residuals: -1.24264, Convergence:   inf\n",
      "Epoch: 100, Loss: 1307.16398, Residuals: -1.27282, Convergence: 0.047308\n",
      "Epoch: 101, Loss: 1260.10418, Residuals: -1.29610, Convergence: 0.037346\n",
      "Epoch: 102, Loss: 1224.51006, Residuals: -1.31254, Convergence: 0.029068\n",
      "Epoch: 103, Loss: 1196.80843, Residuals: -1.32390, Convergence: 0.023146\n",
      "Epoch: 104, Loss: 1174.41036, Residuals: -1.33217, Convergence: 0.019072\n",
      "Epoch: 105, Loss: 1155.81298, Residuals: -1.33842, Convergence: 0.016090\n",
      "Epoch: 106, Loss: 1140.11009, Residuals: -1.34310, Convergence: 0.013773\n",
      "Epoch: 107, Loss: 1126.68789, Residuals: -1.34646, Convergence: 0.011913\n",
      "Epoch: 108, Loss: 1115.09297, Residuals: -1.34864, Convergence: 0.010398\n",
      "Epoch: 109, Loss: 1104.97226, Residuals: -1.34979, Convergence: 0.009159\n",
      "Epoch: 110, Loss: 1096.04385, Residuals: -1.34999, Convergence: 0.008146\n",
      "Epoch: 111, Loss: 1088.07583, Residuals: -1.34935, Convergence: 0.007323\n",
      "Epoch: 112, Loss: 1080.87389, Residuals: -1.34793, Convergence: 0.006663\n",
      "Epoch: 113, Loss: 1074.27153, Residuals: -1.34579, Convergence: 0.006146\n",
      "Epoch: 114, Loss: 1068.12244, Residuals: -1.34297, Convergence: 0.005757\n",
      "Epoch: 115, Loss: 1062.29584, Residuals: -1.33947, Convergence: 0.005485\n",
      "Epoch: 116, Loss: 1056.67440, Residuals: -1.33532, Convergence: 0.005320\n",
      "Epoch: 117, Loss: 1051.15540, Residuals: -1.33050, Convergence: 0.005250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 118, Loss: 1045.65855, Residuals: -1.32504, Convergence: 0.005257\n",
      "Epoch: 119, Loss: 1040.13777, Residuals: -1.31898, Convergence: 0.005308\n",
      "Epoch: 120, Loss: 1034.59698, Residuals: -1.31238, Convergence: 0.005356\n",
      "Epoch: 121, Loss: 1029.09316, Residuals: -1.30533, Convergence: 0.005348\n",
      "Epoch: 122, Loss: 1023.71935, Residuals: -1.29793, Convergence: 0.005249\n",
      "Epoch: 123, Loss: 1018.56849, Residuals: -1.29026, Convergence: 0.005057\n",
      "Epoch: 124, Loss: 1013.70426, Residuals: -1.28241, Convergence: 0.004798\n",
      "Epoch: 125, Loss: 1009.15252, Residuals: -1.27447, Convergence: 0.004510\n",
      "Epoch: 126, Loss: 1004.90857, Residuals: -1.26649, Convergence: 0.004223\n",
      "Epoch: 127, Loss: 1000.95214, Residuals: -1.25854, Convergence: 0.003953\n",
      "Epoch: 128, Loss: 997.25603, Residuals: -1.25067, Convergence: 0.003706\n",
      "Epoch: 129, Loss: 993.79191, Residuals: -1.24291, Convergence: 0.003486\n",
      "Epoch: 130, Loss: 990.53509, Residuals: -1.23530, Convergence: 0.003288\n",
      "Epoch: 131, Loss: 987.46318, Residuals: -1.22786, Convergence: 0.003111\n",
      "Epoch: 132, Loss: 984.55788, Residuals: -1.22062, Convergence: 0.002951\n",
      "Epoch: 133, Loss: 981.80366, Residuals: -1.21358, Convergence: 0.002805\n",
      "Epoch: 134, Loss: 979.18724, Residuals: -1.20676, Convergence: 0.002672\n",
      "Epoch: 135, Loss: 976.69814, Residuals: -1.20017, Convergence: 0.002548\n",
      "Epoch: 136, Loss: 974.32679, Residuals: -1.19381, Convergence: 0.002434\n",
      "Epoch: 137, Loss: 972.06568, Residuals: -1.18769, Convergence: 0.002326\n",
      "Epoch: 138, Loss: 969.90699, Residuals: -1.18181, Convergence: 0.002226\n",
      "Epoch: 139, Loss: 967.84462, Residuals: -1.17617, Convergence: 0.002131\n",
      "Epoch: 140, Loss: 965.87225, Residuals: -1.17076, Convergence: 0.002042\n",
      "Epoch: 141, Loss: 963.98435, Residuals: -1.16558, Convergence: 0.001958\n",
      "Epoch: 142, Loss: 962.17530, Residuals: -1.16063, Convergence: 0.001880\n",
      "Epoch: 143, Loss: 960.44037, Residuals: -1.15589, Convergence: 0.001806\n",
      "Epoch: 144, Loss: 958.77478, Residuals: -1.15136, Convergence: 0.001737\n",
      "Epoch: 145, Loss: 957.17345, Residuals: -1.14702, Convergence: 0.001673\n",
      "Epoch: 146, Loss: 955.63270, Residuals: -1.14288, Convergence: 0.001612\n",
      "Epoch: 147, Loss: 954.14846, Residuals: -1.13892, Convergence: 0.001556\n",
      "Epoch: 148, Loss: 952.71670, Residuals: -1.13513, Convergence: 0.001503\n",
      "Epoch: 149, Loss: 951.33387, Residuals: -1.13149, Convergence: 0.001454\n",
      "Epoch: 150, Loss: 949.99692, Residuals: -1.12802, Convergence: 0.001407\n",
      "Epoch: 151, Loss: 948.70183, Residuals: -1.12468, Convergence: 0.001365\n",
      "Epoch: 152, Loss: 947.44575, Residuals: -1.12148, Convergence: 0.001326\n",
      "Epoch: 153, Loss: 946.22545, Residuals: -1.11840, Convergence: 0.001290\n",
      "Epoch: 154, Loss: 945.03717, Residuals: -1.11543, Convergence: 0.001257\n",
      "Epoch: 155, Loss: 943.87753, Residuals: -1.11257, Convergence: 0.001229\n",
      "Epoch: 156, Loss: 942.74289, Residuals: -1.10980, Convergence: 0.001204\n",
      "Epoch: 157, Loss: 941.62901, Residuals: -1.10712, Convergence: 0.001183\n",
      "Epoch: 158, Loss: 940.53223, Residuals: -1.10452, Convergence: 0.001166\n",
      "Epoch: 159, Loss: 939.44855, Residuals: -1.10197, Convergence: 0.001154\n",
      "Epoch: 160, Loss: 938.37320, Residuals: -1.09948, Convergence: 0.001146\n",
      "Epoch: 161, Loss: 937.30338, Residuals: -1.09704, Convergence: 0.001141\n",
      "Epoch: 162, Loss: 936.23581, Residuals: -1.09463, Convergence: 0.001140\n",
      "Epoch: 163, Loss: 935.16870, Residuals: -1.09224, Convergence: 0.001141\n",
      "Epoch: 164, Loss: 934.10108, Residuals: -1.08988, Convergence: 0.001143\n",
      "Epoch: 165, Loss: 933.03418, Residuals: -1.08754, Convergence: 0.001143\n",
      "Epoch: 166, Loss: 931.97050, Residuals: -1.08523, Convergence: 0.001141\n",
      "Epoch: 167, Loss: 930.91376, Residuals: -1.08294, Convergence: 0.001135\n",
      "Epoch: 168, Loss: 929.86753, Residuals: -1.08069, Convergence: 0.001125\n",
      "Epoch: 169, Loss: 928.83657, Residuals: -1.07848, Convergence: 0.001110\n",
      "Epoch: 170, Loss: 927.82526, Residuals: -1.07631, Convergence: 0.001090\n",
      "Epoch: 171, Loss: 926.83635, Residuals: -1.07420, Convergence: 0.001067\n",
      "Epoch: 172, Loss: 925.87303, Residuals: -1.07215, Convergence: 0.001040\n",
      "Epoch: 173, Loss: 924.93676, Residuals: -1.07015, Convergence: 0.001012\n",
      "Epoch: 174, Loss: 924.02881, Residuals: -1.06822, Convergence: 0.000983\n",
      "Evidence 11049.375\n",
      "\n",
      "Epoch: 174, Evidence: 11049.37500, Convergence: 1.016715\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.75e-01\n",
      "Epoch: 174, Loss: 2340.65849, Residuals: -1.06822, Convergence:   inf\n",
      "Epoch: 175, Loss: 2301.76656, Residuals: -1.07526, Convergence: 0.016897\n",
      "Epoch: 176, Loss: 2274.91514, Residuals: -1.07423, Convergence: 0.011803\n",
      "Epoch: 177, Loss: 2252.65073, Residuals: -1.07231, Convergence: 0.009884\n",
      "Epoch: 178, Loss: 2233.89095, Residuals: -1.07020, Convergence: 0.008398\n",
      "Epoch: 179, Loss: 2217.94371, Residuals: -1.06801, Convergence: 0.007190\n",
      "Epoch: 180, Loss: 2204.27459, Residuals: -1.06578, Convergence: 0.006201\n",
      "Epoch: 181, Loss: 2192.44921, Residuals: -1.06352, Convergence: 0.005394\n",
      "Epoch: 182, Loss: 2182.10587, Residuals: -1.06123, Convergence: 0.004740\n",
      "Epoch: 183, Loss: 2172.94522, Residuals: -1.05890, Convergence: 0.004216\n",
      "Epoch: 184, Loss: 2164.72233, Residuals: -1.05650, Convergence: 0.003799\n",
      "Epoch: 185, Loss: 2157.24961, Residuals: -1.05400, Convergence: 0.003464\n",
      "Epoch: 186, Loss: 2150.39443, Residuals: -1.05141, Convergence: 0.003188\n",
      "Epoch: 187, Loss: 2144.07403, Residuals: -1.04873, Convergence: 0.002948\n",
      "Epoch: 188, Loss: 2138.24105, Residuals: -1.04599, Convergence: 0.002728\n",
      "Epoch: 189, Loss: 2132.86130, Residuals: -1.04322, Convergence: 0.002522\n",
      "Epoch: 190, Loss: 2127.90333, Residuals: -1.04047, Convergence: 0.002330\n",
      "Epoch: 191, Loss: 2123.33561, Residuals: -1.03776, Convergence: 0.002151\n",
      "Epoch: 192, Loss: 2119.12644, Residuals: -1.03512, Convergence: 0.001986\n",
      "Epoch: 193, Loss: 2115.24481, Residuals: -1.03256, Convergence: 0.001835\n",
      "Epoch: 194, Loss: 2111.66169, Residuals: -1.03009, Convergence: 0.001697\n",
      "Epoch: 195, Loss: 2108.35223, Residuals: -1.02772, Convergence: 0.001570\n",
      "Epoch: 196, Loss: 2105.29296, Residuals: -1.02545, Convergence: 0.001453\n",
      "Epoch: 197, Loss: 2102.46273, Residuals: -1.02328, Convergence: 0.001346\n",
      "Epoch: 198, Loss: 2099.84223, Residuals: -1.02121, Convergence: 0.001248\n",
      "Epoch: 199, Loss: 2097.41273, Residuals: -1.01923, Convergence: 0.001158\n",
      "Epoch: 200, Loss: 2095.15721, Residuals: -1.01735, Convergence: 0.001077\n",
      "Epoch: 201, Loss: 2093.05991, Residuals: -1.01557, Convergence: 0.001002\n",
      "Epoch: 202, Loss: 2091.10580, Residuals: -1.01386, Convergence: 0.000934\n",
      "Evidence 14207.509\n",
      "\n",
      "Epoch: 202, Evidence: 14207.50879, Convergence: 0.222286\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.36e-01\n",
      "Epoch: 202, Loss: 2469.06934, Residuals: -1.01386, Convergence:   inf\n",
      "Epoch: 203, Loss: 2455.72463, Residuals: -1.01127, Convergence: 0.005434\n",
      "Epoch: 204, Loss: 2444.77468, Residuals: -1.00814, Convergence: 0.004479\n",
      "Epoch: 205, Loss: 2435.33408, Residuals: -1.00507, Convergence: 0.003877\n",
      "Epoch: 206, Loss: 2427.14473, Residuals: -1.00215, Convergence: 0.003374\n",
      "Epoch: 207, Loss: 2420.00839, Residuals: -0.99944, Convergence: 0.002949\n",
      "Epoch: 208, Loss: 2413.76106, Residuals: -0.99694, Convergence: 0.002588\n",
      "Epoch: 209, Loss: 2408.26738, Residuals: -0.99464, Convergence: 0.002281\n",
      "Epoch: 210, Loss: 2403.41130, Residuals: -0.99253, Convergence: 0.002020\n",
      "Epoch: 211, Loss: 2399.09737, Residuals: -0.99059, Convergence: 0.001798\n",
      "Epoch: 212, Loss: 2395.24439, Residuals: -0.98882, Convergence: 0.001609\n",
      "Epoch: 213, Loss: 2391.78401, Residuals: -0.98718, Convergence: 0.001447\n",
      "Epoch: 214, Loss: 2388.65971, Residuals: -0.98568, Convergence: 0.001308\n",
      "Epoch: 215, Loss: 2385.82381, Residuals: -0.98429, Convergence: 0.001189\n",
      "Epoch: 216, Loss: 2383.23548, Residuals: -0.98302, Convergence: 0.001086\n",
      "Epoch: 217, Loss: 2380.86338, Residuals: -0.98184, Convergence: 0.000996\n",
      "Evidence 14584.031\n",
      "\n",
      "Epoch: 217, Evidence: 14584.03125, Convergence: 0.025817\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.34e-01\n",
      "Epoch: 217, Loss: 2474.78822, Residuals: -0.98184, Convergence:   inf\n",
      "Epoch: 218, Loss: 2468.20731, Residuals: -0.97862, Convergence: 0.002666\n",
      "Epoch: 219, Loss: 2462.75872, Residuals: -0.97577, Convergence: 0.002212\n",
      "Epoch: 220, Loss: 2458.14704, Residuals: -0.97333, Convergence: 0.001876\n",
      "Epoch: 221, Loss: 2454.19314, Residuals: -0.97126, Convergence: 0.001611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 222, Loss: 2450.76137, Residuals: -0.96950, Convergence: 0.001400\n",
      "Epoch: 223, Loss: 2447.74542, Residuals: -0.96800, Convergence: 0.001232\n",
      "Epoch: 224, Loss: 2445.06428, Residuals: -0.96673, Convergence: 0.001097\n",
      "Epoch: 225, Loss: 2442.65438, Residuals: -0.96564, Convergence: 0.000987\n",
      "Evidence 14664.564\n",
      "\n",
      "Epoch: 225, Evidence: 14664.56445, Convergence: 0.005492\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.62e-01\n",
      "Epoch: 225, Loss: 2476.42074, Residuals: -0.96564, Convergence:   inf\n",
      "Epoch: 226, Loss: 2472.40646, Residuals: -0.96322, Convergence: 0.001624\n",
      "Epoch: 227, Loss: 2469.08259, Residuals: -0.96129, Convergence: 0.001346\n",
      "Epoch: 228, Loss: 2466.25425, Residuals: -0.95976, Convergence: 0.001147\n",
      "Epoch: 229, Loss: 2463.79884, Residuals: -0.95853, Convergence: 0.000997\n",
      "Evidence 14693.070\n",
      "\n",
      "Epoch: 229, Evidence: 14693.07031, Convergence: 0.001940\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.11e-01\n",
      "Epoch: 229, Loss: 2477.37935, Residuals: -0.95853, Convergence:   inf\n",
      "Epoch: 230, Loss: 2474.39594, Residuals: -0.95663, Convergence: 0.001206\n",
      "Epoch: 231, Loss: 2471.90722, Residuals: -0.95518, Convergence: 0.001007\n",
      "Epoch: 232, Loss: 2469.76093, Residuals: -0.95405, Convergence: 0.000869\n",
      "Evidence 14707.886\n",
      "\n",
      "Epoch: 232, Evidence: 14707.88574, Convergence: 0.001007\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.74e-01\n",
      "Epoch: 232, Loss: 2478.00972, Residuals: -0.95405, Convergence:   inf\n",
      "Epoch: 233, Loss: 2475.63537, Residuals: -0.95243, Convergence: 0.000959\n",
      "Evidence 14714.492\n",
      "\n",
      "Epoch: 233, Evidence: 14714.49219, Convergence: 0.000449\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.48e-01\n",
      "Epoch: 233, Loss: 2478.49661, Residuals: -0.95243, Convergence:   inf\n",
      "Epoch: 234, Loss: 2474.40394, Residuals: -0.94999, Convergence: 0.001654\n",
      "Epoch: 235, Loss: 2471.30166, Residuals: -0.94859, Convergence: 0.001255\n",
      "Epoch: 236, Loss: 2468.77233, Residuals: -0.94781, Convergence: 0.001025\n",
      "Epoch: 237, Loss: 2466.61879, Residuals: -0.94752, Convergence: 0.000873\n",
      "Evidence 14729.727\n",
      "\n",
      "Epoch: 237, Evidence: 14729.72656, Convergence: 0.001483\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.24e-01\n",
      "Epoch: 237, Loss: 2478.59693, Residuals: -0.94752, Convergence:   inf\n",
      "Epoch: 238, Loss: 2475.87585, Residuals: -0.94544, Convergence: 0.001099\n",
      "Epoch: 239, Loss: 2473.71518, Residuals: -0.94455, Convergence: 0.000873\n",
      "Evidence 14739.395\n",
      "\n",
      "Epoch: 239, Evidence: 14739.39453, Convergence: 0.000656\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.07e-01\n",
      "Epoch: 239, Loss: 2478.68309, Residuals: -0.94455, Convergence:   inf\n",
      "Epoch: 240, Loss: 2474.68429, Residuals: -0.94165, Convergence: 0.001616\n",
      "Epoch: 241, Loss: 2471.80699, Residuals: -0.94269, Convergence: 0.001164\n",
      "Epoch: 242, Loss: 2469.31387, Residuals: -0.94325, Convergence: 0.001010\n",
      "Epoch: 243, Loss: 2467.18203, Residuals: -0.94606, Convergence: 0.000864\n",
      "Evidence 14753.707\n",
      "\n",
      "Epoch: 243, Evidence: 14753.70703, Convergence: 0.001625\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 9.58e-02\n",
      "Epoch: 243, Loss: 2477.86268, Residuals: -0.94606, Convergence:   inf\n",
      "Epoch: 244, Loss: 2476.08600, Residuals: -0.94371, Convergence: 0.000718\n",
      "Evidence 14760.047\n",
      "\n",
      "Epoch: 244, Evidence: 14760.04688, Convergence: 0.000430\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.92e-02\n",
      "Epoch: 244, Loss: 2478.73888, Residuals: -0.94371, Convergence:   inf\n",
      "Epoch: 245, Loss: 2522.59382, Residuals: -0.98764, Convergence: -0.017385\n",
      "Epoch: 245, Loss: 2476.82727, Residuals: -0.94188, Convergence: 0.000772\n",
      "Evidence 14763.714\n",
      "\n",
      "Epoch: 245, Evidence: 14763.71387, Convergence: 0.000678\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.36e-02\n",
      "Epoch: 245, Loss: 2478.08059, Residuals: -0.94188, Convergence:   inf\n",
      "Epoch: 246, Loss: 2482.04021, Residuals: -0.94271, Convergence: -0.001595\n",
      "Epoch: 246, Loss: 2477.95056, Residuals: -0.93964, Convergence: 0.000052\n",
      "Evidence 14765.429\n",
      "\n",
      "Epoch: 246, Evidence: 14765.42871, Convergence: 0.000794\n",
      "Total samples: 183, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 383.99470, Residuals: -4.54980, Convergence:   inf\n",
      "Epoch: 1, Loss: 358.17715, Residuals: -4.42901, Convergence: 0.072080\n",
      "Epoch: 2, Loss: 337.08721, Residuals: -4.26502, Convergence: 0.062565\n",
      "Epoch: 3, Loss: 320.97807, Residuals: -4.10087, Convergence: 0.050188\n",
      "Epoch: 4, Loss: 308.68447, Residuals: -3.95621, Convergence: 0.039826\n",
      "Epoch: 5, Loss: 298.93079, Residuals: -3.82835, Convergence: 0.032629\n",
      "Epoch: 6, Loss: 291.01049, Residuals: -3.71701, Convergence: 0.027217\n",
      "Epoch: 7, Loss: 284.43360, Residuals: -3.62166, Convergence: 0.023123\n",
      "Epoch: 8, Loss: 278.83698, Residuals: -3.54022, Convergence: 0.020071\n",
      "Epoch: 9, Loss: 273.96388, Residuals: -3.47032, Convergence: 0.017787\n",
      "Epoch: 10, Loss: 269.63119, Residuals: -3.40983, Convergence: 0.016069\n",
      "Epoch: 11, Loss: 265.70567, Residuals: -3.35696, Convergence: 0.014774\n",
      "Epoch: 12, Loss: 262.08953, Residuals: -3.31014, Convergence: 0.013797\n",
      "Epoch: 13, Loss: 258.71162, Residuals: -3.26805, Convergence: 0.013057\n",
      "Epoch: 14, Loss: 255.52232, Residuals: -3.22952, Convergence: 0.012482\n",
      "Epoch: 15, Loss: 252.49185, Residuals: -3.19367, Convergence: 0.012002\n",
      "Epoch: 16, Loss: 249.60694, Residuals: -3.15993, Convergence: 0.011558\n",
      "Epoch: 17, Loss: 246.85739, Residuals: -3.12797, Convergence: 0.011138\n",
      "Epoch: 18, Loss: 244.22095, Residuals: -3.09738, Convergence: 0.010795\n",
      "Epoch: 19, Loss: 241.66295, Residuals: -3.06760, Convergence: 0.010585\n",
      "Epoch: 20, Loss: 239.14515, Residuals: -3.03804, Convergence: 0.010528\n",
      "Epoch: 21, Loss: 236.63459, Residuals: -3.00817, Convergence: 0.010609\n",
      "Epoch: 22, Loss: 234.10565, Residuals: -2.97763, Convergence: 0.010803\n",
      "Epoch: 23, Loss: 231.52794, Residuals: -2.94607, Convergence: 0.011133\n",
      "Epoch: 24, Loss: 228.84992, Residuals: -2.91291, Convergence: 0.011702\n",
      "Epoch: 25, Loss: 226.01538, Residuals: -2.87746, Convergence: 0.012541\n",
      "Epoch: 26, Loss: 223.05469, Residuals: -2.83991, Convergence: 0.013273\n",
      "Epoch: 27, Loss: 220.11359, Residuals: -2.80179, Convergence: 0.013362\n",
      "Epoch: 28, Loss: 217.27927, Residuals: -2.76419, Convergence: 0.013045\n",
      "Epoch: 29, Loss: 214.55273, Residuals: -2.72726, Convergence: 0.012708\n",
      "Epoch: 30, Loss: 211.91406, Residuals: -2.69089, Convergence: 0.012452\n",
      "Epoch: 31, Loss: 209.34563, Residuals: -2.65494, Convergence: 0.012269\n",
      "Epoch: 32, Loss: 206.83529, Residuals: -2.61928, Convergence: 0.012137\n",
      "Epoch: 33, Loss: 204.37581, Residuals: -2.58384, Convergence: 0.012034\n",
      "Epoch: 34, Loss: 201.96362, Residuals: -2.54856, Convergence: 0.011944\n",
      "Epoch: 35, Loss: 199.59771, Residuals: -2.51342, Convergence: 0.011853\n",
      "Epoch: 36, Loss: 197.27872, Residuals: -2.47838, Convergence: 0.011755\n",
      "Epoch: 37, Loss: 195.00824, Residuals: -2.44346, Convergence: 0.011643\n",
      "Epoch: 38, Loss: 192.78826, Residuals: -2.40864, Convergence: 0.011515\n",
      "Epoch: 39, Loss: 190.62085, Residuals: -2.37396, Convergence: 0.011370\n",
      "Epoch: 40, Loss: 188.50811, Residuals: -2.33941, Convergence: 0.011208\n",
      "Epoch: 41, Loss: 186.45215, Residuals: -2.30504, Convergence: 0.011027\n",
      "Epoch: 42, Loss: 184.45522, Residuals: -2.27087, Convergence: 0.010826\n",
      "Epoch: 43, Loss: 182.51983, Residuals: -2.23694, Convergence: 0.010604\n",
      "Epoch: 44, Loss: 180.64874, Residuals: -2.20331, Convergence: 0.010358\n",
      "Epoch: 45, Loss: 178.84481, Residuals: -2.17004, Convergence: 0.010087\n",
      "Epoch: 46, Loss: 177.11062, Residuals: -2.13720, Convergence: 0.009792\n",
      "Epoch: 47, Loss: 175.44819, Residuals: -2.10484, Convergence: 0.009475\n",
      "Epoch: 48, Loss: 173.85872, Residuals: -2.07304, Convergence: 0.009142\n",
      "Epoch: 49, Loss: 172.34254, Residuals: -2.04184, Convergence: 0.008798\n",
      "Epoch: 50, Loss: 170.89912, Residuals: -2.01129, Convergence: 0.008446\n",
      "Epoch: 51, Loss: 169.52723, Residuals: -1.98144, Convergence: 0.008092\n",
      "Epoch: 52, Loss: 168.22508, Residuals: -1.95231, Convergence: 0.007741\n",
      "Epoch: 53, Loss: 166.99041, Residuals: -1.92393, Convergence: 0.007394\n",
      "Epoch: 54, Loss: 165.82062, Residuals: -1.89633, Convergence: 0.007054\n",
      "Epoch: 55, Loss: 164.71287, Residuals: -1.86950, Convergence: 0.006725\n",
      "Epoch: 56, Loss: 163.66407, Residuals: -1.84347, Convergence: 0.006408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57, Loss: 162.67095, Residuals: -1.81824, Convergence: 0.006105\n",
      "Epoch: 58, Loss: 161.73014, Residuals: -1.79380, Convergence: 0.005817\n",
      "Epoch: 59, Loss: 160.83813, Residuals: -1.77015, Convergence: 0.005546\n",
      "Epoch: 60, Loss: 159.99138, Residuals: -1.74728, Convergence: 0.005292\n",
      "Epoch: 61, Loss: 159.18637, Residuals: -1.72516, Convergence: 0.005057\n",
      "Epoch: 62, Loss: 158.41967, Residuals: -1.70377, Convergence: 0.004840\n",
      "Epoch: 63, Loss: 157.68793, Residuals: -1.68309, Convergence: 0.004640\n",
      "Epoch: 64, Loss: 156.98799, Residuals: -1.66310, Convergence: 0.004459\n",
      "Epoch: 65, Loss: 156.31692, Residuals: -1.64375, Convergence: 0.004293\n",
      "Epoch: 66, Loss: 155.67197, Residuals: -1.62503, Convergence: 0.004143\n",
      "Epoch: 67, Loss: 155.05074, Residuals: -1.60690, Convergence: 0.004007\n",
      "Epoch: 68, Loss: 154.45110, Residuals: -1.58932, Convergence: 0.003882\n",
      "Epoch: 69, Loss: 153.87133, Residuals: -1.57227, Convergence: 0.003768\n",
      "Epoch: 70, Loss: 153.31005, Residuals: -1.55573, Convergence: 0.003661\n",
      "Epoch: 71, Loss: 152.76625, Residuals: -1.53968, Convergence: 0.003560\n",
      "Epoch: 72, Loss: 152.23923, Residuals: -1.52408, Convergence: 0.003462\n",
      "Epoch: 73, Loss: 151.72851, Residuals: -1.50895, Convergence: 0.003366\n",
      "Epoch: 74, Loss: 151.23382, Residuals: -1.49426, Convergence: 0.003271\n",
      "Epoch: 75, Loss: 150.75499, Residuals: -1.48000, Convergence: 0.003176\n",
      "Epoch: 76, Loss: 150.29194, Residuals: -1.46617, Convergence: 0.003081\n",
      "Epoch: 77, Loss: 149.84461, Residuals: -1.45277, Convergence: 0.002985\n",
      "Epoch: 78, Loss: 149.41297, Residuals: -1.43978, Convergence: 0.002889\n",
      "Epoch: 79, Loss: 148.99696, Residuals: -1.42721, Convergence: 0.002792\n",
      "Epoch: 80, Loss: 148.59652, Residuals: -1.41504, Convergence: 0.002695\n",
      "Epoch: 81, Loss: 148.21154, Residuals: -1.40328, Convergence: 0.002597\n",
      "Epoch: 82, Loss: 147.84191, Residuals: -1.39191, Convergence: 0.002500\n",
      "Epoch: 83, Loss: 147.48748, Residuals: -1.38093, Convergence: 0.002403\n",
      "Epoch: 84, Loss: 147.14808, Residuals: -1.37033, Convergence: 0.002307\n",
      "Epoch: 85, Loss: 146.82350, Residuals: -1.36011, Convergence: 0.002211\n",
      "Epoch: 86, Loss: 146.51349, Residuals: -1.35026, Convergence: 0.002116\n",
      "Epoch: 87, Loss: 146.21783, Residuals: -1.34077, Convergence: 0.002022\n",
      "Epoch: 88, Loss: 145.93621, Residuals: -1.33164, Convergence: 0.001930\n",
      "Epoch: 89, Loss: 145.66836, Residuals: -1.32286, Convergence: 0.001839\n",
      "Epoch: 90, Loss: 145.41394, Residuals: -1.31442, Convergence: 0.001750\n",
      "Epoch: 91, Loss: 145.17262, Residuals: -1.30632, Convergence: 0.001662\n",
      "Epoch: 92, Loss: 144.94407, Residuals: -1.29855, Convergence: 0.001577\n",
      "Epoch: 93, Loss: 144.72793, Residuals: -1.29112, Convergence: 0.001493\n",
      "Epoch: 94, Loss: 144.52382, Residuals: -1.28400, Convergence: 0.001412\n",
      "Epoch: 95, Loss: 144.33136, Residuals: -1.27720, Convergence: 0.001333\n",
      "Epoch: 96, Loss: 144.15014, Residuals: -1.27071, Convergence: 0.001257\n",
      "Epoch: 97, Loss: 143.97971, Residuals: -1.26453, Convergence: 0.001184\n",
      "Epoch: 98, Loss: 143.81959, Residuals: -1.25866, Convergence: 0.001113\n",
      "Epoch: 99, Loss: 143.66925, Residuals: -1.25309, Convergence: 0.001046\n",
      "Epoch: 100, Loss: 143.52804, Residuals: -1.24782, Convergence: 0.000984\n",
      "Evidence -185.221\n",
      "\n",
      "Epoch: 100, Evidence: -185.22079, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.24e-01\n",
      "Epoch: 100, Loss: 1386.99671, Residuals: -1.24782, Convergence:   inf\n",
      "Epoch: 101, Loss: 1323.07329, Residuals: -1.27746, Convergence: 0.048314\n",
      "Epoch: 102, Loss: 1274.50730, Residuals: -1.30054, Convergence: 0.038106\n",
      "Epoch: 103, Loss: 1237.83122, Residuals: -1.31677, Convergence: 0.029629\n",
      "Epoch: 104, Loss: 1209.21854, Residuals: -1.32787, Convergence: 0.023662\n",
      "Epoch: 105, Loss: 1186.00100, Residuals: -1.33588, Convergence: 0.019576\n",
      "Epoch: 106, Loss: 1166.67850, Residuals: -1.34185, Convergence: 0.016562\n",
      "Epoch: 107, Loss: 1150.34070, Residuals: -1.34627, Convergence: 0.014203\n",
      "Epoch: 108, Loss: 1136.35924, Residuals: -1.34936, Convergence: 0.012304\n",
      "Epoch: 109, Loss: 1124.26168, Residuals: -1.35131, Convergence: 0.010760\n",
      "Epoch: 110, Loss: 1113.67634, Residuals: -1.35222, Convergence: 0.009505\n",
      "Epoch: 111, Loss: 1104.30071, Residuals: -1.35220, Convergence: 0.008490\n",
      "Epoch: 112, Loss: 1095.88338, Residuals: -1.35133, Convergence: 0.007681\n",
      "Epoch: 113, Loss: 1088.21000, Residuals: -1.34964, Convergence: 0.007051\n",
      "Epoch: 114, Loss: 1081.09358, Residuals: -1.34717, Convergence: 0.006583\n",
      "Epoch: 115, Loss: 1074.36992, Residuals: -1.34395, Convergence: 0.006258\n",
      "Epoch: 116, Loss: 1067.89597, Residuals: -1.33997, Convergence: 0.006062\n",
      "Epoch: 117, Loss: 1061.55705, Residuals: -1.33527, Convergence: 0.005971\n",
      "Epoch: 118, Loss: 1055.28001, Residuals: -1.32988, Convergence: 0.005948\n",
      "Epoch: 119, Loss: 1049.04679, Residuals: -1.32388, Convergence: 0.005942\n",
      "Epoch: 120, Loss: 1042.90022, Residuals: -1.31737, Convergence: 0.005894\n",
      "Epoch: 121, Loss: 1036.92567, Residuals: -1.31046, Convergence: 0.005762\n",
      "Epoch: 122, Loss: 1031.21369, Residuals: -1.30323, Convergence: 0.005539\n",
      "Epoch: 123, Loss: 1025.82890, Residuals: -1.29578, Convergence: 0.005249\n",
      "Epoch: 124, Loss: 1020.79809, Residuals: -1.28819, Convergence: 0.004928\n",
      "Epoch: 125, Loss: 1016.11665, Residuals: -1.28052, Convergence: 0.004607\n",
      "Epoch: 126, Loss: 1011.76136, Residuals: -1.27283, Convergence: 0.004305\n",
      "Epoch: 127, Loss: 1007.70133, Residuals: -1.26517, Convergence: 0.004029\n",
      "Epoch: 128, Loss: 1003.90485, Residuals: -1.25759, Convergence: 0.003782\n",
      "Epoch: 129, Loss: 1000.34184, Residuals: -1.25012, Convergence: 0.003562\n",
      "Epoch: 130, Loss: 996.98645, Residuals: -1.24279, Convergence: 0.003366\n",
      "Epoch: 131, Loss: 993.81558, Residuals: -1.23561, Convergence: 0.003191\n",
      "Epoch: 132, Loss: 990.81034, Residuals: -1.22861, Convergence: 0.003033\n",
      "Epoch: 133, Loss: 987.95531, Residuals: -1.22180, Convergence: 0.002890\n",
      "Epoch: 134, Loss: 985.23666, Residuals: -1.21519, Convergence: 0.002759\n",
      "Epoch: 135, Loss: 982.64381, Residuals: -1.20878, Convergence: 0.002639\n",
      "Epoch: 136, Loss: 980.16768, Residuals: -1.20259, Convergence: 0.002526\n",
      "Epoch: 137, Loss: 977.79982, Residuals: -1.19662, Convergence: 0.002422\n",
      "Epoch: 138, Loss: 975.53407, Residuals: -1.19086, Convergence: 0.002323\n",
      "Epoch: 139, Loss: 973.36444, Residuals: -1.18532, Convergence: 0.002229\n",
      "Epoch: 140, Loss: 971.28529, Residuals: -1.18000, Convergence: 0.002141\n",
      "Epoch: 141, Loss: 969.29225, Residuals: -1.17489, Convergence: 0.002056\n",
      "Epoch: 142, Loss: 967.38087, Residuals: -1.16998, Convergence: 0.001976\n",
      "Epoch: 143, Loss: 965.54680, Residuals: -1.16528, Convergence: 0.001900\n",
      "Epoch: 144, Loss: 963.78656, Residuals: -1.16078, Convergence: 0.001826\n",
      "Epoch: 145, Loss: 962.09644, Residuals: -1.15647, Convergence: 0.001757\n",
      "Epoch: 146, Loss: 960.47310, Residuals: -1.15234, Convergence: 0.001690\n",
      "Epoch: 147, Loss: 958.91296, Residuals: -1.14839, Convergence: 0.001627\n",
      "Epoch: 148, Loss: 957.41295, Residuals: -1.14461, Convergence: 0.001567\n",
      "Epoch: 149, Loss: 955.96989, Residuals: -1.14099, Convergence: 0.001510\n",
      "Epoch: 150, Loss: 954.58055, Residuals: -1.13752, Convergence: 0.001455\n",
      "Epoch: 151, Loss: 953.24127, Residuals: -1.13419, Convergence: 0.001405\n",
      "Epoch: 152, Loss: 951.94870, Residuals: -1.13100, Convergence: 0.001358\n",
      "Epoch: 153, Loss: 950.69935, Residuals: -1.12793, Convergence: 0.001314\n",
      "Epoch: 154, Loss: 949.48924, Residuals: -1.12497, Convergence: 0.001274\n",
      "Epoch: 155, Loss: 948.31434, Residuals: -1.12213, Convergence: 0.001239\n",
      "Epoch: 156, Loss: 947.17033, Residuals: -1.11938, Convergence: 0.001208\n",
      "Epoch: 157, Loss: 946.05265, Residuals: -1.11671, Convergence: 0.001181\n",
      "Epoch: 158, Loss: 944.95720, Residuals: -1.11412, Convergence: 0.001159\n",
      "Epoch: 159, Loss: 943.87952, Residuals: -1.11160, Convergence: 0.001142\n",
      "Epoch: 160, Loss: 942.81469, Residuals: -1.10912, Convergence: 0.001129\n",
      "Epoch: 161, Loss: 941.75954, Residuals: -1.10669, Convergence: 0.001120\n",
      "Epoch: 162, Loss: 940.71076, Residuals: -1.10430, Convergence: 0.001115\n",
      "Epoch: 163, Loss: 939.66643, Residuals: -1.10193, Convergence: 0.001111\n",
      "Epoch: 164, Loss: 938.62614, Residuals: -1.09958, Convergence: 0.001108\n",
      "Epoch: 165, Loss: 937.59072, Residuals: -1.09726, Convergence: 0.001104\n",
      "Epoch: 166, Loss: 936.56255, Residuals: -1.09497, Convergence: 0.001098\n",
      "Epoch: 167, Loss: 935.54480, Residuals: -1.09270, Convergence: 0.001088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 168, Loss: 934.54121, Residuals: -1.09048, Convergence: 0.001074\n",
      "Epoch: 169, Loss: 933.55577, Residuals: -1.08829, Convergence: 0.001056\n",
      "Epoch: 170, Loss: 932.59224, Residuals: -1.08616, Convergence: 0.001033\n",
      "Epoch: 171, Loss: 931.65318, Residuals: -1.08408, Convergence: 0.001008\n",
      "Epoch: 172, Loss: 930.74089, Residuals: -1.08207, Convergence: 0.000980\n",
      "Evidence 11222.243\n",
      "\n",
      "Epoch: 172, Evidence: 11222.24316, Convergence: 1.016505\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 5.75e-01\n",
      "Epoch: 172, Loss: 2364.38399, Residuals: -1.08207, Convergence:   inf\n",
      "Epoch: 173, Loss: 2325.39592, Residuals: -1.08952, Convergence: 0.016766\n",
      "Epoch: 174, Loss: 2298.82550, Residuals: -1.08799, Convergence: 0.011558\n",
      "Epoch: 175, Loss: 2276.75619, Residuals: -1.08574, Convergence: 0.009693\n",
      "Epoch: 176, Loss: 2258.18533, Residuals: -1.08328, Convergence: 0.008224\n",
      "Epoch: 177, Loss: 2242.39727, Residuals: -1.08073, Convergence: 0.007041\n",
      "Epoch: 178, Loss: 2228.83721, Residuals: -1.07811, Convergence: 0.006084\n",
      "Epoch: 179, Loss: 2217.05606, Residuals: -1.07544, Convergence: 0.005314\n",
      "Epoch: 180, Loss: 2206.69070, Residuals: -1.07271, Convergence: 0.004697\n",
      "Epoch: 181, Loss: 2197.45254, Residuals: -1.06990, Convergence: 0.004204\n",
      "Epoch: 182, Loss: 2189.12573, Residuals: -1.06700, Convergence: 0.003804\n",
      "Epoch: 183, Loss: 2181.56145, Residuals: -1.06401, Convergence: 0.003467\n",
      "Epoch: 184, Loss: 2174.66540, Residuals: -1.06096, Convergence: 0.003171\n",
      "Epoch: 185, Loss: 2168.37272, Residuals: -1.05790, Convergence: 0.002902\n",
      "Epoch: 186, Loss: 2162.63434, Residuals: -1.05486, Convergence: 0.002653\n",
      "Epoch: 187, Loss: 2157.40131, Residuals: -1.05188, Convergence: 0.002426\n",
      "Epoch: 188, Loss: 2152.62401, Residuals: -1.04897, Convergence: 0.002219\n",
      "Epoch: 189, Loss: 2148.25360, Residuals: -1.04617, Convergence: 0.002034\n",
      "Epoch: 190, Loss: 2144.24135, Residuals: -1.04347, Convergence: 0.001871\n",
      "Epoch: 191, Loss: 2140.54471, Residuals: -1.04086, Convergence: 0.001727\n",
      "Epoch: 192, Loss: 2137.12391, Residuals: -1.03836, Convergence: 0.001601\n",
      "Epoch: 193, Loss: 2133.94547, Residuals: -1.03596, Convergence: 0.001489\n",
      "Epoch: 194, Loss: 2130.98131, Residuals: -1.03365, Convergence: 0.001391\n",
      "Epoch: 195, Loss: 2128.20843, Residuals: -1.03143, Convergence: 0.001303\n",
      "Epoch: 196, Loss: 2125.60940, Residuals: -1.02931, Convergence: 0.001223\n",
      "Epoch: 197, Loss: 2123.16767, Residuals: -1.02726, Convergence: 0.001150\n",
      "Epoch: 198, Loss: 2120.87138, Residuals: -1.02531, Convergence: 0.001083\n",
      "Epoch: 199, Loss: 2118.70984, Residuals: -1.02343, Convergence: 0.001020\n",
      "Epoch: 200, Loss: 2116.67392, Residuals: -1.02164, Convergence: 0.000962\n",
      "Evidence 14340.307\n",
      "\n",
      "Epoch: 200, Evidence: 14340.30664, Convergence: 0.217434\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 4.37e-01\n",
      "Epoch: 200, Loss: 2486.21716, Residuals: -1.02164, Convergence:   inf\n",
      "Epoch: 201, Loss: 2472.99315, Residuals: -1.01875, Convergence: 0.005347\n",
      "Epoch: 202, Loss: 2462.08192, Residuals: -1.01551, Convergence: 0.004432\n",
      "Epoch: 203, Loss: 2452.62942, Residuals: -1.01239, Convergence: 0.003854\n",
      "Epoch: 204, Loss: 2444.39860, Residuals: -1.00946, Convergence: 0.003367\n",
      "Epoch: 205, Loss: 2437.20465, Residuals: -1.00675, Convergence: 0.002952\n",
      "Epoch: 206, Loss: 2430.89431, Residuals: -1.00426, Convergence: 0.002596\n",
      "Epoch: 207, Loss: 2425.33713, Residuals: -1.00198, Convergence: 0.002291\n",
      "Epoch: 208, Loss: 2420.42246, Residuals: -0.99988, Convergence: 0.002031\n",
      "Epoch: 209, Loss: 2416.05666, Residuals: -0.99797, Convergence: 0.001807\n",
      "Epoch: 210, Loss: 2412.15755, Residuals: -0.99622, Convergence: 0.001616\n",
      "Epoch: 211, Loss: 2408.65725, Residuals: -0.99461, Convergence: 0.001453\n",
      "Epoch: 212, Loss: 2405.49888, Residuals: -0.99315, Convergence: 0.001313\n",
      "Epoch: 213, Loss: 2402.63419, Residuals: -0.99182, Convergence: 0.001192\n",
      "Epoch: 214, Loss: 2400.02180, Residuals: -0.99060, Convergence: 0.001088\n",
      "Epoch: 215, Loss: 2397.62928, Residuals: -0.98949, Convergence: 0.000998\n",
      "Evidence 14705.201\n",
      "\n",
      "Epoch: 215, Evidence: 14705.20117, Convergence: 0.024814\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 3.34e-01\n",
      "Epoch: 215, Loss: 2491.48394, Residuals: -0.98949, Convergence:   inf\n",
      "Epoch: 216, Loss: 2484.69639, Residuals: -0.98630, Convergence: 0.002732\n",
      "Epoch: 217, Loss: 2479.07915, Residuals: -0.98364, Convergence: 0.002266\n",
      "Epoch: 218, Loss: 2474.31793, Residuals: -0.98142, Convergence: 0.001924\n",
      "Epoch: 219, Loss: 2470.23165, Residuals: -0.97955, Convergence: 0.001654\n",
      "Epoch: 220, Loss: 2466.68233, Residuals: -0.97797, Convergence: 0.001439\n",
      "Epoch: 221, Loss: 2463.56369, Residuals: -0.97662, Convergence: 0.001266\n",
      "Epoch: 222, Loss: 2460.79377, Residuals: -0.97547, Convergence: 0.001126\n",
      "Epoch: 223, Loss: 2458.30801, Residuals: -0.97448, Convergence: 0.001011\n",
      "Epoch: 224, Loss: 2456.05662, Residuals: -0.97363, Convergence: 0.000917\n",
      "Evidence 14790.154\n",
      "\n",
      "Epoch: 224, Evidence: 14790.15430, Convergence: 0.005744\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.61e-01\n",
      "Epoch: 224, Loss: 2492.81559, Residuals: -0.97363, Convergence:   inf\n",
      "Epoch: 225, Loss: 2488.80628, Residuals: -0.97139, Convergence: 0.001611\n",
      "Epoch: 226, Loss: 2485.49227, Residuals: -0.96966, Convergence: 0.001333\n",
      "Epoch: 227, Loss: 2482.67068, Residuals: -0.96829, Convergence: 0.001137\n",
      "Epoch: 228, Loss: 2480.22099, Residuals: -0.96717, Convergence: 0.000988\n",
      "Evidence 14820.035\n",
      "\n",
      "Epoch: 228, Evidence: 14820.03516, Convergence: 0.002016\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.10e-01\n",
      "Epoch: 228, Loss: 2493.77705, Residuals: -0.96717, Convergence:   inf\n",
      "Epoch: 229, Loss: 2490.80252, Residuals: -0.96551, Convergence: 0.001194\n",
      "Epoch: 230, Loss: 2488.33190, Residuals: -0.96422, Convergence: 0.000993\n",
      "Evidence 14832.436\n",
      "\n",
      "Epoch: 230, Evidence: 14832.43555, Convergence: 0.000836\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.74e-01\n",
      "Epoch: 230, Loss: 2494.47517, Residuals: -0.96422, Convergence:   inf\n",
      "Epoch: 231, Loss: 2489.94787, Residuals: -0.96237, Convergence: 0.001818\n",
      "Epoch: 232, Loss: 2486.43284, Residuals: -0.96070, Convergence: 0.001414\n",
      "Epoch: 233, Loss: 2483.59661, Residuals: -0.95945, Convergence: 0.001142\n",
      "Epoch: 234, Loss: 2481.20437, Residuals: -0.95867, Convergence: 0.000964\n",
      "Evidence 14850.394\n",
      "\n",
      "Epoch: 234, Evidence: 14850.39355, Convergence: 0.002044\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.44e-01\n",
      "Epoch: 234, Loss: 2494.56316, Residuals: -0.95867, Convergence:   inf\n",
      "Epoch: 235, Loss: 2491.52629, Residuals: -0.95648, Convergence: 0.001219\n",
      "Epoch: 236, Loss: 2489.12427, Residuals: -0.95522, Convergence: 0.000965\n",
      "Evidence 14861.509\n",
      "\n",
      "Epoch: 236, Evidence: 14861.50879, Convergence: 0.000748\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.22e-01\n",
      "Epoch: 236, Loss: 2494.72223, Residuals: -0.95522, Convergence:   inf\n",
      "Epoch: 237, Loss: 2490.26208, Residuals: -0.95207, Convergence: 0.001791\n",
      "Epoch: 238, Loss: 2487.12603, Residuals: -0.95265, Convergence: 0.001261\n",
      "Epoch: 239, Loss: 2484.53655, Residuals: -0.95305, Convergence: 0.001042\n",
      "Epoch: 240, Loss: 2482.26982, Residuals: -0.95556, Convergence: 0.000913\n",
      "Evidence 14877.279\n",
      "\n",
      "Epoch: 240, Evidence: 14877.27930, Convergence: 0.001807\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.09e-01\n",
      "Epoch: 240, Loss: 2493.99577, Residuals: -0.95556, Convergence:   inf\n",
      "Epoch: 241, Loss: 2492.22493, Residuals: -0.95272, Convergence: 0.000711\n",
      "Evidence 14883.963\n",
      "\n",
      "Epoch: 241, Evidence: 14883.96289, Convergence: 0.000449\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 8.88e-02\n",
      "Epoch: 241, Loss: 2494.96544, Residuals: -0.95272, Convergence:   inf\n",
      "Epoch: 242, Loss: 2537.42262, Residuals: -0.99446, Convergence: -0.016732\n",
      "Epoch: 242, Loss: 2492.64724, Residuals: -0.95152, Convergence: 0.000930\n",
      "Evidence 14888.370\n",
      "\n",
      "Epoch: 242, Evidence: 14888.37012, Convergence: 0.000745\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 8.22e-02\n",
      "Epoch: 242, Loss: 2494.37711, Residuals: -0.95152, Convergence:   inf\n",
      "Epoch: 243, Loss: 2499.69398, Residuals: -0.95382, Convergence: -0.002127\n",
      "Epoch: 243, Loss: 2494.37763, Residuals: -0.95011, Convergence: -0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence 14890.024\n",
      "\n",
      "Epoch: 243, Evidence: 14890.02441, Convergence: 0.000856\n",
      "Total samples: 183, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 381.75287, Residuals: -4.50038, Convergence:   inf\n",
      "Epoch: 1, Loss: 356.01519, Residuals: -4.38132, Convergence: 0.072294\n",
      "Epoch: 2, Loss: 334.90851, Residuals: -4.21773, Convergence: 0.063022\n",
      "Epoch: 3, Loss: 318.81662, Residuals: -4.05393, Convergence: 0.050474\n",
      "Epoch: 4, Loss: 306.54082, Residuals: -3.90945, Convergence: 0.040046\n",
      "Epoch: 5, Loss: 296.80170, Residuals: -3.78160, Convergence: 0.032814\n",
      "Epoch: 6, Loss: 288.89391, Residuals: -3.67023, Convergence: 0.027373\n",
      "Epoch: 7, Loss: 282.32893, Residuals: -3.57494, Convergence: 0.023253\n",
      "Epoch: 8, Loss: 276.74536, Residuals: -3.49366, Convergence: 0.020176\n",
      "Epoch: 9, Loss: 271.88825, Residuals: -3.42405, Convergence: 0.017864\n",
      "Epoch: 10, Loss: 267.57640, Residuals: -3.36398, Convergence: 0.016114\n",
      "Epoch: 11, Loss: 263.67874, Residuals: -3.31165, Convergence: 0.014782\n",
      "Epoch: 12, Loss: 260.09985, Residuals: -3.26556, Convergence: 0.013760\n",
      "Epoch: 13, Loss: 256.77038, Residuals: -3.22437, Convergence: 0.012967\n",
      "Epoch: 14, Loss: 253.64043, Residuals: -3.18695, Convergence: 0.012340\n",
      "Epoch: 15, Loss: 250.67576, Residuals: -3.15233, Convergence: 0.011827\n",
      "Epoch: 16, Loss: 247.85627, Residuals: -3.11980, Convergence: 0.011376\n",
      "Epoch: 17, Loss: 245.16965, Residuals: -3.08897, Convergence: 0.010958\n",
      "Epoch: 18, Loss: 242.59844, Residuals: -3.05950, Convergence: 0.010599\n",
      "Epoch: 19, Loss: 240.11384, Residuals: -3.03092, Convergence: 0.010348\n",
      "Epoch: 20, Loss: 237.68085, Residuals: -3.00269, Convergence: 0.010236\n",
      "Epoch: 21, Loss: 235.26665, Residuals: -2.97430, Convergence: 0.010262\n",
      "Epoch: 22, Loss: 232.84664, Residuals: -2.94536, Convergence: 0.010393\n",
      "Epoch: 23, Loss: 230.40122, Residuals: -2.91562, Convergence: 0.010614\n",
      "Epoch: 24, Loss: 227.90102, Residuals: -2.88477, Convergence: 0.010971\n",
      "Epoch: 25, Loss: 225.29687, Residuals: -2.85227, Convergence: 0.011559\n",
      "Epoch: 26, Loss: 222.54452, Residuals: -2.81755, Convergence: 0.012368\n",
      "Epoch: 27, Loss: 219.68626, Residuals: -2.78100, Convergence: 0.013011\n",
      "Epoch: 28, Loss: 216.85072, Residuals: -2.74402, Convergence: 0.013076\n",
      "Epoch: 29, Loss: 214.10724, Residuals: -2.70752, Convergence: 0.012814\n",
      "Epoch: 30, Loss: 211.45642, Residuals: -2.67164, Convergence: 0.012536\n",
      "Epoch: 31, Loss: 208.88242, Residuals: -2.63627, Convergence: 0.012323\n",
      "Epoch: 32, Loss: 206.37142, Residuals: -2.60129, Convergence: 0.012167\n",
      "Epoch: 33, Loss: 203.91428, Residuals: -2.56660, Convergence: 0.012050\n",
      "Epoch: 34, Loss: 201.50587, Residuals: -2.53212, Convergence: 0.011952\n",
      "Epoch: 35, Loss: 199.14394, Residuals: -2.49780, Convergence: 0.011860\n",
      "Epoch: 36, Loss: 196.82807, Residuals: -2.46361, Convergence: 0.011766\n",
      "Epoch: 37, Loss: 194.55893, Residuals: -2.42952, Convergence: 0.011663\n",
      "Epoch: 38, Loss: 192.33767, Residuals: -2.39554, Convergence: 0.011549\n",
      "Epoch: 39, Loss: 190.16564, Residuals: -2.36165, Convergence: 0.011422\n",
      "Epoch: 40, Loss: 188.04417, Residuals: -2.32786, Convergence: 0.011282\n",
      "Epoch: 41, Loss: 185.97454, Residuals: -2.29418, Convergence: 0.011129\n",
      "Epoch: 42, Loss: 183.95809, Residuals: -2.26063, Convergence: 0.010961\n",
      "Epoch: 43, Loss: 181.99627, Residuals: -2.22723, Convergence: 0.010779\n",
      "Epoch: 44, Loss: 180.09085, Residuals: -2.19402, Convergence: 0.010580\n",
      "Epoch: 45, Loss: 178.24386, Residuals: -2.16104, Convergence: 0.010362\n",
      "Epoch: 46, Loss: 176.45754, Residuals: -2.12833, Convergence: 0.010123\n",
      "Epoch: 47, Loss: 174.73411, Residuals: -2.09597, Convergence: 0.009863\n",
      "Epoch: 48, Loss: 173.07546, Residuals: -2.06401, Convergence: 0.009583\n",
      "Epoch: 49, Loss: 171.48300, Residuals: -2.03251, Convergence: 0.009286\n",
      "Epoch: 50, Loss: 169.95745, Residuals: -2.00153, Convergence: 0.008976\n",
      "Epoch: 51, Loss: 168.49886, Residuals: -1.97112, Convergence: 0.008656\n",
      "Epoch: 52, Loss: 167.10662, Residuals: -1.94133, Convergence: 0.008331\n",
      "Epoch: 53, Loss: 165.77957, Residuals: -1.91220, Convergence: 0.008005\n",
      "Epoch: 54, Loss: 164.51603, Residuals: -1.88378, Convergence: 0.007680\n",
      "Epoch: 55, Loss: 163.31386, Residuals: -1.85609, Convergence: 0.007361\n",
      "Epoch: 56, Loss: 162.17052, Residuals: -1.82914, Convergence: 0.007050\n",
      "Epoch: 57, Loss: 161.08311, Residuals: -1.80296, Convergence: 0.006751\n",
      "Epoch: 58, Loss: 160.04842, Residuals: -1.77754, Convergence: 0.006465\n",
      "Epoch: 59, Loss: 159.06304, Residuals: -1.75287, Convergence: 0.006195\n",
      "Epoch: 60, Loss: 158.12345, Residuals: -1.72894, Convergence: 0.005942\n",
      "Epoch: 61, Loss: 157.22619, Residuals: -1.70572, Convergence: 0.005707\n",
      "Epoch: 62, Loss: 156.36808, Residuals: -1.68319, Convergence: 0.005488\n",
      "Epoch: 63, Loss: 155.54628, Residuals: -1.66131, Convergence: 0.005283\n",
      "Epoch: 64, Loss: 154.75842, Residuals: -1.64006, Convergence: 0.005091\n",
      "Epoch: 65, Loss: 154.00261, Residuals: -1.61941, Convergence: 0.004908\n",
      "Epoch: 66, Loss: 153.27740, Residuals: -1.59936, Convergence: 0.004731\n",
      "Epoch: 67, Loss: 152.58168, Residuals: -1.57988, Convergence: 0.004560\n",
      "Epoch: 68, Loss: 151.91462, Residuals: -1.56099, Convergence: 0.004391\n",
      "Epoch: 69, Loss: 151.27553, Residuals: -1.54267, Convergence: 0.004225\n",
      "Epoch: 70, Loss: 150.66382, Residuals: -1.52492, Convergence: 0.004060\n",
      "Epoch: 71, Loss: 150.07891, Residuals: -1.50775, Convergence: 0.003897\n",
      "Epoch: 72, Loss: 149.52019, Residuals: -1.49115, Convergence: 0.003737\n",
      "Epoch: 73, Loss: 148.98702, Residuals: -1.47512, Convergence: 0.003579\n",
      "Epoch: 74, Loss: 148.47869, Residuals: -1.45966, Convergence: 0.003424\n",
      "Epoch: 75, Loss: 147.99443, Residuals: -1.44476, Convergence: 0.003272\n",
      "Epoch: 76, Loss: 147.53342, Residuals: -1.43042, Convergence: 0.003125\n",
      "Epoch: 77, Loss: 147.09477, Residuals: -1.41661, Convergence: 0.002982\n",
      "Epoch: 78, Loss: 146.67760, Residuals: -1.40334, Convergence: 0.002844\n",
      "Epoch: 79, Loss: 146.28099, Residuals: -1.39059, Convergence: 0.002711\n",
      "Epoch: 80, Loss: 145.90401, Residuals: -1.37835, Convergence: 0.002584\n",
      "Epoch: 81, Loss: 145.54578, Residuals: -1.36659, Convergence: 0.002461\n",
      "Epoch: 82, Loss: 145.20538, Residuals: -1.35532, Convergence: 0.002344\n",
      "Epoch: 83, Loss: 144.88197, Residuals: -1.34450, Convergence: 0.002232\n",
      "Epoch: 84, Loss: 144.57473, Residuals: -1.33413, Convergence: 0.002125\n",
      "Epoch: 85, Loss: 144.28286, Residuals: -1.32419, Convergence: 0.002023\n",
      "Epoch: 86, Loss: 144.00565, Residuals: -1.31466, Convergence: 0.001925\n",
      "Epoch: 87, Loss: 143.74240, Residuals: -1.30552, Convergence: 0.001831\n",
      "Epoch: 88, Loss: 143.49248, Residuals: -1.29677, Convergence: 0.001742\n",
      "Epoch: 89, Loss: 143.25527, Residuals: -1.28839, Convergence: 0.001656\n",
      "Epoch: 90, Loss: 143.03025, Residuals: -1.28036, Convergence: 0.001573\n",
      "Epoch: 91, Loss: 142.81691, Residuals: -1.27267, Convergence: 0.001494\n",
      "Epoch: 92, Loss: 142.61478, Residuals: -1.26530, Convergence: 0.001417\n",
      "Epoch: 93, Loss: 142.42345, Residuals: -1.25825, Convergence: 0.001343\n",
      "Epoch: 94, Loss: 142.24251, Residuals: -1.25150, Convergence: 0.001272\n",
      "Epoch: 95, Loss: 142.07161, Residuals: -1.24505, Convergence: 0.001203\n",
      "Epoch: 96, Loss: 141.91040, Residuals: -1.23887, Convergence: 0.001136\n",
      "Epoch: 97, Loss: 141.75857, Residuals: -1.23298, Convergence: 0.001071\n",
      "Epoch: 98, Loss: 141.61581, Residuals: -1.22735, Convergence: 0.001008\n",
      "Epoch: 99, Loss: 141.48177, Residuals: -1.22199, Convergence: 0.000947\n",
      "Evidence -182.795\n",
      "\n",
      "Epoch: 99, Evidence: -182.79523, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.24e-01\n",
      "Epoch: 99, Loss: 1367.95212, Residuals: -1.22199, Convergence:   inf\n",
      "Epoch: 100, Loss: 1306.33872, Residuals: -1.25125, Convergence: 0.047165\n",
      "Epoch: 101, Loss: 1259.44358, Residuals: -1.27438, Convergence: 0.037235\n",
      "Epoch: 102, Loss: 1224.00067, Residuals: -1.29128, Convergence: 0.028957\n",
      "Epoch: 103, Loss: 1196.46563, Residuals: -1.30334, Convergence: 0.023014\n",
      "Epoch: 104, Loss: 1174.24983, Residuals: -1.31231, Convergence: 0.018919\n",
      "Epoch: 105, Loss: 1155.84360, Residuals: -1.31918, Convergence: 0.015924\n",
      "Epoch: 106, Loss: 1140.33473, Residuals: -1.32442, Convergence: 0.013600\n",
      "Epoch: 107, Loss: 1127.10469, Residuals: -1.32826, Convergence: 0.011738\n",
      "Epoch: 108, Loss: 1115.69364, Residuals: -1.33087, Convergence: 0.010228\n",
      "Epoch: 109, Loss: 1105.74280, Residuals: -1.33236, Convergence: 0.008999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 110, Loss: 1096.96233, Residuals: -1.33284, Convergence: 0.008004\n",
      "Epoch: 111, Loss: 1089.11258, Residuals: -1.33240, Convergence: 0.007207\n",
      "Epoch: 112, Loss: 1081.99157, Residuals: -1.33110, Convergence: 0.006581\n",
      "Epoch: 113, Loss: 1075.42644, Residuals: -1.32902, Convergence: 0.006105\n",
      "Epoch: 114, Loss: 1069.26601, Residuals: -1.32617, Convergence: 0.005761\n",
      "Epoch: 115, Loss: 1063.37846, Residuals: -1.32258, Convergence: 0.005537\n",
      "Epoch: 116, Loss: 1057.64846, Residuals: -1.31827, Convergence: 0.005418\n",
      "Epoch: 117, Loss: 1051.97843, Residuals: -1.31326, Convergence: 0.005390\n",
      "Epoch: 118, Loss: 1046.29427, Residuals: -1.30757, Convergence: 0.005433\n",
      "Epoch: 119, Loss: 1040.55630, Residuals: -1.30125, Convergence: 0.005514\n",
      "Epoch: 120, Loss: 1034.77532, Residuals: -1.29438, Convergence: 0.005587\n",
      "Epoch: 121, Loss: 1029.01739, Residuals: -1.28707, Convergence: 0.005596\n",
      "Epoch: 122, Loss: 1023.38895, Residuals: -1.27942, Convergence: 0.005500\n",
      "Epoch: 123, Loss: 1017.99859, Residuals: -1.27152, Convergence: 0.005295\n",
      "Epoch: 124, Loss: 1012.92267, Residuals: -1.26346, Convergence: 0.005011\n",
      "Epoch: 125, Loss: 1008.19397, Residuals: -1.25533, Convergence: 0.004690\n",
      "Epoch: 126, Loss: 1003.81039, Residuals: -1.24717, Convergence: 0.004367\n",
      "Epoch: 127, Loss: 999.74925, Residuals: -1.23906, Convergence: 0.004062\n",
      "Epoch: 128, Loss: 995.97961, Residuals: -1.23104, Convergence: 0.003785\n",
      "Epoch: 129, Loss: 992.46980, Residuals: -1.22315, Convergence: 0.003536\n",
      "Epoch: 130, Loss: 989.19082, Residuals: -1.21542, Convergence: 0.003315\n",
      "Epoch: 131, Loss: 986.11744, Residuals: -1.20789, Convergence: 0.003117\n",
      "Epoch: 132, Loss: 983.22853, Residuals: -1.20057, Convergence: 0.002938\n",
      "Epoch: 133, Loss: 980.50614, Residuals: -1.19347, Convergence: 0.002777\n",
      "Epoch: 134, Loss: 977.93545, Residuals: -1.18662, Convergence: 0.002629\n",
      "Epoch: 135, Loss: 975.50348, Residuals: -1.18001, Convergence: 0.002493\n",
      "Epoch: 136, Loss: 973.19931, Residuals: -1.17366, Convergence: 0.002368\n",
      "Epoch: 137, Loss: 971.01361, Residuals: -1.16757, Convergence: 0.002251\n",
      "Epoch: 138, Loss: 968.93759, Residuals: -1.16174, Convergence: 0.002143\n",
      "Epoch: 139, Loss: 966.96328, Residuals: -1.15616, Convergence: 0.002042\n",
      "Epoch: 140, Loss: 965.08357, Residuals: -1.15083, Convergence: 0.001948\n",
      "Epoch: 141, Loss: 963.29182, Residuals: -1.14575, Convergence: 0.001860\n",
      "Epoch: 142, Loss: 961.58154, Residuals: -1.14090, Convergence: 0.001779\n",
      "Epoch: 143, Loss: 959.94695, Residuals: -1.13629, Convergence: 0.001703\n",
      "Epoch: 144, Loss: 958.38249, Residuals: -1.13189, Convergence: 0.001632\n",
      "Epoch: 145, Loss: 956.88255, Residuals: -1.12770, Convergence: 0.001568\n",
      "Epoch: 146, Loss: 955.44259, Residuals: -1.12372, Convergence: 0.001507\n",
      "Epoch: 147, Loss: 954.05758, Residuals: -1.11992, Convergence: 0.001452\n",
      "Epoch: 148, Loss: 952.72297, Residuals: -1.11630, Convergence: 0.001401\n",
      "Epoch: 149, Loss: 951.43467, Residuals: -1.11285, Convergence: 0.001354\n",
      "Epoch: 150, Loss: 950.18798, Residuals: -1.10956, Convergence: 0.001312\n",
      "Epoch: 151, Loss: 948.97946, Residuals: -1.10641, Convergence: 0.001273\n",
      "Epoch: 152, Loss: 947.80488, Residuals: -1.10341, Convergence: 0.001239\n",
      "Epoch: 153, Loss: 946.66041, Residuals: -1.10052, Convergence: 0.001209\n",
      "Epoch: 154, Loss: 945.54246, Residuals: -1.09775, Convergence: 0.001182\n",
      "Epoch: 155, Loss: 944.44716, Residuals: -1.09509, Convergence: 0.001160\n",
      "Epoch: 156, Loss: 943.37019, Residuals: -1.09252, Convergence: 0.001142\n",
      "Epoch: 157, Loss: 942.30834, Residuals: -1.09003, Convergence: 0.001127\n",
      "Epoch: 158, Loss: 941.25734, Residuals: -1.08760, Convergence: 0.001117\n",
      "Epoch: 159, Loss: 940.21317, Residuals: -1.08524, Convergence: 0.001111\n",
      "Epoch: 160, Loss: 939.17224, Residuals: -1.08292, Convergence: 0.001108\n",
      "Epoch: 161, Loss: 938.13148, Residuals: -1.08064, Convergence: 0.001109\n",
      "Epoch: 162, Loss: 937.08829, Residuals: -1.07838, Convergence: 0.001113\n",
      "Epoch: 163, Loss: 936.04199, Residuals: -1.07614, Convergence: 0.001118\n",
      "Epoch: 164, Loss: 934.99324, Residuals: -1.07391, Convergence: 0.001122\n",
      "Epoch: 165, Loss: 933.94477, Residuals: -1.07170, Convergence: 0.001123\n",
      "Epoch: 166, Loss: 932.90106, Residuals: -1.06951, Convergence: 0.001119\n",
      "Epoch: 167, Loss: 931.86787, Residuals: -1.06735, Convergence: 0.001109\n",
      "Epoch: 168, Loss: 930.85141, Residuals: -1.06522, Convergence: 0.001092\n",
      "Epoch: 169, Loss: 929.85739, Residuals: -1.06315, Convergence: 0.001069\n",
      "Epoch: 170, Loss: 928.89124, Residuals: -1.06113, Convergence: 0.001040\n",
      "Epoch: 171, Loss: 927.95647, Residuals: -1.05917, Convergence: 0.001007\n",
      "Epoch: 172, Loss: 927.05622, Residuals: -1.05727, Convergence: 0.000971\n",
      "Evidence 11208.454\n",
      "\n",
      "Epoch: 172, Evidence: 11208.45410, Convergence: 1.016309\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 5.75e-01\n",
      "Epoch: 172, Loss: 2356.07789, Residuals: -1.05727, Convergence:   inf\n",
      "Epoch: 173, Loss: 2315.84218, Residuals: -1.06356, Convergence: 0.017374\n",
      "Epoch: 174, Loss: 2288.22094, Residuals: -1.06272, Convergence: 0.012071\n",
      "Epoch: 175, Loss: 2265.43204, Residuals: -1.06101, Convergence: 0.010059\n",
      "Epoch: 176, Loss: 2246.34143, Residuals: -1.05907, Convergence: 0.008499\n",
      "Epoch: 177, Loss: 2230.21842, Residuals: -1.05700, Convergence: 0.007229\n",
      "Epoch: 178, Loss: 2216.50006, Residuals: -1.05487, Convergence: 0.006189\n",
      "Epoch: 179, Loss: 2204.72712, Residuals: -1.05269, Convergence: 0.005340\n",
      "Epoch: 180, Loss: 2194.51995, Residuals: -1.05045, Convergence: 0.004651\n",
      "Epoch: 181, Loss: 2185.56323, Residuals: -1.04817, Convergence: 0.004098\n",
      "Epoch: 182, Loss: 2177.59534, Residuals: -1.04582, Convergence: 0.003659\n",
      "Epoch: 183, Loss: 2170.40448, Residuals: -1.04338, Convergence: 0.003313\n",
      "Epoch: 184, Loss: 2163.82820, Residuals: -1.04083, Convergence: 0.003039\n",
      "Epoch: 185, Loss: 2157.75323, Residuals: -1.03818, Convergence: 0.002815\n",
      "Epoch: 186, Loss: 2152.10990, Residuals: -1.03543, Convergence: 0.002622\n",
      "Epoch: 187, Loss: 2146.85705, Residuals: -1.03262, Convergence: 0.002447\n",
      "Epoch: 188, Loss: 2141.96980, Residuals: -1.02980, Convergence: 0.002282\n",
      "Epoch: 189, Loss: 2137.42472, Residuals: -1.02699, Convergence: 0.002126\n",
      "Epoch: 190, Loss: 2133.19786, Residuals: -1.02425, Convergence: 0.001981\n",
      "Epoch: 191, Loss: 2129.26418, Residuals: -1.02158, Convergence: 0.001847\n",
      "Epoch: 192, Loss: 2125.59727, Residuals: -1.01900, Convergence: 0.001725\n",
      "Epoch: 193, Loss: 2122.17467, Residuals: -1.01652, Convergence: 0.001613\n",
      "Epoch: 194, Loss: 2118.97505, Residuals: -1.01414, Convergence: 0.001510\n",
      "Epoch: 195, Loss: 2115.98039, Residuals: -1.01187, Convergence: 0.001415\n",
      "Epoch: 196, Loss: 2113.17538, Residuals: -1.00971, Convergence: 0.001327\n",
      "Epoch: 197, Loss: 2110.54669, Residuals: -1.00765, Convergence: 0.001246\n",
      "Epoch: 198, Loss: 2108.08198, Residuals: -1.00570, Convergence: 0.001169\n",
      "Epoch: 199, Loss: 2105.77161, Residuals: -1.00384, Convergence: 0.001097\n",
      "Epoch: 200, Loss: 2103.60505, Residuals: -1.00207, Convergence: 0.001030\n",
      "Epoch: 201, Loss: 2101.57194, Residuals: -1.00039, Convergence: 0.000967\n",
      "Evidence 14393.744\n",
      "\n",
      "Epoch: 201, Evidence: 14393.74414, Convergence: 0.221297\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 4.37e-01\n",
      "Epoch: 201, Loss: 2483.48279, Residuals: -1.00039, Convergence:   inf\n",
      "Epoch: 202, Loss: 2469.97458, Residuals: -0.99785, Convergence: 0.005469\n",
      "Epoch: 203, Loss: 2458.87358, Residuals: -0.99492, Convergence: 0.004515\n",
      "Epoch: 204, Loss: 2449.28651, Residuals: -0.99205, Convergence: 0.003914\n",
      "Epoch: 205, Loss: 2440.94980, Residuals: -0.98934, Convergence: 0.003415\n",
      "Epoch: 206, Loss: 2433.66331, Residuals: -0.98682, Convergence: 0.002994\n",
      "Epoch: 207, Loss: 2427.26392, Residuals: -0.98451, Convergence: 0.002636\n",
      "Epoch: 208, Loss: 2421.61967, Residuals: -0.98239, Convergence: 0.002331\n",
      "Epoch: 209, Loss: 2416.61899, Residuals: -0.98044, Convergence: 0.002069\n",
      "Epoch: 210, Loss: 2412.16759, Residuals: -0.97866, Convergence: 0.001845\n",
      "Epoch: 211, Loss: 2408.18695, Residuals: -0.97703, Convergence: 0.001653\n",
      "Epoch: 212, Loss: 2404.60983, Residuals: -0.97552, Convergence: 0.001488\n",
      "Epoch: 213, Loss: 2401.37930, Residuals: -0.97413, Convergence: 0.001345\n",
      "Epoch: 214, Loss: 2398.44803, Residuals: -0.97285, Convergence: 0.001222\n",
      "Epoch: 215, Loss: 2395.77525, Residuals: -0.97166, Convergence: 0.001116\n",
      "Epoch: 216, Loss: 2393.32877, Residuals: -0.97056, Convergence: 0.001022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 217, Loss: 2391.07795, Residuals: -0.96955, Convergence: 0.000941\n",
      "Evidence 14782.588\n",
      "\n",
      "Epoch: 217, Evidence: 14782.58789, Convergence: 0.026304\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 3.33e-01\n",
      "Epoch: 217, Loss: 2488.23129, Residuals: -0.96955, Convergence:   inf\n",
      "Epoch: 218, Loss: 2481.70750, Residuals: -0.96663, Convergence: 0.002629\n",
      "Epoch: 219, Loss: 2476.33976, Residuals: -0.96413, Convergence: 0.002168\n",
      "Epoch: 220, Loss: 2471.80569, Residuals: -0.96204, Convergence: 0.001834\n",
      "Epoch: 221, Loss: 2467.91745, Residuals: -0.96028, Convergence: 0.001576\n",
      "Epoch: 222, Loss: 2464.53639, Residuals: -0.95881, Convergence: 0.001372\n",
      "Epoch: 223, Loss: 2461.56144, Residuals: -0.95758, Convergence: 0.001209\n",
      "Epoch: 224, Loss: 2458.91060, Residuals: -0.95656, Convergence: 0.001078\n",
      "Epoch: 225, Loss: 2456.52668, Residuals: -0.95570, Convergence: 0.000970\n",
      "Evidence 14865.362\n",
      "\n",
      "Epoch: 225, Evidence: 14865.36230, Convergence: 0.005568\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.61e-01\n",
      "Epoch: 225, Loss: 2489.77514, Residuals: -0.95570, Convergence:   inf\n",
      "Epoch: 226, Loss: 2485.87045, Residuals: -0.95352, Convergence: 0.001571\n",
      "Epoch: 227, Loss: 2482.64359, Residuals: -0.95184, Convergence: 0.001300\n",
      "Epoch: 228, Loss: 2479.88924, Residuals: -0.95051, Convergence: 0.001111\n",
      "Epoch: 229, Loss: 2477.48822, Residuals: -0.94947, Convergence: 0.000969\n",
      "Evidence 14893.342\n",
      "\n",
      "Epoch: 229, Evidence: 14893.34180, Convergence: 0.001879\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.10e-01\n",
      "Epoch: 229, Loss: 2490.68461, Residuals: -0.94947, Convergence:   inf\n",
      "Epoch: 230, Loss: 2487.81183, Residuals: -0.94778, Convergence: 0.001155\n",
      "Epoch: 231, Loss: 2485.41107, Residuals: -0.94648, Convergence: 0.000966\n",
      "Evidence 14905.454\n",
      "\n",
      "Epoch: 231, Evidence: 14905.45410, Convergence: 0.000813\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.73e-01\n",
      "Epoch: 231, Loss: 2491.33424, Residuals: -0.94648, Convergence:   inf\n",
      "Epoch: 232, Loss: 2486.85949, Residuals: -0.94425, Convergence: 0.001799\n",
      "Epoch: 233, Loss: 2483.40968, Residuals: -0.94264, Convergence: 0.001389\n",
      "Epoch: 234, Loss: 2480.60824, Residuals: -0.94168, Convergence: 0.001129\n",
      "Epoch: 235, Loss: 2478.23142, Residuals: -0.94125, Convergence: 0.000959\n",
      "Evidence 14923.073\n",
      "\n",
      "Epoch: 235, Evidence: 14923.07324, Convergence: 0.001992\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.43e-01\n",
      "Epoch: 235, Loss: 2491.35295, Residuals: -0.94125, Convergence:   inf\n",
      "Epoch: 236, Loss: 2488.34447, Residuals: -0.93894, Convergence: 0.001209\n",
      "Epoch: 237, Loss: 2485.96696, Residuals: -0.93785, Convergence: 0.000956\n",
      "Evidence 14934.002\n",
      "\n",
      "Epoch: 237, Evidence: 14934.00195, Convergence: 0.000732\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.21e-01\n",
      "Epoch: 237, Loss: 2491.47696, Residuals: -0.93785, Convergence:   inf\n",
      "Epoch: 238, Loss: 2487.06238, Residuals: -0.93446, Convergence: 0.001775\n",
      "Epoch: 239, Loss: 2483.95207, Residuals: -0.93586, Convergence: 0.001252\n",
      "Epoch: 240, Loss: 2481.36810, Residuals: -0.93673, Convergence: 0.001041\n",
      "Epoch: 241, Loss: 2479.11540, Residuals: -0.93971, Convergence: 0.000909\n",
      "Evidence 14949.631\n",
      "\n",
      "Epoch: 241, Evidence: 14949.63086, Convergence: 0.001776\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.07e-01\n",
      "Epoch: 241, Loss: 2490.65893, Residuals: -0.93971, Convergence:   inf\n",
      "Epoch: 242, Loss: 2488.85576, Residuals: -0.93676, Convergence: 0.000724\n",
      "Evidence 14956.502\n",
      "\n",
      "Epoch: 242, Evidence: 14956.50195, Convergence: 0.000459\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 8.80e-02\n",
      "Epoch: 242, Loss: 2491.56326, Residuals: -0.93676, Convergence:   inf\n",
      "Epoch: 243, Loss: 2531.42789, Residuals: -0.97335, Convergence: -0.015748\n",
      "Epoch: 243, Loss: 2489.35014, Residuals: -0.93495, Convergence: 0.000889\n",
      "Evidence 14960.764\n",
      "\n",
      "Epoch: 243, Evidence: 14960.76367, Convergence: 0.000744\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 8.11e-02\n",
      "Epoch: 243, Loss: 2490.95716, Residuals: -0.93495, Convergence:   inf\n",
      "Epoch: 244, Loss: 2496.60950, Residuals: -0.93568, Convergence: -0.002264\n",
      "Epoch: 244, Loss: 2491.06345, Residuals: -0.93300, Convergence: -0.000043\n",
      "Evidence 14962.329\n",
      "\n",
      "Epoch: 244, Evidence: 14962.32910, Convergence: 0.000849\n",
      "Total samples: 181, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 383.08513, Residuals: -4.55782, Convergence:   inf\n",
      "Epoch: 1, Loss: 357.32361, Residuals: -4.43684, Convergence: 0.072096\n",
      "Epoch: 2, Loss: 336.20812, Residuals: -4.27133, Convergence: 0.062805\n",
      "Epoch: 3, Loss: 320.08263, Residuals: -4.10552, Convergence: 0.050379\n",
      "Epoch: 4, Loss: 307.78405, Residuals: -3.95927, Convergence: 0.039958\n",
      "Epoch: 5, Loss: 298.02952, Residuals: -3.82990, Convergence: 0.032730\n",
      "Epoch: 6, Loss: 290.11199, Residuals: -3.71718, Convergence: 0.027291\n",
      "Epoch: 7, Loss: 283.54323, Residuals: -3.62066, Convergence: 0.023167\n",
      "Epoch: 8, Loss: 277.96108, Residuals: -3.53829, Convergence: 0.020083\n",
      "Epoch: 9, Loss: 273.10961, Residuals: -3.46771, Convergence: 0.017764\n",
      "Epoch: 10, Loss: 268.80682, Residuals: -3.40679, Convergence: 0.016007\n",
      "Epoch: 11, Loss: 264.92085, Residuals: -3.35375, Convergence: 0.014668\n",
      "Epoch: 12, Loss: 261.35541, Residuals: -3.30706, Convergence: 0.013642\n",
      "Epoch: 13, Loss: 258.04033, Residuals: -3.26541, Convergence: 0.012847\n",
      "Epoch: 14, Loss: 254.92499, Residuals: -3.22764, Convergence: 0.012221\n",
      "Epoch: 15, Loss: 251.97480, Residuals: -3.19278, Convergence: 0.011708\n",
      "Epoch: 16, Loss: 249.16976, Residuals: -3.16012, Convergence: 0.011258\n",
      "Epoch: 17, Loss: 246.49766, Residuals: -3.12924, Convergence: 0.010840\n",
      "Epoch: 18, Loss: 243.94055, Residuals: -3.09977, Convergence: 0.010483\n",
      "Epoch: 19, Loss: 241.46862, Residuals: -3.07121, Convergence: 0.010237\n",
      "Epoch: 20, Loss: 239.04562, Residuals: -3.04299, Convergence: 0.010136\n",
      "Epoch: 21, Loss: 236.63748, Residuals: -3.01453, Convergence: 0.010176\n",
      "Epoch: 22, Loss: 234.21834, Residuals: -2.98543, Convergence: 0.010329\n",
      "Epoch: 23, Loss: 231.76696, Residuals: -2.95541, Convergence: 0.010577\n",
      "Epoch: 24, Loss: 229.25095, Residuals: -2.92409, Convergence: 0.010975\n",
      "Epoch: 25, Loss: 226.61731, Residuals: -2.89088, Convergence: 0.011622\n",
      "Epoch: 26, Loss: 223.82595, Residuals: -2.85526, Convergence: 0.012471\n",
      "Epoch: 27, Loss: 220.94170, Residuals: -2.81783, Convergence: 0.013054\n",
      "Epoch: 28, Loss: 218.10261, Residuals: -2.78013, Convergence: 0.013017\n",
      "Epoch: 29, Loss: 215.36768, Residuals: -2.74298, Convergence: 0.012699\n",
      "Epoch: 30, Loss: 212.73210, Residuals: -2.70647, Convergence: 0.012389\n",
      "Epoch: 31, Loss: 210.17872, Residuals: -2.67052, Convergence: 0.012149\n",
      "Epoch: 32, Loss: 207.69297, Residuals: -2.63501, Convergence: 0.011968\n",
      "Epoch: 33, Loss: 205.26480, Residuals: -2.59986, Convergence: 0.011829\n",
      "Epoch: 34, Loss: 202.88789, Residuals: -2.56501, Convergence: 0.011715\n",
      "Epoch: 35, Loss: 200.55877, Residuals: -2.53039, Convergence: 0.011613\n",
      "Epoch: 36, Loss: 198.27594, Residuals: -2.49597, Convergence: 0.011513\n",
      "Epoch: 37, Loss: 196.03923, Residuals: -2.46171, Convergence: 0.011409\n",
      "Epoch: 38, Loss: 193.84930, Residuals: -2.42761, Convergence: 0.011297\n",
      "Epoch: 39, Loss: 191.70724, Residuals: -2.39365, Convergence: 0.011174\n",
      "Epoch: 40, Loss: 189.61432, Residuals: -2.35983, Convergence: 0.011038\n",
      "Epoch: 41, Loss: 187.57186, Residuals: -2.32615, Convergence: 0.010889\n",
      "Epoch: 42, Loss: 185.58118, Residuals: -2.29264, Convergence: 0.010727\n",
      "Epoch: 43, Loss: 183.64366, Residuals: -2.25930, Convergence: 0.010550\n",
      "Epoch: 44, Loss: 181.76077, Residuals: -2.22618, Convergence: 0.010359\n",
      "Epoch: 45, Loss: 179.93419, Residuals: -2.19332, Convergence: 0.010151\n",
      "Epoch: 46, Loss: 178.16575, Residuals: -2.16076, Convergence: 0.009926\n",
      "Epoch: 47, Loss: 176.45738, Residuals: -2.12856, Convergence: 0.009681\n",
      "Epoch: 48, Loss: 174.81084, Residuals: -2.09680, Convergence: 0.009419\n",
      "Epoch: 49, Loss: 173.22743, Residuals: -2.06553, Convergence: 0.009141\n",
      "Epoch: 50, Loss: 171.70774, Residuals: -2.03481, Convergence: 0.008850\n",
      "Epoch: 51, Loss: 170.25147, Residuals: -2.00470, Convergence: 0.008554\n",
      "Epoch: 52, Loss: 168.85749, Residuals: -1.97523, Convergence: 0.008255\n",
      "Epoch: 53, Loss: 167.52386, Residuals: -1.94643, Convergence: 0.007961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54, Loss: 166.24806, Residuals: -1.91831, Convergence: 0.007674\n",
      "Epoch: 55, Loss: 165.02728, Residuals: -1.89086, Convergence: 0.007397\n",
      "Epoch: 56, Loss: 163.85851, Residuals: -1.86409, Convergence: 0.007133\n",
      "Epoch: 57, Loss: 162.73888, Residuals: -1.83796, Convergence: 0.006880\n",
      "Epoch: 58, Loss: 161.66565, Residuals: -1.81248, Convergence: 0.006639\n",
      "Epoch: 59, Loss: 160.63639, Residuals: -1.78763, Convergence: 0.006407\n",
      "Epoch: 60, Loss: 159.64898, Residuals: -1.76339, Convergence: 0.006185\n",
      "Epoch: 61, Loss: 158.70157, Residuals: -1.73976, Convergence: 0.005970\n",
      "Epoch: 62, Loss: 157.79264, Residuals: -1.71675, Convergence: 0.005760\n",
      "Epoch: 63, Loss: 156.92086, Residuals: -1.69433, Convergence: 0.005556\n",
      "Epoch: 64, Loss: 156.08508, Residuals: -1.67253, Convergence: 0.005355\n",
      "Epoch: 65, Loss: 155.28432, Residuals: -1.65132, Convergence: 0.005157\n",
      "Epoch: 66, Loss: 154.51767, Residuals: -1.63073, Convergence: 0.004962\n",
      "Epoch: 67, Loss: 153.78427, Residuals: -1.61075, Convergence: 0.004769\n",
      "Epoch: 68, Loss: 153.08331, Residuals: -1.59138, Convergence: 0.004579\n",
      "Epoch: 69, Loss: 152.41394, Residuals: -1.57262, Convergence: 0.004392\n",
      "Epoch: 70, Loss: 151.77531, Residuals: -1.55447, Convergence: 0.004208\n",
      "Epoch: 71, Loss: 151.16655, Residuals: -1.53693, Convergence: 0.004027\n",
      "Epoch: 72, Loss: 150.58671, Residuals: -1.51999, Convergence: 0.003851\n",
      "Epoch: 73, Loss: 150.03484, Residuals: -1.50365, Convergence: 0.003678\n",
      "Epoch: 74, Loss: 149.50993, Residuals: -1.48790, Convergence: 0.003511\n",
      "Epoch: 75, Loss: 149.01092, Residuals: -1.47273, Convergence: 0.003349\n",
      "Epoch: 76, Loss: 148.53674, Residuals: -1.45814, Convergence: 0.003192\n",
      "Epoch: 77, Loss: 148.08629, Residuals: -1.44410, Convergence: 0.003042\n",
      "Epoch: 78, Loss: 147.65847, Residuals: -1.43061, Convergence: 0.002897\n",
      "Epoch: 79, Loss: 147.25218, Residuals: -1.41765, Convergence: 0.002759\n",
      "Epoch: 80, Loss: 146.86634, Residuals: -1.40521, Convergence: 0.002627\n",
      "Epoch: 81, Loss: 146.49990, Residuals: -1.39327, Convergence: 0.002501\n",
      "Epoch: 82, Loss: 146.15184, Residuals: -1.38181, Convergence: 0.002382\n",
      "Epoch: 83, Loss: 145.82118, Residuals: -1.37082, Convergence: 0.002268\n",
      "Epoch: 84, Loss: 145.50699, Residuals: -1.36029, Convergence: 0.002159\n",
      "Epoch: 85, Loss: 145.20839, Residuals: -1.35019, Convergence: 0.002056\n",
      "Epoch: 86, Loss: 144.92456, Residuals: -1.34051, Convergence: 0.001959\n",
      "Epoch: 87, Loss: 144.65473, Residuals: -1.33123, Convergence: 0.001865\n",
      "Epoch: 88, Loss: 144.39819, Residuals: -1.32233, Convergence: 0.001777\n",
      "Epoch: 89, Loss: 144.15427, Residuals: -1.31381, Convergence: 0.001692\n",
      "Epoch: 90, Loss: 143.92235, Residuals: -1.30564, Convergence: 0.001611\n",
      "Epoch: 91, Loss: 143.70189, Residuals: -1.29780, Convergence: 0.001534\n",
      "Epoch: 92, Loss: 143.49237, Residuals: -1.29030, Convergence: 0.001460\n",
      "Epoch: 93, Loss: 143.29333, Residuals: -1.28310, Convergence: 0.001389\n",
      "Epoch: 94, Loss: 143.10432, Residuals: -1.27621, Convergence: 0.001321\n",
      "Epoch: 95, Loss: 142.92497, Residuals: -1.26960, Convergence: 0.001255\n",
      "Epoch: 96, Loss: 142.75492, Residuals: -1.26327, Convergence: 0.001191\n",
      "Epoch: 97, Loss: 142.59386, Residuals: -1.25720, Convergence: 0.001130\n",
      "Epoch: 98, Loss: 142.44149, Residuals: -1.25139, Convergence: 0.001070\n",
      "Epoch: 99, Loss: 142.29754, Residuals: -1.24582, Convergence: 0.001012\n",
      "Epoch: 100, Loss: 142.16177, Residuals: -1.24049, Convergence: 0.000955\n",
      "Evidence -184.036\n",
      "\n",
      "Epoch: 100, Evidence: -184.03615, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 7.25e-01\n",
      "Epoch: 100, Loss: 1362.36309, Residuals: -1.24049, Convergence:   inf\n",
      "Epoch: 101, Loss: 1301.02527, Residuals: -1.27041, Convergence: 0.047146\n",
      "Epoch: 102, Loss: 1254.32316, Residuals: -1.29383, Convergence: 0.037233\n",
      "Epoch: 103, Loss: 1218.99579, Residuals: -1.31065, Convergence: 0.028981\n",
      "Epoch: 104, Loss: 1191.50621, Residuals: -1.32245, Convergence: 0.023071\n",
      "Epoch: 105, Loss: 1169.28246, Residuals: -1.33111, Convergence: 0.019006\n",
      "Epoch: 106, Loss: 1150.83039, Residuals: -1.33770, Convergence: 0.016034\n",
      "Epoch: 107, Loss: 1135.24979, Residuals: -1.34268, Convergence: 0.013724\n",
      "Epoch: 108, Loss: 1121.93163, Residuals: -1.34629, Convergence: 0.011871\n",
      "Epoch: 109, Loss: 1110.42558, Residuals: -1.34870, Convergence: 0.010362\n",
      "Epoch: 110, Loss: 1100.38118, Residuals: -1.35003, Convergence: 0.009128\n",
      "Epoch: 111, Loss: 1091.51739, Residuals: -1.35039, Convergence: 0.008121\n",
      "Epoch: 112, Loss: 1083.60376, Residuals: -1.34987, Convergence: 0.007303\n",
      "Epoch: 113, Loss: 1076.44712, Residuals: -1.34854, Convergence: 0.006648\n",
      "Epoch: 114, Loss: 1069.88254, Residuals: -1.34645, Convergence: 0.006136\n",
      "Epoch: 115, Loss: 1063.76696, Residuals: -1.34364, Convergence: 0.005749\n",
      "Epoch: 116, Loss: 1057.97443, Residuals: -1.34013, Convergence: 0.005475\n",
      "Epoch: 117, Loss: 1052.39319, Residuals: -1.33592, Convergence: 0.005303\n",
      "Epoch: 118, Loss: 1046.92626, Residuals: -1.33104, Convergence: 0.005222\n",
      "Epoch: 119, Loss: 1041.49392, Residuals: -1.32549, Convergence: 0.005216\n",
      "Epoch: 120, Loss: 1036.04284, Residuals: -1.31931, Convergence: 0.005261\n",
      "Epoch: 121, Loss: 1030.55861, Residuals: -1.31257, Convergence: 0.005322\n",
      "Epoch: 122, Loss: 1025.07547, Residuals: -1.30535, Convergence: 0.005349\n",
      "Epoch: 123, Loss: 1019.67190, Residuals: -1.29776, Convergence: 0.005299\n",
      "Epoch: 124, Loss: 1014.44327, Residuals: -1.28989, Convergence: 0.005154\n",
      "Epoch: 125, Loss: 1009.46931, Residuals: -1.28184, Convergence: 0.004927\n",
      "Epoch: 126, Loss: 1004.79479, Residuals: -1.27370, Convergence: 0.004652\n",
      "Epoch: 127, Loss: 1000.42992, Residuals: -1.26552, Convergence: 0.004363\n",
      "Epoch: 128, Loss: 996.36191, Residuals: -1.25738, Convergence: 0.004083\n",
      "Epoch: 129, Loss: 992.56777, Residuals: -1.24931, Convergence: 0.003823\n",
      "Epoch: 130, Loss: 989.02078, Residuals: -1.24136, Convergence: 0.003586\n",
      "Epoch: 131, Loss: 985.69528, Residuals: -1.23357, Convergence: 0.003374\n",
      "Epoch: 132, Loss: 982.56903, Residuals: -1.22596, Convergence: 0.003182\n",
      "Epoch: 133, Loss: 979.62285, Residuals: -1.21855, Convergence: 0.003007\n",
      "Epoch: 134, Loss: 976.83982, Residuals: -1.21137, Convergence: 0.002849\n",
      "Epoch: 135, Loss: 974.20599, Residuals: -1.20442, Convergence: 0.002704\n",
      "Epoch: 136, Loss: 971.70941, Residuals: -1.19773, Convergence: 0.002569\n",
      "Epoch: 137, Loss: 969.33979, Residuals: -1.19128, Convergence: 0.002445\n",
      "Epoch: 138, Loss: 967.08821, Residuals: -1.18509, Convergence: 0.002328\n",
      "Epoch: 139, Loss: 964.94544, Residuals: -1.17916, Convergence: 0.002221\n",
      "Epoch: 140, Loss: 962.90407, Residuals: -1.17349, Convergence: 0.002120\n",
      "Epoch: 141, Loss: 960.95729, Residuals: -1.16808, Convergence: 0.002026\n",
      "Epoch: 142, Loss: 959.09764, Residuals: -1.16291, Convergence: 0.001939\n",
      "Epoch: 143, Loss: 957.31961, Residuals: -1.15797, Convergence: 0.001857\n",
      "Epoch: 144, Loss: 955.61657, Residuals: -1.15327, Convergence: 0.001782\n",
      "Epoch: 145, Loss: 953.98368, Residuals: -1.14879, Convergence: 0.001712\n",
      "Epoch: 146, Loss: 952.41535, Residuals: -1.14452, Convergence: 0.001647\n",
      "Epoch: 147, Loss: 950.90725, Residuals: -1.14044, Convergence: 0.001586\n",
      "Epoch: 148, Loss: 949.45492, Residuals: -1.13655, Convergence: 0.001530\n",
      "Epoch: 149, Loss: 948.05451, Residuals: -1.13284, Convergence: 0.001477\n",
      "Epoch: 150, Loss: 946.70217, Residuals: -1.12930, Convergence: 0.001428\n",
      "Epoch: 151, Loss: 945.39463, Residuals: -1.12591, Convergence: 0.001383\n",
      "Epoch: 152, Loss: 944.12854, Residuals: -1.12266, Convergence: 0.001341\n",
      "Epoch: 153, Loss: 942.90134, Residuals: -1.11955, Convergence: 0.001302\n",
      "Epoch: 154, Loss: 941.70934, Residuals: -1.11657, Convergence: 0.001266\n",
      "Epoch: 155, Loss: 940.54989, Residuals: -1.11370, Convergence: 0.001233\n",
      "Epoch: 156, Loss: 939.41996, Residuals: -1.11094, Convergence: 0.001203\n",
      "Epoch: 157, Loss: 938.31609, Residuals: -1.10827, Convergence: 0.001176\n",
      "Epoch: 158, Loss: 937.23523, Residuals: -1.10569, Convergence: 0.001153\n",
      "Epoch: 159, Loss: 936.17405, Residuals: -1.10320, Convergence: 0.001134\n",
      "Epoch: 160, Loss: 935.12894, Residuals: -1.10077, Convergence: 0.001118\n",
      "Epoch: 161, Loss: 934.09595, Residuals: -1.09840, Convergence: 0.001106\n",
      "Epoch: 162, Loss: 933.07183, Residuals: -1.09608, Convergence: 0.001098\n",
      "Epoch: 163, Loss: 932.05294, Residuals: -1.09381, Convergence: 0.001093\n",
      "Epoch: 164, Loss: 931.03580, Residuals: -1.09156, Convergence: 0.001092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 165, Loss: 930.01780, Residuals: -1.08934, Convergence: 0.001095\n",
      "Epoch: 166, Loss: 928.99667, Residuals: -1.08714, Convergence: 0.001099\n",
      "Epoch: 167, Loss: 927.97185, Residuals: -1.08494, Convergence: 0.001104\n",
      "Epoch: 168, Loss: 926.94368, Residuals: -1.08276, Convergence: 0.001109\n",
      "Epoch: 169, Loss: 925.91387, Residuals: -1.08058, Convergence: 0.001112\n",
      "Epoch: 170, Loss: 924.88576, Residuals: -1.07842, Convergence: 0.001112\n",
      "Epoch: 171, Loss: 923.86372, Residuals: -1.07628, Convergence: 0.001106\n",
      "Epoch: 172, Loss: 922.85305, Residuals: -1.07417, Convergence: 0.001095\n",
      "Epoch: 173, Loss: 921.85851, Residuals: -1.07210, Convergence: 0.001079\n",
      "Epoch: 174, Loss: 920.88481, Residuals: -1.07008, Convergence: 0.001057\n",
      "Epoch: 175, Loss: 919.93586, Residuals: -1.06810, Convergence: 0.001032\n",
      "Epoch: 176, Loss: 919.01395, Residuals: -1.06619, Convergence: 0.001003\n",
      "Epoch: 177, Loss: 918.12111, Residuals: -1.06435, Convergence: 0.000972\n",
      "Evidence 11062.572\n",
      "\n",
      "Epoch: 177, Evidence: 11062.57227, Convergence: 1.016636\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 5.75e-01\n",
      "Epoch: 177, Loss: 2332.90524, Residuals: -1.06435, Convergence:   inf\n",
      "Epoch: 178, Loss: 2294.33401, Residuals: -1.07162, Convergence: 0.016812\n",
      "Epoch: 179, Loss: 2268.16006, Residuals: -1.07034, Convergence: 0.011540\n",
      "Epoch: 180, Loss: 2246.46743, Residuals: -1.06830, Convergence: 0.009656\n",
      "Epoch: 181, Loss: 2228.22891, Residuals: -1.06613, Convergence: 0.008185\n",
      "Epoch: 182, Loss: 2212.75970, Residuals: -1.06392, Convergence: 0.006991\n",
      "Epoch: 183, Loss: 2199.53054, Residuals: -1.06168, Convergence: 0.006015\n",
      "Epoch: 184, Loss: 2188.10985, Residuals: -1.05944, Convergence: 0.005219\n",
      "Epoch: 185, Loss: 2178.13966, Residuals: -1.05717, Convergence: 0.004577\n",
      "Epoch: 186, Loss: 2169.32446, Residuals: -1.05486, Convergence: 0.004064\n",
      "Epoch: 187, Loss: 2161.42523, Residuals: -1.05247, Convergence: 0.003655\n",
      "Epoch: 188, Loss: 2154.26236, Residuals: -1.05000, Convergence: 0.003325\n",
      "Epoch: 189, Loss: 2147.71071, Residuals: -1.04742, Convergence: 0.003051\n",
      "Epoch: 190, Loss: 2141.69292, Residuals: -1.04477, Convergence: 0.002810\n",
      "Epoch: 191, Loss: 2136.15924, Residuals: -1.04207, Convergence: 0.002590\n",
      "Epoch: 192, Loss: 2131.07304, Residuals: -1.03936, Convergence: 0.002387\n",
      "Epoch: 193, Loss: 2126.39882, Residuals: -1.03668, Convergence: 0.002198\n",
      "Epoch: 194, Loss: 2122.10152, Residuals: -1.03404, Convergence: 0.002025\n",
      "Epoch: 195, Loss: 2118.14614, Residuals: -1.03147, Convergence: 0.001867\n",
      "Epoch: 196, Loss: 2114.49842, Residuals: -1.02897, Convergence: 0.001725\n",
      "Epoch: 197, Loss: 2111.12883, Residuals: -1.02656, Convergence: 0.001596\n",
      "Epoch: 198, Loss: 2108.01116, Residuals: -1.02423, Convergence: 0.001479\n",
      "Epoch: 199, Loss: 2105.12198, Residuals: -1.02199, Convergence: 0.001372\n",
      "Epoch: 200, Loss: 2102.44086, Residuals: -1.01984, Convergence: 0.001275\n",
      "Epoch: 201, Loss: 2099.95026, Residuals: -1.01778, Convergence: 0.001186\n",
      "Epoch: 202, Loss: 2097.63341, Residuals: -1.01581, Convergence: 0.001105\n",
      "Epoch: 203, Loss: 2095.47482, Residuals: -1.01392, Convergence: 0.001030\n",
      "Epoch: 204, Loss: 2093.46116, Residuals: -1.01211, Convergence: 0.000962\n",
      "Evidence 14225.518\n",
      "\n",
      "Epoch: 204, Evidence: 14225.51758, Convergence: 0.222343\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 4.37e-01\n",
      "Epoch: 204, Loss: 2461.56337, Residuals: -1.01211, Convergence:   inf\n",
      "Epoch: 205, Loss: 2448.44037, Residuals: -1.00994, Convergence: 0.005360\n",
      "Epoch: 206, Loss: 2437.70158, Residuals: -1.00707, Convergence: 0.004405\n",
      "Epoch: 207, Loss: 2428.40741, Residuals: -1.00418, Convergence: 0.003827\n",
      "Epoch: 208, Loss: 2420.31062, Residuals: -1.00140, Convergence: 0.003345\n",
      "Epoch: 209, Loss: 2413.22428, Residuals: -0.99879, Convergence: 0.002936\n",
      "Epoch: 210, Loss: 2406.99472, Residuals: -0.99636, Convergence: 0.002588\n",
      "Epoch: 211, Loss: 2401.49491, Residuals: -0.99410, Convergence: 0.002290\n",
      "Epoch: 212, Loss: 2396.61682, Residuals: -0.99201, Convergence: 0.002035\n",
      "Epoch: 213, Loss: 2392.26913, Residuals: -0.99007, Convergence: 0.001817\n",
      "Epoch: 214, Loss: 2388.37523, Residuals: -0.98828, Convergence: 0.001630\n",
      "Epoch: 215, Loss: 2384.87085, Residuals: -0.98660, Convergence: 0.001469\n",
      "Epoch: 216, Loss: 2381.70014, Residuals: -0.98504, Convergence: 0.001331\n",
      "Epoch: 217, Loss: 2378.81811, Residuals: -0.98357, Convergence: 0.001212\n",
      "Epoch: 218, Loss: 2376.18508, Residuals: -0.98220, Convergence: 0.001108\n",
      "Epoch: 219, Loss: 2373.76938, Residuals: -0.98091, Convergence: 0.001018\n",
      "Epoch: 220, Loss: 2371.54327, Residuals: -0.97969, Convergence: 0.000939\n",
      "Evidence 14598.488\n",
      "\n",
      "Epoch: 220, Evidence: 14598.48828, Convergence: 0.025549\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 3.35e-01\n",
      "Epoch: 220, Loss: 2466.90268, Residuals: -0.97969, Convergence:   inf\n",
      "Epoch: 221, Loss: 2460.34121, Residuals: -0.97662, Convergence: 0.002667\n",
      "Epoch: 222, Loss: 2454.89858, Residuals: -0.97387, Convergence: 0.002217\n",
      "Epoch: 223, Loss: 2450.27789, Residuals: -0.97150, Convergence: 0.001886\n",
      "Epoch: 224, Loss: 2446.30685, Residuals: -0.96946, Convergence: 0.001623\n",
      "Epoch: 225, Loss: 2442.85119, Residuals: -0.96770, Convergence: 0.001415\n",
      "Epoch: 226, Loss: 2439.80971, Residuals: -0.96617, Convergence: 0.001247\n",
      "Epoch: 227, Loss: 2437.10260, Residuals: -0.96484, Convergence: 0.001111\n",
      "Epoch: 228, Loss: 2434.66845, Residuals: -0.96366, Convergence: 0.001000\n",
      "Evidence 14681.328\n",
      "\n",
      "Epoch: 228, Evidence: 14681.32812, Convergence: 0.005643\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 2.63e-01\n",
      "Epoch: 228, Loss: 2468.51329, Residuals: -0.96366, Convergence:   inf\n",
      "Epoch: 229, Loss: 2464.42145, Residuals: -0.96127, Convergence: 0.001660\n",
      "Epoch: 230, Loss: 2461.02485, Residuals: -0.95934, Convergence: 0.001380\n",
      "Epoch: 231, Loss: 2458.13209, Residuals: -0.95778, Convergence: 0.001177\n",
      "Epoch: 232, Loss: 2455.62120, Residuals: -0.95650, Convergence: 0.001023\n",
      "Epoch: 233, Loss: 2453.40541, Residuals: -0.95544, Convergence: 0.000903\n",
      "Evidence 14712.720\n",
      "\n",
      "Epoch: 233, Evidence: 14712.71973, Convergence: 0.002134\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 2.11e-01\n",
      "Epoch: 233, Loss: 2469.33762, Residuals: -0.95544, Convergence:   inf\n",
      "Epoch: 234, Loss: 2466.41956, Residuals: -0.95368, Convergence: 0.001183\n",
      "Epoch: 235, Loss: 2463.98632, Residuals: -0.95233, Convergence: 0.000988\n",
      "Evidence 14725.932\n",
      "\n",
      "Epoch: 235, Evidence: 14725.93164, Convergence: 0.000897\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.75e-01\n",
      "Epoch: 235, Loss: 2470.00678, Residuals: -0.95233, Convergence:   inf\n",
      "Epoch: 236, Loss: 2465.45687, Residuals: -0.95021, Convergence: 0.001845\n",
      "Epoch: 237, Loss: 2461.99296, Residuals: -0.94870, Convergence: 0.001407\n",
      "Epoch: 238, Loss: 2459.20625, Residuals: -0.94763, Convergence: 0.001133\n",
      "Epoch: 239, Loss: 2456.85476, Residuals: -0.94697, Convergence: 0.000957\n",
      "Evidence 14743.845\n",
      "\n",
      "Epoch: 239, Evidence: 14743.84473, Convergence: 0.002111\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.45e-01\n",
      "Epoch: 239, Loss: 2470.07757, Residuals: -0.94697, Convergence:   inf\n",
      "Epoch: 240, Loss: 2467.04970, Residuals: -0.94493, Convergence: 0.001227\n",
      "Epoch: 241, Loss: 2464.67219, Residuals: -0.94390, Convergence: 0.000965\n",
      "Evidence 14755.059\n",
      "\n",
      "Epoch: 241, Evidence: 14755.05859, Convergence: 0.000760\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.22e-01\n",
      "Epoch: 241, Loss: 2470.23278, Residuals: -0.94390, Convergence:   inf\n",
      "Epoch: 242, Loss: 2465.82060, Residuals: -0.94106, Convergence: 0.001789\n",
      "Epoch: 243, Loss: 2462.70661, Residuals: -0.94201, Convergence: 0.001264\n",
      "Epoch: 244, Loss: 2460.13426, Residuals: -0.94242, Convergence: 0.001046\n",
      "Epoch: 245, Loss: 2457.87144, Residuals: -0.94483, Convergence: 0.000921\n",
      "Evidence 14770.729\n",
      "\n",
      "Epoch: 245, Evidence: 14770.72852, Convergence: 0.001820\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.08e-01\n",
      "Epoch: 245, Loss: 2469.42107, Residuals: -0.94483, Convergence:   inf\n",
      "Epoch: 246, Loss: 2467.56129, Residuals: -0.94246, Convergence: 0.000754\n",
      "Evidence 14777.706\n",
      "\n",
      "Epoch: 246, Evidence: 14777.70605, Convergence: 0.000472\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 8.88e-02\n",
      "Epoch: 246, Loss: 2470.36844, Residuals: -0.94246, Convergence:   inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 247, Loss: 2507.57650, Residuals: -0.98038, Convergence: -0.014838\n",
      "Epoch: 247, Loss: 2468.14139, Residuals: -0.94065, Convergence: 0.000902\n",
      "Evidence 14781.950\n",
      "\n",
      "Epoch: 247, Evidence: 14781.95020, Convergence: 0.000759\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 8.19e-02\n",
      "Epoch: 247, Loss: 2469.78398, Residuals: -0.94065, Convergence:   inf\n",
      "Epoch: 248, Loss: 2474.63089, Residuals: -0.94321, Convergence: -0.001959\n",
      "Epoch: 248, Loss: 2469.72242, Residuals: -0.93959, Convergence: 0.000025\n",
      "Evidence 14783.713\n",
      "\n",
      "Epoch: 248, Evidence: 14783.71289, Convergence: 0.000878\n",
      "Total samples: 181, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 385.29591, Residuals: -4.58858, Convergence:   inf\n",
      "Epoch: 1, Loss: 359.54196, Residuals: -4.46851, Convergence: 0.071630\n",
      "Epoch: 2, Loss: 338.42498, Residuals: -4.30389, Convergence: 0.062398\n",
      "Epoch: 3, Loss: 322.29394, Residuals: -4.13867, Convergence: 0.050051\n",
      "Epoch: 4, Loss: 309.98545, Residuals: -3.99310, Convergence: 0.039707\n",
      "Epoch: 5, Loss: 300.21526, Residuals: -3.86433, Convergence: 0.032544\n",
      "Epoch: 6, Loss: 292.27648, Residuals: -3.75211, Convergence: 0.027162\n",
      "Epoch: 7, Loss: 285.67958, Residuals: -3.65596, Convergence: 0.023092\n",
      "Epoch: 8, Loss: 280.06279, Residuals: -3.57380, Convergence: 0.020055\n",
      "Epoch: 9, Loss: 275.17090, Residuals: -3.50328, Convergence: 0.017778\n",
      "Epoch: 10, Loss: 270.82242, Residuals: -3.44225, Convergence: 0.016057\n",
      "Epoch: 11, Loss: 266.88570, Residuals: -3.38890, Convergence: 0.014751\n",
      "Epoch: 12, Loss: 263.26428, Residuals: -3.34169, Convergence: 0.013756\n",
      "Epoch: 13, Loss: 259.88775, Residuals: -3.29927, Convergence: 0.012992\n",
      "Epoch: 14, Loss: 256.70605, Residuals: -3.26048, Convergence: 0.012394\n",
      "Epoch: 15, Loss: 253.68723, Residuals: -3.22440, Convergence: 0.011900\n",
      "Epoch: 16, Loss: 250.81538, Residuals: -3.19042, Convergence: 0.011450\n",
      "Epoch: 17, Loss: 248.08026, Residuals: -3.15822, Convergence: 0.011025\n",
      "Epoch: 18, Loss: 245.46241, Residuals: -3.12746, Convergence: 0.010665\n",
      "Epoch: 19, Loss: 242.92988, Residuals: -3.09766, Convergence: 0.010425\n",
      "Epoch: 20, Loss: 240.44495, Residuals: -3.06824, Convergence: 0.010335\n",
      "Epoch: 21, Loss: 237.97178, Residuals: -3.03865, Convergence: 0.010393\n",
      "Epoch: 22, Loss: 235.47989, Residuals: -3.00843, Convergence: 0.010582\n",
      "Epoch: 23, Loss: 232.93759, Residuals: -2.97718, Convergence: 0.010914\n",
      "Epoch: 24, Loss: 230.29813, Residuals: -2.94436, Convergence: 0.011461\n",
      "Epoch: 25, Loss: 227.51225, Residuals: -2.90939, Convergence: 0.012245\n",
      "Epoch: 26, Loss: 224.61058, Residuals: -2.87250, Convergence: 0.012919\n",
      "Epoch: 27, Loss: 221.72590, Residuals: -2.83512, Convergence: 0.013010\n",
      "Epoch: 28, Loss: 218.93507, Residuals: -2.79823, Convergence: 0.012747\n",
      "Epoch: 29, Loss: 216.23614, Residuals: -2.76195, Convergence: 0.012481\n",
      "Epoch: 30, Loss: 213.60833, Residuals: -2.72614, Convergence: 0.012302\n",
      "Epoch: 31, Loss: 211.03374, Residuals: -2.69062, Convergence: 0.012200\n",
      "Epoch: 32, Loss: 208.50054, Residuals: -2.65525, Convergence: 0.012150\n",
      "Epoch: 33, Loss: 206.00238, Residuals: -2.61992, Convergence: 0.012127\n",
      "Epoch: 34, Loss: 203.53708, Residuals: -2.58459, Convergence: 0.012112\n",
      "Epoch: 35, Loss: 201.10545, Residuals: -2.54920, Convergence: 0.012091\n",
      "Epoch: 36, Loss: 198.71016, Residuals: -2.51376, Convergence: 0.012054\n",
      "Epoch: 37, Loss: 196.35497, Residuals: -2.47828, Convergence: 0.011995\n",
      "Epoch: 38, Loss: 194.04412, Residuals: -2.44278, Convergence: 0.011909\n",
      "Epoch: 39, Loss: 191.78191, Residuals: -2.40730, Convergence: 0.011796\n",
      "Epoch: 40, Loss: 189.57251, Residuals: -2.37189, Convergence: 0.011655\n",
      "Epoch: 41, Loss: 187.41986, Residuals: -2.33660, Convergence: 0.011486\n",
      "Epoch: 42, Loss: 185.32762, Residuals: -2.30149, Convergence: 0.011289\n",
      "Epoch: 43, Loss: 183.29927, Residuals: -2.26661, Convergence: 0.011066\n",
      "Epoch: 44, Loss: 181.33803, Residuals: -2.23205, Convergence: 0.010815\n",
      "Epoch: 45, Loss: 179.44684, Residuals: -2.19786, Convergence: 0.010539\n",
      "Epoch: 46, Loss: 177.62814, Residuals: -2.16414, Convergence: 0.010239\n",
      "Epoch: 47, Loss: 175.88360, Residuals: -2.13095, Convergence: 0.009919\n",
      "Epoch: 48, Loss: 174.21397, Residuals: -2.09835, Convergence: 0.009584\n",
      "Epoch: 49, Loss: 172.61895, Residuals: -2.06641, Convergence: 0.009240\n",
      "Epoch: 50, Loss: 171.09728, Residuals: -2.03517, Convergence: 0.008894\n",
      "Epoch: 51, Loss: 169.64696, Residuals: -2.00465, Convergence: 0.008549\n",
      "Epoch: 52, Loss: 168.26545, Residuals: -1.97486, Convergence: 0.008210\n",
      "Epoch: 53, Loss: 166.94994, Residuals: -1.94583, Convergence: 0.007880\n",
      "Epoch: 54, Loss: 165.69751, Residuals: -1.91754, Convergence: 0.007559\n",
      "Epoch: 55, Loss: 164.50519, Residuals: -1.89001, Convergence: 0.007248\n",
      "Epoch: 56, Loss: 163.37002, Residuals: -1.86322, Convergence: 0.006948\n",
      "Epoch: 57, Loss: 162.28900, Residuals: -1.83719, Convergence: 0.006661\n",
      "Epoch: 58, Loss: 161.25914, Residuals: -1.81190, Convergence: 0.006386\n",
      "Epoch: 59, Loss: 160.27737, Residuals: -1.78735, Convergence: 0.006125\n",
      "Epoch: 60, Loss: 159.34072, Residuals: -1.76352, Convergence: 0.005878\n",
      "Epoch: 61, Loss: 158.44627, Residuals: -1.74040, Convergence: 0.005645\n",
      "Epoch: 62, Loss: 157.59131, Residuals: -1.71797, Convergence: 0.005425\n",
      "Epoch: 63, Loss: 156.77335, Residuals: -1.69620, Convergence: 0.005218\n",
      "Epoch: 64, Loss: 155.99016, Residuals: -1.67508, Convergence: 0.005021\n",
      "Epoch: 65, Loss: 155.23980, Residuals: -1.65458, Convergence: 0.004834\n",
      "Epoch: 66, Loss: 154.52058, Residuals: -1.63471, Convergence: 0.004654\n",
      "Epoch: 67, Loss: 153.83108, Residuals: -1.61543, Convergence: 0.004482\n",
      "Epoch: 68, Loss: 153.17004, Residuals: -1.59673, Convergence: 0.004316\n",
      "Epoch: 69, Loss: 152.53639, Residuals: -1.57862, Convergence: 0.004154\n",
      "Epoch: 70, Loss: 151.92916, Residuals: -1.56107, Convergence: 0.003997\n",
      "Epoch: 71, Loss: 151.34746, Residuals: -1.54407, Convergence: 0.003843\n",
      "Epoch: 72, Loss: 150.79051, Residuals: -1.52762, Convergence: 0.003694\n",
      "Epoch: 73, Loss: 150.25753, Residuals: -1.51171, Convergence: 0.003547\n",
      "Epoch: 74, Loss: 149.74781, Residuals: -1.49633, Convergence: 0.003404\n",
      "Epoch: 75, Loss: 149.26060, Residuals: -1.48147, Convergence: 0.003264\n",
      "Epoch: 76, Loss: 148.79523, Residuals: -1.46711, Convergence: 0.003128\n",
      "Epoch: 77, Loss: 148.35097, Residuals: -1.45326, Convergence: 0.002995\n",
      "Epoch: 78, Loss: 147.92711, Residuals: -1.43990, Convergence: 0.002865\n",
      "Epoch: 79, Loss: 147.52295, Residuals: -1.42702, Convergence: 0.002740\n",
      "Epoch: 80, Loss: 147.13777, Residuals: -1.41460, Convergence: 0.002618\n",
      "Epoch: 81, Loss: 146.77086, Residuals: -1.40264, Convergence: 0.002500\n",
      "Epoch: 82, Loss: 146.42151, Residuals: -1.39113, Convergence: 0.002386\n",
      "Epoch: 83, Loss: 146.08901, Residuals: -1.38006, Convergence: 0.002276\n",
      "Epoch: 84, Loss: 145.77268, Residuals: -1.36941, Convergence: 0.002170\n",
      "Epoch: 85, Loss: 145.47184, Residuals: -1.35917, Convergence: 0.002068\n",
      "Epoch: 86, Loss: 145.18584, Residuals: -1.34932, Convergence: 0.001970\n",
      "Epoch: 87, Loss: 144.91406, Residuals: -1.33987, Convergence: 0.001875\n",
      "Epoch: 88, Loss: 144.65590, Residuals: -1.33079, Convergence: 0.001785\n",
      "Epoch: 89, Loss: 144.41080, Residuals: -1.32208, Convergence: 0.001697\n",
      "Epoch: 90, Loss: 144.17825, Residuals: -1.31372, Convergence: 0.001613\n",
      "Epoch: 91, Loss: 143.95774, Residuals: -1.30570, Convergence: 0.001532\n",
      "Epoch: 92, Loss: 143.74883, Residuals: -1.29802, Convergence: 0.001453\n",
      "Epoch: 93, Loss: 143.55108, Residuals: -1.29066, Convergence: 0.001378\n",
      "Epoch: 94, Loss: 143.36412, Residuals: -1.28362, Convergence: 0.001304\n",
      "Epoch: 95, Loss: 143.18754, Residuals: -1.27688, Convergence: 0.001233\n",
      "Epoch: 96, Loss: 143.02101, Residuals: -1.27045, Convergence: 0.001164\n",
      "Epoch: 97, Loss: 142.86415, Residuals: -1.26431, Convergence: 0.001098\n",
      "Epoch: 98, Loss: 142.71659, Residuals: -1.25846, Convergence: 0.001034\n",
      "Epoch: 99, Loss: 142.57791, Residuals: -1.25290, Convergence: 0.000973\n",
      "Evidence -184.500\n",
      "\n",
      "Epoch: 99, Evidence: -184.50027, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 7.25e-01\n",
      "Epoch: 99, Loss: 1360.17866, Residuals: -1.25290, Convergence:   inf\n",
      "Epoch: 100, Loss: 1298.48107, Residuals: -1.28353, Convergence: 0.047515\n",
      "Epoch: 101, Loss: 1251.46115, Residuals: -1.30742, Convergence: 0.037572\n",
      "Epoch: 102, Loss: 1215.92745, Residuals: -1.32448, Convergence: 0.029224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 103, Loss: 1188.35529, Residuals: -1.33634, Convergence: 0.023202\n",
      "Epoch: 104, Loss: 1166.12865, Residuals: -1.34496, Convergence: 0.019060\n",
      "Epoch: 105, Loss: 1147.71799, Residuals: -1.35144, Convergence: 0.016041\n",
      "Epoch: 106, Loss: 1132.20053, Residuals: -1.35629, Convergence: 0.013706\n",
      "Epoch: 107, Loss: 1118.95365, Residuals: -1.35976, Convergence: 0.011839\n",
      "Epoch: 108, Loss: 1107.51984, Residuals: -1.36203, Convergence: 0.010324\n",
      "Epoch: 109, Loss: 1097.54450, Residuals: -1.36321, Convergence: 0.009089\n",
      "Epoch: 110, Loss: 1088.74482, Residuals: -1.36343, Convergence: 0.008082\n",
      "Epoch: 111, Loss: 1080.89042, Residuals: -1.36276, Convergence: 0.007267\n",
      "Epoch: 112, Loss: 1073.78795, Residuals: -1.36129, Convergence: 0.006614\n",
      "Epoch: 113, Loss: 1067.27439, Residuals: -1.35907, Convergence: 0.006103\n",
      "Epoch: 114, Loss: 1061.20796, Residuals: -1.35614, Convergence: 0.005717\n",
      "Epoch: 115, Loss: 1055.46403, Residuals: -1.35253, Convergence: 0.005442\n",
      "Epoch: 116, Loss: 1049.93362, Residuals: -1.34825, Convergence: 0.005267\n",
      "Epoch: 117, Loss: 1044.52222, Residuals: -1.34330, Convergence: 0.005181\n",
      "Epoch: 118, Loss: 1039.15552, Residuals: -1.33771, Convergence: 0.005164\n",
      "Epoch: 119, Loss: 1033.78605, Residuals: -1.33152, Convergence: 0.005194\n",
      "Epoch: 120, Loss: 1028.40461, Residuals: -1.32477, Convergence: 0.005233\n",
      "Epoch: 121, Loss: 1023.04568, Residuals: -1.31757, Convergence: 0.005238\n",
      "Epoch: 122, Loss: 1017.77988, Residuals: -1.31000, Convergence: 0.005174\n",
      "Epoch: 123, Loss: 1012.68961, Residuals: -1.30217, Convergence: 0.005026\n",
      "Epoch: 124, Loss: 1007.84247, Residuals: -1.29416, Convergence: 0.004809\n",
      "Epoch: 125, Loss: 1003.27543, Residuals: -1.28605, Convergence: 0.004552\n",
      "Epoch: 126, Loss: 998.99681, Residuals: -1.27791, Convergence: 0.004283\n",
      "Epoch: 127, Loss: 994.99454, Residuals: -1.26981, Convergence: 0.004022\n",
      "Epoch: 128, Loss: 991.24727, Residuals: -1.26178, Convergence: 0.003780\n",
      "Epoch: 129, Loss: 987.73013, Residuals: -1.25386, Convergence: 0.003561\n",
      "Epoch: 130, Loss: 984.41995, Residuals: -1.24610, Convergence: 0.003363\n",
      "Epoch: 131, Loss: 981.29484, Residuals: -1.23850, Convergence: 0.003185\n",
      "Epoch: 132, Loss: 978.33737, Residuals: -1.23110, Convergence: 0.003023\n",
      "Epoch: 133, Loss: 975.53148, Residuals: -1.22390, Convergence: 0.002876\n",
      "Epoch: 134, Loss: 972.86444, Residuals: -1.21693, Convergence: 0.002741\n",
      "Epoch: 135, Loss: 970.32587, Residuals: -1.21018, Convergence: 0.002616\n",
      "Epoch: 136, Loss: 967.90664, Residuals: -1.20368, Convergence: 0.002499\n",
      "Epoch: 137, Loss: 965.59855, Residuals: -1.19742, Convergence: 0.002390\n",
      "Epoch: 138, Loss: 963.39538, Residuals: -1.19141, Convergence: 0.002287\n",
      "Epoch: 139, Loss: 961.29076, Residuals: -1.18564, Convergence: 0.002189\n",
      "Epoch: 140, Loss: 959.27901, Residuals: -1.18013, Convergence: 0.002097\n",
      "Epoch: 141, Loss: 957.35491, Residuals: -1.17485, Convergence: 0.002010\n",
      "Epoch: 142, Loss: 955.51358, Residuals: -1.16981, Convergence: 0.001927\n",
      "Epoch: 143, Loss: 953.74953, Residuals: -1.16501, Convergence: 0.001850\n",
      "Epoch: 144, Loss: 952.05857, Residuals: -1.16042, Convergence: 0.001776\n",
      "Epoch: 145, Loss: 950.43556, Residuals: -1.15606, Convergence: 0.001708\n",
      "Epoch: 146, Loss: 948.87636, Residuals: -1.15190, Convergence: 0.001643\n",
      "Epoch: 147, Loss: 947.37676, Residuals: -1.14793, Convergence: 0.001583\n",
      "Epoch: 148, Loss: 945.93291, Residuals: -1.14415, Convergence: 0.001526\n",
      "Epoch: 149, Loss: 944.54090, Residuals: -1.14055, Convergence: 0.001474\n",
      "Epoch: 150, Loss: 943.19742, Residuals: -1.13711, Convergence: 0.001424\n",
      "Epoch: 151, Loss: 941.89936, Residuals: -1.13382, Convergence: 0.001378\n",
      "Epoch: 152, Loss: 940.64364, Residuals: -1.13068, Convergence: 0.001335\n",
      "Epoch: 153, Loss: 939.42736, Residuals: -1.12768, Convergence: 0.001295\n",
      "Epoch: 154, Loss: 938.24853, Residuals: -1.12480, Convergence: 0.001256\n",
      "Epoch: 155, Loss: 937.10408, Residuals: -1.12204, Convergence: 0.001221\n",
      "Epoch: 156, Loss: 935.99129, Residuals: -1.11939, Convergence: 0.001189\n",
      "Epoch: 157, Loss: 934.90787, Residuals: -1.11684, Convergence: 0.001159\n",
      "Epoch: 158, Loss: 933.85107, Residuals: -1.11438, Convergence: 0.001132\n",
      "Epoch: 159, Loss: 932.81798, Residuals: -1.11200, Convergence: 0.001107\n",
      "Epoch: 160, Loss: 931.80551, Residuals: -1.10970, Convergence: 0.001087\n",
      "Epoch: 161, Loss: 930.81083, Residuals: -1.10746, Convergence: 0.001069\n",
      "Epoch: 162, Loss: 929.83017, Residuals: -1.10527, Convergence: 0.001055\n",
      "Epoch: 163, Loss: 928.86064, Residuals: -1.10313, Convergence: 0.001044\n",
      "Epoch: 164, Loss: 927.89908, Residuals: -1.10103, Convergence: 0.001036\n",
      "Epoch: 165, Loss: 926.94218, Residuals: -1.09895, Convergence: 0.001032\n",
      "Epoch: 166, Loss: 925.98767, Residuals: -1.09690, Convergence: 0.001031\n",
      "Epoch: 167, Loss: 925.03407, Residuals: -1.09486, Convergence: 0.001031\n",
      "Epoch: 168, Loss: 924.08089, Residuals: -1.09283, Convergence: 0.001031\n",
      "Epoch: 169, Loss: 923.12944, Residuals: -1.09082, Convergence: 0.001031\n",
      "Epoch: 170, Loss: 922.18158, Residuals: -1.08881, Convergence: 0.001028\n",
      "Epoch: 171, Loss: 921.24106, Residuals: -1.08682, Convergence: 0.001021\n",
      "Epoch: 172, Loss: 920.31225, Residuals: -1.08486, Convergence: 0.001009\n",
      "Epoch: 173, Loss: 919.40002, Residuals: -1.08293, Convergence: 0.000992\n",
      "Evidence 10984.676\n",
      "\n",
      "Epoch: 173, Evidence: 10984.67578, Convergence: 1.016796\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 5.75e-01\n",
      "Epoch: 173, Loss: 2329.78922, Residuals: -1.08293, Convergence:   inf\n",
      "Epoch: 174, Loss: 2289.96593, Residuals: -1.09027, Convergence: 0.017390\n",
      "Epoch: 175, Loss: 2263.04242, Residuals: -1.08888, Convergence: 0.011897\n",
      "Epoch: 176, Loss: 2240.72773, Residuals: -1.08657, Convergence: 0.009959\n",
      "Epoch: 177, Loss: 2221.91244, Residuals: -1.08406, Convergence: 0.008468\n",
      "Epoch: 178, Loss: 2205.90450, Residuals: -1.08149, Convergence: 0.007257\n",
      "Epoch: 179, Loss: 2192.17242, Residuals: -1.07888, Convergence: 0.006264\n",
      "Epoch: 180, Loss: 2180.28054, Residuals: -1.07624, Convergence: 0.005454\n",
      "Epoch: 181, Loss: 2169.86260, Residuals: -1.07356, Convergence: 0.004801\n",
      "Epoch: 182, Loss: 2160.61728, Residuals: -1.07084, Convergence: 0.004279\n",
      "Epoch: 183, Loss: 2152.29962, Residuals: -1.06804, Convergence: 0.003865\n",
      "Epoch: 184, Loss: 2144.72801, Residuals: -1.06517, Convergence: 0.003530\n",
      "Epoch: 185, Loss: 2137.78444, Residuals: -1.06221, Convergence: 0.003248\n",
      "Epoch: 186, Loss: 2131.39774, Residuals: -1.05921, Convergence: 0.002996\n",
      "Epoch: 187, Loss: 2125.52692, Residuals: -1.05620, Convergence: 0.002762\n",
      "Epoch: 188, Loss: 2120.13851, Residuals: -1.05324, Convergence: 0.002542\n",
      "Epoch: 189, Loss: 2115.19870, Residuals: -1.05035, Convergence: 0.002335\n",
      "Epoch: 190, Loss: 2110.67110, Residuals: -1.04756, Convergence: 0.002145\n",
      "Epoch: 191, Loss: 2106.51648, Residuals: -1.04489, Convergence: 0.001972\n",
      "Epoch: 192, Loss: 2102.69546, Residuals: -1.04233, Convergence: 0.001817\n",
      "Epoch: 193, Loss: 2099.17229, Residuals: -1.03988, Convergence: 0.001678\n",
      "Epoch: 194, Loss: 2095.91283, Residuals: -1.03755, Convergence: 0.001555\n",
      "Epoch: 195, Loss: 2092.88880, Residuals: -1.03533, Convergence: 0.001445\n",
      "Epoch: 196, Loss: 2090.07612, Residuals: -1.03321, Convergence: 0.001346\n",
      "Epoch: 197, Loss: 2087.45406, Residuals: -1.03118, Convergence: 0.001256\n",
      "Epoch: 198, Loss: 2085.00481, Residuals: -1.02925, Convergence: 0.001175\n",
      "Epoch: 199, Loss: 2082.71436, Residuals: -1.02741, Convergence: 0.001100\n",
      "Epoch: 200, Loss: 2080.56982, Residuals: -1.02565, Convergence: 0.001031\n",
      "Epoch: 201, Loss: 2078.55827, Residuals: -1.02397, Convergence: 0.000968\n",
      "Evidence 14135.970\n",
      "\n",
      "Epoch: 201, Evidence: 14135.96973, Convergence: 0.222927\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 4.37e-01\n",
      "Epoch: 201, Loss: 2456.29265, Residuals: -1.02397, Convergence:   inf\n",
      "Epoch: 202, Loss: 2442.64321, Residuals: -1.02106, Convergence: 0.005588\n",
      "Epoch: 203, Loss: 2431.38439, Residuals: -1.01770, Convergence: 0.004631\n",
      "Epoch: 204, Loss: 2421.61368, Residuals: -1.01444, Convergence: 0.004035\n",
      "Epoch: 205, Loss: 2413.09081, Residuals: -1.01137, Convergence: 0.003532\n",
      "Epoch: 206, Loss: 2405.62770, Residuals: -1.00852, Convergence: 0.003102\n",
      "Epoch: 207, Loss: 2399.06943, Residuals: -1.00591, Convergence: 0.002734\n",
      "Epoch: 208, Loss: 2393.28447, Residuals: -1.00352, Convergence: 0.002417\n",
      "Epoch: 209, Loss: 2388.16190, Residuals: -1.00133, Convergence: 0.002145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 210, Loss: 2383.60506, Residuals: -0.99934, Convergence: 0.001912\n",
      "Epoch: 211, Loss: 2379.53317, Residuals: -0.99751, Convergence: 0.001711\n",
      "Epoch: 212, Loss: 2375.87676, Residuals: -0.99585, Convergence: 0.001539\n",
      "Epoch: 213, Loss: 2372.57689, Residuals: -0.99432, Convergence: 0.001391\n",
      "Epoch: 214, Loss: 2369.58388, Residuals: -0.99293, Convergence: 0.001263\n",
      "Epoch: 215, Loss: 2366.85578, Residuals: -0.99166, Convergence: 0.001153\n",
      "Epoch: 216, Loss: 2364.35688, Residuals: -0.99049, Convergence: 0.001057\n",
      "Epoch: 217, Loss: 2362.05780, Residuals: -0.98942, Convergence: 0.000973\n",
      "Evidence 14523.067\n",
      "\n",
      "Epoch: 217, Evidence: 14523.06738, Convergence: 0.026654\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 3.33e-01\n",
      "Epoch: 217, Loss: 2461.19397, Residuals: -0.98942, Convergence:   inf\n",
      "Epoch: 218, Loss: 2454.34248, Residuals: -0.98602, Convergence: 0.002792\n",
      "Epoch: 219, Loss: 2448.67148, Residuals: -0.98313, Convergence: 0.002316\n",
      "Epoch: 220, Loss: 2443.86834, Residuals: -0.98072, Convergence: 0.001965\n",
      "Epoch: 221, Loss: 2439.74441, Residuals: -0.97870, Convergence: 0.001690\n",
      "Epoch: 222, Loss: 2436.16069, Residuals: -0.97701, Convergence: 0.001471\n",
      "Epoch: 223, Loss: 2433.00997, Residuals: -0.97558, Convergence: 0.001295\n",
      "Epoch: 224, Loss: 2430.20820, Residuals: -0.97436, Convergence: 0.001153\n",
      "Epoch: 225, Loss: 2427.69237, Residuals: -0.97333, Convergence: 0.001036\n",
      "Epoch: 226, Loss: 2425.41161, Residuals: -0.97244, Convergence: 0.000940\n",
      "Evidence 14611.420\n",
      "\n",
      "Epoch: 226, Evidence: 14611.41992, Convergence: 0.006047\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 2.60e-01\n",
      "Epoch: 226, Loss: 2462.48635, Residuals: -0.97244, Convergence:   inf\n",
      "Epoch: 227, Loss: 2458.47067, Residuals: -0.96996, Convergence: 0.001633\n",
      "Epoch: 228, Loss: 2455.14603, Residuals: -0.96805, Convergence: 0.001354\n",
      "Epoch: 229, Loss: 2452.31100, Residuals: -0.96653, Convergence: 0.001156\n",
      "Epoch: 230, Loss: 2449.84483, Residuals: -0.96532, Convergence: 0.001007\n",
      "Epoch: 231, Loss: 2447.66329, Residuals: -0.96433, Convergence: 0.000891\n",
      "Evidence 14643.663\n",
      "\n",
      "Epoch: 231, Evidence: 14643.66309, Convergence: 0.002202\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 2.07e-01\n",
      "Epoch: 231, Loss: 2463.26369, Residuals: -0.96433, Convergence:   inf\n",
      "Epoch: 232, Loss: 2460.44654, Residuals: -0.96249, Convergence: 0.001145\n",
      "Epoch: 233, Loss: 2458.09375, Residuals: -0.96109, Convergence: 0.000957\n",
      "Evidence 14656.496\n",
      "\n",
      "Epoch: 233, Evidence: 14656.49609, Convergence: 0.000876\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.71e-01\n",
      "Epoch: 233, Loss: 2463.88563, Residuals: -0.96109, Convergence:   inf\n",
      "Epoch: 234, Loss: 2459.52776, Residuals: -0.95869, Convergence: 0.001772\n",
      "Epoch: 235, Loss: 2456.16474, Residuals: -0.95697, Convergence: 0.001369\n",
      "Epoch: 236, Loss: 2453.42987, Residuals: -0.95578, Convergence: 0.001115\n",
      "Epoch: 237, Loss: 2451.10172, Residuals: -0.95507, Convergence: 0.000950\n",
      "Evidence 14673.822\n",
      "\n",
      "Epoch: 237, Evidence: 14673.82227, Convergence: 0.002055\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.41e-01\n",
      "Epoch: 237, Loss: 2463.96899, Residuals: -0.95507, Convergence:   inf\n",
      "Epoch: 238, Loss: 2461.06802, Residuals: -0.95258, Convergence: 0.001179\n",
      "Epoch: 239, Loss: 2458.75996, Residuals: -0.95126, Convergence: 0.000939\n",
      "Evidence 14684.535\n",
      "\n",
      "Epoch: 239, Evidence: 14684.53516, Convergence: 0.000730\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.19e-01\n",
      "Epoch: 239, Loss: 2464.13201, Residuals: -0.95126, Convergence:   inf\n",
      "Epoch: 240, Loss: 2459.90048, Residuals: -0.94755, Convergence: 0.001720\n",
      "Epoch: 241, Loss: 2456.83739, Residuals: -0.94824, Convergence: 0.001247\n",
      "Epoch: 242, Loss: 2454.22415, Residuals: -0.94844, Convergence: 0.001065\n",
      "Epoch: 243, Loss: 2451.98032, Residuals: -0.95107, Convergence: 0.000915\n",
      "Evidence 14699.795\n",
      "\n",
      "Epoch: 243, Evidence: 14699.79492, Convergence: 0.001767\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.06e-01\n",
      "Epoch: 243, Loss: 2463.34580, Residuals: -0.95107, Convergence:   inf\n",
      "Epoch: 244, Loss: 2461.49206, Residuals: -0.94855, Convergence: 0.000753\n",
      "Evidence 14706.531\n",
      "\n",
      "Epoch: 244, Evidence: 14706.53125, Convergence: 0.000458\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 8.68e-02\n",
      "Epoch: 244, Loss: 2464.35092, Residuals: -0.94855, Convergence:   inf\n",
      "Epoch: 245, Loss: 2501.93396, Residuals: -0.99489, Convergence: -0.015022\n",
      "Epoch: 245, Loss: 2462.22243, Residuals: -0.94651, Convergence: 0.000864\n",
      "Evidence 14710.623\n",
      "\n",
      "Epoch: 245, Evidence: 14710.62305, Convergence: 0.000736\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 8.03e-02\n",
      "Epoch: 245, Loss: 2463.71072, Residuals: -0.94651, Convergence:   inf\n",
      "Epoch: 246, Loss: 2467.94602, Residuals: -0.94966, Convergence: -0.001716\n",
      "Epoch: 246, Loss: 2463.46686, Residuals: -0.94515, Convergence: 0.000099\n",
      "Evidence 14712.516\n",
      "\n",
      "Epoch: 246, Evidence: 14712.51562, Convergence: 0.000865\n",
      "Total samples: 181, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 381.47067, Residuals: -4.52701, Convergence:   inf\n",
      "Epoch: 1, Loss: 355.81858, Residuals: -4.40797, Convergence: 0.072093\n",
      "Epoch: 2, Loss: 334.80398, Residuals: -4.24466, Convergence: 0.062767\n",
      "Epoch: 3, Loss: 318.74169, Residuals: -4.08063, Convergence: 0.050393\n",
      "Epoch: 4, Loss: 306.49297, Residuals: -3.93596, Convergence: 0.039964\n",
      "Epoch: 5, Loss: 296.78047, Residuals: -3.80804, Convergence: 0.032726\n",
      "Epoch: 6, Loss: 288.89680, Residuals: -3.69654, Convergence: 0.027289\n",
      "Epoch: 7, Loss: 282.35320, Residuals: -3.60096, Convergence: 0.023175\n",
      "Epoch: 8, Loss: 276.78710, Residuals: -3.51925, Convergence: 0.020110\n",
      "Epoch: 9, Loss: 271.94203, Residuals: -3.44909, Convergence: 0.017817\n",
      "Epoch: 10, Loss: 267.63569, Residuals: -3.38837, Convergence: 0.016090\n",
      "Epoch: 11, Loss: 263.73610, Residuals: -3.33533, Convergence: 0.014786\n",
      "Epoch: 12, Loss: 260.14713, Residuals: -3.28843, Convergence: 0.013796\n",
      "Epoch: 13, Loss: 256.79940, Residuals: -3.24635, Convergence: 0.013036\n",
      "Epoch: 14, Loss: 253.64484, Residuals: -3.20795, Convergence: 0.012437\n",
      "Epoch: 15, Loss: 250.65421, Residuals: -3.17233, Convergence: 0.011931\n",
      "Epoch: 16, Loss: 247.81426, Residuals: -3.13893, Convergence: 0.011460\n",
      "Epoch: 17, Loss: 245.11568, Residuals: -3.10741, Convergence: 0.011009\n",
      "Epoch: 18, Loss: 242.53837, Residuals: -3.07742, Convergence: 0.010626\n",
      "Epoch: 19, Loss: 240.04981, Residuals: -3.04844, Convergence: 0.010367\n",
      "Epoch: 20, Loss: 237.61317, Residuals: -3.01989, Convergence: 0.010255\n",
      "Epoch: 21, Loss: 235.19611, Residuals: -2.99123, Convergence: 0.010277\n",
      "Epoch: 22, Loss: 232.77563, Residuals: -2.96210, Convergence: 0.010398\n",
      "Epoch: 23, Loss: 230.33189, Residuals: -2.93224, Convergence: 0.010610\n",
      "Epoch: 24, Loss: 227.83232, Residuals: -2.90126, Convergence: 0.010971\n",
      "Epoch: 25, Loss: 225.22593, Residuals: -2.86858, Convergence: 0.011572\n",
      "Epoch: 26, Loss: 222.47837, Residuals: -2.83370, Convergence: 0.012350\n",
      "Epoch: 27, Loss: 219.65335, Residuals: -2.79725, Convergence: 0.012861\n",
      "Epoch: 28, Loss: 216.87443, Residuals: -2.76063, Convergence: 0.012813\n",
      "Epoch: 29, Loss: 214.19091, Residuals: -2.72452, Convergence: 0.012529\n",
      "Epoch: 30, Loss: 211.59544, Residuals: -2.68897, Convergence: 0.012266\n",
      "Epoch: 31, Loss: 209.07056, Residuals: -2.65384, Convergence: 0.012077\n",
      "Epoch: 32, Loss: 206.60239, Residuals: -2.61900, Convergence: 0.011946\n",
      "Epoch: 33, Loss: 204.18225, Residuals: -2.58435, Convergence: 0.011853\n",
      "Epoch: 34, Loss: 201.80577, Residuals: -2.54985, Convergence: 0.011776\n",
      "Epoch: 35, Loss: 199.47163, Residuals: -2.51545, Convergence: 0.011702\n",
      "Epoch: 36, Loss: 197.18048, Residuals: -2.48114, Convergence: 0.011620\n",
      "Epoch: 37, Loss: 194.93396, Residuals: -2.44692, Convergence: 0.011524\n",
      "Epoch: 38, Loss: 192.73414, Residuals: -2.41282, Convergence: 0.011414\n",
      "Epoch: 39, Loss: 190.58307, Residuals: -2.37885, Convergence: 0.011287\n",
      "Epoch: 40, Loss: 188.48263, Residuals: -2.34503, Convergence: 0.011144\n",
      "Epoch: 41, Loss: 186.43455, Residuals: -2.31139, Convergence: 0.010986\n",
      "Epoch: 42, Loss: 184.44041, Residuals: -2.27797, Convergence: 0.010812\n",
      "Epoch: 43, Loss: 182.50177, Residuals: -2.24480, Convergence: 0.010623\n",
      "Epoch: 44, Loss: 180.62020, Residuals: -2.21193, Convergence: 0.010417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45, Loss: 178.79724, Residuals: -2.17938, Convergence: 0.010196\n",
      "Epoch: 46, Loss: 177.03438, Residuals: -2.14719, Convergence: 0.009958\n",
      "Epoch: 47, Loss: 175.33292, Residuals: -2.11540, Convergence: 0.009704\n",
      "Epoch: 48, Loss: 173.69402, Residuals: -2.08402, Convergence: 0.009436\n",
      "Epoch: 49, Loss: 172.11850, Residuals: -2.05311, Convergence: 0.009154\n",
      "Epoch: 50, Loss: 170.60682, Residuals: -2.02269, Convergence: 0.008861\n",
      "Epoch: 51, Loss: 169.15900, Residuals: -1.99281, Convergence: 0.008559\n",
      "Epoch: 52, Loss: 167.77456, Residuals: -1.96349, Convergence: 0.008252\n",
      "Epoch: 53, Loss: 166.45250, Residuals: -1.93479, Convergence: 0.007943\n",
      "Epoch: 54, Loss: 165.19133, Residuals: -1.90673, Convergence: 0.007635\n",
      "Epoch: 55, Loss: 163.98906, Residuals: -1.87933, Convergence: 0.007331\n",
      "Epoch: 56, Loss: 162.84333, Residuals: -1.85262, Convergence: 0.007036\n",
      "Epoch: 57, Loss: 161.75143, Residuals: -1.82661, Convergence: 0.006750\n",
      "Epoch: 58, Loss: 160.71047, Residuals: -1.80129, Convergence: 0.006477\n",
      "Epoch: 59, Loss: 159.71744, Residuals: -1.77667, Convergence: 0.006217\n",
      "Epoch: 60, Loss: 158.76940, Residuals: -1.75272, Convergence: 0.005971\n",
      "Epoch: 61, Loss: 157.86355, Residuals: -1.72945, Convergence: 0.005738\n",
      "Epoch: 62, Loss: 156.99729, Residuals: -1.70683, Convergence: 0.005518\n",
      "Epoch: 63, Loss: 156.16834, Residuals: -1.68485, Convergence: 0.005308\n",
      "Epoch: 64, Loss: 155.37466, Residuals: -1.66350, Convergence: 0.005108\n",
      "Epoch: 65, Loss: 154.61451, Residuals: -1.64276, Convergence: 0.004916\n",
      "Epoch: 66, Loss: 153.88637, Residuals: -1.62263, Convergence: 0.004732\n",
      "Epoch: 67, Loss: 153.18891, Residuals: -1.60310, Convergence: 0.004553\n",
      "Epoch: 68, Loss: 152.52097, Residuals: -1.58415, Convergence: 0.004379\n",
      "Epoch: 69, Loss: 151.88152, Residuals: -1.56580, Convergence: 0.004210\n",
      "Epoch: 70, Loss: 151.26961, Residuals: -1.54801, Convergence: 0.004045\n",
      "Epoch: 71, Loss: 150.68434, Residuals: -1.53080, Convergence: 0.003884\n",
      "Epoch: 72, Loss: 150.12490, Residuals: -1.51416, Convergence: 0.003727\n",
      "Epoch: 73, Loss: 149.59044, Residuals: -1.49806, Convergence: 0.003573\n",
      "Epoch: 74, Loss: 149.08020, Residuals: -1.48252, Convergence: 0.003423\n",
      "Epoch: 75, Loss: 148.59338, Residuals: -1.46751, Convergence: 0.003276\n",
      "Epoch: 76, Loss: 148.12922, Residuals: -1.45304, Convergence: 0.003133\n",
      "Epoch: 77, Loss: 147.68693, Residuals: -1.43908, Convergence: 0.002995\n",
      "Epoch: 78, Loss: 147.26572, Residuals: -1.42564, Convergence: 0.002860\n",
      "Epoch: 79, Loss: 146.86482, Residuals: -1.41269, Convergence: 0.002730\n",
      "Epoch: 80, Loss: 146.48343, Residuals: -1.40022, Convergence: 0.002604\n",
      "Epoch: 81, Loss: 146.12076, Residuals: -1.38824, Convergence: 0.002482\n",
      "Epoch: 82, Loss: 145.77603, Residuals: -1.37671, Convergence: 0.002365\n",
      "Epoch: 83, Loss: 145.44847, Residuals: -1.36564, Convergence: 0.002252\n",
      "Epoch: 84, Loss: 145.13731, Residuals: -1.35500, Convergence: 0.002144\n",
      "Epoch: 85, Loss: 144.84181, Residuals: -1.34478, Convergence: 0.002040\n",
      "Epoch: 86, Loss: 144.56124, Residuals: -1.33498, Convergence: 0.001941\n",
      "Epoch: 87, Loss: 144.29492, Residuals: -1.32557, Convergence: 0.001846\n",
      "Epoch: 88, Loss: 144.04219, Residuals: -1.31655, Convergence: 0.001755\n",
      "Epoch: 89, Loss: 143.80242, Residuals: -1.30789, Convergence: 0.001667\n",
      "Epoch: 90, Loss: 143.57503, Residuals: -1.29960, Convergence: 0.001584\n",
      "Epoch: 91, Loss: 143.35946, Residuals: -1.29166, Convergence: 0.001504\n",
      "Epoch: 92, Loss: 143.15520, Residuals: -1.28405, Convergence: 0.001427\n",
      "Epoch: 93, Loss: 142.96180, Residuals: -1.27676, Convergence: 0.001353\n",
      "Epoch: 94, Loss: 142.77882, Residuals: -1.26979, Convergence: 0.001282\n",
      "Epoch: 95, Loss: 142.60584, Residuals: -1.26313, Convergence: 0.001213\n",
      "Epoch: 96, Loss: 142.44253, Residuals: -1.25677, Convergence: 0.001147\n",
      "Epoch: 97, Loss: 142.28851, Residuals: -1.25069, Convergence: 0.001082\n",
      "Epoch: 98, Loss: 142.14346, Residuals: -1.24491, Convergence: 0.001020\n",
      "Epoch: 99, Loss: 142.00703, Residuals: -1.23940, Convergence: 0.000961\n",
      "Evidence -183.585\n",
      "\n",
      "Epoch: 99, Evidence: -183.58511, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 7.25e-01\n",
      "Epoch: 99, Loss: 1370.24481, Residuals: -1.23940, Convergence:   inf\n",
      "Epoch: 100, Loss: 1307.78679, Residuals: -1.26950, Convergence: 0.047759\n",
      "Epoch: 101, Loss: 1259.64797, Residuals: -1.29300, Convergence: 0.038216\n",
      "Epoch: 102, Loss: 1222.88682, Residuals: -1.30983, Convergence: 0.030061\n",
      "Epoch: 103, Loss: 1194.19712, Residuals: -1.32153, Convergence: 0.024024\n",
      "Epoch: 104, Loss: 1171.03213, Residuals: -1.33005, Convergence: 0.019782\n",
      "Epoch: 105, Loss: 1151.84501, Residuals: -1.33647, Convergence: 0.016658\n",
      "Epoch: 106, Loss: 1135.67881, Residuals: -1.34130, Convergence: 0.014235\n",
      "Epoch: 107, Loss: 1121.87768, Residuals: -1.34476, Convergence: 0.012302\n",
      "Epoch: 108, Loss: 1109.95538, Residuals: -1.34700, Convergence: 0.010741\n",
      "Epoch: 109, Loss: 1099.53313, Residuals: -1.34814, Convergence: 0.009479\n",
      "Epoch: 110, Loss: 1090.30655, Residuals: -1.34826, Convergence: 0.008462\n",
      "Epoch: 111, Loss: 1082.02471, Residuals: -1.34743, Convergence: 0.007654\n",
      "Epoch: 112, Loss: 1074.47758, Residuals: -1.34572, Convergence: 0.007024\n",
      "Epoch: 113, Loss: 1067.48707, Residuals: -1.34315, Convergence: 0.006549\n",
      "Epoch: 114, Loss: 1060.90063, Residuals: -1.33978, Convergence: 0.006208\n",
      "Epoch: 115, Loss: 1054.58817, Residuals: -1.33562, Convergence: 0.005986\n",
      "Epoch: 116, Loss: 1048.44207, Residuals: -1.33069, Convergence: 0.005862\n",
      "Epoch: 117, Loss: 1042.38246, Residuals: -1.32505, Convergence: 0.005813\n",
      "Epoch: 118, Loss: 1036.36747, Residuals: -1.31876, Convergence: 0.005804\n",
      "Epoch: 119, Loss: 1030.40456, Residuals: -1.31191, Convergence: 0.005787\n",
      "Epoch: 120, Loss: 1024.55024, Residuals: -1.30460, Convergence: 0.005714\n",
      "Epoch: 121, Loss: 1018.88753, Residuals: -1.29694, Convergence: 0.005558\n",
      "Epoch: 122, Loss: 1013.49438, Residuals: -1.28903, Convergence: 0.005321\n",
      "Epoch: 123, Loss: 1008.41684, Residuals: -1.28096, Convergence: 0.005035\n",
      "Epoch: 124, Loss: 1003.66769, Residuals: -1.27278, Convergence: 0.004732\n",
      "Epoch: 125, Loss: 999.23407, Residuals: -1.26458, Convergence: 0.004437\n",
      "Epoch: 126, Loss: 995.08969, Residuals: -1.25640, Convergence: 0.004165\n",
      "Epoch: 127, Loss: 991.20456, Residuals: -1.24828, Convergence: 0.003920\n",
      "Epoch: 128, Loss: 987.54949, Residuals: -1.24027, Convergence: 0.003701\n",
      "Epoch: 129, Loss: 984.09861, Residuals: -1.23239, Convergence: 0.003507\n",
      "Epoch: 130, Loss: 980.83048, Residuals: -1.22467, Convergence: 0.003332\n",
      "Epoch: 131, Loss: 977.72659, Residuals: -1.21714, Convergence: 0.003175\n",
      "Epoch: 132, Loss: 974.77321, Residuals: -1.20980, Convergence: 0.003030\n",
      "Epoch: 133, Loss: 971.95864, Residuals: -1.20268, Convergence: 0.002896\n",
      "Epoch: 134, Loss: 969.27383, Residuals: -1.19579, Convergence: 0.002770\n",
      "Epoch: 135, Loss: 966.71156, Residuals: -1.18913, Convergence: 0.002650\n",
      "Epoch: 136, Loss: 964.26518, Residuals: -1.18273, Convergence: 0.002537\n",
      "Epoch: 137, Loss: 961.92904, Residuals: -1.17658, Convergence: 0.002429\n",
      "Epoch: 138, Loss: 959.69782, Residuals: -1.17068, Convergence: 0.002325\n",
      "Epoch: 139, Loss: 957.56608, Residuals: -1.16503, Convergence: 0.002226\n",
      "Epoch: 140, Loss: 955.52880, Residuals: -1.15963, Convergence: 0.002132\n",
      "Epoch: 141, Loss: 953.58054, Residuals: -1.15448, Convergence: 0.002043\n",
      "Epoch: 142, Loss: 951.71626, Residuals: -1.14956, Convergence: 0.001959\n",
      "Epoch: 143, Loss: 949.93062, Residuals: -1.14486, Convergence: 0.001880\n",
      "Epoch: 144, Loss: 948.21873, Residuals: -1.14039, Convergence: 0.001805\n",
      "Epoch: 145, Loss: 946.57539, Residuals: -1.13611, Convergence: 0.001736\n",
      "Epoch: 146, Loss: 944.99610, Residuals: -1.13204, Convergence: 0.001671\n",
      "Epoch: 147, Loss: 943.47596, Residuals: -1.12815, Convergence: 0.001611\n",
      "Epoch: 148, Loss: 942.01038, Residuals: -1.12442, Convergence: 0.001556\n",
      "Epoch: 149, Loss: 940.59447, Residuals: -1.12086, Convergence: 0.001505\n",
      "Epoch: 150, Loss: 939.22395, Residuals: -1.11745, Convergence: 0.001459\n",
      "Epoch: 151, Loss: 937.89404, Residuals: -1.11417, Convergence: 0.001418\n",
      "Epoch: 152, Loss: 936.59955, Residuals: -1.11102, Convergence: 0.001382\n",
      "Epoch: 153, Loss: 935.33636, Residuals: -1.10797, Convergence: 0.001351\n",
      "Epoch: 154, Loss: 934.09896, Residuals: -1.10503, Convergence: 0.001325\n",
      "Epoch: 155, Loss: 932.88249, Residuals: -1.10217, Convergence: 0.001304\n",
      "Epoch: 156, Loss: 931.68296, Residuals: -1.09938, Convergence: 0.001287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 157, Loss: 930.49592, Residuals: -1.09666, Convergence: 0.001276\n",
      "Epoch: 158, Loss: 929.31896, Residuals: -1.09399, Convergence: 0.001266\n",
      "Epoch: 159, Loss: 928.15073, Residuals: -1.09136, Convergence: 0.001259\n",
      "Epoch: 160, Loss: 926.99165, Residuals: -1.08877, Convergence: 0.001250\n",
      "Epoch: 161, Loss: 925.84421, Residuals: -1.08623, Convergence: 0.001239\n",
      "Epoch: 162, Loss: 924.71255, Residuals: -1.08374, Convergence: 0.001224\n",
      "Epoch: 163, Loss: 923.60086, Residuals: -1.08129, Convergence: 0.001204\n",
      "Epoch: 164, Loss: 922.51438, Residuals: -1.07891, Convergence: 0.001178\n",
      "Epoch: 165, Loss: 921.45740, Residuals: -1.07660, Convergence: 0.001147\n",
      "Epoch: 166, Loss: 920.43409, Residuals: -1.07436, Convergence: 0.001112\n",
      "Epoch: 167, Loss: 919.44696, Residuals: -1.07219, Convergence: 0.001074\n",
      "Epoch: 168, Loss: 918.49781, Residuals: -1.07010, Convergence: 0.001033\n",
      "Epoch: 169, Loss: 917.58740, Residuals: -1.06809, Convergence: 0.000992\n",
      "Evidence 11092.951\n",
      "\n",
      "Epoch: 169, Evidence: 11092.95117, Convergence: 1.016550\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 5.75e-01\n",
      "Epoch: 169, Loss: 2339.52639, Residuals: -1.06809, Convergence:   inf\n",
      "Epoch: 170, Loss: 2300.07687, Residuals: -1.07565, Convergence: 0.017151\n",
      "Epoch: 171, Loss: 2273.14013, Residuals: -1.07389, Convergence: 0.011850\n",
      "Epoch: 172, Loss: 2250.80014, Residuals: -1.07136, Convergence: 0.009925\n",
      "Epoch: 173, Loss: 2232.00369, Residuals: -1.06863, Convergence: 0.008421\n",
      "Epoch: 174, Loss: 2216.04582, Residuals: -1.06581, Convergence: 0.007201\n",
      "Epoch: 175, Loss: 2202.37868, Residuals: -1.06294, Convergence: 0.006206\n",
      "Epoch: 176, Loss: 2190.55441, Residuals: -1.06003, Convergence: 0.005398\n",
      "Epoch: 177, Loss: 2180.20140, Residuals: -1.05707, Convergence: 0.004749\n",
      "Epoch: 178, Loss: 2171.01645, Residuals: -1.05404, Convergence: 0.004231\n",
      "Epoch: 179, Loss: 2162.75847, Residuals: -1.05093, Convergence: 0.003818\n",
      "Epoch: 180, Loss: 2155.25066, Residuals: -1.04773, Convergence: 0.003483\n",
      "Epoch: 181, Loss: 2148.37429, Residuals: -1.04446, Convergence: 0.003201\n",
      "Epoch: 182, Loss: 2142.05330, Residuals: -1.04115, Convergence: 0.002951\n",
      "Epoch: 183, Loss: 2136.23509, Residuals: -1.03783, Convergence: 0.002724\n",
      "Epoch: 184, Loss: 2130.87598, Residuals: -1.03454, Convergence: 0.002515\n",
      "Epoch: 185, Loss: 2125.93463, Residuals: -1.03132, Convergence: 0.002324\n",
      "Epoch: 186, Loss: 2121.37136, Residuals: -1.02818, Convergence: 0.002151\n",
      "Epoch: 187, Loss: 2117.14779, Residuals: -1.02515, Convergence: 0.001995\n",
      "Epoch: 188, Loss: 2113.22910, Residuals: -1.02224, Convergence: 0.001854\n",
      "Epoch: 189, Loss: 2109.58456, Residuals: -1.01945, Convergence: 0.001728\n",
      "Epoch: 190, Loss: 2106.18690, Residuals: -1.01679, Convergence: 0.001613\n",
      "Epoch: 191, Loss: 2103.01484, Residuals: -1.01426, Convergence: 0.001508\n",
      "Epoch: 192, Loss: 2100.04871, Residuals: -1.01186, Convergence: 0.001412\n",
      "Epoch: 193, Loss: 2097.27181, Residuals: -1.00959, Convergence: 0.001324\n",
      "Epoch: 194, Loss: 2094.67065, Residuals: -1.00744, Convergence: 0.001242\n",
      "Epoch: 195, Loss: 2092.23251, Residuals: -1.00541, Convergence: 0.001165\n",
      "Epoch: 196, Loss: 2089.94561, Residuals: -1.00350, Convergence: 0.001094\n",
      "Epoch: 197, Loss: 2087.80029, Residuals: -1.00168, Convergence: 0.001028\n",
      "Epoch: 198, Loss: 2085.78620, Residuals: -0.99996, Convergence: 0.000966\n",
      "Evidence 14254.794\n",
      "\n",
      "Epoch: 198, Evidence: 14254.79395, Convergence: 0.221809\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 4.37e-01\n",
      "Epoch: 198, Loss: 2464.56694, Residuals: -0.99996, Convergence:   inf\n",
      "Epoch: 199, Loss: 2451.21653, Residuals: -0.99674, Convergence: 0.005446\n",
      "Epoch: 200, Loss: 2440.29116, Residuals: -0.99316, Convergence: 0.004477\n",
      "Epoch: 201, Loss: 2430.88242, Residuals: -0.98975, Convergence: 0.003871\n",
      "Epoch: 202, Loss: 2422.73881, Residuals: -0.98659, Convergence: 0.003361\n",
      "Epoch: 203, Loss: 2415.65699, Residuals: -0.98369, Convergence: 0.002932\n",
      "Epoch: 204, Loss: 2409.47035, Residuals: -0.98106, Convergence: 0.002568\n",
      "Epoch: 205, Loss: 2404.03609, Residuals: -0.97868, Convergence: 0.002260\n",
      "Epoch: 206, Loss: 2399.23595, Residuals: -0.97651, Convergence: 0.002001\n",
      "Epoch: 207, Loss: 2394.97256, Residuals: -0.97455, Convergence: 0.001780\n",
      "Epoch: 208, Loss: 2391.16254, Residuals: -0.97276, Convergence: 0.001593\n",
      "Epoch: 209, Loss: 2387.73827, Residuals: -0.97114, Convergence: 0.001434\n",
      "Epoch: 210, Loss: 2384.64331, Residuals: -0.96965, Convergence: 0.001298\n",
      "Epoch: 211, Loss: 2381.83043, Residuals: -0.96830, Convergence: 0.001181\n",
      "Epoch: 212, Loss: 2379.26131, Residuals: -0.96706, Convergence: 0.001080\n",
      "Epoch: 213, Loss: 2376.90327, Residuals: -0.96592, Convergence: 0.000992\n",
      "Evidence 14625.085\n",
      "\n",
      "Epoch: 213, Evidence: 14625.08496, Convergence: 0.025319\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 3.34e-01\n",
      "Epoch: 213, Loss: 2470.18905, Residuals: -0.96592, Convergence:   inf\n",
      "Epoch: 214, Loss: 2463.62114, Residuals: -0.96271, Convergence: 0.002666\n",
      "Epoch: 215, Loss: 2458.19221, Residuals: -0.95999, Convergence: 0.002209\n",
      "Epoch: 216, Loss: 2453.59880, Residuals: -0.95770, Convergence: 0.001872\n",
      "Epoch: 217, Loss: 2449.65538, Residuals: -0.95577, Convergence: 0.001610\n",
      "Epoch: 218, Loss: 2446.22488, Residuals: -0.95415, Convergence: 0.001402\n",
      "Epoch: 219, Loss: 2443.20192, Residuals: -0.95279, Convergence: 0.001237\n",
      "Epoch: 220, Loss: 2440.50724, Residuals: -0.95163, Convergence: 0.001104\n",
      "Epoch: 221, Loss: 2438.08061, Residuals: -0.95066, Convergence: 0.000995\n",
      "Evidence 14704.963\n",
      "\n",
      "Epoch: 221, Evidence: 14704.96289, Convergence: 0.005432\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 2.61e-01\n",
      "Epoch: 221, Loss: 2471.91162, Residuals: -0.95066, Convergence:   inf\n",
      "Epoch: 222, Loss: 2467.91547, Residuals: -0.94836, Convergence: 0.001619\n",
      "Epoch: 223, Loss: 2464.60311, Residuals: -0.94657, Convergence: 0.001344\n",
      "Epoch: 224, Loss: 2461.77258, Residuals: -0.94514, Convergence: 0.001150\n",
      "Epoch: 225, Loss: 2459.30320, Residuals: -0.94399, Convergence: 0.001004\n",
      "Epoch: 226, Loss: 2457.11207, Residuals: -0.94306, Convergence: 0.000892\n",
      "Evidence 14735.558\n",
      "\n",
      "Epoch: 226, Evidence: 14735.55762, Convergence: 0.002076\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 2.08e-01\n",
      "Epoch: 226, Loss: 2472.85247, Residuals: -0.94306, Convergence:   inf\n",
      "Epoch: 227, Loss: 2470.02695, Residuals: -0.94134, Convergence: 0.001144\n",
      "Epoch: 228, Loss: 2467.65648, Residuals: -0.94002, Convergence: 0.000961\n",
      "Evidence 14748.273\n",
      "\n",
      "Epoch: 228, Evidence: 14748.27344, Convergence: 0.000862\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.72e-01\n",
      "Epoch: 228, Loss: 2473.56642, Residuals: -0.94002, Convergence:   inf\n",
      "Epoch: 229, Loss: 2469.09639, Residuals: -0.93771, Convergence: 0.001810\n",
      "Epoch: 230, Loss: 2465.67873, Residuals: -0.93603, Convergence: 0.001386\n",
      "Epoch: 231, Loss: 2462.89772, Residuals: -0.93491, Convergence: 0.001129\n",
      "Epoch: 232, Loss: 2460.53111, Residuals: -0.93427, Convergence: 0.000962\n",
      "Evidence 14765.764\n",
      "\n",
      "Epoch: 232, Evidence: 14765.76367, Convergence: 0.002046\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.42e-01\n",
      "Epoch: 232, Loss: 2473.71115, Residuals: -0.93427, Convergence:   inf\n",
      "Epoch: 233, Loss: 2470.71605, Residuals: -0.93191, Convergence: 0.001212\n",
      "Epoch: 234, Loss: 2468.33859, Residuals: -0.93066, Convergence: 0.000963\n",
      "Evidence 14776.577\n",
      "\n",
      "Epoch: 234, Evidence: 14776.57715, Convergence: 0.000732\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.21e-01\n",
      "Epoch: 234, Loss: 2473.90932, Residuals: -0.93066, Convergence:   inf\n",
      "Epoch: 235, Loss: 2469.51589, Residuals: -0.92716, Convergence: 0.001779\n",
      "Epoch: 236, Loss: 2466.35654, Residuals: -0.92792, Convergence: 0.001281\n",
      "Epoch: 237, Loss: 2463.69383, Residuals: -0.92838, Convergence: 0.001081\n",
      "Epoch: 238, Loss: 2461.38988, Residuals: -0.93091, Convergence: 0.000936\n",
      "Evidence 14792.247\n",
      "\n",
      "Epoch: 238, Evidence: 14792.24707, Convergence: 0.001790\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.07e-01\n",
      "Epoch: 238, Loss: 2473.17896, Residuals: -0.93091, Convergence:   inf\n",
      "Epoch: 239, Loss: 2471.27975, Residuals: -0.92801, Convergence: 0.000769\n",
      "Evidence 14799.055\n",
      "\n",
      "Epoch: 239, Evidence: 14799.05469, Convergence: 0.000460\n",
      "Updating hyper-parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 181, Updated regularization: 8.78e-02\n",
      "Epoch: 239, Loss: 2474.12138, Residuals: -0.92801, Convergence:   inf\n",
      "Epoch: 240, Loss: 2513.27941, Residuals: -0.96704, Convergence: -0.015580\n",
      "Epoch: 240, Loss: 2471.97246, Residuals: -0.92621, Convergence: 0.000869\n",
      "Evidence 14803.180\n",
      "\n",
      "Epoch: 240, Evidence: 14803.17969, Convergence: 0.000739\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 8.11e-02\n",
      "Epoch: 240, Loss: 2473.53580, Residuals: -0.92621, Convergence:   inf\n",
      "Epoch: 241, Loss: 2478.54440, Residuals: -0.92678, Convergence: -0.002021\n",
      "Epoch: 241, Loss: 2473.55952, Residuals: -0.92413, Convergence: -0.000010\n",
      "Evidence 14804.786\n",
      "\n",
      "Epoch: 241, Evidence: 14804.78613, Convergence: 0.000847\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 381.37759, Residuals: -4.50216, Convergence:   inf\n",
      "Epoch: 1, Loss: 355.78147, Residuals: -4.38397, Convergence: 0.071943\n",
      "Epoch: 2, Loss: 334.81540, Residuals: -4.22180, Convergence: 0.062620\n",
      "Epoch: 3, Loss: 318.77997, Residuals: -4.05858, Convergence: 0.050302\n",
      "Epoch: 4, Loss: 306.54759, Residuals: -3.91458, Convergence: 0.039904\n",
      "Epoch: 5, Loss: 296.84275, Residuals: -3.78719, Convergence: 0.032694\n",
      "Epoch: 6, Loss: 288.96031, Residuals: -3.67616, Convergence: 0.027279\n",
      "Epoch: 7, Loss: 282.41095, Residuals: -3.58101, Convergence: 0.023191\n",
      "Epoch: 8, Loss: 276.83221, Residuals: -3.49971, Convergence: 0.020152\n",
      "Epoch: 9, Loss: 271.96849, Residuals: -3.42995, Convergence: 0.017883\n",
      "Epoch: 10, Loss: 267.63833, Residuals: -3.36964, Convergence: 0.016179\n",
      "Epoch: 11, Loss: 263.71068, Residuals: -3.31703, Convergence: 0.014894\n",
      "Epoch: 12, Loss: 260.09028, Residuals: -3.27059, Convergence: 0.013920\n",
      "Epoch: 13, Loss: 256.70843, Residuals: -3.22901, Convergence: 0.013174\n",
      "Epoch: 14, Loss: 253.51682, Residuals: -3.19113, Convergence: 0.012589\n",
      "Epoch: 15, Loss: 250.48456, Residuals: -3.15602, Convergence: 0.012106\n",
      "Epoch: 16, Loss: 247.59585, Residuals: -3.12302, Convergence: 0.011667\n",
      "Epoch: 17, Loss: 244.84009, Residuals: -3.09174, Convergence: 0.011255\n",
      "Epoch: 18, Loss: 242.19744, Residuals: -3.06178, Convergence: 0.010911\n",
      "Epoch: 19, Loss: 239.63594, Residuals: -3.03263, Convergence: 0.010689\n",
      "Epoch: 20, Loss: 237.12001, Residuals: -3.00371, Convergence: 0.010610\n",
      "Epoch: 21, Loss: 234.62052, Residuals: -2.97456, Convergence: 0.010653\n",
      "Epoch: 22, Loss: 232.11858, Residuals: -2.94492, Convergence: 0.010779\n",
      "Epoch: 23, Loss: 229.59470, Residuals: -2.91460, Convergence: 0.010993\n",
      "Epoch: 24, Loss: 227.01146, Residuals: -2.88318, Convergence: 0.011379\n",
      "Epoch: 25, Loss: 224.31112, Residuals: -2.84998, Convergence: 0.012038\n",
      "Epoch: 26, Loss: 221.45332, Residuals: -2.81447, Convergence: 0.012905\n",
      "Epoch: 27, Loss: 218.50530, Residuals: -2.77726, Convergence: 0.013492\n",
      "Epoch: 28, Loss: 215.60309, Residuals: -2.73982, Convergence: 0.013461\n",
      "Epoch: 29, Loss: 212.80102, Residuals: -2.70289, Convergence: 0.013168\n",
      "Epoch: 30, Loss: 210.09218, Residuals: -2.66651, Convergence: 0.012894\n",
      "Epoch: 31, Loss: 207.45976, Residuals: -2.63055, Convergence: 0.012689\n",
      "Epoch: 32, Loss: 204.89101, Residuals: -2.59490, Convergence: 0.012537\n",
      "Epoch: 33, Loss: 202.37862, Residuals: -2.55946, Convergence: 0.012414\n",
      "Epoch: 34, Loss: 199.91952, Residuals: -2.52419, Convergence: 0.012300\n",
      "Epoch: 35, Loss: 197.51344, Residuals: -2.48907, Convergence: 0.012182\n",
      "Epoch: 36, Loss: 195.16166, Residuals: -2.45410, Convergence: 0.012050\n",
      "Epoch: 37, Loss: 192.86614, Residuals: -2.41927, Convergence: 0.011902\n",
      "Epoch: 38, Loss: 190.62888, Residuals: -2.38462, Convergence: 0.011736\n",
      "Epoch: 39, Loss: 188.45161, Residuals: -2.35015, Convergence: 0.011553\n",
      "Epoch: 40, Loss: 186.33568, Residuals: -2.31589, Convergence: 0.011355\n",
      "Epoch: 41, Loss: 184.28211, Residuals: -2.28187, Convergence: 0.011144\n",
      "Epoch: 42, Loss: 182.29167, Residuals: -2.24810, Convergence: 0.010919\n",
      "Epoch: 43, Loss: 180.36511, Residuals: -2.21462, Convergence: 0.010681\n",
      "Epoch: 44, Loss: 178.50322, Residuals: -2.18146, Convergence: 0.010431\n",
      "Epoch: 45, Loss: 176.70685, Residuals: -2.14867, Convergence: 0.010166\n",
      "Epoch: 46, Loss: 174.97681, Residuals: -2.11629, Convergence: 0.009887\n",
      "Epoch: 47, Loss: 173.31367, Residuals: -2.08436, Convergence: 0.009596\n",
      "Epoch: 48, Loss: 171.71758, Residuals: -2.05294, Convergence: 0.009295\n",
      "Epoch: 49, Loss: 170.18816, Residuals: -2.02208, Convergence: 0.008987\n",
      "Epoch: 50, Loss: 168.72447, Residuals: -1.99179, Convergence: 0.008675\n",
      "Epoch: 51, Loss: 167.32506, Residuals: -1.96212, Convergence: 0.008363\n",
      "Epoch: 52, Loss: 165.98821, Residuals: -1.93308, Convergence: 0.008054\n",
      "Epoch: 53, Loss: 164.71201, Residuals: -1.90468, Convergence: 0.007748\n",
      "Epoch: 54, Loss: 163.49447, Residuals: -1.87693, Convergence: 0.007447\n",
      "Epoch: 55, Loss: 162.33362, Residuals: -1.84985, Convergence: 0.007151\n",
      "Epoch: 56, Loss: 161.22746, Residuals: -1.82344, Convergence: 0.006861\n",
      "Epoch: 57, Loss: 160.17387, Residuals: -1.79772, Convergence: 0.006578\n",
      "Epoch: 58, Loss: 159.17066, Residuals: -1.77268, Convergence: 0.006303\n",
      "Epoch: 59, Loss: 158.21545, Residuals: -1.74834, Convergence: 0.006037\n",
      "Epoch: 60, Loss: 157.30571, Residuals: -1.72470, Convergence: 0.005783\n",
      "Epoch: 61, Loss: 156.43879, Residuals: -1.70174, Convergence: 0.005542\n",
      "Epoch: 62, Loss: 155.61199, Residuals: -1.67947, Convergence: 0.005313\n",
      "Epoch: 63, Loss: 154.82262, Residuals: -1.65786, Convergence: 0.005099\n",
      "Epoch: 64, Loss: 154.06814, Residuals: -1.63689, Convergence: 0.004897\n",
      "Epoch: 65, Loss: 153.34619, Residuals: -1.61655, Convergence: 0.004708\n",
      "Epoch: 66, Loss: 152.65469, Residuals: -1.59682, Convergence: 0.004530\n",
      "Epoch: 67, Loss: 151.99183, Residuals: -1.57768, Convergence: 0.004361\n",
      "Epoch: 68, Loss: 151.35607, Residuals: -1.55911, Convergence: 0.004200\n",
      "Epoch: 69, Loss: 150.74612, Residuals: -1.54111, Convergence: 0.004046\n",
      "Epoch: 70, Loss: 150.16090, Residuals: -1.52366, Convergence: 0.003897\n",
      "Epoch: 71, Loss: 149.59949, Residuals: -1.50676, Convergence: 0.003753\n",
      "Epoch: 72, Loss: 149.06107, Residuals: -1.49039, Convergence: 0.003612\n",
      "Epoch: 73, Loss: 148.54493, Residuals: -1.47456, Convergence: 0.003475\n",
      "Epoch: 74, Loss: 148.05045, Residuals: -1.45925, Convergence: 0.003340\n",
      "Epoch: 75, Loss: 147.57696, Residuals: -1.44446, Convergence: 0.003208\n",
      "Epoch: 76, Loss: 147.12390, Residuals: -1.43018, Convergence: 0.003079\n",
      "Epoch: 77, Loss: 146.69064, Residuals: -1.41641, Convergence: 0.002954\n",
      "Epoch: 78, Loss: 146.27662, Residuals: -1.40313, Convergence: 0.002830\n",
      "Epoch: 79, Loss: 145.88121, Residuals: -1.39034, Convergence: 0.002710\n",
      "Epoch: 80, Loss: 145.50381, Residuals: -1.37804, Convergence: 0.002594\n",
      "Epoch: 81, Loss: 145.14378, Residuals: -1.36620, Convergence: 0.002481\n",
      "Epoch: 82, Loss: 144.80049, Residuals: -1.35482, Convergence: 0.002371\n",
      "Epoch: 83, Loss: 144.47329, Residuals: -1.34388, Convergence: 0.002265\n",
      "Epoch: 84, Loss: 144.16156, Residuals: -1.33339, Convergence: 0.002162\n",
      "Epoch: 85, Loss: 143.86463, Residuals: -1.32331, Convergence: 0.002064\n",
      "Epoch: 86, Loss: 143.58189, Residuals: -1.31365, Convergence: 0.001969\n",
      "Epoch: 87, Loss: 143.31269, Residuals: -1.30438, Convergence: 0.001878\n",
      "Epoch: 88, Loss: 143.05645, Residuals: -1.29550, Convergence: 0.001791\n",
      "Epoch: 89, Loss: 142.81257, Residuals: -1.28699, Convergence: 0.001708\n",
      "Epoch: 90, Loss: 142.58050, Residuals: -1.27884, Convergence: 0.001628\n",
      "Epoch: 91, Loss: 142.35971, Residuals: -1.27104, Convergence: 0.001551\n",
      "Epoch: 92, Loss: 142.14970, Residuals: -1.26357, Convergence: 0.001477\n",
      "Epoch: 93, Loss: 141.95001, Residuals: -1.25642, Convergence: 0.001407\n",
      "Epoch: 94, Loss: 141.76020, Residuals: -1.24958, Convergence: 0.001339\n",
      "Epoch: 95, Loss: 141.57990, Residuals: -1.24304, Convergence: 0.001274\n",
      "Epoch: 96, Loss: 141.40872, Residuals: -1.23679, Convergence: 0.001210\n",
      "Epoch: 97, Loss: 141.24636, Residuals: -1.23081, Convergence: 0.001149\n",
      "Epoch: 98, Loss: 141.09252, Residuals: -1.22511, Convergence: 0.001090\n",
      "Epoch: 99, Loss: 140.94691, Residuals: -1.21968, Convergence: 0.001033\n",
      "Epoch: 100, Loss: 140.80930, Residuals: -1.21450, Convergence: 0.000977\n",
      "Evidence -182.628\n",
      "\n",
      "Epoch: 100, Evidence: -182.62840, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.25e-01\n",
      "Epoch: 100, Loss: 1372.24902, Residuals: -1.21450, Convergence:   inf\n",
      "Epoch: 101, Loss: 1309.64394, Residuals: -1.24346, Convergence: 0.047803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 102, Loss: 1261.20963, Residuals: -1.26667, Convergence: 0.038403\n",
      "Epoch: 103, Loss: 1224.14009, Residuals: -1.28396, Convergence: 0.030282\n",
      "Epoch: 104, Loss: 1195.21725, Residuals: -1.29654, Convergence: 0.024199\n",
      "Epoch: 105, Loss: 1171.91212, Residuals: -1.30607, Convergence: 0.019886\n",
      "Epoch: 106, Loss: 1152.66327, Residuals: -1.31355, Convergence: 0.016699\n",
      "Epoch: 107, Loss: 1136.50206, Residuals: -1.31941, Convergence: 0.014220\n",
      "Epoch: 108, Loss: 1122.76475, Residuals: -1.32389, Convergence: 0.012235\n",
      "Epoch: 109, Loss: 1110.96117, Residuals: -1.32713, Convergence: 0.010625\n",
      "Epoch: 110, Loss: 1100.70926, Residuals: -1.32926, Convergence: 0.009314\n",
      "Epoch: 111, Loss: 1091.70307, Residuals: -1.33035, Convergence: 0.008250\n",
      "Epoch: 112, Loss: 1083.68963, Residuals: -1.33048, Convergence: 0.007395\n",
      "Epoch: 113, Loss: 1076.45616, Residuals: -1.32972, Convergence: 0.006720\n",
      "Epoch: 114, Loss: 1069.81872, Residuals: -1.32811, Convergence: 0.006204\n",
      "Epoch: 115, Loss: 1063.61440, Residuals: -1.32569, Convergence: 0.005833\n",
      "Epoch: 116, Loss: 1057.69875, Residuals: -1.32246, Convergence: 0.005593\n",
      "Epoch: 117, Loss: 1051.94625, Residuals: -1.31845, Convergence: 0.005468\n",
      "Epoch: 118, Loss: 1046.25874, Residuals: -1.31369, Convergence: 0.005436\n",
      "Epoch: 119, Loss: 1040.57884, Residuals: -1.30823, Convergence: 0.005458\n",
      "Epoch: 120, Loss: 1034.90455, Residuals: -1.30216, Convergence: 0.005483\n",
      "Epoch: 121, Loss: 1029.28683, Residuals: -1.29557, Convergence: 0.005458\n",
      "Epoch: 122, Loss: 1023.80876, Residuals: -1.28858, Convergence: 0.005351\n",
      "Epoch: 123, Loss: 1018.55177, Residuals: -1.28129, Convergence: 0.005161\n",
      "Epoch: 124, Loss: 1013.57076, Residuals: -1.27377, Convergence: 0.004914\n",
      "Epoch: 125, Loss: 1008.88705, Residuals: -1.26611, Convergence: 0.004642\n",
      "Epoch: 126, Loss: 1004.49738, Residuals: -1.25838, Convergence: 0.004370\n",
      "Epoch: 127, Loss: 1000.38274, Residuals: -1.25064, Convergence: 0.004113\n",
      "Epoch: 128, Loss: 996.51892, Residuals: -1.24293, Convergence: 0.003877\n",
      "Epoch: 129, Loss: 992.88038, Residuals: -1.23530, Convergence: 0.003665\n",
      "Epoch: 130, Loss: 989.44392, Residuals: -1.22778, Convergence: 0.003473\n",
      "Epoch: 131, Loss: 986.18826, Residuals: -1.22040, Convergence: 0.003301\n",
      "Epoch: 132, Loss: 983.09547, Residuals: -1.21318, Convergence: 0.003146\n",
      "Epoch: 133, Loss: 980.15078, Residuals: -1.20613, Convergence: 0.003004\n",
      "Epoch: 134, Loss: 977.34164, Residuals: -1.19927, Convergence: 0.002874\n",
      "Epoch: 135, Loss: 974.65757, Residuals: -1.19262, Convergence: 0.002754\n",
      "Epoch: 136, Loss: 972.08995, Residuals: -1.18617, Convergence: 0.002641\n",
      "Epoch: 137, Loss: 969.63147, Residuals: -1.17994, Convergence: 0.002535\n",
      "Epoch: 138, Loss: 967.27554, Residuals: -1.17392, Convergence: 0.002436\n",
      "Epoch: 139, Loss: 965.01692, Residuals: -1.16812, Convergence: 0.002341\n",
      "Epoch: 140, Loss: 962.85013, Residuals: -1.16254, Convergence: 0.002250\n",
      "Epoch: 141, Loss: 960.77098, Residuals: -1.15718, Convergence: 0.002164\n",
      "Epoch: 142, Loss: 958.77473, Residuals: -1.15203, Convergence: 0.002082\n",
      "Epoch: 143, Loss: 956.85688, Residuals: -1.14708, Convergence: 0.002004\n",
      "Epoch: 144, Loss: 955.01379, Residuals: -1.14234, Convergence: 0.001930\n",
      "Epoch: 145, Loss: 953.24167, Residuals: -1.13779, Convergence: 0.001859\n",
      "Epoch: 146, Loss: 951.53627, Residuals: -1.13343, Convergence: 0.001792\n",
      "Epoch: 147, Loss: 949.89394, Residuals: -1.12925, Convergence: 0.001729\n",
      "Epoch: 148, Loss: 948.31050, Residuals: -1.12524, Convergence: 0.001670\n",
      "Epoch: 149, Loss: 946.78257, Residuals: -1.12139, Convergence: 0.001614\n",
      "Epoch: 150, Loss: 945.30570, Residuals: -1.11769, Convergence: 0.001562\n",
      "Epoch: 151, Loss: 943.87578, Residuals: -1.11414, Convergence: 0.001515\n",
      "Epoch: 152, Loss: 942.48826, Residuals: -1.11073, Convergence: 0.001472\n",
      "Epoch: 153, Loss: 941.13859, Residuals: -1.10744, Convergence: 0.001434\n",
      "Epoch: 154, Loss: 939.82150, Residuals: -1.10426, Convergence: 0.001401\n",
      "Epoch: 155, Loss: 938.53221, Residuals: -1.10118, Convergence: 0.001374\n",
      "Epoch: 156, Loss: 937.26573, Residuals: -1.09819, Convergence: 0.001351\n",
      "Epoch: 157, Loss: 936.01710, Residuals: -1.09528, Convergence: 0.001334\n",
      "Epoch: 158, Loss: 934.78268, Residuals: -1.09243, Convergence: 0.001321\n",
      "Epoch: 159, Loss: 933.55942, Residuals: -1.08963, Convergence: 0.001310\n",
      "Epoch: 160, Loss: 932.34659, Residuals: -1.08689, Convergence: 0.001301\n",
      "Epoch: 161, Loss: 931.14441, Residuals: -1.08420, Convergence: 0.001291\n",
      "Epoch: 162, Loss: 929.95611, Residuals: -1.08156, Convergence: 0.001278\n",
      "Epoch: 163, Loss: 928.78558, Residuals: -1.07897, Convergence: 0.001260\n",
      "Epoch: 164, Loss: 927.63767, Residuals: -1.07644, Convergence: 0.001237\n",
      "Epoch: 165, Loss: 926.51743, Residuals: -1.07398, Convergence: 0.001209\n",
      "Epoch: 166, Loss: 925.42930, Residuals: -1.07160, Convergence: 0.001176\n",
      "Epoch: 167, Loss: 924.37617, Residuals: -1.06929, Convergence: 0.001139\n",
      "Epoch: 168, Loss: 923.36068, Residuals: -1.06707, Convergence: 0.001100\n",
      "Epoch: 169, Loss: 922.38423, Residuals: -1.06492, Convergence: 0.001059\n",
      "Epoch: 170, Loss: 921.44704, Residuals: -1.06286, Convergence: 0.001017\n",
      "Epoch: 171, Loss: 920.54943, Residuals: -1.06088, Convergence: 0.000975\n",
      "Evidence 11188.456\n",
      "\n",
      "Epoch: 171, Evidence: 11188.45605, Convergence: 1.016323\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.76e-01\n",
      "Epoch: 171, Loss: 2348.04986, Residuals: -1.06088, Convergence:   inf\n",
      "Epoch: 172, Loss: 2309.29312, Residuals: -1.06936, Convergence: 0.016783\n",
      "Epoch: 173, Loss: 2283.30581, Residuals: -1.06861, Convergence: 0.011381\n",
      "Epoch: 174, Loss: 2261.70118, Residuals: -1.06708, Convergence: 0.009552\n",
      "Epoch: 175, Loss: 2243.50385, Residuals: -1.06526, Convergence: 0.008111\n",
      "Epoch: 176, Loss: 2228.03396, Residuals: -1.06324, Convergence: 0.006943\n",
      "Epoch: 177, Loss: 2214.75967, Residuals: -1.06106, Convergence: 0.005994\n",
      "Epoch: 178, Loss: 2203.24858, Residuals: -1.05875, Convergence: 0.005225\n",
      "Epoch: 179, Loss: 2193.14478, Residuals: -1.05630, Convergence: 0.004607\n",
      "Epoch: 180, Loss: 2184.16102, Residuals: -1.05371, Convergence: 0.004113\n",
      "Epoch: 181, Loss: 2176.07325, Residuals: -1.05098, Convergence: 0.003717\n",
      "Epoch: 182, Loss: 2168.72238, Residuals: -1.04811, Convergence: 0.003389\n",
      "Epoch: 183, Loss: 2162.00044, Residuals: -1.04513, Convergence: 0.003109\n",
      "Epoch: 184, Loss: 2155.83863, Residuals: -1.04208, Convergence: 0.002858\n",
      "Epoch: 185, Loss: 2150.18504, Residuals: -1.03900, Convergence: 0.002629\n",
      "Epoch: 186, Loss: 2144.99546, Residuals: -1.03595, Convergence: 0.002419\n",
      "Epoch: 187, Loss: 2140.22577, Residuals: -1.03295, Convergence: 0.002229\n",
      "Epoch: 188, Loss: 2135.83228, Residuals: -1.03003, Convergence: 0.002057\n",
      "Epoch: 189, Loss: 2131.77370, Residuals: -1.02720, Convergence: 0.001904\n",
      "Epoch: 190, Loss: 2128.01028, Residuals: -1.02447, Convergence: 0.001769\n",
      "Epoch: 191, Loss: 2124.50826, Residuals: -1.02185, Convergence: 0.001648\n",
      "Epoch: 192, Loss: 2121.23724, Residuals: -1.01935, Convergence: 0.001542\n",
      "Epoch: 193, Loss: 2118.17181, Residuals: -1.01695, Convergence: 0.001447\n",
      "Epoch: 194, Loss: 2115.29054, Residuals: -1.01467, Convergence: 0.001362\n",
      "Epoch: 195, Loss: 2112.57671, Residuals: -1.01250, Convergence: 0.001285\n",
      "Epoch: 196, Loss: 2110.01563, Residuals: -1.01043, Convergence: 0.001214\n",
      "Epoch: 197, Loss: 2107.59610, Residuals: -1.00848, Convergence: 0.001148\n",
      "Epoch: 198, Loss: 2105.30934, Residuals: -1.00663, Convergence: 0.001086\n",
      "Epoch: 199, Loss: 2103.14730, Residuals: -1.00488, Convergence: 0.001028\n",
      "Epoch: 200, Loss: 2101.10361, Residuals: -1.00323, Convergence: 0.000973\n",
      "Evidence 14358.873\n",
      "\n",
      "Epoch: 200, Evidence: 14358.87305, Convergence: 0.220798\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.41e-01\n",
      "Epoch: 200, Loss: 2472.91412, Residuals: -1.00323, Convergence:   inf\n",
      "Epoch: 201, Loss: 2459.75956, Residuals: -1.00046, Convergence: 0.005348\n",
      "Epoch: 202, Loss: 2448.98433, Residuals: -0.99724, Convergence: 0.004400\n",
      "Epoch: 203, Loss: 2439.64974, Residuals: -0.99411, Convergence: 0.003826\n",
      "Epoch: 204, Loss: 2431.52270, Residuals: -0.99116, Convergence: 0.003342\n",
      "Epoch: 205, Loss: 2424.42116, Residuals: -0.98844, Convergence: 0.002929\n",
      "Epoch: 206, Loss: 2418.19375, Residuals: -0.98595, Convergence: 0.002575\n",
      "Epoch: 207, Loss: 2412.71185, Residuals: -0.98369, Convergence: 0.002272\n",
      "Epoch: 208, Loss: 2407.86361, Residuals: -0.98162, Convergence: 0.002014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 209, Loss: 2403.55588, Residuals: -0.97975, Convergence: 0.001792\n",
      "Epoch: 210, Loss: 2399.70758, Residuals: -0.97804, Convergence: 0.001604\n",
      "Epoch: 211, Loss: 2396.25092, Residuals: -0.97649, Convergence: 0.001443\n",
      "Epoch: 212, Loss: 2393.12985, Residuals: -0.97506, Convergence: 0.001304\n",
      "Epoch: 213, Loss: 2390.29558, Residuals: -0.97376, Convergence: 0.001186\n",
      "Epoch: 214, Loss: 2387.70995, Residuals: -0.97256, Convergence: 0.001083\n",
      "Epoch: 215, Loss: 2385.33840, Residuals: -0.97145, Convergence: 0.000994\n",
      "Evidence 14722.949\n",
      "\n",
      "Epoch: 215, Evidence: 14722.94922, Convergence: 0.024728\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.40e-01\n",
      "Epoch: 215, Loss: 2478.19185, Residuals: -0.97145, Convergence:   inf\n",
      "Epoch: 216, Loss: 2471.54287, Residuals: -0.96863, Convergence: 0.002690\n",
      "Epoch: 217, Loss: 2466.03728, Residuals: -0.96612, Convergence: 0.002233\n",
      "Epoch: 218, Loss: 2461.36824, Residuals: -0.96396, Convergence: 0.001897\n",
      "Epoch: 219, Loss: 2457.35964, Residuals: -0.96212, Convergence: 0.001631\n",
      "Epoch: 220, Loss: 2453.87823, Residuals: -0.96055, Convergence: 0.001419\n",
      "Epoch: 221, Loss: 2450.81861, Residuals: -0.95921, Convergence: 0.001248\n",
      "Epoch: 222, Loss: 2448.09897, Residuals: -0.95805, Convergence: 0.001111\n",
      "Epoch: 223, Loss: 2445.65589, Residuals: -0.95705, Convergence: 0.000999\n",
      "Evidence 14803.977\n",
      "\n",
      "Epoch: 223, Evidence: 14803.97656, Convergence: 0.005473\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.68e-01\n",
      "Epoch: 223, Loss: 2479.77422, Residuals: -0.95705, Convergence:   inf\n",
      "Epoch: 224, Loss: 2475.66361, Residuals: -0.95500, Convergence: 0.001660\n",
      "Epoch: 225, Loss: 2472.26009, Residuals: -0.95335, Convergence: 0.001377\n",
      "Epoch: 226, Loss: 2469.36588, Residuals: -0.95202, Convergence: 0.001172\n",
      "Epoch: 227, Loss: 2466.85750, Residuals: -0.95093, Convergence: 0.001017\n",
      "Epoch: 228, Loss: 2464.64493, Residuals: -0.95004, Convergence: 0.000898\n",
      "Evidence 14835.475\n",
      "\n",
      "Epoch: 228, Evidence: 14835.47461, Convergence: 0.002123\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.16e-01\n",
      "Epoch: 228, Loss: 2480.68516, Residuals: -0.95004, Convergence:   inf\n",
      "Epoch: 229, Loss: 2477.76675, Residuals: -0.94859, Convergence: 0.001178\n",
      "Epoch: 230, Loss: 2475.33744, Residuals: -0.94748, Convergence: 0.000981\n",
      "Evidence 14848.684\n",
      "\n",
      "Epoch: 230, Evidence: 14848.68359, Convergence: 0.000890\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.79e-01\n",
      "Epoch: 230, Loss: 2481.38299, Residuals: -0.94748, Convergence:   inf\n",
      "Epoch: 231, Loss: 2476.82531, Residuals: -0.94608, Convergence: 0.001840\n",
      "Epoch: 232, Loss: 2473.37134, Residuals: -0.94484, Convergence: 0.001396\n",
      "Epoch: 233, Loss: 2470.57769, Residuals: -0.94381, Convergence: 0.001131\n",
      "Epoch: 234, Loss: 2468.21034, Residuals: -0.94307, Convergence: 0.000959\n",
      "Evidence 14866.867\n",
      "\n",
      "Epoch: 234, Evidence: 14866.86719, Convergence: 0.002112\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.49e-01\n",
      "Epoch: 234, Loss: 2481.63437, Residuals: -0.94307, Convergence:   inf\n",
      "Epoch: 235, Loss: 2478.58702, Residuals: -0.94156, Convergence: 0.001229\n",
      "Epoch: 236, Loss: 2476.18186, Residuals: -0.94068, Convergence: 0.000971\n",
      "Evidence 14878.109\n",
      "\n",
      "Epoch: 236, Evidence: 14878.10938, Convergence: 0.000756\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.26e-01\n",
      "Epoch: 236, Loss: 2481.87946, Residuals: -0.94068, Convergence:   inf\n",
      "Epoch: 237, Loss: 2477.37677, Residuals: -0.93962, Convergence: 0.001818\n",
      "Epoch: 238, Loss: 2474.14105, Residuals: -0.94012, Convergence: 0.001308\n",
      "Epoch: 239, Loss: 2471.46720, Residuals: -0.93900, Convergence: 0.001082\n",
      "Epoch: 240, Loss: 2469.14573, Residuals: -0.94047, Convergence: 0.000940\n",
      "Evidence 14894.446\n",
      "\n",
      "Epoch: 240, Evidence: 14894.44629, Convergence: 0.001852\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.12e-01\n",
      "Epoch: 240, Loss: 2481.32471, Residuals: -0.94047, Convergence:   inf\n",
      "Epoch: 241, Loss: 2479.07287, Residuals: -0.93911, Convergence: 0.000908\n",
      "Evidence 14901.613\n",
      "\n",
      "Epoch: 241, Evidence: 14901.61328, Convergence: 0.000481\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 9.17e-02\n",
      "Epoch: 241, Loss: 2482.29125, Residuals: -0.93911, Convergence:   inf\n",
      "Epoch: 242, Loss: 2522.11744, Residuals: -0.97401, Convergence: -0.015791\n",
      "Epoch: 242, Loss: 2479.85641, Residuals: -0.93962, Convergence: 0.000982\n",
      "Evidence 14906.120\n",
      "\n",
      "Epoch: 242, Evidence: 14906.12012, Convergence: 0.000783\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.39e-02\n",
      "Epoch: 242, Loss: 2481.78445, Residuals: -0.93962, Convergence:   inf\n",
      "Epoch: 243, Loss: 2484.31587, Residuals: -0.94026, Convergence: -0.001019\n",
      "Epoch: 243, Loss: 2481.00580, Residuals: -0.93797, Convergence: 0.000314\n",
      "Evidence 14908.555\n",
      "\n",
      "Epoch: 243, Evidence: 14908.55469, Convergence: 0.000946\n",
      "Total samples: 183, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 382.06911, Residuals: -4.50860, Convergence:   inf\n",
      "Epoch: 1, Loss: 356.36533, Residuals: -4.38862, Convergence: 0.072128\n",
      "Epoch: 2, Loss: 335.25155, Residuals: -4.22409, Convergence: 0.062979\n",
      "Epoch: 3, Loss: 319.09613, Residuals: -4.05843, Convergence: 0.050629\n",
      "Epoch: 4, Loss: 306.77040, Residuals: -3.91233, Convergence: 0.040179\n",
      "Epoch: 5, Loss: 296.99073, Residuals: -3.78297, Convergence: 0.032929\n",
      "Epoch: 6, Loss: 289.05102, Residuals: -3.67026, Convergence: 0.027468\n",
      "Epoch: 7, Loss: 282.45740, Residuals: -3.57374, Convergence: 0.023344\n",
      "Epoch: 8, Loss: 276.84358, Residuals: -3.49130, Convergence: 0.020278\n",
      "Epoch: 9, Loss: 271.95183, Residuals: -3.42063, Convergence: 0.017988\n",
      "Epoch: 10, Loss: 267.59956, Residuals: -3.35964, Convergence: 0.016264\n",
      "Epoch: 11, Loss: 263.65529, Residuals: -3.30659, Convergence: 0.014960\n",
      "Epoch: 12, Loss: 260.02391, Residuals: -3.25998, Convergence: 0.013966\n",
      "Epoch: 13, Loss: 256.63716, Residuals: -3.21853, Convergence: 0.013197\n",
      "Epoch: 14, Loss: 253.44649, Residuals: -3.18109, Convergence: 0.012589\n",
      "Epoch: 15, Loss: 250.41806, Residuals: -3.14665, Convergence: 0.012093\n",
      "Epoch: 16, Loss: 247.52973, Residuals: -3.11443, Convergence: 0.011669\n",
      "Epoch: 17, Loss: 244.76571, Residuals: -3.08386, Convergence: 0.011293\n",
      "Epoch: 18, Loss: 242.10685, Residuals: -3.05449, Convergence: 0.010982\n",
      "Epoch: 19, Loss: 239.52538, Residuals: -3.02580, Convergence: 0.010777\n",
      "Epoch: 20, Loss: 236.98945, Residuals: -2.99725, Convergence: 0.010701\n",
      "Epoch: 21, Loss: 234.47160, Residuals: -2.96839, Convergence: 0.010738\n",
      "Epoch: 22, Loss: 231.95284, Residuals: -2.93895, Convergence: 0.010859\n",
      "Epoch: 23, Loss: 229.41325, Residuals: -2.90872, Convergence: 0.011070\n",
      "Epoch: 24, Loss: 226.81533, Residuals: -2.87733, Convergence: 0.011454\n",
      "Epoch: 25, Loss: 224.10397, Residuals: -2.84414, Convergence: 0.012099\n",
      "Epoch: 26, Loss: 221.25563, Residuals: -2.80881, Convergence: 0.012874\n",
      "Epoch: 27, Loss: 218.35937, Residuals: -2.77227, Convergence: 0.013264\n",
      "Epoch: 28, Loss: 215.53534, Residuals: -2.73589, Convergence: 0.013102\n",
      "Epoch: 29, Loss: 212.81610, Residuals: -2.70017, Convergence: 0.012777\n",
      "Epoch: 30, Loss: 210.18786, Residuals: -2.66506, Convergence: 0.012504\n",
      "Epoch: 31, Loss: 207.63151, Residuals: -2.63038, Convergence: 0.012312\n",
      "Epoch: 32, Loss: 205.13262, Residuals: -2.59598, Convergence: 0.012182\n",
      "Epoch: 33, Loss: 202.68216, Residuals: -2.56175, Convergence: 0.012090\n",
      "Epoch: 34, Loss: 200.27556, Residuals: -2.52760, Convergence: 0.012016\n",
      "Epoch: 35, Loss: 197.91162, Residuals: -2.49351, Convergence: 0.011944\n",
      "Epoch: 36, Loss: 195.59134, Residuals: -2.45946, Convergence: 0.011863\n",
      "Epoch: 37, Loss: 193.31717, Residuals: -2.42546, Convergence: 0.011764\n",
      "Epoch: 38, Loss: 191.09224, Residuals: -2.39154, Convergence: 0.011643\n",
      "Epoch: 39, Loss: 188.91986, Residuals: -2.35772, Convergence: 0.011499\n",
      "Epoch: 40, Loss: 186.80323, Residuals: -2.32407, Convergence: 0.011331\n",
      "Epoch: 41, Loss: 184.74516, Residuals: -2.29062, Convergence: 0.011140\n",
      "Epoch: 42, Loss: 182.74801, Residuals: -2.25741, Convergence: 0.010928\n",
      "Epoch: 43, Loss: 180.81361, Residuals: -2.22451, Convergence: 0.010698\n",
      "Epoch: 44, Loss: 178.94324, Residuals: -2.19194, Convergence: 0.010452\n",
      "Epoch: 45, Loss: 177.13767, Residuals: -2.15976, Convergence: 0.010193\n",
      "Epoch: 46, Loss: 175.39713, Residuals: -2.12799, Convergence: 0.009923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47, Loss: 173.72136, Residuals: -2.09668, Convergence: 0.009646\n",
      "Epoch: 48, Loss: 172.10967, Residuals: -2.06585, Convergence: 0.009364\n",
      "Epoch: 49, Loss: 170.56098, Residuals: -2.03552, Convergence: 0.009080\n",
      "Epoch: 50, Loss: 169.07390, Residuals: -2.00571, Convergence: 0.008795\n",
      "Epoch: 51, Loss: 167.64682, Residuals: -1.97642, Convergence: 0.008512\n",
      "Epoch: 52, Loss: 166.27805, Residuals: -1.94767, Convergence: 0.008232\n",
      "Epoch: 53, Loss: 164.96584, Residuals: -1.91946, Convergence: 0.007954\n",
      "Epoch: 54, Loss: 163.70848, Residuals: -1.89180, Convergence: 0.007680\n",
      "Epoch: 55, Loss: 162.50434, Residuals: -1.86469, Convergence: 0.007410\n",
      "Epoch: 56, Loss: 161.35183, Residuals: -1.83815, Convergence: 0.007143\n",
      "Epoch: 57, Loss: 160.24946, Residuals: -1.81219, Convergence: 0.006879\n",
      "Epoch: 58, Loss: 159.19581, Residuals: -1.78680, Convergence: 0.006619\n",
      "Epoch: 59, Loss: 158.18952, Residuals: -1.76201, Convergence: 0.006361\n",
      "Epoch: 60, Loss: 157.22930, Residuals: -1.73781, Convergence: 0.006107\n",
      "Epoch: 61, Loss: 156.31392, Residuals: -1.71422, Convergence: 0.005856\n",
      "Epoch: 62, Loss: 155.44223, Residuals: -1.69125, Convergence: 0.005608\n",
      "Epoch: 63, Loss: 154.61313, Residuals: -1.66891, Convergence: 0.005362\n",
      "Epoch: 64, Loss: 153.82556, Residuals: -1.64719, Convergence: 0.005120\n",
      "Epoch: 65, Loss: 153.07851, Residuals: -1.62612, Convergence: 0.004880\n",
      "Epoch: 66, Loss: 152.37090, Residuals: -1.60570, Convergence: 0.004644\n",
      "Epoch: 67, Loss: 151.70165, Residuals: -1.58593, Convergence: 0.004412\n",
      "Epoch: 68, Loss: 151.06951, Residuals: -1.56683, Convergence: 0.004184\n",
      "Epoch: 69, Loss: 150.47316, Residuals: -1.54839, Convergence: 0.003963\n",
      "Epoch: 70, Loss: 149.91109, Residuals: -1.53063, Convergence: 0.003749\n",
      "Epoch: 71, Loss: 149.38160, Residuals: -1.51353, Convergence: 0.003545\n",
      "Epoch: 72, Loss: 148.88282, Residuals: -1.49709, Convergence: 0.003350\n",
      "Epoch: 73, Loss: 148.41275, Residuals: -1.48131, Convergence: 0.003167\n",
      "Epoch: 74, Loss: 147.96921, Residuals: -1.46615, Convergence: 0.002997\n",
      "Epoch: 75, Loss: 147.55000, Residuals: -1.45161, Convergence: 0.002841\n",
      "Epoch: 76, Loss: 147.15294, Residuals: -1.43765, Convergence: 0.002698\n",
      "Epoch: 77, Loss: 146.77595, Residuals: -1.42424, Convergence: 0.002568\n",
      "Epoch: 78, Loss: 146.41720, Residuals: -1.41135, Convergence: 0.002450\n",
      "Epoch: 79, Loss: 146.07510, Residuals: -1.39895, Convergence: 0.002342\n",
      "Epoch: 80, Loss: 145.74835, Residuals: -1.38701, Convergence: 0.002242\n",
      "Epoch: 81, Loss: 145.43595, Residuals: -1.37552, Convergence: 0.002148\n",
      "Epoch: 82, Loss: 145.13714, Residuals: -1.36446, Convergence: 0.002059\n",
      "Epoch: 83, Loss: 144.85135, Residuals: -1.35381, Convergence: 0.001973\n",
      "Epoch: 84, Loss: 144.57813, Residuals: -1.34356, Convergence: 0.001890\n",
      "Epoch: 85, Loss: 144.31715, Residuals: -1.33372, Convergence: 0.001808\n",
      "Epoch: 86, Loss: 144.06808, Residuals: -1.32426, Convergence: 0.001729\n",
      "Epoch: 87, Loss: 143.83061, Residuals: -1.31519, Convergence: 0.001651\n",
      "Epoch: 88, Loss: 143.60437, Residuals: -1.30650, Convergence: 0.001575\n",
      "Epoch: 89, Loss: 143.38893, Residuals: -1.29819, Convergence: 0.001502\n",
      "Epoch: 90, Loss: 143.18375, Residuals: -1.29025, Convergence: 0.001433\n",
      "Epoch: 91, Loss: 142.98815, Residuals: -1.28267, Convergence: 0.001368\n",
      "Epoch: 92, Loss: 142.80134, Residuals: -1.27545, Convergence: 0.001308\n",
      "Epoch: 93, Loss: 142.62238, Residuals: -1.26857, Convergence: 0.001255\n",
      "Epoch: 94, Loss: 142.45023, Residuals: -1.26202, Convergence: 0.001208\n",
      "Epoch: 95, Loss: 142.28379, Residuals: -1.25577, Convergence: 0.001170\n",
      "Epoch: 96, Loss: 142.12197, Residuals: -1.24979, Convergence: 0.001139\n",
      "Epoch: 97, Loss: 141.96374, Residuals: -1.24405, Convergence: 0.001115\n",
      "Epoch: 98, Loss: 141.80825, Residuals: -1.23854, Convergence: 0.001097\n",
      "Epoch: 99, Loss: 141.65480, Residuals: -1.23322, Convergence: 0.001083\n",
      "Epoch: 100, Loss: 141.50292, Residuals: -1.22808, Convergence: 0.001073\n",
      "Epoch: 101, Loss: 141.35233, Residuals: -1.22309, Convergence: 0.001065\n",
      "Epoch: 102, Loss: 141.20293, Residuals: -1.21823, Convergence: 0.001058\n",
      "Epoch: 103, Loss: 141.05475, Residuals: -1.21351, Convergence: 0.001051\n",
      "Epoch: 104, Loss: 140.90793, Residuals: -1.20890, Convergence: 0.001042\n",
      "Epoch: 105, Loss: 140.76265, Residuals: -1.20441, Convergence: 0.001032\n",
      "Epoch: 106, Loss: 140.61912, Residuals: -1.20003, Convergence: 0.001021\n",
      "Epoch: 107, Loss: 140.47758, Residuals: -1.19576, Convergence: 0.001008\n",
      "Epoch: 108, Loss: 140.33825, Residuals: -1.19159, Convergence: 0.000993\n",
      "Evidence -181.101\n",
      "\n",
      "Epoch: 108, Evidence: -181.10114, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.23e-01\n",
      "Epoch: 108, Loss: 1373.13048, Residuals: -1.19159, Convergence:   inf\n",
      "Epoch: 109, Loss: 1316.04091, Residuals: -1.21749, Convergence: 0.043380\n",
      "Epoch: 110, Loss: 1271.94583, Residuals: -1.23951, Convergence: 0.034667\n",
      "Epoch: 111, Loss: 1238.05806, Residuals: -1.25697, Convergence: 0.027372\n",
      "Epoch: 112, Loss: 1211.26924, Residuals: -1.27056, Convergence: 0.022116\n",
      "Epoch: 113, Loss: 1189.36264, Residuals: -1.28144, Convergence: 0.018419\n",
      "Epoch: 114, Loss: 1171.06853, Residuals: -1.29023, Convergence: 0.015622\n",
      "Epoch: 115, Loss: 1155.59473, Residuals: -1.29723, Convergence: 0.013390\n",
      "Epoch: 116, Loss: 1142.37691, Residuals: -1.30263, Convergence: 0.011570\n",
      "Epoch: 117, Loss: 1130.98279, Residuals: -1.30659, Convergence: 0.010075\n",
      "Epoch: 118, Loss: 1121.06687, Residuals: -1.30926, Convergence: 0.008845\n",
      "Epoch: 119, Loss: 1112.34724, Residuals: -1.31079, Convergence: 0.007839\n",
      "Epoch: 120, Loss: 1104.58966, Residuals: -1.31130, Convergence: 0.007023\n",
      "Epoch: 121, Loss: 1097.59489, Residuals: -1.31089, Convergence: 0.006373\n",
      "Epoch: 122, Loss: 1091.19028, Residuals: -1.30963, Convergence: 0.005869\n",
      "Epoch: 123, Loss: 1085.22127, Residuals: -1.30758, Convergence: 0.005500\n",
      "Epoch: 124, Loss: 1079.54624, Residuals: -1.30477, Convergence: 0.005257\n",
      "Epoch: 125, Loss: 1074.03170, Residuals: -1.30121, Convergence: 0.005134\n",
      "Epoch: 126, Loss: 1068.55652, Residuals: -1.29691, Convergence: 0.005124\n",
      "Epoch: 127, Loss: 1063.01775, Residuals: -1.29188, Convergence: 0.005210\n",
      "Epoch: 128, Loss: 1057.35308, Residuals: -1.28616, Convergence: 0.005357\n",
      "Epoch: 129, Loss: 1051.56406, Residuals: -1.27983, Convergence: 0.005505\n",
      "Epoch: 130, Loss: 1045.72983, Residuals: -1.27299, Convergence: 0.005579\n",
      "Epoch: 131, Loss: 1039.98049, Residuals: -1.26574, Convergence: 0.005528\n",
      "Epoch: 132, Loss: 1034.44620, Residuals: -1.25819, Convergence: 0.005350\n",
      "Epoch: 133, Loss: 1029.21275, Residuals: -1.25042, Convergence: 0.005085\n",
      "Epoch: 134, Loss: 1024.31291, Residuals: -1.24253, Convergence: 0.004784\n",
      "Epoch: 135, Loss: 1019.74187, Residuals: -1.23458, Convergence: 0.004483\n",
      "Epoch: 136, Loss: 1015.47427, Residuals: -1.22665, Convergence: 0.004203\n",
      "Epoch: 137, Loss: 1011.47953, Residuals: -1.21878, Convergence: 0.003949\n",
      "Epoch: 138, Loss: 1007.72616, Residuals: -1.21101, Convergence: 0.003725\n",
      "Epoch: 139, Loss: 1004.18719, Residuals: -1.20339, Convergence: 0.003524\n",
      "Epoch: 140, Loss: 1000.84017, Residuals: -1.19593, Convergence: 0.003344\n",
      "Epoch: 141, Loss: 997.66701, Residuals: -1.18868, Convergence: 0.003181\n",
      "Epoch: 142, Loss: 994.65337, Residuals: -1.18164, Convergence: 0.003030\n",
      "Epoch: 143, Loss: 991.78806, Residuals: -1.17483, Convergence: 0.002889\n",
      "Epoch: 144, Loss: 989.06231, Residuals: -1.16827, Convergence: 0.002756\n",
      "Epoch: 145, Loss: 986.46853, Residuals: -1.16196, Convergence: 0.002629\n",
      "Epoch: 146, Loss: 984.00017, Residuals: -1.15592, Convergence: 0.002508\n",
      "Epoch: 147, Loss: 981.65120, Residuals: -1.15014, Convergence: 0.002393\n",
      "Epoch: 148, Loss: 979.41612, Residuals: -1.14462, Convergence: 0.002282\n",
      "Epoch: 149, Loss: 977.28894, Residuals: -1.13936, Convergence: 0.002177\n",
      "Epoch: 150, Loss: 975.26463, Residuals: -1.13436, Convergence: 0.002076\n",
      "Epoch: 151, Loss: 973.33737, Residuals: -1.12959, Convergence: 0.001980\n",
      "Epoch: 152, Loss: 971.50176, Residuals: -1.12506, Convergence: 0.001889\n",
      "Epoch: 153, Loss: 969.75260, Residuals: -1.12075, Convergence: 0.001804\n",
      "Epoch: 154, Loss: 968.08444, Residuals: -1.11665, Convergence: 0.001723\n",
      "Epoch: 155, Loss: 966.49204, Residuals: -1.11275, Convergence: 0.001648\n",
      "Epoch: 156, Loss: 964.96999, Residuals: -1.10904, Convergence: 0.001577\n",
      "Epoch: 157, Loss: 963.51290, Residuals: -1.10549, Convergence: 0.001512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 158, Loss: 962.11562, Residuals: -1.10211, Convergence: 0.001452\n",
      "Epoch: 159, Loss: 960.77226, Residuals: -1.09887, Convergence: 0.001398\n",
      "Epoch: 160, Loss: 959.47738, Residuals: -1.09577, Convergence: 0.001350\n",
      "Epoch: 161, Loss: 958.22519, Residuals: -1.09279, Convergence: 0.001307\n",
      "Epoch: 162, Loss: 957.00923, Residuals: -1.08991, Convergence: 0.001271\n",
      "Epoch: 163, Loss: 955.82359, Residuals: -1.08713, Convergence: 0.001240\n",
      "Epoch: 164, Loss: 954.66234, Residuals: -1.08442, Convergence: 0.001216\n",
      "Epoch: 165, Loss: 953.51896, Residuals: -1.08178, Convergence: 0.001199\n",
      "Epoch: 166, Loss: 952.38823, Residuals: -1.07919, Convergence: 0.001187\n",
      "Epoch: 167, Loss: 951.26441, Residuals: -1.07663, Convergence: 0.001181\n",
      "Epoch: 168, Loss: 950.14450, Residuals: -1.07410, Convergence: 0.001179\n",
      "Epoch: 169, Loss: 949.02676, Residuals: -1.07158, Convergence: 0.001178\n",
      "Epoch: 170, Loss: 947.91099, Residuals: -1.06909, Convergence: 0.001177\n",
      "Epoch: 171, Loss: 946.80044, Residuals: -1.06661, Convergence: 0.001173\n",
      "Epoch: 172, Loss: 945.69952, Residuals: -1.06415, Convergence: 0.001164\n",
      "Epoch: 173, Loss: 944.61418, Residuals: -1.06173, Convergence: 0.001149\n",
      "Epoch: 174, Loss: 943.55015, Residuals: -1.05935, Convergence: 0.001128\n",
      "Epoch: 175, Loss: 942.51363, Residuals: -1.05703, Convergence: 0.001100\n",
      "Epoch: 176, Loss: 941.50926, Residuals: -1.05477, Convergence: 0.001067\n",
      "Epoch: 177, Loss: 940.54116, Residuals: -1.05258, Convergence: 0.001029\n",
      "Epoch: 178, Loss: 939.61118, Residuals: -1.05047, Convergence: 0.000990\n",
      "Evidence 11437.889\n",
      "\n",
      "Epoch: 178, Evidence: 11437.88867, Convergence: 1.015833\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 5.71e-01\n",
      "Epoch: 178, Loss: 2354.69338, Residuals: -1.05047, Convergence:   inf\n",
      "Epoch: 179, Loss: 2317.63664, Residuals: -1.05750, Convergence: 0.015989\n",
      "Epoch: 180, Loss: 2292.27339, Residuals: -1.05634, Convergence: 0.011065\n",
      "Epoch: 181, Loss: 2271.08034, Residuals: -1.05477, Convergence: 0.009332\n",
      "Epoch: 182, Loss: 2253.26704, Residuals: -1.05305, Convergence: 0.007906\n",
      "Epoch: 183, Loss: 2238.17356, Residuals: -1.05124, Convergence: 0.006744\n",
      "Epoch: 184, Loss: 2225.25555, Residuals: -1.04936, Convergence: 0.005805\n",
      "Epoch: 185, Loss: 2214.06355, Residuals: -1.04741, Convergence: 0.005055\n",
      "Epoch: 186, Loss: 2204.22667, Residuals: -1.04539, Convergence: 0.004463\n",
      "Epoch: 187, Loss: 2195.44256, Residuals: -1.04326, Convergence: 0.004001\n",
      "Epoch: 188, Loss: 2187.48078, Residuals: -1.04100, Convergence: 0.003640\n",
      "Epoch: 189, Loss: 2180.18167, Residuals: -1.03859, Convergence: 0.003348\n",
      "Epoch: 190, Loss: 2173.45008, Residuals: -1.03606, Convergence: 0.003097\n",
      "Epoch: 191, Loss: 2167.23474, Residuals: -1.03345, Convergence: 0.002868\n",
      "Epoch: 192, Loss: 2161.50760, Residuals: -1.03079, Convergence: 0.002650\n",
      "Epoch: 193, Loss: 2156.24185, Residuals: -1.02815, Convergence: 0.002442\n",
      "Epoch: 194, Loss: 2151.40921, Residuals: -1.02556, Convergence: 0.002246\n",
      "Epoch: 195, Loss: 2146.97614, Residuals: -1.02305, Convergence: 0.002065\n",
      "Epoch: 196, Loss: 2142.90811, Residuals: -1.02063, Convergence: 0.001898\n",
      "Epoch: 197, Loss: 2139.17000, Residuals: -1.01833, Convergence: 0.001747\n",
      "Epoch: 198, Loss: 2135.72700, Residuals: -1.01614, Convergence: 0.001612\n",
      "Epoch: 199, Loss: 2132.54695, Residuals: -1.01406, Convergence: 0.001491\n",
      "Epoch: 200, Loss: 2129.59942, Residuals: -1.01209, Convergence: 0.001384\n",
      "Epoch: 201, Loss: 2126.85827, Residuals: -1.01023, Convergence: 0.001289\n",
      "Epoch: 202, Loss: 2124.29822, Residuals: -1.00846, Convergence: 0.001205\n",
      "Epoch: 203, Loss: 2121.89919, Residuals: -1.00679, Convergence: 0.001131\n",
      "Epoch: 204, Loss: 2119.64309, Residuals: -1.00520, Convergence: 0.001064\n",
      "Epoch: 205, Loss: 2117.51479, Residuals: -1.00369, Convergence: 0.001005\n",
      "Epoch: 206, Loss: 2115.50154, Residuals: -1.00226, Convergence: 0.000952\n",
      "Evidence 14513.211\n",
      "\n",
      "Epoch: 206, Evidence: 14513.21094, Convergence: 0.211898\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 4.31e-01\n",
      "Epoch: 206, Loss: 2476.64940, Residuals: -1.00226, Convergence:   inf\n",
      "Epoch: 207, Loss: 2463.32753, Residuals: -0.99875, Convergence: 0.005408\n",
      "Epoch: 208, Loss: 2452.55192, Residuals: -0.99526, Convergence: 0.004394\n",
      "Epoch: 209, Loss: 2443.37302, Residuals: -0.99209, Convergence: 0.003757\n",
      "Epoch: 210, Loss: 2435.48931, Residuals: -0.98927, Convergence: 0.003237\n",
      "Epoch: 211, Loss: 2428.66276, Residuals: -0.98679, Convergence: 0.002811\n",
      "Epoch: 212, Loss: 2422.70775, Residuals: -0.98463, Convergence: 0.002458\n",
      "Epoch: 213, Loss: 2417.47696, Residuals: -0.98273, Convergence: 0.002164\n",
      "Epoch: 214, Loss: 2412.85032, Residuals: -0.98106, Convergence: 0.001918\n",
      "Epoch: 215, Loss: 2408.73106, Residuals: -0.97959, Convergence: 0.001710\n",
      "Epoch: 216, Loss: 2405.04082, Residuals: -0.97829, Convergence: 0.001534\n",
      "Epoch: 217, Loss: 2401.71310, Residuals: -0.97712, Convergence: 0.001386\n",
      "Epoch: 218, Loss: 2398.69647, Residuals: -0.97608, Convergence: 0.001258\n",
      "Epoch: 219, Loss: 2395.94638, Residuals: -0.97514, Convergence: 0.001148\n",
      "Epoch: 220, Loss: 2393.42664, Residuals: -0.97429, Convergence: 0.001053\n",
      "Epoch: 221, Loss: 2391.10776, Residuals: -0.97351, Convergence: 0.000970\n",
      "Evidence 14852.918\n",
      "\n",
      "Epoch: 221, Evidence: 14852.91797, Convergence: 0.022871\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 3.28e-01\n",
      "Epoch: 221, Loss: 2482.22418, Residuals: -0.97351, Convergence:   inf\n",
      "Epoch: 222, Loss: 2475.83300, Residuals: -0.97045, Convergence: 0.002581\n",
      "Epoch: 223, Loss: 2470.59511, Residuals: -0.96798, Convergence: 0.002120\n",
      "Epoch: 224, Loss: 2466.16797, Residuals: -0.96602, Convergence: 0.001795\n",
      "Epoch: 225, Loss: 2462.36247, Residuals: -0.96444, Convergence: 0.001545\n",
      "Epoch: 226, Loss: 2459.04358, Residuals: -0.96316, Convergence: 0.001350\n",
      "Epoch: 227, Loss: 2456.11241, Residuals: -0.96210, Convergence: 0.001193\n",
      "Epoch: 228, Loss: 2453.49510, Residuals: -0.96122, Convergence: 0.001067\n",
      "Epoch: 229, Loss: 2451.13409, Residuals: -0.96047, Convergence: 0.000963\n",
      "Evidence 14929.845\n",
      "\n",
      "Epoch: 229, Evidence: 14929.84473, Convergence: 0.005153\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.55e-01\n",
      "Epoch: 229, Loss: 2483.90206, Residuals: -0.96047, Convergence:   inf\n",
      "Epoch: 230, Loss: 2480.05582, Residuals: -0.95822, Convergence: 0.001551\n",
      "Epoch: 231, Loss: 2476.86509, Residuals: -0.95656, Convergence: 0.001288\n",
      "Epoch: 232, Loss: 2474.12826, Residuals: -0.95528, Convergence: 0.001106\n",
      "Epoch: 233, Loss: 2471.73199, Residuals: -0.95427, Convergence: 0.000969\n",
      "Evidence 14957.246\n",
      "\n",
      "Epoch: 233, Evidence: 14957.24609, Convergence: 0.001832\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.04e-01\n",
      "Epoch: 233, Loss: 2484.94204, Residuals: -0.95427, Convergence:   inf\n",
      "Epoch: 234, Loss: 2482.07063, Residuals: -0.95246, Convergence: 0.001157\n",
      "Epoch: 235, Loss: 2479.65991, Residuals: -0.95112, Convergence: 0.000972\n",
      "Evidence 14969.209\n",
      "\n",
      "Epoch: 235, Evidence: 14969.20898, Convergence: 0.000799\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.69e-01\n",
      "Epoch: 235, Loss: 2485.70069, Residuals: -0.95112, Convergence:   inf\n",
      "Epoch: 236, Loss: 2481.13912, Residuals: -0.94867, Convergence: 0.001838\n",
      "Epoch: 237, Loss: 2477.69048, Residuals: -0.94704, Convergence: 0.001392\n",
      "Epoch: 238, Loss: 2474.90773, Residuals: -0.94586, Convergence: 0.001124\n",
      "Epoch: 239, Loss: 2472.54976, Residuals: -0.94512, Convergence: 0.000954\n",
      "Evidence 14986.726\n",
      "\n",
      "Epoch: 239, Evidence: 14986.72559, Convergence: 0.001967\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.39e-01\n",
      "Epoch: 239, Loss: 2485.79982, Residuals: -0.94512, Convergence:   inf\n",
      "Epoch: 240, Loss: 2482.82533, Residuals: -0.94249, Convergence: 0.001198\n",
      "Epoch: 241, Loss: 2480.47972, Residuals: -0.94101, Convergence: 0.000946\n",
      "Evidence 14997.538\n",
      "\n",
      "Epoch: 241, Evidence: 14997.53809, Convergence: 0.000721\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.18e-01\n",
      "Epoch: 241, Loss: 2485.98869, Residuals: -0.94101, Convergence:   inf\n",
      "Epoch: 242, Loss: 2481.69331, Residuals: -0.93674, Convergence: 0.001731\n",
      "Epoch: 243, Loss: 2478.65236, Residuals: -0.93702, Convergence: 0.001227\n",
      "Epoch: 244, Loss: 2476.07227, Residuals: -0.93725, Convergence: 0.001042\n",
      "Epoch: 245, Loss: 2473.86088, Residuals: -0.93966, Convergence: 0.000894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence 15012.699\n",
      "\n",
      "Epoch: 245, Evidence: 15012.69922, Convergence: 0.001730\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.05e-01\n",
      "Epoch: 245, Loss: 2485.24135, Residuals: -0.93966, Convergence:   inf\n",
      "Epoch: 246, Loss: 2483.98569, Residuals: -0.93727, Convergence: 0.000506\n",
      "Evidence 15018.968\n",
      "\n",
      "Epoch: 246, Evidence: 15018.96777, Convergence: 0.000417\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 8.51e-02\n",
      "Epoch: 246, Loss: 2486.25776, Residuals: -0.93727, Convergence:   inf\n",
      "Epoch: 247, Loss: 2535.50425, Residuals: -0.97943, Convergence: -0.019423\n",
      "Epoch: 247, Loss: 2484.31138, Residuals: -0.93395, Convergence: 0.000783\n",
      "Evidence 15022.958\n",
      "\n",
      "Epoch: 247, Evidence: 15022.95801, Convergence: 0.000683\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.88e-02\n",
      "Epoch: 247, Loss: 2485.59850, Residuals: -0.93395, Convergence:   inf\n",
      "Epoch: 248, Loss: 2493.09669, Residuals: -0.93464, Convergence: -0.003008\n",
      "Epoch: 248, Loss: 2486.40059, Residuals: -0.93128, Convergence: -0.000323\n",
      "Evidence 15023.811\n",
      "\n",
      "Epoch: 248, Evidence: 15023.81055, Convergence: 0.000740\n",
      "Total samples: 183, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 381.90763, Residuals: -4.51184, Convergence:   inf\n",
      "Epoch: 1, Loss: 356.12176, Residuals: -4.39130, Convergence: 0.072407\n",
      "Epoch: 2, Loss: 334.97357, Residuals: -4.22606, Convergence: 0.063134\n",
      "Epoch: 3, Loss: 318.81595, Residuals: -4.06048, Convergence: 0.050680\n",
      "Epoch: 4, Loss: 306.49114, Residuals: -3.91444, Convergence: 0.040213\n",
      "Epoch: 5, Loss: 296.71409, Residuals: -3.78525, Convergence: 0.032951\n",
      "Epoch: 6, Loss: 288.77439, Residuals: -3.67274, Convergence: 0.027494\n",
      "Epoch: 7, Loss: 282.17990, Residuals: -3.57645, Convergence: 0.023370\n",
      "Epoch: 8, Loss: 276.56683, Residuals: -3.49430, Convergence: 0.020296\n",
      "Epoch: 9, Loss: 271.67891, Residuals: -3.42392, Convergence: 0.017992\n",
      "Epoch: 10, Loss: 267.33430, Residuals: -3.36318, Convergence: 0.016252\n",
      "Epoch: 11, Loss: 263.40185, Residuals: -3.31028, Convergence: 0.014929\n",
      "Epoch: 12, Loss: 259.78660, Residuals: -3.26370, Convergence: 0.013916\n",
      "Epoch: 13, Loss: 256.42026, Residuals: -3.22210, Convergence: 0.013128\n",
      "Epoch: 14, Loss: 253.25425, Residuals: -3.18432, Convergence: 0.012501\n",
      "Epoch: 15, Loss: 250.25526, Residuals: -3.14937, Convergence: 0.011984\n",
      "Epoch: 16, Loss: 247.40281, Residuals: -3.11657, Convergence: 0.011530\n",
      "Epoch: 17, Loss: 244.68271, Residuals: -3.08546, Convergence: 0.011117\n",
      "Epoch: 18, Loss: 242.07572, Residuals: -3.05568, Convergence: 0.010769\n",
      "Epoch: 19, Loss: 239.55268, Residuals: -3.02677, Convergence: 0.010532\n",
      "Epoch: 20, Loss: 237.07945, Residuals: -2.99823, Convergence: 0.010432\n",
      "Epoch: 21, Loss: 234.62407, Residuals: -2.96956, Convergence: 0.010465\n",
      "Epoch: 22, Loss: 232.16114, Residuals: -2.94037, Convergence: 0.010609\n",
      "Epoch: 23, Loss: 229.66775, Residuals: -2.91039, Convergence: 0.010857\n",
      "Epoch: 24, Loss: 227.10990, Residuals: -2.87925, Convergence: 0.011263\n",
      "Epoch: 25, Loss: 224.43353, Residuals: -2.84633, Convergence: 0.011925\n",
      "Epoch: 26, Loss: 221.59257, Residuals: -2.81104, Convergence: 0.012821\n",
      "Epoch: 27, Loss: 218.64327, Residuals: -2.77388, Convergence: 0.013489\n",
      "Epoch: 28, Loss: 215.73297, Residuals: -2.73642, Convergence: 0.013490\n",
      "Epoch: 29, Loss: 212.93175, Residuals: -2.69957, Convergence: 0.013155\n",
      "Epoch: 30, Loss: 210.23666, Residuals: -2.66345, Convergence: 0.012819\n",
      "Epoch: 31, Loss: 207.63031, Residuals: -2.62793, Convergence: 0.012553\n",
      "Epoch: 32, Loss: 205.09836, Residuals: -2.59291, Convergence: 0.012345\n",
      "Epoch: 33, Loss: 202.63141, Residuals: -2.55830, Convergence: 0.012175\n",
      "Epoch: 34, Loss: 200.22399, Residuals: -2.52406, Convergence: 0.012024\n",
      "Epoch: 35, Loss: 197.87322, Residuals: -2.49014, Convergence: 0.011880\n",
      "Epoch: 36, Loss: 195.57783, Residuals: -2.45652, Convergence: 0.011736\n",
      "Epoch: 37, Loss: 193.33731, Residuals: -2.42319, Convergence: 0.011589\n",
      "Epoch: 38, Loss: 191.15142, Residuals: -2.39014, Convergence: 0.011435\n",
      "Epoch: 39, Loss: 189.01995, Residuals: -2.35736, Convergence: 0.011276\n",
      "Epoch: 40, Loss: 186.94262, Residuals: -2.32484, Convergence: 0.011112\n",
      "Epoch: 41, Loss: 184.91917, Residuals: -2.29257, Convergence: 0.010942\n",
      "Epoch: 42, Loss: 182.94949, Residuals: -2.26052, Convergence: 0.010766\n",
      "Epoch: 43, Loss: 181.03370, Residuals: -2.22869, Convergence: 0.010582\n",
      "Epoch: 44, Loss: 179.17224, Residuals: -2.19706, Convergence: 0.010389\n",
      "Epoch: 45, Loss: 177.36578, Residuals: -2.16565, Convergence: 0.010185\n",
      "Epoch: 46, Loss: 175.61527, Residuals: -2.13447, Convergence: 0.009968\n",
      "Epoch: 47, Loss: 173.92186, Residuals: -2.10355, Convergence: 0.009737\n",
      "Epoch: 48, Loss: 172.28697, Residuals: -2.07292, Convergence: 0.009489\n",
      "Epoch: 49, Loss: 170.71218, Residuals: -2.04263, Convergence: 0.009225\n",
      "Epoch: 50, Loss: 169.19907, Residuals: -2.01274, Convergence: 0.008943\n",
      "Epoch: 51, Loss: 167.74899, Residuals: -1.98330, Convergence: 0.008644\n",
      "Epoch: 52, Loss: 166.36275, Residuals: -1.95436, Convergence: 0.008333\n",
      "Epoch: 53, Loss: 165.04047, Residuals: -1.92597, Convergence: 0.008012\n",
      "Epoch: 54, Loss: 163.78152, Residuals: -1.89818, Convergence: 0.007687\n",
      "Epoch: 55, Loss: 162.58456, Residuals: -1.87103, Convergence: 0.007362\n",
      "Epoch: 56, Loss: 161.44760, Residuals: -1.84453, Convergence: 0.007042\n",
      "Epoch: 57, Loss: 160.36819, Residuals: -1.81871, Convergence: 0.006731\n",
      "Epoch: 58, Loss: 159.34349, Residuals: -1.79358, Convergence: 0.006431\n",
      "Epoch: 59, Loss: 158.37037, Residuals: -1.76915, Convergence: 0.006145\n",
      "Epoch: 60, Loss: 157.44560, Residuals: -1.74541, Convergence: 0.005874\n",
      "Epoch: 61, Loss: 156.56583, Residuals: -1.72235, Convergence: 0.005619\n",
      "Epoch: 62, Loss: 155.72784, Residuals: -1.69996, Convergence: 0.005381\n",
      "Epoch: 63, Loss: 154.92851, Residuals: -1.67821, Convergence: 0.005159\n",
      "Epoch: 64, Loss: 154.16501, Residuals: -1.65708, Convergence: 0.004952\n",
      "Epoch: 65, Loss: 153.43481, Residuals: -1.63657, Convergence: 0.004759\n",
      "Epoch: 66, Loss: 152.73570, Residuals: -1.61664, Convergence: 0.004577\n",
      "Epoch: 67, Loss: 152.06579, Residuals: -1.59729, Convergence: 0.004405\n",
      "Epoch: 68, Loss: 151.42351, Residuals: -1.57849, Convergence: 0.004242\n",
      "Epoch: 69, Loss: 150.80752, Residuals: -1.56025, Convergence: 0.004085\n",
      "Epoch: 70, Loss: 150.21668, Residuals: -1.54255, Convergence: 0.003933\n",
      "Epoch: 71, Loss: 149.65005, Residuals: -1.52538, Convergence: 0.003786\n",
      "Epoch: 72, Loss: 149.10675, Residuals: -1.50875, Convergence: 0.003644\n",
      "Epoch: 73, Loss: 148.58603, Residuals: -1.49264, Convergence: 0.003505\n",
      "Epoch: 74, Loss: 148.08716, Residuals: -1.47705, Convergence: 0.003369\n",
      "Epoch: 75, Loss: 147.60946, Residuals: -1.46197, Convergence: 0.003236\n",
      "Epoch: 76, Loss: 147.15226, Residuals: -1.44740, Convergence: 0.003107\n",
      "Epoch: 77, Loss: 146.71490, Residuals: -1.43333, Convergence: 0.002981\n",
      "Epoch: 78, Loss: 146.29672, Residuals: -1.41975, Convergence: 0.002858\n",
      "Epoch: 79, Loss: 145.89705, Residuals: -1.40666, Convergence: 0.002739\n",
      "Epoch: 80, Loss: 145.51522, Residuals: -1.39404, Convergence: 0.002624\n",
      "Epoch: 81, Loss: 145.15056, Residuals: -1.38189, Convergence: 0.002512\n",
      "Epoch: 82, Loss: 144.80239, Residuals: -1.37019, Convergence: 0.002404\n",
      "Epoch: 83, Loss: 144.47003, Residuals: -1.35893, Convergence: 0.002301\n",
      "Epoch: 84, Loss: 144.15280, Residuals: -1.34810, Convergence: 0.002201\n",
      "Epoch: 85, Loss: 143.85005, Residuals: -1.33769, Convergence: 0.002105\n",
      "Epoch: 86, Loss: 143.56112, Residuals: -1.32768, Convergence: 0.002013\n",
      "Epoch: 87, Loss: 143.28539, Residuals: -1.31807, Convergence: 0.001924\n",
      "Epoch: 88, Loss: 143.02224, Residuals: -1.30883, Convergence: 0.001840\n",
      "Epoch: 89, Loss: 142.77110, Residuals: -1.29995, Convergence: 0.001759\n",
      "Epoch: 90, Loss: 142.53141, Residuals: -1.29143, Convergence: 0.001682\n",
      "Epoch: 91, Loss: 142.30266, Residuals: -1.28325, Convergence: 0.001607\n",
      "Epoch: 92, Loss: 142.08436, Residuals: -1.27538, Convergence: 0.001536\n",
      "Epoch: 93, Loss: 141.87604, Residuals: -1.26783, Convergence: 0.001468\n",
      "Epoch: 94, Loss: 141.67730, Residuals: -1.26059, Convergence: 0.001403\n",
      "Epoch: 95, Loss: 141.48775, Residuals: -1.25362, Convergence: 0.001340\n",
      "Epoch: 96, Loss: 141.30704, Residuals: -1.24694, Convergence: 0.001279\n",
      "Epoch: 97, Loss: 141.13484, Residuals: -1.24052, Convergence: 0.001220\n",
      "Epoch: 98, Loss: 140.97086, Residuals: -1.23435, Convergence: 0.001163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99, Loss: 140.81485, Residuals: -1.22844, Convergence: 0.001108\n",
      "Epoch: 100, Loss: 140.66659, Residuals: -1.22275, Convergence: 0.001054\n",
      "Epoch: 101, Loss: 140.52584, Residuals: -1.21730, Convergence: 0.001002\n",
      "Epoch: 102, Loss: 140.39245, Residuals: -1.21206, Convergence: 0.000950\n",
      "Evidence -181.972\n",
      "\n",
      "Epoch: 102, Evidence: -181.97208, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.25e-01\n",
      "Epoch: 102, Loss: 1372.89642, Residuals: -1.21206, Convergence:   inf\n",
      "Epoch: 103, Loss: 1310.89461, Residuals: -1.24239, Convergence: 0.047297\n",
      "Epoch: 104, Loss: 1262.95718, Residuals: -1.26640, Convergence: 0.037956\n",
      "Epoch: 105, Loss: 1226.18724, Residuals: -1.28424, Convergence: 0.029987\n",
      "Epoch: 106, Loss: 1197.45137, Residuals: -1.29733, Convergence: 0.023998\n",
      "Epoch: 107, Loss: 1174.27658, Residuals: -1.30733, Convergence: 0.019735\n",
      "Epoch: 108, Loss: 1155.10652, Residuals: -1.31522, Convergence: 0.016596\n",
      "Epoch: 109, Loss: 1138.97373, Residuals: -1.32145, Convergence: 0.014164\n",
      "Epoch: 110, Loss: 1125.22131, Residuals: -1.32624, Convergence: 0.012222\n",
      "Epoch: 111, Loss: 1113.36467, Residuals: -1.32973, Convergence: 0.010649\n",
      "Epoch: 112, Loss: 1103.02901, Residuals: -1.33204, Convergence: 0.009370\n",
      "Epoch: 113, Loss: 1093.91362, Residuals: -1.33326, Convergence: 0.008333\n",
      "Epoch: 114, Loss: 1085.77234, Residuals: -1.33348, Convergence: 0.007498\n",
      "Epoch: 115, Loss: 1078.40094, Residuals: -1.33277, Convergence: 0.006835\n",
      "Epoch: 116, Loss: 1071.62829, Residuals: -1.33119, Convergence: 0.006320\n",
      "Epoch: 117, Loss: 1065.30953, Residuals: -1.32878, Convergence: 0.005931\n",
      "Epoch: 118, Loss: 1059.32354, Residuals: -1.32559, Convergence: 0.005651\n",
      "Epoch: 119, Loss: 1053.56749, Residuals: -1.32165, Convergence: 0.005463\n",
      "Epoch: 120, Loss: 1047.95584, Residuals: -1.31698, Convergence: 0.005355\n",
      "Epoch: 121, Loss: 1042.41751, Residuals: -1.31163, Convergence: 0.005313\n",
      "Epoch: 122, Loss: 1036.89940, Residuals: -1.30564, Convergence: 0.005322\n",
      "Epoch: 123, Loss: 1031.37656, Residuals: -1.29908, Convergence: 0.005355\n",
      "Epoch: 124, Loss: 1025.86355, Residuals: -1.29204, Convergence: 0.005374\n",
      "Epoch: 125, Loss: 1020.41762, Residuals: -1.28462, Convergence: 0.005337\n",
      "Epoch: 126, Loss: 1015.12088, Residuals: -1.27691, Convergence: 0.005218\n",
      "Epoch: 127, Loss: 1010.05035, Residuals: -1.26900, Convergence: 0.005020\n",
      "Epoch: 128, Loss: 1005.25340, Residuals: -1.26096, Convergence: 0.004772\n",
      "Epoch: 129, Loss: 1000.74489, Residuals: -1.25287, Convergence: 0.004505\n",
      "Epoch: 130, Loss: 996.51544, Residuals: -1.24476, Convergence: 0.004244\n",
      "Epoch: 131, Loss: 992.54348, Residuals: -1.23670, Convergence: 0.004002\n",
      "Epoch: 132, Loss: 988.80369, Residuals: -1.22872, Convergence: 0.003782\n",
      "Epoch: 133, Loss: 985.27188, Residuals: -1.22085, Convergence: 0.003585\n",
      "Epoch: 134, Loss: 981.92650, Residuals: -1.21312, Convergence: 0.003407\n",
      "Epoch: 135, Loss: 978.74961, Residuals: -1.20556, Convergence: 0.003246\n",
      "Epoch: 136, Loss: 975.72638, Residuals: -1.19818, Convergence: 0.003098\n",
      "Epoch: 137, Loss: 972.84531, Residuals: -1.19100, Convergence: 0.002961\n",
      "Epoch: 138, Loss: 970.09635, Residuals: -1.18402, Convergence: 0.002834\n",
      "Epoch: 139, Loss: 967.47134, Residuals: -1.17728, Convergence: 0.002713\n",
      "Epoch: 140, Loss: 964.96310, Residuals: -1.17075, Convergence: 0.002599\n",
      "Epoch: 141, Loss: 962.56593, Residuals: -1.16446, Convergence: 0.002490\n",
      "Epoch: 142, Loss: 960.27386, Residuals: -1.15841, Convergence: 0.002387\n",
      "Epoch: 143, Loss: 958.08149, Residuals: -1.15259, Convergence: 0.002288\n",
      "Epoch: 144, Loss: 955.98458, Residuals: -1.14700, Convergence: 0.002193\n",
      "Epoch: 145, Loss: 953.97780, Residuals: -1.14163, Convergence: 0.002104\n",
      "Epoch: 146, Loss: 952.05639, Residuals: -1.13649, Convergence: 0.002018\n",
      "Epoch: 147, Loss: 950.21559, Residuals: -1.13155, Convergence: 0.001937\n",
      "Epoch: 148, Loss: 948.45038, Residuals: -1.12683, Convergence: 0.001861\n",
      "Epoch: 149, Loss: 946.75583, Residuals: -1.12230, Convergence: 0.001790\n",
      "Epoch: 150, Loss: 945.12641, Residuals: -1.11796, Convergence: 0.001724\n",
      "Epoch: 151, Loss: 943.55679, Residuals: -1.11380, Convergence: 0.001664\n",
      "Epoch: 152, Loss: 942.04104, Residuals: -1.10980, Convergence: 0.001609\n",
      "Epoch: 153, Loss: 940.57314, Residuals: -1.10595, Convergence: 0.001561\n",
      "Epoch: 154, Loss: 939.14692, Residuals: -1.10225, Convergence: 0.001519\n",
      "Epoch: 155, Loss: 937.75557, Residuals: -1.09867, Convergence: 0.001484\n",
      "Epoch: 156, Loss: 936.39321, Residuals: -1.09520, Convergence: 0.001455\n",
      "Epoch: 157, Loss: 935.05356, Residuals: -1.09183, Convergence: 0.001433\n",
      "Epoch: 158, Loss: 933.73179, Residuals: -1.08855, Convergence: 0.001416\n",
      "Epoch: 159, Loss: 932.42418, Residuals: -1.08535, Convergence: 0.001402\n",
      "Epoch: 160, Loss: 931.12926, Residuals: -1.08221, Convergence: 0.001391\n",
      "Epoch: 161, Loss: 929.84764, Residuals: -1.07914, Convergence: 0.001378\n",
      "Epoch: 162, Loss: 928.58224, Residuals: -1.07614, Convergence: 0.001363\n",
      "Epoch: 163, Loss: 927.33821, Residuals: -1.07321, Convergence: 0.001342\n",
      "Epoch: 164, Loss: 926.12113, Residuals: -1.07036, Convergence: 0.001314\n",
      "Epoch: 165, Loss: 924.93697, Residuals: -1.06761, Convergence: 0.001280\n",
      "Epoch: 166, Loss: 923.79058, Residuals: -1.06494, Convergence: 0.001241\n",
      "Epoch: 167, Loss: 922.68562, Residuals: -1.06238, Convergence: 0.001198\n",
      "Epoch: 168, Loss: 921.62449, Residuals: -1.05992, Convergence: 0.001151\n",
      "Epoch: 169, Loss: 920.60794, Residuals: -1.05756, Convergence: 0.001104\n",
      "Epoch: 170, Loss: 919.63604, Residuals: -1.05530, Convergence: 0.001057\n",
      "Epoch: 171, Loss: 918.70783, Residuals: -1.05314, Convergence: 0.001010\n",
      "Epoch: 172, Loss: 917.82174, Residuals: -1.05107, Convergence: 0.000965\n",
      "Evidence 11288.319\n",
      "\n",
      "Epoch: 172, Evidence: 11288.31934, Convergence: 1.016120\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 5.75e-01\n",
      "Epoch: 172, Loss: 2349.83706, Residuals: -1.05107, Convergence:   inf\n",
      "Epoch: 173, Loss: 2313.57524, Residuals: -1.05942, Convergence: 0.015673\n",
      "Epoch: 174, Loss: 2287.29182, Residuals: -1.05830, Convergence: 0.011491\n",
      "Epoch: 175, Loss: 2265.41566, Residuals: -1.05624, Convergence: 0.009657\n",
      "Epoch: 176, Loss: 2246.97499, Residuals: -1.05391, Convergence: 0.008207\n",
      "Epoch: 177, Loss: 2231.27651, Residuals: -1.05140, Convergence: 0.007036\n",
      "Epoch: 178, Loss: 2217.78470, Residuals: -1.04876, Convergence: 0.006083\n",
      "Epoch: 179, Loss: 2206.06457, Residuals: -1.04601, Convergence: 0.005313\n",
      "Epoch: 180, Loss: 2195.75634, Residuals: -1.04316, Convergence: 0.004695\n",
      "Epoch: 181, Loss: 2186.56641, Residuals: -1.04018, Convergence: 0.004203\n",
      "Epoch: 182, Loss: 2178.26137, Residuals: -1.03709, Convergence: 0.003813\n",
      "Epoch: 183, Loss: 2170.66593, Residuals: -1.03386, Convergence: 0.003499\n",
      "Epoch: 184, Loss: 2163.66226, Residuals: -1.03051, Convergence: 0.003237\n",
      "Epoch: 185, Loss: 2157.18057, Residuals: -1.02708, Convergence: 0.003005\n",
      "Epoch: 186, Loss: 2151.17926, Residuals: -1.02360, Convergence: 0.002790\n",
      "Epoch: 187, Loss: 2145.62913, Residuals: -1.02014, Convergence: 0.002587\n",
      "Epoch: 188, Loss: 2140.50563, Residuals: -1.01674, Convergence: 0.002394\n",
      "Epoch: 189, Loss: 2135.78105, Residuals: -1.01343, Convergence: 0.002212\n",
      "Epoch: 190, Loss: 2131.42705, Residuals: -1.01024, Convergence: 0.002043\n",
      "Epoch: 191, Loss: 2127.41381, Residuals: -1.00718, Convergence: 0.001886\n",
      "Epoch: 192, Loss: 2123.71116, Residuals: -1.00427, Convergence: 0.001743\n",
      "Epoch: 193, Loss: 2120.28924, Residuals: -1.00150, Convergence: 0.001614\n",
      "Epoch: 194, Loss: 2117.12146, Residuals: -0.99888, Convergence: 0.001496\n",
      "Epoch: 195, Loss: 2114.18077, Residuals: -0.99641, Convergence: 0.001391\n",
      "Epoch: 196, Loss: 2111.44378, Residuals: -0.99409, Convergence: 0.001296\n",
      "Epoch: 197, Loss: 2108.88907, Residuals: -0.99190, Convergence: 0.001211\n",
      "Epoch: 198, Loss: 2106.49795, Residuals: -0.98985, Convergence: 0.001135\n",
      "Epoch: 199, Loss: 2104.25436, Residuals: -0.98793, Convergence: 0.001066\n",
      "Epoch: 200, Loss: 2102.14405, Residuals: -0.98614, Convergence: 0.001004\n",
      "Epoch: 201, Loss: 2100.15530, Residuals: -0.98447, Convergence: 0.000947\n",
      "Evidence 14500.437\n",
      "\n",
      "Epoch: 201, Evidence: 14500.43652, Convergence: 0.221519\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 4.37e-01\n",
      "Epoch: 201, Loss: 2475.65373, Residuals: -0.98447, Convergence:   inf\n",
      "Epoch: 202, Loss: 2462.76759, Residuals: -0.98110, Convergence: 0.005232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 203, Loss: 2452.23281, Residuals: -0.97756, Convergence: 0.004296\n",
      "Epoch: 204, Loss: 2443.18900, Residuals: -0.97426, Convergence: 0.003702\n",
      "Epoch: 205, Loss: 2435.37176, Residuals: -0.97124, Convergence: 0.003210\n",
      "Epoch: 206, Loss: 2428.57573, Residuals: -0.96853, Convergence: 0.002798\n",
      "Epoch: 207, Loss: 2422.63411, Residuals: -0.96609, Convergence: 0.002453\n",
      "Epoch: 208, Loss: 2417.40883, Residuals: -0.96391, Convergence: 0.002162\n",
      "Epoch: 209, Loss: 2412.78433, Residuals: -0.96196, Convergence: 0.001917\n",
      "Epoch: 210, Loss: 2408.66647, Residuals: -0.96022, Convergence: 0.001710\n",
      "Epoch: 211, Loss: 2404.97670, Residuals: -0.95867, Convergence: 0.001534\n",
      "Epoch: 212, Loss: 2401.65005, Residuals: -0.95728, Convergence: 0.001385\n",
      "Epoch: 213, Loss: 2398.63287, Residuals: -0.95603, Convergence: 0.001258\n",
      "Epoch: 214, Loss: 2395.88111, Residuals: -0.95492, Convergence: 0.001149\n",
      "Epoch: 215, Loss: 2393.35931, Residuals: -0.95392, Convergence: 0.001054\n",
      "Epoch: 216, Loss: 2391.03681, Residuals: -0.95301, Convergence: 0.000971\n",
      "Evidence 14862.043\n",
      "\n",
      "Epoch: 216, Evidence: 14862.04297, Convergence: 0.024331\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 3.34e-01\n",
      "Epoch: 216, Loss: 2480.64575, Residuals: -0.95301, Convergence:   inf\n",
      "Epoch: 217, Loss: 2474.38080, Residuals: -0.95026, Convergence: 0.002532\n",
      "Epoch: 218, Loss: 2469.22548, Residuals: -0.94795, Convergence: 0.002088\n",
      "Epoch: 219, Loss: 2464.86038, Residuals: -0.94604, Convergence: 0.001771\n",
      "Epoch: 220, Loss: 2461.10746, Residuals: -0.94447, Convergence: 0.001525\n",
      "Epoch: 221, Loss: 2457.83522, Residuals: -0.94317, Convergence: 0.001331\n",
      "Epoch: 222, Loss: 2454.94530, Residuals: -0.94208, Convergence: 0.001177\n",
      "Epoch: 223, Loss: 2452.36238, Residuals: -0.94118, Convergence: 0.001053\n",
      "Epoch: 224, Loss: 2450.03015, Residuals: -0.94043, Convergence: 0.000952\n",
      "Evidence 14938.989\n",
      "\n",
      "Epoch: 224, Evidence: 14938.98926, Convergence: 0.005151\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.61e-01\n",
      "Epoch: 224, Loss: 2482.24830, Residuals: -0.94043, Convergence:   inf\n",
      "Epoch: 225, Loss: 2478.40604, Residuals: -0.93857, Convergence: 0.001550\n",
      "Epoch: 226, Loss: 2475.22495, Residuals: -0.93714, Convergence: 0.001285\n",
      "Epoch: 227, Loss: 2472.50068, Residuals: -0.93605, Convergence: 0.001102\n",
      "Epoch: 228, Loss: 2470.11768, Residuals: -0.93520, Convergence: 0.000965\n",
      "Evidence 14966.725\n",
      "\n",
      "Epoch: 228, Evidence: 14966.72461, Convergence: 0.001853\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.10e-01\n",
      "Epoch: 228, Loss: 2483.33933, Residuals: -0.93520, Convergence:   inf\n",
      "Epoch: 229, Loss: 2480.44459, Residuals: -0.93380, Convergence: 0.001167\n",
      "Epoch: 230, Loss: 2478.01940, Residuals: -0.93277, Convergence: 0.000979\n",
      "Evidence 14978.980\n",
      "\n",
      "Epoch: 230, Evidence: 14978.98047, Convergence: 0.000818\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.74e-01\n",
      "Epoch: 230, Loss: 2484.16130, Residuals: -0.93277, Convergence:   inf\n",
      "Epoch: 231, Loss: 2479.53009, Residuals: -0.93114, Convergence: 0.001868\n",
      "Epoch: 232, Loss: 2476.03435, Residuals: -0.93013, Convergence: 0.001412\n",
      "Epoch: 233, Loss: 2473.21024, Residuals: -0.92951, Convergence: 0.001142\n",
      "Epoch: 234, Loss: 2470.82079, Residuals: -0.92925, Convergence: 0.000967\n",
      "Evidence 14997.008\n",
      "\n",
      "Epoch: 234, Evidence: 14997.00781, Convergence: 0.002019\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.44e-01\n",
      "Epoch: 234, Loss: 2484.33789, Residuals: -0.92925, Convergence:   inf\n",
      "Epoch: 235, Loss: 2481.23010, Residuals: -0.92775, Convergence: 0.001253\n",
      "Epoch: 236, Loss: 2478.78377, Residuals: -0.92718, Convergence: 0.000987\n",
      "Evidence 15008.318\n",
      "\n",
      "Epoch: 236, Evidence: 15008.31836, Convergence: 0.000754\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.22e-01\n",
      "Epoch: 236, Loss: 2484.57813, Residuals: -0.92718, Convergence:   inf\n",
      "Epoch: 237, Loss: 2479.97738, Residuals: -0.92560, Convergence: 0.001855\n",
      "Epoch: 238, Loss: 2476.74987, Residuals: -0.92712, Convergence: 0.001303\n",
      "Epoch: 239, Loss: 2474.11428, Residuals: -0.92792, Convergence: 0.001065\n",
      "Epoch: 240, Loss: 2471.79774, Residuals: -0.93072, Convergence: 0.000937\n",
      "Evidence 15024.421\n",
      "\n",
      "Epoch: 240, Evidence: 15024.42090, Convergence: 0.001825\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.09e-01\n",
      "Epoch: 240, Loss: 2483.87241, Residuals: -0.93072, Convergence:   inf\n",
      "Epoch: 241, Loss: 2481.82074, Residuals: -0.93053, Convergence: 0.000827\n",
      "Evidence 15031.568\n",
      "\n",
      "Epoch: 241, Evidence: 15031.56836, Convergence: 0.000475\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 8.93e-02\n",
      "Epoch: 241, Loss: 2484.82740, Residuals: -0.93053, Convergence:   inf\n",
      "Epoch: 242, Loss: 2525.36641, Residuals: -0.97653, Convergence: -0.016053\n",
      "Epoch: 242, Loss: 2482.39701, Residuals: -0.93073, Convergence: 0.000979\n",
      "Evidence 15036.098\n",
      "\n",
      "Epoch: 242, Evidence: 15036.09766, Convergence: 0.000777\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 8.27e-02\n",
      "Epoch: 242, Loss: 2484.24580, Residuals: -0.93073, Convergence:   inf\n",
      "Epoch: 243, Loss: 2488.39754, Residuals: -0.93679, Convergence: -0.001668\n",
      "Epoch: 243, Loss: 2483.93751, Residuals: -0.93115, Convergence: 0.000124\n",
      "Evidence 15038.088\n",
      "\n",
      "Epoch: 243, Evidence: 15038.08789, Convergence: 0.000909\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 380.95516, Residuals: -4.52208, Convergence:   inf\n",
      "Epoch: 1, Loss: 355.30833, Residuals: -4.40156, Convergence: 0.072182\n",
      "Epoch: 2, Loss: 334.33005, Residuals: -4.23761, Convergence: 0.062747\n",
      "Epoch: 3, Loss: 318.29675, Residuals: -4.07264, Convergence: 0.050372\n",
      "Epoch: 4, Loss: 306.08106, Residuals: -3.92743, Convergence: 0.039910\n",
      "Epoch: 5, Loss: 296.40104, Residuals: -3.79912, Convergence: 0.032659\n",
      "Epoch: 6, Loss: 288.54655, Residuals: -3.68723, Convergence: 0.027221\n",
      "Epoch: 7, Loss: 282.02829, Residuals: -3.59124, Convergence: 0.023112\n",
      "Epoch: 8, Loss: 276.48428, Residuals: -3.50914, Convergence: 0.020052\n",
      "Epoch: 9, Loss: 271.65860, Residuals: -3.43862, Convergence: 0.017764\n",
      "Epoch: 10, Loss: 267.36953, Residuals: -3.37761, Convergence: 0.016042\n",
      "Epoch: 11, Loss: 263.48578, Residuals: -3.32435, Convergence: 0.014740\n",
      "Epoch: 12, Loss: 259.91222, Residuals: -3.27736, Convergence: 0.013749\n",
      "Epoch: 13, Loss: 256.58067, Residuals: -3.23536, Convergence: 0.012984\n",
      "Epoch: 14, Loss: 253.44365, Residuals: -3.19723, Convergence: 0.012378\n",
      "Epoch: 15, Loss: 250.47038, Residuals: -3.16202, Convergence: 0.011871\n",
      "Epoch: 16, Loss: 247.64411, Residuals: -3.12904, Convergence: 0.011413\n",
      "Epoch: 17, Loss: 244.95444, Residuals: -3.09788, Convergence: 0.010980\n",
      "Epoch: 18, Loss: 242.38462, Residuals: -3.06815, Convergence: 0.010602\n",
      "Epoch: 19, Loss: 239.90655, Residuals: -3.03939, Convergence: 0.010329\n",
      "Epoch: 20, Loss: 237.48636, Residuals: -3.01106, Convergence: 0.010191\n",
      "Epoch: 21, Loss: 235.09289, Residuals: -2.98267, Convergence: 0.010181\n",
      "Epoch: 22, Loss: 232.70276, Residuals: -2.95385, Convergence: 0.010271\n",
      "Epoch: 23, Loss: 230.29462, Residuals: -2.92435, Convergence: 0.010457\n",
      "Epoch: 24, Loss: 227.83329, Residuals: -2.89380, Convergence: 0.010803\n",
      "Epoch: 25, Loss: 225.26477, Residuals: -2.86157, Convergence: 0.011402\n",
      "Epoch: 26, Loss: 222.55716, Residuals: -2.82724, Convergence: 0.012166\n",
      "Epoch: 27, Loss: 219.78546, Residuals: -2.79154, Convergence: 0.012611\n",
      "Epoch: 28, Loss: 217.07372, Residuals: -2.75590, Convergence: 0.012492\n",
      "Epoch: 29, Loss: 214.46325, Residuals: -2.72092, Convergence: 0.012172\n",
      "Epoch: 30, Loss: 211.94203, Residuals: -2.68658, Convergence: 0.011896\n",
      "Epoch: 31, Loss: 209.48980, Residuals: -2.65271, Convergence: 0.011706\n",
      "Epoch: 32, Loss: 207.09034, Residuals: -2.61915, Convergence: 0.011587\n",
      "Epoch: 33, Loss: 204.73286, Residuals: -2.58578, Convergence: 0.011515\n",
      "Epoch: 34, Loss: 202.41130, Residuals: -2.55250, Convergence: 0.011470\n",
      "Epoch: 35, Loss: 200.12311, Residuals: -2.51925, Convergence: 0.011434\n",
      "Epoch: 36, Loss: 197.86821, Residuals: -2.48600, Convergence: 0.011396\n",
      "Epoch: 37, Loss: 195.64799, Residuals: -2.45274, Convergence: 0.011348\n",
      "Epoch: 38, Loss: 193.46466, Residuals: -2.41947, Convergence: 0.011285\n",
      "Epoch: 39, Loss: 191.32061, Residuals: -2.38619, Convergence: 0.011207\n",
      "Epoch: 40, Loss: 189.21817, Residuals: -2.35294, Convergence: 0.011111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41, Loss: 187.15936, Residuals: -2.31972, Convergence: 0.011000\n",
      "Epoch: 42, Loss: 185.14588, Residuals: -2.28655, Convergence: 0.010875\n",
      "Epoch: 43, Loss: 183.17920, Residuals: -2.25347, Convergence: 0.010736\n",
      "Epoch: 44, Loss: 181.26072, Residuals: -2.22049, Convergence: 0.010584\n",
      "Epoch: 45, Loss: 179.39189, Residuals: -2.18764, Convergence: 0.010418\n",
      "Epoch: 46, Loss: 177.57434, Residuals: -2.15495, Convergence: 0.010235\n",
      "Epoch: 47, Loss: 175.80984, Residuals: -2.12247, Convergence: 0.010036\n",
      "Epoch: 48, Loss: 174.10023, Residuals: -2.09024, Convergence: 0.009820\n",
      "Epoch: 49, Loss: 172.44723, Residuals: -2.05831, Convergence: 0.009586\n",
      "Epoch: 50, Loss: 170.85229, Residuals: -2.02673, Convergence: 0.009335\n",
      "Epoch: 51, Loss: 169.31645, Residuals: -1.99556, Convergence: 0.009071\n",
      "Epoch: 52, Loss: 167.84032, Residuals: -1.96485, Convergence: 0.008795\n",
      "Epoch: 53, Loss: 166.42405, Residuals: -1.93465, Convergence: 0.008510\n",
      "Epoch: 54, Loss: 165.06738, Residuals: -1.90501, Convergence: 0.008219\n",
      "Epoch: 55, Loss: 163.76965, Residuals: -1.87596, Convergence: 0.007924\n",
      "Epoch: 56, Loss: 162.52996, Residuals: -1.84755, Convergence: 0.007627\n",
      "Epoch: 57, Loss: 161.34709, Residuals: -1.81981, Convergence: 0.007331\n",
      "Epoch: 58, Loss: 160.21958, Residuals: -1.79278, Convergence: 0.007037\n",
      "Epoch: 59, Loss: 159.14576, Residuals: -1.76647, Convergence: 0.006747\n",
      "Epoch: 60, Loss: 158.12374, Residuals: -1.74090, Convergence: 0.006463\n",
      "Epoch: 61, Loss: 157.15143, Residuals: -1.71610, Convergence: 0.006187\n",
      "Epoch: 62, Loss: 156.22656, Residuals: -1.69205, Convergence: 0.005920\n",
      "Epoch: 63, Loss: 155.34674, Residuals: -1.66876, Convergence: 0.005664\n",
      "Epoch: 64, Loss: 154.50953, Residuals: -1.64622, Convergence: 0.005418\n",
      "Epoch: 65, Loss: 153.71255, Residuals: -1.62441, Convergence: 0.005185\n",
      "Epoch: 66, Loss: 152.95353, Residuals: -1.60332, Convergence: 0.004962\n",
      "Epoch: 67, Loss: 152.23041, Residuals: -1.58293, Convergence: 0.004750\n",
      "Epoch: 68, Loss: 151.54138, Residuals: -1.56322, Convergence: 0.004547\n",
      "Epoch: 69, Loss: 150.88484, Residuals: -1.54418, Convergence: 0.004351\n",
      "Epoch: 70, Loss: 150.25939, Residuals: -1.52580, Convergence: 0.004162\n",
      "Epoch: 71, Loss: 149.66377, Residuals: -1.50806, Convergence: 0.003980\n",
      "Epoch: 72, Loss: 149.09683, Residuals: -1.49095, Convergence: 0.003802\n",
      "Epoch: 73, Loss: 148.55748, Residuals: -1.47448, Convergence: 0.003631\n",
      "Epoch: 74, Loss: 148.04465, Residuals: -1.45861, Convergence: 0.003464\n",
      "Epoch: 75, Loss: 147.55731, Residuals: -1.44336, Convergence: 0.003303\n",
      "Epoch: 76, Loss: 147.09441, Residuals: -1.42869, Convergence: 0.003147\n",
      "Epoch: 77, Loss: 146.65495, Residuals: -1.41461, Convergence: 0.002997\n",
      "Epoch: 78, Loss: 146.23791, Residuals: -1.40109, Convergence: 0.002852\n",
      "Epoch: 79, Loss: 145.84231, Residuals: -1.38812, Convergence: 0.002712\n",
      "Epoch: 80, Loss: 145.46720, Residuals: -1.37569, Convergence: 0.002579\n",
      "Epoch: 81, Loss: 145.11164, Residuals: -1.36377, Convergence: 0.002450\n",
      "Epoch: 82, Loss: 144.77475, Residuals: -1.35237, Convergence: 0.002327\n",
      "Epoch: 83, Loss: 144.45567, Residuals: -1.34145, Convergence: 0.002209\n",
      "Epoch: 84, Loss: 144.15363, Residuals: -1.33100, Convergence: 0.002095\n",
      "Epoch: 85, Loss: 143.86785, Residuals: -1.32101, Convergence: 0.001986\n",
      "Epoch: 86, Loss: 143.59764, Residuals: -1.31146, Convergence: 0.001882\n",
      "Epoch: 87, Loss: 143.34232, Residuals: -1.30233, Convergence: 0.001781\n",
      "Epoch: 88, Loss: 143.10128, Residuals: -1.29362, Convergence: 0.001684\n",
      "Epoch: 89, Loss: 142.87391, Residuals: -1.28531, Convergence: 0.001591\n",
      "Epoch: 90, Loss: 142.65965, Residuals: -1.27739, Convergence: 0.001502\n",
      "Epoch: 91, Loss: 142.45793, Residuals: -1.26985, Convergence: 0.001416\n",
      "Epoch: 92, Loss: 142.26821, Residuals: -1.26268, Convergence: 0.001334\n",
      "Epoch: 93, Loss: 142.08990, Residuals: -1.25587, Convergence: 0.001255\n",
      "Epoch: 94, Loss: 141.92237, Residuals: -1.24941, Convergence: 0.001180\n",
      "Epoch: 95, Loss: 141.76495, Residuals: -1.24330, Convergence: 0.001110\n",
      "Epoch: 96, Loss: 141.61683, Residuals: -1.23752, Convergence: 0.001046\n",
      "Epoch: 97, Loss: 141.47717, Residuals: -1.23207, Convergence: 0.000987\n",
      "Evidence -183.081\n",
      "\n",
      "Epoch: 97, Evidence: -183.08144, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.24e-01\n",
      "Epoch: 97, Loss: 1358.20640, Residuals: -1.23207, Convergence:   inf\n",
      "Epoch: 98, Loss: 1295.57283, Residuals: -1.26159, Convergence: 0.048344\n",
      "Epoch: 99, Loss: 1248.16603, Residuals: -1.28492, Convergence: 0.037981\n",
      "Epoch: 100, Loss: 1212.56265, Residuals: -1.30161, Convergence: 0.029362\n",
      "Epoch: 101, Loss: 1184.94907, Residuals: -1.31315, Convergence: 0.023304\n",
      "Epoch: 102, Loss: 1162.65414, Residuals: -1.32146, Convergence: 0.019176\n",
      "Epoch: 103, Loss: 1144.17433, Residuals: -1.32762, Convergence: 0.016151\n",
      "Epoch: 104, Loss: 1128.60483, Residuals: -1.33213, Convergence: 0.013795\n",
      "Epoch: 105, Loss: 1115.32614, Residuals: -1.33524, Convergence: 0.011906\n",
      "Epoch: 106, Loss: 1103.87655, Residuals: -1.33714, Convergence: 0.010372\n",
      "Epoch: 107, Loss: 1093.89531, Residuals: -1.33797, Convergence: 0.009124\n",
      "Epoch: 108, Loss: 1085.08998, Residuals: -1.33785, Convergence: 0.008115\n",
      "Epoch: 109, Loss: 1077.21751, Residuals: -1.33685, Convergence: 0.007308\n",
      "Epoch: 110, Loss: 1070.07016, Residuals: -1.33505, Convergence: 0.006679\n",
      "Epoch: 111, Loss: 1063.46442, Residuals: -1.33248, Convergence: 0.006212\n",
      "Epoch: 112, Loss: 1057.23500, Residuals: -1.32918, Convergence: 0.005892\n",
      "Epoch: 113, Loss: 1051.23122, Residuals: -1.32515, Convergence: 0.005711\n",
      "Epoch: 114, Loss: 1045.32312, Residuals: -1.32041, Convergence: 0.005652\n",
      "Epoch: 115, Loss: 1039.41351, Residuals: -1.31496, Convergence: 0.005686\n",
      "Epoch: 116, Loss: 1033.46060, Residuals: -1.30888, Convergence: 0.005760\n",
      "Epoch: 117, Loss: 1027.49143, Residuals: -1.30223, Convergence: 0.005809\n",
      "Epoch: 118, Loss: 1021.59272, Residuals: -1.29512, Convergence: 0.005774\n",
      "Epoch: 119, Loss: 1015.87472, Residuals: -1.28763, Convergence: 0.005629\n",
      "Epoch: 120, Loss: 1010.42868, Residuals: -1.27987, Convergence: 0.005390\n",
      "Epoch: 121, Loss: 1005.30591, Residuals: -1.27192, Convergence: 0.005096\n",
      "Epoch: 122, Loss: 1000.52009, Residuals: -1.26387, Convergence: 0.004783\n",
      "Epoch: 123, Loss: 996.06011, Residuals: -1.25579, Convergence: 0.004478\n",
      "Epoch: 124, Loss: 991.90284, Residuals: -1.24774, Convergence: 0.004191\n",
      "Epoch: 125, Loss: 988.02105, Residuals: -1.23977, Convergence: 0.003929\n",
      "Epoch: 126, Loss: 984.38904, Residuals: -1.23192, Convergence: 0.003690\n",
      "Epoch: 127, Loss: 980.98208, Residuals: -1.22423, Convergence: 0.003473\n",
      "Epoch: 128, Loss: 977.77955, Residuals: -1.21671, Convergence: 0.003275\n",
      "Epoch: 129, Loss: 974.76335, Residuals: -1.20939, Convergence: 0.003094\n",
      "Epoch: 130, Loss: 971.91803, Residuals: -1.20228, Convergence: 0.002928\n",
      "Epoch: 131, Loss: 969.23089, Residuals: -1.19541, Convergence: 0.002772\n",
      "Epoch: 132, Loss: 966.69050, Residuals: -1.18877, Convergence: 0.002628\n",
      "Epoch: 133, Loss: 964.28690, Residuals: -1.18239, Convergence: 0.002493\n",
      "Epoch: 134, Loss: 962.01118, Residuals: -1.17626, Convergence: 0.002366\n",
      "Epoch: 135, Loss: 959.85526, Residuals: -1.17038, Convergence: 0.002246\n",
      "Epoch: 136, Loss: 957.81111, Residuals: -1.16476, Convergence: 0.002134\n",
      "Epoch: 137, Loss: 955.87188, Residuals: -1.15939, Convergence: 0.002029\n",
      "Epoch: 138, Loss: 954.02978, Residuals: -1.15427, Convergence: 0.001931\n",
      "Epoch: 139, Loss: 952.27864, Residuals: -1.14940, Convergence: 0.001839\n",
      "Epoch: 140, Loss: 950.61157, Residuals: -1.14475, Convergence: 0.001754\n",
      "Epoch: 141, Loss: 949.02244, Residuals: -1.14034, Convergence: 0.001674\n",
      "Epoch: 142, Loss: 947.50547, Residuals: -1.13615, Convergence: 0.001601\n",
      "Epoch: 143, Loss: 946.05533, Residuals: -1.13216, Convergence: 0.001533\n",
      "Epoch: 144, Loss: 944.66668, Residuals: -1.12837, Convergence: 0.001470\n",
      "Epoch: 145, Loss: 943.33453, Residuals: -1.12476, Convergence: 0.001412\n",
      "Epoch: 146, Loss: 942.05501, Residuals: -1.12134, Convergence: 0.001358\n",
      "Epoch: 147, Loss: 940.82399, Residuals: -1.11808, Convergence: 0.001308\n",
      "Epoch: 148, Loss: 939.63766, Residuals: -1.11497, Convergence: 0.001263\n",
      "Epoch: 149, Loss: 938.49288, Residuals: -1.11202, Convergence: 0.001220\n",
      "Epoch: 150, Loss: 937.38646, Residuals: -1.10920, Convergence: 0.001180\n",
      "Epoch: 151, Loss: 936.31610, Residuals: -1.10650, Convergence: 0.001143\n",
      "Epoch: 152, Loss: 935.27892, Residuals: -1.10393, Convergence: 0.001109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 153, Loss: 934.27295, Residuals: -1.10147, Convergence: 0.001077\n",
      "Epoch: 154, Loss: 933.29616, Residuals: -1.09912, Convergence: 0.001047\n",
      "Epoch: 155, Loss: 932.34666, Residuals: -1.09686, Convergence: 0.001018\n",
      "Epoch: 156, Loss: 931.42266, Residuals: -1.09469, Convergence: 0.000992\n",
      "Evidence 11048.186\n",
      "\n",
      "Epoch: 156, Evidence: 11048.18555, Convergence: 1.016571\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.78e-01\n",
      "Epoch: 156, Loss: 2341.09919, Residuals: -1.09469, Convergence:   inf\n",
      "Epoch: 157, Loss: 2300.17605, Residuals: -1.10294, Convergence: 0.017791\n",
      "Epoch: 158, Loss: 2271.69591, Residuals: -1.10187, Convergence: 0.012537\n",
      "Epoch: 159, Loss: 2247.72956, Residuals: -1.09981, Convergence: 0.010662\n",
      "Epoch: 160, Loss: 2227.30161, Residuals: -1.09737, Convergence: 0.009172\n",
      "Epoch: 161, Loss: 2209.76068, Residuals: -1.09468, Convergence: 0.007938\n",
      "Epoch: 162, Loss: 2194.59879, Residuals: -1.09181, Convergence: 0.006909\n",
      "Epoch: 163, Loss: 2181.40017, Residuals: -1.08878, Convergence: 0.006051\n",
      "Epoch: 164, Loss: 2169.82249, Residuals: -1.08563, Convergence: 0.005336\n",
      "Epoch: 165, Loss: 2159.57766, Residuals: -1.08236, Convergence: 0.004744\n",
      "Epoch: 166, Loss: 2150.42486, Residuals: -1.07900, Convergence: 0.004256\n",
      "Epoch: 167, Loss: 2142.16171, Residuals: -1.07554, Convergence: 0.003857\n",
      "Epoch: 168, Loss: 2134.61985, Residuals: -1.07200, Convergence: 0.003533\n",
      "Epoch: 169, Loss: 2127.66493, Residuals: -1.06836, Convergence: 0.003269\n",
      "Epoch: 170, Loss: 2121.19869, Residuals: -1.06464, Convergence: 0.003048\n",
      "Epoch: 171, Loss: 2115.15705, Residuals: -1.06086, Convergence: 0.002856\n",
      "Epoch: 172, Loss: 2109.50496, Residuals: -1.05705, Convergence: 0.002679\n",
      "Epoch: 173, Loss: 2104.22352, Residuals: -1.05324, Convergence: 0.002510\n",
      "Epoch: 174, Loss: 2099.30063, Residuals: -1.04948, Convergence: 0.002345\n",
      "Epoch: 175, Loss: 2094.71782, Residuals: -1.04581, Convergence: 0.002188\n",
      "Epoch: 176, Loss: 2090.45523, Residuals: -1.04225, Convergence: 0.002039\n",
      "Epoch: 177, Loss: 2086.48856, Residuals: -1.03882, Convergence: 0.001901\n",
      "Epoch: 178, Loss: 2082.79300, Residuals: -1.03553, Convergence: 0.001774\n",
      "Epoch: 179, Loss: 2079.34391, Residuals: -1.03239, Convergence: 0.001659\n",
      "Epoch: 180, Loss: 2076.11916, Residuals: -1.02942, Convergence: 0.001553\n",
      "Epoch: 181, Loss: 2073.09935, Residuals: -1.02659, Convergence: 0.001457\n",
      "Epoch: 182, Loss: 2070.26707, Residuals: -1.02393, Convergence: 0.001368\n",
      "Epoch: 183, Loss: 2067.60799, Residuals: -1.02142, Convergence: 0.001286\n",
      "Epoch: 184, Loss: 2065.10953, Residuals: -1.01905, Convergence: 0.001210\n",
      "Epoch: 185, Loss: 2062.76108, Residuals: -1.01682, Convergence: 0.001138\n",
      "Epoch: 186, Loss: 2060.55206, Residuals: -1.01472, Convergence: 0.001072\n",
      "Epoch: 187, Loss: 2058.47364, Residuals: -1.01275, Convergence: 0.001010\n",
      "Epoch: 188, Loss: 2056.51732, Residuals: -1.01089, Convergence: 0.000951\n",
      "Evidence 14204.711\n",
      "\n",
      "Epoch: 188, Evidence: 14204.71094, Convergence: 0.222217\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.42e-01\n",
      "Epoch: 188, Loss: 2468.56141, Residuals: -1.01089, Convergence:   inf\n",
      "Epoch: 189, Loss: 2453.78050, Residuals: -1.00783, Convergence: 0.006024\n",
      "Epoch: 190, Loss: 2441.71226, Residuals: -1.00417, Convergence: 0.004943\n",
      "Epoch: 191, Loss: 2431.31875, Residuals: -1.00060, Convergence: 0.004275\n",
      "Epoch: 192, Loss: 2422.30713, Residuals: -0.99721, Convergence: 0.003720\n",
      "Epoch: 193, Loss: 2414.45586, Residuals: -0.99406, Convergence: 0.003252\n",
      "Epoch: 194, Loss: 2407.58397, Residuals: -0.99114, Convergence: 0.002854\n",
      "Epoch: 195, Loss: 2401.54033, Residuals: -0.98846, Convergence: 0.002517\n",
      "Epoch: 196, Loss: 2396.19946, Residuals: -0.98598, Convergence: 0.002229\n",
      "Epoch: 197, Loss: 2391.45351, Residuals: -0.98371, Convergence: 0.001985\n",
      "Epoch: 198, Loss: 2387.21243, Residuals: -0.98161, Convergence: 0.001777\n",
      "Epoch: 199, Loss: 2383.40115, Residuals: -0.97968, Convergence: 0.001599\n",
      "Epoch: 200, Loss: 2379.95652, Residuals: -0.97789, Convergence: 0.001447\n",
      "Epoch: 201, Loss: 2376.82587, Residuals: -0.97623, Convergence: 0.001317\n",
      "Epoch: 202, Loss: 2373.96540, Residuals: -0.97468, Convergence: 0.001205\n",
      "Epoch: 203, Loss: 2371.33865, Residuals: -0.97325, Convergence: 0.001108\n",
      "Epoch: 204, Loss: 2368.91527, Residuals: -0.97190, Convergence: 0.001023\n",
      "Epoch: 205, Loss: 2366.66865, Residuals: -0.97065, Convergence: 0.000949\n",
      "Evidence 14642.800\n",
      "\n",
      "Epoch: 205, Evidence: 14642.79980, Convergence: 0.029918\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.38e-01\n",
      "Epoch: 205, Loss: 2474.14885, Residuals: -0.97065, Convergence:   inf\n",
      "Epoch: 206, Loss: 2467.19203, Residuals: -0.96730, Convergence: 0.002820\n",
      "Epoch: 207, Loss: 2461.45388, Residuals: -0.96429, Convergence: 0.002331\n",
      "Epoch: 208, Loss: 2456.59364, Residuals: -0.96169, Convergence: 0.001978\n",
      "Epoch: 209, Loss: 2452.41218, Residuals: -0.95944, Convergence: 0.001705\n",
      "Epoch: 210, Loss: 2448.76597, Residuals: -0.95748, Convergence: 0.001489\n",
      "Epoch: 211, Loss: 2445.54580, Residuals: -0.95578, Convergence: 0.001317\n",
      "Epoch: 212, Loss: 2442.66997, Residuals: -0.95429, Convergence: 0.001177\n",
      "Epoch: 213, Loss: 2440.07437, Residuals: -0.95299, Convergence: 0.001064\n",
      "Epoch: 214, Loss: 2437.71096, Residuals: -0.95184, Convergence: 0.000970\n",
      "Evidence 14734.893\n",
      "\n",
      "Epoch: 214, Evidence: 14734.89258, Convergence: 0.006250\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.65e-01\n",
      "Epoch: 214, Loss: 2475.70094, Residuals: -0.95184, Convergence:   inf\n",
      "Epoch: 215, Loss: 2471.65588, Residuals: -0.94931, Convergence: 0.001637\n",
      "Epoch: 216, Loss: 2468.29388, Residuals: -0.94726, Convergence: 0.001362\n",
      "Epoch: 217, Loss: 2465.40640, Residuals: -0.94558, Convergence: 0.001171\n",
      "Epoch: 218, Loss: 2462.87564, Residuals: -0.94418, Convergence: 0.001028\n",
      "Epoch: 219, Loss: 2460.62104, Residuals: -0.94303, Convergence: 0.000916\n",
      "Evidence 14767.059\n",
      "\n",
      "Epoch: 219, Evidence: 14767.05859, Convergence: 0.002178\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.13e-01\n",
      "Epoch: 219, Loss: 2476.60486, Residuals: -0.94303, Convergence:   inf\n",
      "Epoch: 220, Loss: 2473.74541, Residuals: -0.94117, Convergence: 0.001156\n",
      "Epoch: 221, Loss: 2471.33773, Residuals: -0.93971, Convergence: 0.000974\n",
      "Evidence 14779.801\n",
      "\n",
      "Epoch: 221, Evidence: 14779.80078, Convergence: 0.000862\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.76e-01\n",
      "Epoch: 221, Loss: 2477.29887, Residuals: -0.93971, Convergence:   inf\n",
      "Epoch: 222, Loss: 2472.78673, Residuals: -0.93736, Convergence: 0.001825\n",
      "Epoch: 223, Loss: 2469.31005, Residuals: -0.93556, Convergence: 0.001408\n",
      "Epoch: 224, Loss: 2466.46351, Residuals: -0.93437, Convergence: 0.001154\n",
      "Epoch: 225, Loss: 2464.03143, Residuals: -0.93370, Convergence: 0.000987\n",
      "Evidence 14797.602\n",
      "\n",
      "Epoch: 225, Evidence: 14797.60156, Convergence: 0.002064\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.46e-01\n",
      "Epoch: 225, Loss: 2477.50373, Residuals: -0.93370, Convergence:   inf\n",
      "Epoch: 226, Loss: 2474.52009, Residuals: -0.93157, Convergence: 0.001206\n",
      "Epoch: 227, Loss: 2472.13471, Residuals: -0.93054, Convergence: 0.000965\n",
      "Evidence 14808.318\n",
      "\n",
      "Epoch: 227, Evidence: 14808.31836, Convergence: 0.000724\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.23e-01\n",
      "Epoch: 227, Loss: 2477.73996, Residuals: -0.93054, Convergence:   inf\n",
      "Epoch: 228, Loss: 2473.39202, Residuals: -0.92757, Convergence: 0.001758\n",
      "Epoch: 229, Loss: 2470.22310, Residuals: -0.92907, Convergence: 0.001283\n",
      "Epoch: 230, Loss: 2467.59952, Residuals: -0.92986, Convergence: 0.001063\n",
      "Epoch: 231, Loss: 2465.32723, Residuals: -0.93259, Convergence: 0.000922\n",
      "Evidence 14823.802\n",
      "\n",
      "Epoch: 231, Evidence: 14823.80176, Convergence: 0.001767\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.09e-01\n",
      "Epoch: 231, Loss: 2477.12026, Residuals: -0.93259, Convergence:   inf\n",
      "Epoch: 232, Loss: 2475.74587, Residuals: -0.93080, Convergence: 0.000555\n",
      "Evidence 14829.854\n",
      "\n",
      "Epoch: 232, Evidence: 14829.85352, Convergence: 0.000408\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.91e-02\n",
      "Epoch: 232, Loss: 2478.15270, Residuals: -0.93080, Convergence:   inf\n",
      "Epoch: 233, Loss: 2525.90125, Residuals: -0.97124, Convergence: -0.018904\n",
      "Epoch: 233, Loss: 2475.90680, Residuals: -0.92810, Convergence: 0.000907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence 14834.146\n",
      "\n",
      "Epoch: 233, Evidence: 14834.14648, Convergence: 0.000697\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.17e-02\n",
      "Epoch: 233, Loss: 2477.60128, Residuals: -0.92810, Convergence:   inf\n",
      "Epoch: 234, Loss: 2482.83338, Residuals: -0.92832, Convergence: -0.002107\n",
      "Epoch: 234, Loss: 2477.73405, Residuals: -0.92633, Convergence: -0.000054\n",
      "Evidence 14835.512\n",
      "\n",
      "Epoch: 234, Evidence: 14835.51172, Convergence: 0.000789\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 382.20863, Residuals: -4.51243, Convergence:   inf\n",
      "Epoch: 1, Loss: 356.50062, Residuals: -4.39235, Convergence: 0.072112\n",
      "Epoch: 2, Loss: 335.47015, Residuals: -4.22864, Convergence: 0.062690\n",
      "Epoch: 3, Loss: 319.38087, Residuals: -4.06413, Convergence: 0.050376\n",
      "Epoch: 4, Loss: 307.10223, Residuals: -3.91886, Convergence: 0.039982\n",
      "Epoch: 5, Loss: 297.36030, Residuals: -3.79019, Convergence: 0.032761\n",
      "Epoch: 6, Loss: 289.45087, Residuals: -3.67804, Convergence: 0.027326\n",
      "Epoch: 7, Loss: 282.88384, Residuals: -3.58198, Convergence: 0.023215\n",
      "Epoch: 8, Loss: 277.29555, Residuals: -3.49998, Convergence: 0.020153\n",
      "Epoch: 9, Loss: 272.42967, Residuals: -3.42968, Convergence: 0.017861\n",
      "Epoch: 10, Loss: 268.10428, Residuals: -3.36897, Convergence: 0.016133\n",
      "Epoch: 11, Loss: 264.18813, Residuals: -3.31605, Convergence: 0.014823\n",
      "Epoch: 12, Loss: 260.58607, Residuals: -3.26941, Convergence: 0.013823\n",
      "Epoch: 13, Loss: 257.22973, Residuals: -3.22773, Convergence: 0.013048\n",
      "Epoch: 14, Loss: 254.07082, Residuals: -3.18988, Convergence: 0.012433\n",
      "Epoch: 15, Loss: 251.07727, Residuals: -3.15489, Convergence: 0.011923\n",
      "Epoch: 16, Loss: 248.23112, Residuals: -3.12207, Convergence: 0.011466\n",
      "Epoch: 17, Loss: 245.52112, Residuals: -3.09102, Convergence: 0.011038\n",
      "Epoch: 18, Loss: 242.92956, Residuals: -3.06138, Convergence: 0.010668\n",
      "Epoch: 19, Loss: 240.42706, Residuals: -3.03270, Convergence: 0.010409\n",
      "Epoch: 20, Loss: 237.97852, Residuals: -3.00443, Convergence: 0.010289\n",
      "Epoch: 21, Loss: 235.55200, Residuals: -2.97608, Convergence: 0.010301\n",
      "Epoch: 22, Loss: 233.12452, Residuals: -2.94730, Convergence: 0.010413\n",
      "Epoch: 23, Loss: 230.67704, Residuals: -2.91787, Convergence: 0.010610\n",
      "Epoch: 24, Loss: 228.17879, Residuals: -2.88746, Convergence: 0.010949\n",
      "Epoch: 25, Loss: 225.57883, Residuals: -2.85548, Convergence: 0.011526\n",
      "Epoch: 26, Loss: 222.83195, Residuals: -2.82136, Convergence: 0.012327\n",
      "Epoch: 27, Loss: 219.97961, Residuals: -2.78543, Convergence: 0.012966\n",
      "Epoch: 28, Loss: 217.14875, Residuals: -2.74905, Convergence: 0.013037\n",
      "Epoch: 29, Loss: 214.40698, Residuals: -2.71308, Convergence: 0.012788\n",
      "Epoch: 30, Loss: 211.75423, Residuals: -2.67762, Convergence: 0.012527\n",
      "Epoch: 31, Loss: 209.17501, Residuals: -2.64255, Convergence: 0.012330\n",
      "Epoch: 32, Loss: 206.65637, Residuals: -2.60775, Convergence: 0.012188\n",
      "Epoch: 33, Loss: 204.19036, Residuals: -2.57314, Convergence: 0.012077\n",
      "Epoch: 34, Loss: 201.77316, Residuals: -2.53866, Convergence: 0.011980\n",
      "Epoch: 35, Loss: 199.40381, Residuals: -2.50428, Convergence: 0.011882\n",
      "Epoch: 36, Loss: 197.08308, Residuals: -2.47000, Convergence: 0.011775\n",
      "Epoch: 37, Loss: 194.81260, Residuals: -2.43581, Convergence: 0.011655\n",
      "Epoch: 38, Loss: 192.59431, Residuals: -2.40172, Convergence: 0.011518\n",
      "Epoch: 39, Loss: 190.43009, Residuals: -2.36775, Convergence: 0.011365\n",
      "Epoch: 40, Loss: 188.32153, Residuals: -2.33392, Convergence: 0.011197\n",
      "Epoch: 41, Loss: 186.26997, Residuals: -2.30025, Convergence: 0.011014\n",
      "Epoch: 42, Loss: 184.27647, Residuals: -2.26675, Convergence: 0.010818\n",
      "Epoch: 43, Loss: 182.34205, Residuals: -2.23347, Convergence: 0.010609\n",
      "Epoch: 44, Loss: 180.46769, Residuals: -2.20042, Convergence: 0.010386\n",
      "Epoch: 45, Loss: 178.65456, Residuals: -2.16766, Convergence: 0.010149\n",
      "Epoch: 46, Loss: 176.90392, Residuals: -2.13522, Convergence: 0.009896\n",
      "Epoch: 47, Loss: 175.21715, Residuals: -2.10317, Convergence: 0.009627\n",
      "Epoch: 48, Loss: 173.59547, Residuals: -2.07155, Convergence: 0.009342\n",
      "Epoch: 49, Loss: 172.03971, Residuals: -2.04044, Convergence: 0.009043\n",
      "Epoch: 50, Loss: 170.55023, Residuals: -2.00987, Convergence: 0.008733\n",
      "Epoch: 51, Loss: 169.12660, Residuals: -1.97991, Convergence: 0.008417\n",
      "Epoch: 52, Loss: 167.76773, Residuals: -1.95061, Convergence: 0.008100\n",
      "Epoch: 53, Loss: 166.47173, Residuals: -1.92198, Convergence: 0.007785\n",
      "Epoch: 54, Loss: 165.23606, Residuals: -1.89406, Convergence: 0.007478\n",
      "Epoch: 55, Loss: 164.05763, Residuals: -1.86686, Convergence: 0.007183\n",
      "Epoch: 56, Loss: 162.93295, Residuals: -1.84036, Convergence: 0.006903\n",
      "Epoch: 57, Loss: 161.85835, Residuals: -1.81457, Convergence: 0.006639\n",
      "Epoch: 58, Loss: 160.83018, Residuals: -1.78945, Convergence: 0.006393\n",
      "Epoch: 59, Loss: 159.84503, Residuals: -1.76499, Convergence: 0.006163\n",
      "Epoch: 60, Loss: 158.89989, Residuals: -1.74115, Convergence: 0.005948\n",
      "Epoch: 61, Loss: 157.99220, Residuals: -1.71792, Convergence: 0.005745\n",
      "Epoch: 62, Loss: 157.11987, Residuals: -1.69528, Convergence: 0.005552\n",
      "Epoch: 63, Loss: 156.28128, Residuals: -1.67322, Convergence: 0.005366\n",
      "Epoch: 64, Loss: 155.47513, Residuals: -1.65173, Convergence: 0.005185\n",
      "Epoch: 65, Loss: 154.70043, Residuals: -1.63081, Convergence: 0.005008\n",
      "Epoch: 66, Loss: 153.95636, Residuals: -1.61046, Convergence: 0.004833\n",
      "Epoch: 67, Loss: 153.24225, Residuals: -1.59069, Convergence: 0.004660\n",
      "Epoch: 68, Loss: 152.55747, Residuals: -1.57149, Convergence: 0.004489\n",
      "Epoch: 69, Loss: 151.90143, Residuals: -1.55286, Convergence: 0.004319\n",
      "Epoch: 70, Loss: 151.27356, Residuals: -1.53482, Convergence: 0.004151\n",
      "Epoch: 71, Loss: 150.67323, Residuals: -1.51735, Convergence: 0.003984\n",
      "Epoch: 72, Loss: 150.09980, Residuals: -1.50046, Convergence: 0.003820\n",
      "Epoch: 73, Loss: 149.55256, Residuals: -1.48414, Convergence: 0.003659\n",
      "Epoch: 74, Loss: 149.03078, Residuals: -1.46839, Convergence: 0.003501\n",
      "Epoch: 75, Loss: 148.53365, Residuals: -1.45320, Convergence: 0.003347\n",
      "Epoch: 76, Loss: 148.06035, Residuals: -1.43856, Convergence: 0.003197\n",
      "Epoch: 77, Loss: 147.61001, Residuals: -1.42447, Convergence: 0.003051\n",
      "Epoch: 78, Loss: 147.18173, Residuals: -1.41092, Convergence: 0.002910\n",
      "Epoch: 79, Loss: 146.77461, Residuals: -1.39788, Convergence: 0.002774\n",
      "Epoch: 80, Loss: 146.38770, Residuals: -1.38535, Convergence: 0.002643\n",
      "Epoch: 81, Loss: 146.02011, Residuals: -1.37332, Convergence: 0.002517\n",
      "Epoch: 82, Loss: 145.67093, Residuals: -1.36177, Convergence: 0.002397\n",
      "Epoch: 83, Loss: 145.33927, Residuals: -1.35069, Convergence: 0.002282\n",
      "Epoch: 84, Loss: 145.02427, Residuals: -1.34006, Convergence: 0.002172\n",
      "Epoch: 85, Loss: 144.72514, Residuals: -1.32986, Convergence: 0.002067\n",
      "Epoch: 86, Loss: 144.44110, Residuals: -1.32009, Convergence: 0.001966\n",
      "Epoch: 87, Loss: 144.17141, Residuals: -1.31073, Convergence: 0.001871\n",
      "Epoch: 88, Loss: 143.91540, Residuals: -1.30176, Convergence: 0.001779\n",
      "Epoch: 89, Loss: 143.67246, Residuals: -1.29317, Convergence: 0.001691\n",
      "Epoch: 90, Loss: 143.44201, Residuals: -1.28494, Convergence: 0.001607\n",
      "Epoch: 91, Loss: 143.22351, Residuals: -1.27707, Convergence: 0.001526\n",
      "Epoch: 92, Loss: 143.01648, Residuals: -1.26954, Convergence: 0.001448\n",
      "Epoch: 93, Loss: 142.82049, Residuals: -1.26235, Convergence: 0.001372\n",
      "Epoch: 94, Loss: 142.63513, Residuals: -1.25548, Convergence: 0.001300\n",
      "Epoch: 95, Loss: 142.45999, Residuals: -1.24892, Convergence: 0.001229\n",
      "Epoch: 96, Loss: 142.29470, Residuals: -1.24267, Convergence: 0.001162\n",
      "Epoch: 97, Loss: 142.13888, Residuals: -1.23673, Convergence: 0.001096\n",
      "Epoch: 98, Loss: 141.99211, Residuals: -1.23108, Convergence: 0.001034\n",
      "Epoch: 99, Loss: 141.85394, Residuals: -1.22572, Convergence: 0.000974\n",
      "Evidence -183.199\n",
      "\n",
      "Epoch: 99, Evidence: -183.19901, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.24e-01\n",
      "Epoch: 99, Loss: 1364.78682, Residuals: -1.22572, Convergence:   inf\n",
      "Epoch: 100, Loss: 1302.79034, Residuals: -1.25513, Convergence: 0.047587\n",
      "Epoch: 101, Loss: 1255.78091, Residuals: -1.27851, Convergence: 0.037434\n",
      "Epoch: 102, Loss: 1220.40244, Residuals: -1.29544, Convergence: 0.028989\n",
      "Epoch: 103, Loss: 1192.93715, Residuals: -1.30737, Convergence: 0.023023\n",
      "Epoch: 104, Loss: 1170.76245, Residuals: -1.31616, Convergence: 0.018940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 105, Loss: 1152.38604, Residuals: -1.32286, Convergence: 0.015946\n",
      "Epoch: 106, Loss: 1136.90299, Residuals: -1.32792, Convergence: 0.013619\n",
      "Epoch: 107, Loss: 1123.69374, Residuals: -1.33158, Convergence: 0.011755\n",
      "Epoch: 108, Loss: 1112.29738, Residuals: -1.33402, Convergence: 0.010246\n",
      "Epoch: 109, Loss: 1102.35387, Residuals: -1.33535, Convergence: 0.009020\n",
      "Epoch: 110, Loss: 1093.57224, Residuals: -1.33567, Convergence: 0.008030\n",
      "Epoch: 111, Loss: 1085.71103, Residuals: -1.33508, Convergence: 0.007241\n",
      "Epoch: 112, Loss: 1078.56601, Residuals: -1.33363, Convergence: 0.006625\n",
      "Epoch: 113, Loss: 1071.95837, Residuals: -1.33136, Convergence: 0.006164\n",
      "Epoch: 114, Loss: 1065.73006, Residuals: -1.32831, Convergence: 0.005844\n",
      "Epoch: 115, Loss: 1059.74001, Residuals: -1.32449, Convergence: 0.005652\n",
      "Epoch: 116, Loss: 1053.86571, Residuals: -1.31990, Convergence: 0.005574\n",
      "Epoch: 117, Loss: 1048.01174, Residuals: -1.31458, Convergence: 0.005586\n",
      "Epoch: 118, Loss: 1042.12510, Residuals: -1.30856, Convergence: 0.005649\n",
      "Epoch: 119, Loss: 1036.20995, Residuals: -1.30195, Convergence: 0.005708\n",
      "Epoch: 120, Loss: 1030.32888, Residuals: -1.29483, Convergence: 0.005708\n",
      "Epoch: 121, Loss: 1024.58340, Residuals: -1.28732, Convergence: 0.005608\n",
      "Epoch: 122, Loss: 1019.07402, Residuals: -1.27953, Convergence: 0.005406\n",
      "Epoch: 123, Loss: 1013.86946, Residuals: -1.27155, Convergence: 0.005133\n",
      "Epoch: 124, Loss: 1008.99900, Residuals: -1.26345, Convergence: 0.004827\n",
      "Epoch: 125, Loss: 1004.45952, Residuals: -1.25532, Convergence: 0.004519\n",
      "Epoch: 126, Loss: 1000.22953, Residuals: -1.24720, Convergence: 0.004229\n",
      "Epoch: 127, Loss: 996.28055, Residuals: -1.23915, Convergence: 0.003964\n",
      "Epoch: 128, Loss: 992.58275, Residuals: -1.23121, Convergence: 0.003725\n",
      "Epoch: 129, Loss: 989.10853, Residuals: -1.22341, Convergence: 0.003512\n",
      "Epoch: 130, Loss: 985.83356, Residuals: -1.21578, Convergence: 0.003322\n",
      "Epoch: 131, Loss: 982.73703, Residuals: -1.20834, Convergence: 0.003151\n",
      "Epoch: 132, Loss: 979.80257, Residuals: -1.20111, Convergence: 0.002995\n",
      "Epoch: 133, Loss: 977.01571, Residuals: -1.19410, Convergence: 0.002852\n",
      "Epoch: 134, Loss: 974.36495, Residuals: -1.18732, Convergence: 0.002720\n",
      "Epoch: 135, Loss: 971.84093, Residuals: -1.18079, Convergence: 0.002597\n",
      "Epoch: 136, Loss: 969.43546, Residuals: -1.17450, Convergence: 0.002481\n",
      "Epoch: 137, Loss: 967.14173, Residuals: -1.16846, Convergence: 0.002372\n",
      "Epoch: 138, Loss: 964.95310, Residuals: -1.16267, Convergence: 0.002268\n",
      "Epoch: 139, Loss: 962.86431, Residuals: -1.15713, Convergence: 0.002169\n",
      "Epoch: 140, Loss: 960.87002, Residuals: -1.15183, Convergence: 0.002076\n",
      "Epoch: 141, Loss: 958.96478, Residuals: -1.14676, Convergence: 0.001987\n",
      "Epoch: 142, Loss: 957.14405, Residuals: -1.14193, Convergence: 0.001902\n",
      "Epoch: 143, Loss: 955.40299, Residuals: -1.13731, Convergence: 0.001822\n",
      "Epoch: 144, Loss: 953.73712, Residuals: -1.13292, Convergence: 0.001747\n",
      "Epoch: 145, Loss: 952.14222, Residuals: -1.12872, Convergence: 0.001675\n",
      "Epoch: 146, Loss: 950.61382, Residuals: -1.12472, Convergence: 0.001608\n",
      "Epoch: 147, Loss: 949.14798, Residuals: -1.12091, Convergence: 0.001544\n",
      "Epoch: 148, Loss: 947.74098, Residuals: -1.11727, Convergence: 0.001485\n",
      "Epoch: 149, Loss: 946.38892, Residuals: -1.11380, Convergence: 0.001429\n",
      "Epoch: 150, Loss: 945.08840, Residuals: -1.11049, Convergence: 0.001376\n",
      "Epoch: 151, Loss: 943.83523, Residuals: -1.10732, Convergence: 0.001328\n",
      "Epoch: 152, Loss: 942.62658, Residuals: -1.10429, Convergence: 0.001282\n",
      "Epoch: 153, Loss: 941.45822, Residuals: -1.10139, Convergence: 0.001241\n",
      "Epoch: 154, Loss: 940.32668, Residuals: -1.09861, Convergence: 0.001203\n",
      "Epoch: 155, Loss: 939.22785, Residuals: -1.09593, Convergence: 0.001170\n",
      "Epoch: 156, Loss: 938.15829, Residuals: -1.09336, Convergence: 0.001140\n",
      "Epoch: 157, Loss: 937.11391, Residuals: -1.09087, Convergence: 0.001114\n",
      "Epoch: 158, Loss: 936.09068, Residuals: -1.08846, Convergence: 0.001093\n",
      "Epoch: 159, Loss: 935.08442, Residuals: -1.08612, Convergence: 0.001076\n",
      "Epoch: 160, Loss: 934.09071, Residuals: -1.08383, Convergence: 0.001064\n",
      "Epoch: 161, Loss: 933.10649, Residuals: -1.08160, Convergence: 0.001055\n",
      "Epoch: 162, Loss: 932.12777, Residuals: -1.07940, Convergence: 0.001050\n",
      "Epoch: 163, Loss: 931.15221, Residuals: -1.07723, Convergence: 0.001048\n",
      "Epoch: 164, Loss: 930.17818, Residuals: -1.07508, Convergence: 0.001047\n",
      "Epoch: 165, Loss: 929.20548, Residuals: -1.07295, Convergence: 0.001047\n",
      "Epoch: 166, Loss: 928.23549, Residuals: -1.07083, Convergence: 0.001045\n",
      "Epoch: 167, Loss: 927.27140, Residuals: -1.06874, Convergence: 0.001040\n",
      "Epoch: 168, Loss: 926.31717, Residuals: -1.06667, Convergence: 0.001030\n",
      "Epoch: 169, Loss: 925.37794, Residuals: -1.06463, Convergence: 0.001015\n",
      "Epoch: 170, Loss: 924.45939, Residuals: -1.06264, Convergence: 0.000994\n",
      "Evidence 11161.065\n",
      "\n",
      "Epoch: 170, Evidence: 11161.06543, Convergence: 1.016414\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.76e-01\n",
      "Epoch: 170, Loss: 2342.47522, Residuals: -1.06264, Convergence:   inf\n",
      "Epoch: 171, Loss: 2305.66689, Residuals: -1.07091, Convergence: 0.015964\n",
      "Epoch: 172, Loss: 2279.71054, Residuals: -1.06893, Convergence: 0.011386\n",
      "Epoch: 173, Loss: 2258.02540, Residuals: -1.06627, Convergence: 0.009604\n",
      "Epoch: 174, Loss: 2239.76104, Residuals: -1.06346, Convergence: 0.008155\n",
      "Epoch: 175, Loss: 2224.26065, Residuals: -1.06059, Convergence: 0.006969\n",
      "Epoch: 176, Loss: 2211.00475, Residuals: -1.05770, Convergence: 0.005995\n",
      "Epoch: 177, Loss: 2199.56758, Residuals: -1.05480, Convergence: 0.005200\n",
      "Epoch: 178, Loss: 2189.59269, Residuals: -1.05189, Convergence: 0.004556\n",
      "Epoch: 179, Loss: 2180.78398, Residuals: -1.04895, Convergence: 0.004039\n",
      "Epoch: 180, Loss: 2172.89503, Residuals: -1.04597, Convergence: 0.003631\n",
      "Epoch: 181, Loss: 2165.73295, Residuals: -1.04291, Convergence: 0.003307\n",
      "Epoch: 182, Loss: 2159.15520, Residuals: -1.03976, Convergence: 0.003046\n",
      "Epoch: 183, Loss: 2153.06744, Residuals: -1.03653, Convergence: 0.002827\n",
      "Epoch: 184, Loss: 2147.41332, Residuals: -1.03324, Convergence: 0.002633\n",
      "Epoch: 185, Loss: 2142.15648, Residuals: -1.02995, Convergence: 0.002454\n",
      "Epoch: 186, Loss: 2137.26885, Residuals: -1.02667, Convergence: 0.002287\n",
      "Epoch: 187, Loss: 2132.72278, Residuals: -1.02346, Convergence: 0.002132\n",
      "Epoch: 188, Loss: 2128.49156, Residuals: -1.02033, Convergence: 0.001988\n",
      "Epoch: 189, Loss: 2124.54794, Residuals: -1.01730, Convergence: 0.001856\n",
      "Epoch: 190, Loss: 2120.86735, Residuals: -1.01440, Convergence: 0.001735\n",
      "Epoch: 191, Loss: 2117.42690, Residuals: -1.01162, Convergence: 0.001625\n",
      "Epoch: 192, Loss: 2114.20730, Residuals: -1.00897, Convergence: 0.001523\n",
      "Epoch: 193, Loss: 2111.19152, Residuals: -1.00646, Convergence: 0.001428\n",
      "Epoch: 194, Loss: 2108.36456, Residuals: -1.00408, Convergence: 0.001341\n",
      "Epoch: 195, Loss: 2105.71417, Residuals: -1.00183, Convergence: 0.001259\n",
      "Epoch: 196, Loss: 2103.22757, Residuals: -0.99970, Convergence: 0.001182\n",
      "Epoch: 197, Loss: 2100.89382, Residuals: -0.99769, Convergence: 0.001111\n",
      "Epoch: 198, Loss: 2098.70362, Residuals: -0.99579, Convergence: 0.001044\n",
      "Epoch: 199, Loss: 2096.64692, Residuals: -0.99400, Convergence: 0.000981\n",
      "Evidence 14292.002\n",
      "\n",
      "Epoch: 199, Evidence: 14292.00195, Convergence: 0.219069\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.38e-01\n",
      "Epoch: 199, Loss: 2468.36448, Residuals: -0.99400, Convergence:   inf\n",
      "Epoch: 200, Loss: 2455.16557, Residuals: -0.99084, Convergence: 0.005376\n",
      "Epoch: 201, Loss: 2444.37889, Residuals: -0.98716, Convergence: 0.004413\n",
      "Epoch: 202, Loss: 2435.09173, Residuals: -0.98361, Convergence: 0.003814\n",
      "Epoch: 203, Loss: 2427.04204, Residuals: -0.98029, Convergence: 0.003317\n",
      "Epoch: 204, Loss: 2420.02817, Residuals: -0.97723, Convergence: 0.002898\n",
      "Epoch: 205, Loss: 2413.88403, Residuals: -0.97442, Convergence: 0.002545\n",
      "Epoch: 206, Loss: 2408.47579, Residuals: -0.97185, Convergence: 0.002246\n",
      "Epoch: 207, Loss: 2403.68905, Residuals: -0.96950, Convergence: 0.001991\n",
      "Epoch: 208, Loss: 2399.43000, Residuals: -0.96736, Convergence: 0.001775\n",
      "Epoch: 209, Loss: 2395.61861, Residuals: -0.96539, Convergence: 0.001591\n",
      "Epoch: 210, Loss: 2392.19047, Residuals: -0.96359, Convergence: 0.001433\n",
      "Epoch: 211, Loss: 2389.08964, Residuals: -0.96193, Convergence: 0.001298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 212, Loss: 2386.27041, Residuals: -0.96040, Convergence: 0.001181\n",
      "Epoch: 213, Loss: 2383.69516, Residuals: -0.95899, Convergence: 0.001080\n",
      "Epoch: 214, Loss: 2381.33156, Residuals: -0.95769, Convergence: 0.000993\n",
      "Evidence 14653.510\n",
      "\n",
      "Epoch: 214, Evidence: 14653.50977, Convergence: 0.024670\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.36e-01\n",
      "Epoch: 214, Loss: 2474.04513, Residuals: -0.95769, Convergence:   inf\n",
      "Epoch: 215, Loss: 2467.54156, Residuals: -0.95429, Convergence: 0.002636\n",
      "Epoch: 216, Loss: 2462.15962, Residuals: -0.95128, Convergence: 0.002186\n",
      "Epoch: 217, Loss: 2457.59134, Residuals: -0.94871, Convergence: 0.001859\n",
      "Epoch: 218, Loss: 2453.65733, Residuals: -0.94650, Convergence: 0.001603\n",
      "Epoch: 219, Loss: 2450.22590, Residuals: -0.94462, Convergence: 0.001400\n",
      "Epoch: 220, Loss: 2447.19806, Residuals: -0.94299, Convergence: 0.001237\n",
      "Epoch: 221, Loss: 2444.49761, Residuals: -0.94159, Convergence: 0.001105\n",
      "Epoch: 222, Loss: 2442.06465, Residuals: -0.94039, Convergence: 0.000996\n",
      "Evidence 14732.525\n",
      "\n",
      "Epoch: 222, Evidence: 14732.52539, Convergence: 0.005363\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.63e-01\n",
      "Epoch: 222, Loss: 2475.78059, Residuals: -0.94039, Convergence:   inf\n",
      "Epoch: 223, Loss: 2471.80813, Residuals: -0.93776, Convergence: 0.001607\n",
      "Epoch: 224, Loss: 2468.49694, Residuals: -0.93564, Convergence: 0.001341\n",
      "Epoch: 225, Loss: 2465.65698, Residuals: -0.93392, Convergence: 0.001152\n",
      "Epoch: 226, Loss: 2463.17518, Residuals: -0.93251, Convergence: 0.001008\n",
      "Epoch: 227, Loss: 2460.97216, Residuals: -0.93136, Convergence: 0.000895\n",
      "Evidence 14762.941\n",
      "\n",
      "Epoch: 227, Evidence: 14762.94141, Convergence: 0.002060\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.12e-01\n",
      "Epoch: 227, Loss: 2476.72746, Residuals: -0.93136, Convergence:   inf\n",
      "Epoch: 228, Loss: 2473.90032, Residuals: -0.92934, Convergence: 0.001143\n",
      "Epoch: 229, Loss: 2471.51764, Residuals: -0.92776, Convergence: 0.000964\n",
      "Evidence 14775.582\n",
      "\n",
      "Epoch: 229, Evidence: 14775.58203, Convergence: 0.000856\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.76e-01\n",
      "Epoch: 229, Loss: 2477.43430, Residuals: -0.92776, Convergence:   inf\n",
      "Epoch: 230, Loss: 2472.92681, Residuals: -0.92488, Convergence: 0.001823\n",
      "Epoch: 231, Loss: 2469.49249, Residuals: -0.92296, Convergence: 0.001391\n",
      "Epoch: 232, Loss: 2466.70545, Residuals: -0.92174, Convergence: 0.001130\n",
      "Epoch: 233, Loss: 2464.34072, Residuals: -0.92110, Convergence: 0.000960\n",
      "Evidence 14793.200\n",
      "\n",
      "Epoch: 233, Evidence: 14793.20020, Convergence: 0.002045\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.46e-01\n",
      "Epoch: 233, Loss: 2477.64956, Residuals: -0.92110, Convergence:   inf\n",
      "Epoch: 234, Loss: 2474.63823, Residuals: -0.91859, Convergence: 0.001217\n",
      "Epoch: 235, Loss: 2472.25986, Residuals: -0.91737, Convergence: 0.000962\n",
      "Evidence 14804.090\n",
      "\n",
      "Epoch: 235, Evidence: 14804.08984, Convergence: 0.000736\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.23e-01\n",
      "Epoch: 235, Loss: 2477.84934, Residuals: -0.91737, Convergence:   inf\n",
      "Epoch: 236, Loss: 2473.43475, Residuals: -0.91379, Convergence: 0.001785\n",
      "Epoch: 237, Loss: 2470.31755, Residuals: -0.91496, Convergence: 0.001262\n",
      "Epoch: 238, Loss: 2467.74747, Residuals: -0.91587, Convergence: 0.001041\n",
      "Epoch: 239, Loss: 2465.46618, Residuals: -0.91841, Convergence: 0.000925\n",
      "Evidence 14819.708\n",
      "\n",
      "Epoch: 239, Evidence: 14819.70801, Convergence: 0.001789\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.09e-01\n",
      "Epoch: 239, Loss: 2477.21635, Residuals: -0.91841, Convergence:   inf\n",
      "Epoch: 240, Loss: 2475.42057, Residuals: -0.91574, Convergence: 0.000725\n",
      "Evidence 14826.460\n",
      "\n",
      "Epoch: 240, Evidence: 14826.45996, Convergence: 0.000455\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 9.00e-02\n",
      "Epoch: 240, Loss: 2478.10040, Residuals: -0.91574, Convergence:   inf\n",
      "Epoch: 241, Loss: 2520.83011, Residuals: -0.95807, Convergence: -0.016951\n",
      "Epoch: 241, Loss: 2475.84233, Residuals: -0.91371, Convergence: 0.000912\n",
      "Evidence 14830.697\n",
      "\n",
      "Epoch: 241, Evidence: 14830.69727, Convergence: 0.000741\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.30e-02\n",
      "Epoch: 241, Loss: 2477.59766, Residuals: -0.91371, Convergence:   inf\n",
      "Epoch: 242, Loss: 2481.28238, Residuals: -0.91752, Convergence: -0.001485\n",
      "Epoch: 242, Loss: 2477.26858, Residuals: -0.91308, Convergence: 0.000133\n",
      "Evidence 14832.642\n",
      "\n",
      "Epoch: 242, Evidence: 14832.64160, Convergence: 0.000872\n",
      "Total samples: 181, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 382.90163, Residuals: -4.56061, Convergence:   inf\n",
      "Epoch: 1, Loss: 357.18069, Residuals: -4.44007, Convergence: 0.072011\n",
      "Epoch: 2, Loss: 336.11634, Residuals: -4.27508, Convergence: 0.062670\n",
      "Epoch: 3, Loss: 320.02310, Residuals: -4.10970, Convergence: 0.050288\n",
      "Epoch: 4, Loss: 307.74645, Residuals: -3.96389, Convergence: 0.039892\n",
      "Epoch: 5, Loss: 298.00627, Residuals: -3.83477, Convergence: 0.032684\n",
      "Epoch: 6, Loss: 290.09761, Residuals: -3.72216, Convergence: 0.027262\n",
      "Epoch: 7, Loss: 283.53091, Residuals: -3.62567, Convergence: 0.023160\n",
      "Epoch: 8, Loss: 277.94340, Residuals: -3.54328, Convergence: 0.020103\n",
      "Epoch: 9, Loss: 273.07908, Residuals: -3.47262, Convergence: 0.017813\n",
      "Epoch: 10, Loss: 268.75610, Residuals: -3.41159, Convergence: 0.016085\n",
      "Epoch: 11, Loss: 264.84284, Residuals: -3.35836, Convergence: 0.014776\n",
      "Epoch: 12, Loss: 261.24347, Residuals: -3.31138, Convergence: 0.013778\n",
      "Epoch: 13, Loss: 257.88851, Residuals: -3.26931, Convergence: 0.013009\n",
      "Epoch: 14, Loss: 254.72831, Residuals: -3.23098, Convergence: 0.012406\n",
      "Epoch: 15, Loss: 251.72974, Residuals: -3.19539, Convergence: 0.011912\n",
      "Epoch: 16, Loss: 248.87458, Residuals: -3.16190, Convergence: 0.011472\n",
      "Epoch: 17, Loss: 246.15199, Residuals: -3.13011, Convergence: 0.011061\n",
      "Epoch: 18, Loss: 243.54432, Residuals: -3.09972, Convergence: 0.010707\n",
      "Epoch: 19, Loss: 241.02155, Residuals: -3.07025, Convergence: 0.010467\n",
      "Epoch: 20, Loss: 238.54754, Residuals: -3.04116, Convergence: 0.010371\n",
      "Epoch: 21, Loss: 236.08950, Residuals: -3.01193, Convergence: 0.010411\n",
      "Epoch: 22, Loss: 233.62410, Residuals: -2.98217, Convergence: 0.010553\n",
      "Epoch: 23, Loss: 231.13230, Residuals: -2.95166, Convergence: 0.010781\n",
      "Epoch: 24, Loss: 228.58287, Residuals: -2.92003, Convergence: 0.011153\n",
      "Epoch: 25, Loss: 225.92513, Residuals: -2.88670, Convergence: 0.011764\n",
      "Epoch: 26, Loss: 223.12163, Residuals: -2.85115, Convergence: 0.012565\n",
      "Epoch: 27, Loss: 220.23083, Residuals: -2.81393, Convergence: 0.013126\n",
      "Epoch: 28, Loss: 217.37925, Residuals: -2.77645, Convergence: 0.013118\n",
      "Epoch: 29, Loss: 214.62252, Residuals: -2.73950, Convergence: 0.012845\n",
      "Epoch: 30, Loss: 211.95653, Residuals: -2.70315, Convergence: 0.012578\n",
      "Epoch: 31, Loss: 209.36555, Residuals: -2.66731, Convergence: 0.012375\n",
      "Epoch: 32, Loss: 206.83671, Residuals: -2.63187, Convergence: 0.012226\n",
      "Epoch: 33, Loss: 204.36173, Residuals: -2.59675, Convergence: 0.012111\n",
      "Epoch: 34, Loss: 201.93616, Residuals: -2.56186, Convergence: 0.012012\n",
      "Epoch: 35, Loss: 199.55828, Residuals: -2.52718, Convergence: 0.011916\n",
      "Epoch: 36, Loss: 197.22821, Residuals: -2.49268, Convergence: 0.011814\n",
      "Epoch: 37, Loss: 194.94707, Residuals: -2.45834, Convergence: 0.011701\n",
      "Epoch: 38, Loss: 192.71648, Residuals: -2.42416, Convergence: 0.011574\n",
      "Epoch: 39, Loss: 190.53813, Residuals: -2.39015, Convergence: 0.011433\n",
      "Epoch: 40, Loss: 188.41358, Residuals: -2.35634, Convergence: 0.011276\n",
      "Epoch: 41, Loss: 186.34414, Residuals: -2.32273, Convergence: 0.011105\n",
      "Epoch: 42, Loss: 184.33085, Residuals: -2.28935, Convergence: 0.010922\n",
      "Epoch: 43, Loss: 182.37470, Residuals: -2.25623, Convergence: 0.010726\n",
      "Epoch: 44, Loss: 180.47669, Residuals: -2.22338, Convergence: 0.010517\n",
      "Epoch: 45, Loss: 178.63810, Residuals: -2.19082, Convergence: 0.010292\n",
      "Epoch: 46, Loss: 176.86048, Residuals: -2.15859, Convergence: 0.010051\n",
      "Epoch: 47, Loss: 175.14551, Residuals: -2.12672, Convergence: 0.009792\n",
      "Epoch: 48, Loss: 173.49481, Residuals: -2.09525, Convergence: 0.009514\n",
      "Epoch: 49, Loss: 171.90966, Residuals: -2.06422, Convergence: 0.009221\n",
      "Epoch: 50, Loss: 170.39077, Residuals: -2.03368, Convergence: 0.008914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51, Loss: 168.93818, Residuals: -2.00368, Convergence: 0.008598\n",
      "Epoch: 52, Loss: 167.55125, Residuals: -1.97425, Convergence: 0.008278\n",
      "Epoch: 53, Loss: 166.22868, Residuals: -1.94543, Convergence: 0.007956\n",
      "Epoch: 54, Loss: 164.96863, Residuals: -1.91727, Convergence: 0.007638\n",
      "Epoch: 55, Loss: 163.76877, Residuals: -1.88977, Convergence: 0.007327\n",
      "Epoch: 56, Loss: 162.62641, Residuals: -1.86296, Convergence: 0.007024\n",
      "Epoch: 57, Loss: 161.53860, Residuals: -1.83684, Convergence: 0.006734\n",
      "Epoch: 58, Loss: 160.50220, Residuals: -1.81141, Convergence: 0.006457\n",
      "Epoch: 59, Loss: 159.51405, Residuals: -1.78668, Convergence: 0.006195\n",
      "Epoch: 60, Loss: 158.57102, Residuals: -1.76263, Convergence: 0.005947\n",
      "Epoch: 61, Loss: 157.67016, Residuals: -1.73923, Convergence: 0.005714\n",
      "Epoch: 62, Loss: 156.80878, Residuals: -1.71649, Convergence: 0.005493\n",
      "Epoch: 63, Loss: 155.98445, Residuals: -1.69438, Convergence: 0.005285\n",
      "Epoch: 64, Loss: 155.19507, Residuals: -1.67289, Convergence: 0.005086\n",
      "Epoch: 65, Loss: 154.43884, Residuals: -1.65201, Convergence: 0.004897\n",
      "Epoch: 66, Loss: 153.71422, Residuals: -1.63173, Convergence: 0.004714\n",
      "Epoch: 67, Loss: 153.01990, Residuals: -1.61204, Convergence: 0.004537\n",
      "Epoch: 68, Loss: 152.35472, Residuals: -1.59294, Convergence: 0.004366\n",
      "Epoch: 69, Loss: 151.71769, Residuals: -1.57441, Convergence: 0.004199\n",
      "Epoch: 70, Loss: 151.10787, Residuals: -1.55645, Convergence: 0.004036\n",
      "Epoch: 71, Loss: 150.52443, Residuals: -1.53907, Convergence: 0.003876\n",
      "Epoch: 72, Loss: 149.96654, Residuals: -1.52225, Convergence: 0.003720\n",
      "Epoch: 73, Loss: 149.43340, Residuals: -1.50599, Convergence: 0.003568\n",
      "Epoch: 74, Loss: 148.92423, Residuals: -1.49027, Convergence: 0.003419\n",
      "Epoch: 75, Loss: 148.43822, Residuals: -1.47511, Convergence: 0.003274\n",
      "Epoch: 76, Loss: 147.97455, Residuals: -1.46047, Convergence: 0.003133\n",
      "Epoch: 77, Loss: 147.53241, Residuals: -1.44636, Convergence: 0.002997\n",
      "Epoch: 78, Loss: 147.11097, Residuals: -1.43277, Convergence: 0.002865\n",
      "Epoch: 79, Loss: 146.70938, Residuals: -1.41968, Convergence: 0.002737\n",
      "Epoch: 80, Loss: 146.32680, Residuals: -1.40708, Convergence: 0.002615\n",
      "Epoch: 81, Loss: 145.96237, Residuals: -1.39497, Convergence: 0.002497\n",
      "Epoch: 82, Loss: 145.61527, Residuals: -1.38331, Convergence: 0.002384\n",
      "Epoch: 83, Loss: 145.28466, Residuals: -1.37212, Convergence: 0.002276\n",
      "Epoch: 84, Loss: 144.96975, Residuals: -1.36136, Convergence: 0.002172\n",
      "Epoch: 85, Loss: 144.66975, Residuals: -1.35102, Convergence: 0.002074\n",
      "Epoch: 86, Loss: 144.38393, Residuals: -1.34109, Convergence: 0.001980\n",
      "Epoch: 87, Loss: 144.11159, Residuals: -1.33156, Convergence: 0.001890\n",
      "Epoch: 88, Loss: 143.85204, Residuals: -1.32241, Convergence: 0.001804\n",
      "Epoch: 89, Loss: 143.60469, Residuals: -1.31363, Convergence: 0.001722\n",
      "Epoch: 90, Loss: 143.36895, Residuals: -1.30520, Convergence: 0.001644\n",
      "Epoch: 91, Loss: 143.14428, Residuals: -1.29711, Convergence: 0.001569\n",
      "Epoch: 92, Loss: 142.93021, Residuals: -1.28934, Convergence: 0.001498\n",
      "Epoch: 93, Loss: 142.72628, Residuals: -1.28189, Convergence: 0.001429\n",
      "Epoch: 94, Loss: 142.53210, Residuals: -1.27474, Convergence: 0.001362\n",
      "Epoch: 95, Loss: 142.34730, Residuals: -1.26788, Convergence: 0.001298\n",
      "Epoch: 96, Loss: 142.17155, Residuals: -1.26130, Convergence: 0.001236\n",
      "Epoch: 97, Loss: 142.00457, Residuals: -1.25498, Convergence: 0.001176\n",
      "Epoch: 98, Loss: 141.84608, Residuals: -1.24893, Convergence: 0.001117\n",
      "Epoch: 99, Loss: 141.69587, Residuals: -1.24312, Convergence: 0.001060\n",
      "Epoch: 100, Loss: 141.55370, Residuals: -1.23756, Convergence: 0.001004\n",
      "Epoch: 101, Loss: 141.41942, Residuals: -1.23223, Convergence: 0.000950\n",
      "Evidence -183.109\n",
      "\n",
      "Epoch: 101, Evidence: -183.10941, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 7.25e-01\n",
      "Epoch: 101, Loss: 1359.36430, Residuals: -1.23223, Convergence:   inf\n",
      "Epoch: 102, Loss: 1297.87428, Residuals: -1.26376, Convergence: 0.047377\n",
      "Epoch: 103, Loss: 1251.05545, Residuals: -1.28858, Convergence: 0.037423\n",
      "Epoch: 104, Loss: 1215.68162, Residuals: -1.30662, Convergence: 0.029098\n",
      "Epoch: 105, Loss: 1188.22157, Residuals: -1.31948, Convergence: 0.023110\n",
      "Epoch: 106, Loss: 1166.07760, Residuals: -1.32909, Convergence: 0.018990\n",
      "Epoch: 107, Loss: 1147.73547, Residuals: -1.33654, Convergence: 0.015981\n",
      "Epoch: 108, Loss: 1132.28246, Residuals: -1.34230, Convergence: 0.013648\n",
      "Epoch: 109, Loss: 1119.09937, Residuals: -1.34658, Convergence: 0.011780\n",
      "Epoch: 110, Loss: 1107.72890, Residuals: -1.34956, Convergence: 0.010265\n",
      "Epoch: 111, Loss: 1097.81605, Residuals: -1.35135, Convergence: 0.009030\n",
      "Epoch: 112, Loss: 1089.07468, Residuals: -1.35206, Convergence: 0.008026\n",
      "Epoch: 113, Loss: 1081.27164, Residuals: -1.35178, Convergence: 0.007217\n",
      "Epoch: 114, Loss: 1074.20946, Residuals: -1.35058, Convergence: 0.006574\n",
      "Epoch: 115, Loss: 1067.72045, Residuals: -1.34854, Convergence: 0.006077\n",
      "Epoch: 116, Loss: 1061.65780, Residuals: -1.34569, Convergence: 0.005711\n",
      "Epoch: 117, Loss: 1055.89202, Residuals: -1.34208, Convergence: 0.005461\n",
      "Epoch: 118, Loss: 1050.30959, Residuals: -1.33771, Convergence: 0.005315\n",
      "Epoch: 119, Loss: 1044.81598, Residuals: -1.33262, Convergence: 0.005258\n",
      "Epoch: 120, Loss: 1039.34062, Residuals: -1.32684, Convergence: 0.005268\n",
      "Epoch: 121, Loss: 1033.84722, Residuals: -1.32041, Convergence: 0.005314\n",
      "Epoch: 122, Loss: 1028.34396, Residuals: -1.31342, Convergence: 0.005352\n",
      "Epoch: 123, Loss: 1022.88387, Residuals: -1.30598, Convergence: 0.005338\n",
      "Epoch: 124, Loss: 1017.55158, Residuals: -1.29817, Convergence: 0.005240\n",
      "Epoch: 125, Loss: 1012.43191, Residuals: -1.29011, Convergence: 0.005057\n",
      "Epoch: 126, Loss: 1007.58649, Residuals: -1.28189, Convergence: 0.004809\n",
      "Epoch: 127, Loss: 1003.04240, Residuals: -1.27360, Convergence: 0.004530\n",
      "Epoch: 128, Loss: 998.79888, Residuals: -1.26529, Convergence: 0.004249\n",
      "Epoch: 129, Loss: 994.83748, Residuals: -1.25703, Convergence: 0.003982\n",
      "Epoch: 130, Loss: 991.13203, Residuals: -1.24886, Convergence: 0.003739\n",
      "Epoch: 131, Loss: 987.65478, Residuals: -1.24082, Convergence: 0.003521\n",
      "Epoch: 132, Loss: 984.37987, Residuals: -1.23294, Convergence: 0.003327\n",
      "Epoch: 133, Loss: 981.28421, Residuals: -1.22523, Convergence: 0.003155\n",
      "Epoch: 134, Loss: 978.34816, Residuals: -1.21772, Convergence: 0.003001\n",
      "Epoch: 135, Loss: 975.55589, Residuals: -1.21042, Convergence: 0.002862\n",
      "Epoch: 136, Loss: 972.89415, Residuals: -1.20333, Convergence: 0.002736\n",
      "Epoch: 137, Loss: 970.35220, Residuals: -1.19648, Convergence: 0.002620\n",
      "Epoch: 138, Loss: 967.92145, Residuals: -1.18987, Convergence: 0.002511\n",
      "Epoch: 139, Loss: 965.59468, Residuals: -1.18349, Convergence: 0.002410\n",
      "Epoch: 140, Loss: 963.36597, Residuals: -1.17736, Convergence: 0.002313\n",
      "Epoch: 141, Loss: 961.23058, Residuals: -1.17147, Convergence: 0.002222\n",
      "Epoch: 142, Loss: 959.18363, Residuals: -1.16582, Convergence: 0.002134\n",
      "Epoch: 143, Loss: 957.22167, Residuals: -1.16041, Convergence: 0.002050\n",
      "Epoch: 144, Loss: 955.33974, Residuals: -1.15523, Convergence: 0.001970\n",
      "Epoch: 145, Loss: 953.53569, Residuals: -1.15028, Convergence: 0.001892\n",
      "Epoch: 146, Loss: 951.80504, Residuals: -1.14555, Convergence: 0.001818\n",
      "Epoch: 147, Loss: 950.14493, Residuals: -1.14102, Convergence: 0.001747\n",
      "Epoch: 148, Loss: 948.55161, Residuals: -1.13670, Convergence: 0.001680\n",
      "Epoch: 149, Loss: 947.02199, Residuals: -1.13258, Convergence: 0.001615\n",
      "Epoch: 150, Loss: 945.55228, Residuals: -1.12864, Convergence: 0.001554\n",
      "Epoch: 151, Loss: 944.13938, Residuals: -1.12487, Convergence: 0.001496\n",
      "Epoch: 152, Loss: 942.77899, Residuals: -1.12127, Convergence: 0.001443\n",
      "Epoch: 153, Loss: 941.46775, Residuals: -1.11782, Convergence: 0.001393\n",
      "Epoch: 154, Loss: 940.20139, Residuals: -1.11452, Convergence: 0.001347\n",
      "Epoch: 155, Loss: 938.97598, Residuals: -1.11135, Convergence: 0.001305\n",
      "Epoch: 156, Loss: 937.78646, Residuals: -1.10831, Convergence: 0.001268\n",
      "Epoch: 157, Loss: 936.62890, Residuals: -1.10538, Convergence: 0.001236\n",
      "Epoch: 158, Loss: 935.49772, Residuals: -1.10255, Convergence: 0.001209\n",
      "Epoch: 159, Loss: 934.38877, Residuals: -1.09980, Convergence: 0.001187\n",
      "Epoch: 160, Loss: 933.29764, Residuals: -1.09713, Convergence: 0.001169\n",
      "Epoch: 161, Loss: 932.22014, Residuals: -1.09453, Convergence: 0.001156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 162, Loss: 931.15303, Residuals: -1.09198, Convergence: 0.001146\n",
      "Epoch: 163, Loss: 930.09506, Residuals: -1.08948, Convergence: 0.001137\n",
      "Epoch: 164, Loss: 929.04529, Residuals: -1.08703, Convergence: 0.001130\n",
      "Epoch: 165, Loss: 928.00600, Residuals: -1.08461, Convergence: 0.001120\n",
      "Epoch: 166, Loss: 926.98014, Residuals: -1.08225, Convergence: 0.001107\n",
      "Epoch: 167, Loss: 925.97156, Residuals: -1.07994, Convergence: 0.001089\n",
      "Epoch: 168, Loss: 924.98506, Residuals: -1.07768, Convergence: 0.001067\n",
      "Epoch: 169, Loss: 924.02507, Residuals: -1.07550, Convergence: 0.001039\n",
      "Epoch: 170, Loss: 923.09500, Residuals: -1.07338, Convergence: 0.001008\n",
      "Epoch: 171, Loss: 922.19773, Residuals: -1.07134, Convergence: 0.000973\n",
      "Evidence 11078.436\n",
      "\n",
      "Epoch: 171, Evidence: 11078.43555, Convergence: 1.016528\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 5.77e-01\n",
      "Epoch: 171, Loss: 2330.09900, Residuals: -1.07134, Convergence:   inf\n",
      "Epoch: 172, Loss: 2294.92663, Residuals: -1.08000, Convergence: 0.015326\n",
      "Epoch: 173, Loss: 2269.06042, Residuals: -1.07915, Convergence: 0.011400\n",
      "Epoch: 174, Loss: 2247.50270, Residuals: -1.07712, Convergence: 0.009592\n",
      "Epoch: 175, Loss: 2229.32781, Residuals: -1.07480, Convergence: 0.008153\n",
      "Epoch: 176, Loss: 2213.85582, Residuals: -1.07232, Convergence: 0.006989\n",
      "Epoch: 177, Loss: 2200.56229, Residuals: -1.06973, Convergence: 0.006041\n",
      "Epoch: 178, Loss: 2189.01991, Residuals: -1.06705, Convergence: 0.005273\n",
      "Epoch: 179, Loss: 2178.87693, Residuals: -1.06427, Convergence: 0.004655\n",
      "Epoch: 180, Loss: 2169.84188, Residuals: -1.06138, Convergence: 0.004164\n",
      "Epoch: 181, Loss: 2161.68092, Residuals: -1.05838, Convergence: 0.003775\n",
      "Epoch: 182, Loss: 2154.21818, Residuals: -1.05525, Convergence: 0.003464\n",
      "Epoch: 183, Loss: 2147.33483, Residuals: -1.05199, Convergence: 0.003206\n",
      "Epoch: 184, Loss: 2140.95939, Residuals: -1.04863, Convergence: 0.002978\n",
      "Epoch: 185, Loss: 2135.05329, Residuals: -1.04522, Convergence: 0.002766\n",
      "Epoch: 186, Loss: 2129.59198, Residuals: -1.04181, Convergence: 0.002564\n",
      "Epoch: 187, Loss: 2124.55169, Residuals: -1.03845, Convergence: 0.002372\n",
      "Epoch: 188, Loss: 2119.90855, Residuals: -1.03518, Convergence: 0.002190\n",
      "Epoch: 189, Loss: 2115.63525, Residuals: -1.03201, Convergence: 0.002020\n",
      "Epoch: 190, Loss: 2111.70174, Residuals: -1.02898, Convergence: 0.001863\n",
      "Epoch: 191, Loss: 2108.07906, Residuals: -1.02608, Convergence: 0.001718\n",
      "Epoch: 192, Loss: 2104.73704, Residuals: -1.02332, Convergence: 0.001588\n",
      "Epoch: 193, Loss: 2101.64778, Residuals: -1.02071, Convergence: 0.001470\n",
      "Epoch: 194, Loss: 2098.78435, Residuals: -1.01823, Convergence: 0.001364\n",
      "Epoch: 195, Loss: 2096.12210, Residuals: -1.01588, Convergence: 0.001270\n",
      "Epoch: 196, Loss: 2093.63882, Residuals: -1.01366, Convergence: 0.001186\n",
      "Epoch: 197, Loss: 2091.31445, Residuals: -1.01156, Convergence: 0.001111\n",
      "Epoch: 198, Loss: 2089.13119, Residuals: -1.00957, Convergence: 0.001045\n",
      "Epoch: 199, Loss: 2087.07499, Residuals: -1.00768, Convergence: 0.000985\n",
      "Evidence 14215.489\n",
      "\n",
      "Epoch: 199, Evidence: 14215.48926, Convergence: 0.220679\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 4.39e-01\n",
      "Epoch: 199, Loss: 2457.33898, Residuals: -1.00768, Convergence:   inf\n",
      "Epoch: 200, Loss: 2444.17977, Residuals: -1.00444, Convergence: 0.005384\n",
      "Epoch: 201, Loss: 2433.40701, Residuals: -1.00050, Convergence: 0.004427\n",
      "Epoch: 202, Loss: 2424.15080, Residuals: -0.99667, Convergence: 0.003818\n",
      "Epoch: 203, Loss: 2416.12761, Residuals: -0.99306, Convergence: 0.003321\n",
      "Epoch: 204, Loss: 2409.12605, Residuals: -0.98971, Convergence: 0.002906\n",
      "Epoch: 205, Loss: 2402.97947, Residuals: -0.98662, Convergence: 0.002558\n",
      "Epoch: 206, Loss: 2397.55065, Residuals: -0.98378, Convergence: 0.002264\n",
      "Epoch: 207, Loss: 2392.72765, Residuals: -0.98117, Convergence: 0.002016\n",
      "Epoch: 208, Loss: 2388.41872, Residuals: -0.97878, Convergence: 0.001804\n",
      "Epoch: 209, Loss: 2384.54557, Residuals: -0.97658, Convergence: 0.001624\n",
      "Epoch: 210, Loss: 2381.04556, Residuals: -0.97456, Convergence: 0.001470\n",
      "Epoch: 211, Loss: 2377.86596, Residuals: -0.97271, Convergence: 0.001337\n",
      "Epoch: 212, Loss: 2374.96276, Residuals: -0.97101, Convergence: 0.001222\n",
      "Epoch: 213, Loss: 2372.29935, Residuals: -0.96944, Convergence: 0.001123\n",
      "Epoch: 214, Loss: 2369.84653, Residuals: -0.96800, Convergence: 0.001035\n",
      "Epoch: 215, Loss: 2367.57805, Residuals: -0.96669, Convergence: 0.000958\n",
      "Evidence 14580.336\n",
      "\n",
      "Epoch: 215, Evidence: 14580.33594, Convergence: 0.025023\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 3.36e-01\n",
      "Epoch: 215, Loss: 2462.51475, Residuals: -0.96669, Convergence:   inf\n",
      "Epoch: 216, Loss: 2456.26718, Residuals: -0.96327, Convergence: 0.002544\n",
      "Epoch: 217, Loss: 2451.10751, Residuals: -0.96023, Convergence: 0.002105\n",
      "Epoch: 218, Loss: 2446.72313, Residuals: -0.95759, Convergence: 0.001792\n",
      "Epoch: 219, Loss: 2442.93726, Residuals: -0.95531, Convergence: 0.001550\n",
      "Epoch: 220, Loss: 2439.62237, Residuals: -0.95334, Convergence: 0.001359\n",
      "Epoch: 221, Loss: 2436.68392, Residuals: -0.95162, Convergence: 0.001206\n",
      "Epoch: 222, Loss: 2434.05155, Residuals: -0.95014, Convergence: 0.001081\n",
      "Epoch: 223, Loss: 2431.67157, Residuals: -0.94886, Convergence: 0.000979\n",
      "Evidence 14659.523\n",
      "\n",
      "Epoch: 223, Evidence: 14659.52344, Convergence: 0.005402\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 2.63e-01\n",
      "Epoch: 223, Loss: 2464.42824, Residuals: -0.94886, Convergence:   inf\n",
      "Epoch: 224, Loss: 2460.63021, Residuals: -0.94631, Convergence: 0.001544\n",
      "Epoch: 225, Loss: 2457.46532, Residuals: -0.94422, Convergence: 0.001288\n",
      "Epoch: 226, Loss: 2454.74250, Residuals: -0.94249, Convergence: 0.001109\n",
      "Epoch: 227, Loss: 2452.35201, Residuals: -0.94105, Convergence: 0.000975\n",
      "Evidence 14686.723\n",
      "\n",
      "Epoch: 227, Evidence: 14686.72266, Convergence: 0.001852\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 2.12e-01\n",
      "Epoch: 227, Loss: 2465.59203, Residuals: -0.94105, Convergence:   inf\n",
      "Epoch: 228, Loss: 2462.74803, Residuals: -0.93903, Convergence: 0.001155\n",
      "Epoch: 229, Loss: 2460.35178, Residuals: -0.93739, Convergence: 0.000974\n",
      "Evidence 14698.639\n",
      "\n",
      "Epoch: 229, Evidence: 14698.63867, Convergence: 0.000811\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.76e-01\n",
      "Epoch: 229, Loss: 2466.42182, Residuals: -0.93739, Convergence:   inf\n",
      "Epoch: 230, Loss: 2461.89873, Residuals: -0.93457, Convergence: 0.001837\n",
      "Epoch: 231, Loss: 2458.44342, Residuals: -0.93250, Convergence: 0.001405\n",
      "Epoch: 232, Loss: 2455.63935, Residuals: -0.93112, Convergence: 0.001142\n",
      "Epoch: 233, Loss: 2453.26439, Residuals: -0.93037, Convergence: 0.000968\n",
      "Evidence 14716.413\n",
      "\n",
      "Epoch: 233, Evidence: 14716.41309, Convergence: 0.002018\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.46e-01\n",
      "Epoch: 233, Loss: 2466.69525, Residuals: -0.93037, Convergence:   inf\n",
      "Epoch: 234, Loss: 2463.67358, Residuals: -0.92803, Convergence: 0.001226\n",
      "Epoch: 235, Loss: 2461.28054, Residuals: -0.92684, Convergence: 0.000972\n",
      "Evidence 14727.382\n",
      "\n",
      "Epoch: 235, Evidence: 14727.38184, Convergence: 0.000745\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.24e-01\n",
      "Epoch: 235, Loss: 2466.94305, Residuals: -0.92684, Convergence:   inf\n",
      "Epoch: 236, Loss: 2462.51253, Residuals: -0.92396, Convergence: 0.001799\n",
      "Epoch: 237, Loss: 2459.33697, Residuals: -0.92499, Convergence: 0.001291\n",
      "Epoch: 238, Loss: 2456.77752, Residuals: -0.92631, Convergence: 0.001042\n",
      "Epoch: 239, Loss: 2454.45673, Residuals: -0.92911, Convergence: 0.000946\n",
      "Evidence 14743.256\n",
      "\n",
      "Epoch: 239, Evidence: 14743.25586, Convergence: 0.001821\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.10e-01\n",
      "Epoch: 239, Loss: 2466.38221, Residuals: -0.92911, Convergence:   inf\n",
      "Epoch: 240, Loss: 2464.54055, Residuals: -0.92763, Convergence: 0.000747\n",
      "Evidence 14750.039\n",
      "\n",
      "Epoch: 240, Evidence: 14750.03906, Convergence: 0.000460\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 9.04e-02\n",
      "Epoch: 240, Loss: 2467.27301, Residuals: -0.92763, Convergence:   inf\n",
      "Epoch: 241, Loss: 2503.19792, Residuals: -0.97095, Convergence: -0.014352\n",
      "Epoch: 241, Loss: 2464.94186, Residuals: -0.92720, Convergence: 0.000946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence 14754.428\n",
      "\n",
      "Epoch: 241, Evidence: 14754.42773, Convergence: 0.000757\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 8.40e-02\n",
      "Epoch: 241, Loss: 2466.72101, Residuals: -0.92720, Convergence:   inf\n",
      "Epoch: 242, Loss: 2472.07051, Residuals: -0.93302, Convergence: -0.002164\n",
      "Epoch: 242, Loss: 2466.61216, Residuals: -0.92768, Convergence: 0.000044\n",
      "Evidence 14756.212\n",
      "\n",
      "Epoch: 242, Evidence: 14756.21191, Convergence: 0.000878\n",
      "Total samples: 183, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 385.49132, Residuals: -4.53403, Convergence:   inf\n",
      "Epoch: 1, Loss: 359.61602, Residuals: -4.41358, Convergence: 0.071953\n",
      "Epoch: 2, Loss: 338.42041, Residuals: -4.24852, Convergence: 0.062631\n",
      "Epoch: 3, Loss: 322.21487, Residuals: -4.08302, Convergence: 0.050294\n",
      "Epoch: 4, Loss: 309.83850, Residuals: -3.93716, Convergence: 0.039945\n",
      "Epoch: 5, Loss: 300.00441, Residuals: -3.80811, Convergence: 0.032780\n",
      "Epoch: 6, Loss: 292.00357, Residuals: -3.69556, Convergence: 0.027400\n",
      "Epoch: 7, Loss: 285.34681, Residuals: -3.59906, Convergence: 0.023329\n",
      "Epoch: 8, Loss: 279.67201, Residuals: -3.51661, Convergence: 0.020291\n",
      "Epoch: 9, Loss: 274.72308, Residuals: -3.44584, Convergence: 0.018014\n",
      "Epoch: 10, Loss: 270.31753, Residuals: -3.38464, Convergence: 0.016298\n",
      "Epoch: 11, Loss: 266.32287, Residuals: -3.33119, Convergence: 0.014999\n",
      "Epoch: 12, Loss: 262.64227, Residuals: -3.28395, Convergence: 0.014014\n",
      "Epoch: 13, Loss: 259.20545, Residuals: -3.24159, Convergence: 0.013259\n",
      "Epoch: 14, Loss: 255.96250, Residuals: -3.20294, Convergence: 0.012670\n",
      "Epoch: 15, Loss: 252.88080, Residuals: -3.16705, Convergence: 0.012186\n",
      "Epoch: 16, Loss: 249.94208, Residuals: -3.13330, Convergence: 0.011758\n",
      "Epoch: 17, Loss: 247.13271, Residuals: -3.10127, Convergence: 0.011368\n",
      "Epoch: 18, Loss: 244.43043, Residuals: -3.07058, Convergence: 0.011055\n",
      "Epoch: 19, Loss: 241.80207, Residuals: -3.04070, Convergence: 0.010870\n",
      "Epoch: 20, Loss: 239.21123, Residuals: -3.01108, Convergence: 0.010831\n",
      "Epoch: 21, Loss: 236.62705, Residuals: -2.98124, Convergence: 0.010921\n",
      "Epoch: 22, Loss: 234.02669, Residuals: -2.95084, Convergence: 0.011111\n",
      "Epoch: 23, Loss: 231.38443, Residuals: -2.91959, Convergence: 0.011419\n",
      "Epoch: 24, Loss: 228.65512, Residuals: -2.88699, Convergence: 0.011936\n",
      "Epoch: 25, Loss: 225.77745, Residuals: -2.85229, Convergence: 0.012746\n",
      "Epoch: 26, Loss: 222.73638, Residuals: -2.81519, Convergence: 0.013653\n",
      "Epoch: 27, Loss: 219.64691, Residuals: -2.77681, Convergence: 0.014066\n",
      "Epoch: 28, Loss: 216.63434, Residuals: -2.73856, Convergence: 0.013906\n",
      "Epoch: 29, Loss: 213.72596, Residuals: -2.70087, Convergence: 0.013608\n",
      "Epoch: 30, Loss: 210.90891, Residuals: -2.66369, Convergence: 0.013357\n",
      "Epoch: 31, Loss: 208.16822, Residuals: -2.62691, Convergence: 0.013166\n",
      "Epoch: 32, Loss: 205.49386, Residuals: -2.59041, Convergence: 0.013014\n",
      "Epoch: 33, Loss: 202.88060, Residuals: -2.55415, Convergence: 0.012881\n",
      "Epoch: 34, Loss: 200.32644, Residuals: -2.51809, Convergence: 0.012750\n",
      "Epoch: 35, Loss: 197.83150, Residuals: -2.48222, Convergence: 0.012611\n",
      "Epoch: 36, Loss: 195.39705, Residuals: -2.44653, Convergence: 0.012459\n",
      "Epoch: 37, Loss: 193.02483, Residuals: -2.41104, Convergence: 0.012290\n",
      "Epoch: 38, Loss: 190.71679, Residuals: -2.37577, Convergence: 0.012102\n",
      "Epoch: 39, Loss: 188.47477, Residuals: -2.34074, Convergence: 0.011896\n",
      "Epoch: 40, Loss: 186.30038, Residuals: -2.30598, Convergence: 0.011671\n",
      "Epoch: 41, Loss: 184.19500, Residuals: -2.27152, Convergence: 0.011430\n",
      "Epoch: 42, Loss: 182.15965, Residuals: -2.23740, Convergence: 0.011173\n",
      "Epoch: 43, Loss: 180.19508, Residuals: -2.20366, Convergence: 0.010902\n",
      "Epoch: 44, Loss: 178.30168, Residuals: -2.17032, Convergence: 0.010619\n",
      "Epoch: 45, Loss: 176.47948, Residuals: -2.13744, Convergence: 0.010325\n",
      "Epoch: 46, Loss: 174.72819, Residuals: -2.10504, Convergence: 0.010023\n",
      "Epoch: 47, Loss: 173.04718, Residuals: -2.07316, Convergence: 0.009714\n",
      "Epoch: 48, Loss: 171.43546, Residuals: -2.04182, Convergence: 0.009401\n",
      "Epoch: 49, Loss: 169.89179, Residuals: -2.01106, Convergence: 0.009086\n",
      "Epoch: 50, Loss: 168.41471, Residuals: -1.98088, Convergence: 0.008770\n",
      "Epoch: 51, Loss: 167.00260, Residuals: -1.95132, Convergence: 0.008456\n",
      "Epoch: 52, Loss: 165.65383, Residuals: -1.92238, Convergence: 0.008142\n",
      "Epoch: 53, Loss: 164.36679, Residuals: -1.89409, Convergence: 0.007830\n",
      "Epoch: 54, Loss: 163.14002, Residuals: -1.86646, Convergence: 0.007520\n",
      "Epoch: 55, Loss: 161.97209, Residuals: -1.83950, Convergence: 0.007211\n",
      "Epoch: 56, Loss: 160.86167, Residuals: -1.81324, Convergence: 0.006903\n",
      "Epoch: 57, Loss: 159.80721, Residuals: -1.78769, Convergence: 0.006598\n",
      "Epoch: 58, Loss: 158.80694, Residuals: -1.76287, Convergence: 0.006299\n",
      "Epoch: 59, Loss: 157.85860, Residuals: -1.73877, Convergence: 0.006008\n",
      "Epoch: 60, Loss: 156.95951, Residuals: -1.71541, Convergence: 0.005728\n",
      "Epoch: 61, Loss: 156.10657, Residuals: -1.69277, Convergence: 0.005464\n",
      "Epoch: 62, Loss: 155.29636, Residuals: -1.67084, Convergence: 0.005217\n",
      "Epoch: 63, Loss: 154.52533, Residuals: -1.64959, Convergence: 0.004990\n",
      "Epoch: 64, Loss: 153.78994, Residuals: -1.62899, Convergence: 0.004782\n",
      "Epoch: 65, Loss: 153.08686, Residuals: -1.60900, Convergence: 0.004593\n",
      "Epoch: 66, Loss: 152.41312, Residuals: -1.58961, Convergence: 0.004420\n",
      "Epoch: 67, Loss: 151.76616, Residuals: -1.57078, Convergence: 0.004263\n",
      "Epoch: 68, Loss: 151.14391, Residuals: -1.55248, Convergence: 0.004117\n",
      "Epoch: 69, Loss: 150.54476, Residuals: -1.53470, Convergence: 0.003980\n",
      "Epoch: 70, Loss: 149.96749, Residuals: -1.51743, Convergence: 0.003849\n",
      "Epoch: 71, Loss: 149.41118, Residuals: -1.50065, Convergence: 0.003723\n",
      "Epoch: 72, Loss: 148.87516, Residuals: -1.48436, Convergence: 0.003600\n",
      "Epoch: 73, Loss: 148.35895, Residuals: -1.46856, Convergence: 0.003480\n",
      "Epoch: 74, Loss: 147.86212, Residuals: -1.45324, Convergence: 0.003360\n",
      "Epoch: 75, Loss: 147.38434, Residuals: -1.43840, Convergence: 0.003242\n",
      "Epoch: 76, Loss: 146.92528, Residuals: -1.42403, Convergence: 0.003124\n",
      "Epoch: 77, Loss: 146.48460, Residuals: -1.41015, Convergence: 0.003008\n",
      "Epoch: 78, Loss: 146.06198, Residuals: -1.39673, Convergence: 0.002893\n",
      "Epoch: 79, Loss: 145.65703, Residuals: -1.38378, Convergence: 0.002780\n",
      "Epoch: 80, Loss: 145.26935, Residuals: -1.37129, Convergence: 0.002669\n",
      "Epoch: 81, Loss: 144.89854, Residuals: -1.35926, Convergence: 0.002559\n",
      "Epoch: 82, Loss: 144.54415, Residuals: -1.34767, Convergence: 0.002452\n",
      "Epoch: 83, Loss: 144.20570, Residuals: -1.33652, Convergence: 0.002347\n",
      "Epoch: 84, Loss: 143.88274, Residuals: -1.32579, Convergence: 0.002245\n",
      "Epoch: 85, Loss: 143.57478, Residuals: -1.31549, Convergence: 0.002145\n",
      "Epoch: 86, Loss: 143.28132, Residuals: -1.30559, Convergence: 0.002048\n",
      "Epoch: 87, Loss: 143.00188, Residuals: -1.29608, Convergence: 0.001954\n",
      "Epoch: 88, Loss: 142.73597, Residuals: -1.28695, Convergence: 0.001863\n",
      "Epoch: 89, Loss: 142.48310, Residuals: -1.27819, Convergence: 0.001775\n",
      "Epoch: 90, Loss: 142.24281, Residuals: -1.26979, Convergence: 0.001689\n",
      "Epoch: 91, Loss: 142.01465, Residuals: -1.26174, Convergence: 0.001607\n",
      "Epoch: 92, Loss: 141.79816, Residuals: -1.25402, Convergence: 0.001527\n",
      "Epoch: 93, Loss: 141.59292, Residuals: -1.24661, Convergence: 0.001449\n",
      "Epoch: 94, Loss: 141.39853, Residuals: -1.23951, Convergence: 0.001375\n",
      "Epoch: 95, Loss: 141.21459, Residuals: -1.23271, Convergence: 0.001303\n",
      "Epoch: 96, Loss: 141.04073, Residuals: -1.22620, Convergence: 0.001233\n",
      "Epoch: 97, Loss: 140.87661, Residuals: -1.21996, Convergence: 0.001165\n",
      "Epoch: 98, Loss: 140.72189, Residuals: -1.21398, Convergence: 0.001099\n",
      "Epoch: 99, Loss: 140.57627, Residuals: -1.20826, Convergence: 0.001036\n",
      "Epoch: 100, Loss: 140.43944, Residuals: -1.20279, Convergence: 0.000974\n",
      "Evidence -182.325\n",
      "\n",
      "Epoch: 100, Evidence: -182.32478, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.25e-01\n",
      "Epoch: 100, Loss: 1364.28802, Residuals: -1.20279, Convergence:   inf\n",
      "Epoch: 101, Loss: 1302.16505, Residuals: -1.23455, Convergence: 0.047707\n",
      "Epoch: 102, Loss: 1255.43235, Residuals: -1.25918, Convergence: 0.037224\n",
      "Epoch: 103, Loss: 1220.31550, Residuals: -1.27675, Convergence: 0.028777\n",
      "Epoch: 104, Loss: 1192.99678, Residuals: -1.28925, Convergence: 0.022899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 105, Loss: 1170.90048, Residuals: -1.29865, Convergence: 0.018871\n",
      "Epoch: 106, Loss: 1152.57723, Residuals: -1.30599, Convergence: 0.015898\n",
      "Epoch: 107, Loss: 1137.15000, Residuals: -1.31169, Convergence: 0.013567\n",
      "Epoch: 108, Loss: 1124.01527, Residuals: -1.31599, Convergence: 0.011686\n",
      "Epoch: 109, Loss: 1112.72229, Residuals: -1.31906, Convergence: 0.010149\n",
      "Epoch: 110, Loss: 1102.91658, Residuals: -1.32103, Convergence: 0.008891\n",
      "Epoch: 111, Loss: 1094.31300, Residuals: -1.32200, Convergence: 0.007862\n",
      "Epoch: 112, Loss: 1086.67585, Residuals: -1.32207, Convergence: 0.007028\n",
      "Epoch: 113, Loss: 1079.80792, Residuals: -1.32132, Convergence: 0.006360\n",
      "Epoch: 114, Loss: 1073.53954, Residuals: -1.31981, Convergence: 0.005839\n",
      "Epoch: 115, Loss: 1067.72225, Residuals: -1.31759, Convergence: 0.005448\n",
      "Epoch: 116, Loss: 1062.22166, Residuals: -1.31468, Convergence: 0.005178\n",
      "Epoch: 117, Loss: 1056.91443, Residuals: -1.31109, Convergence: 0.005021\n",
      "Epoch: 118, Loss: 1051.68731, Residuals: -1.30681, Convergence: 0.004970\n",
      "Epoch: 119, Loss: 1046.44123, Residuals: -1.30186, Convergence: 0.005013\n",
      "Epoch: 120, Loss: 1041.10294, Residuals: -1.29624, Convergence: 0.005128\n",
      "Epoch: 121, Loss: 1035.64283, Residuals: -1.29000, Convergence: 0.005272\n",
      "Epoch: 122, Loss: 1030.09281, Residuals: -1.28322, Convergence: 0.005388\n",
      "Epoch: 123, Loss: 1024.54390, Residuals: -1.27599, Convergence: 0.005416\n",
      "Epoch: 124, Loss: 1019.11778, Residuals: -1.26843, Convergence: 0.005324\n",
      "Epoch: 125, Loss: 1013.92259, Residuals: -1.26063, Convergence: 0.005124\n",
      "Epoch: 126, Loss: 1009.02484, Residuals: -1.25269, Convergence: 0.004854\n",
      "Epoch: 127, Loss: 1004.44771, Residuals: -1.24468, Convergence: 0.004557\n",
      "Epoch: 128, Loss: 1000.18441, Residuals: -1.23667, Convergence: 0.004263\n",
      "Epoch: 129, Loss: 996.21140, Residuals: -1.22872, Convergence: 0.003988\n",
      "Epoch: 130, Loss: 992.50101, Residuals: -1.22087, Convergence: 0.003738\n",
      "Epoch: 131, Loss: 989.02503, Residuals: -1.21315, Convergence: 0.003515\n",
      "Epoch: 132, Loss: 985.75861, Residuals: -1.20560, Convergence: 0.003314\n",
      "Epoch: 133, Loss: 982.68026, Residuals: -1.19824, Convergence: 0.003133\n",
      "Epoch: 134, Loss: 979.77266, Residuals: -1.19109, Convergence: 0.002968\n",
      "Epoch: 135, Loss: 977.02214, Residuals: -1.18417, Convergence: 0.002815\n",
      "Epoch: 136, Loss: 974.41662, Residuals: -1.17748, Convergence: 0.002674\n",
      "Epoch: 137, Loss: 971.94663, Residuals: -1.17105, Convergence: 0.002541\n",
      "Epoch: 138, Loss: 969.60424, Residuals: -1.16488, Convergence: 0.002416\n",
      "Epoch: 139, Loss: 967.38144, Residuals: -1.15896, Convergence: 0.002298\n",
      "Epoch: 140, Loss: 965.27200, Residuals: -1.15331, Convergence: 0.002185\n",
      "Epoch: 141, Loss: 963.26888, Residuals: -1.14792, Convergence: 0.002080\n",
      "Epoch: 142, Loss: 961.36599, Residuals: -1.14279, Convergence: 0.001979\n",
      "Epoch: 143, Loss: 959.55634, Residuals: -1.13791, Convergence: 0.001886\n",
      "Epoch: 144, Loss: 957.83420, Residuals: -1.13327, Convergence: 0.001798\n",
      "Epoch: 145, Loss: 956.19251, Residuals: -1.12887, Convergence: 0.001717\n",
      "Epoch: 146, Loss: 954.62571, Residuals: -1.12468, Convergence: 0.001641\n",
      "Epoch: 147, Loss: 953.12788, Residuals: -1.12071, Convergence: 0.001571\n",
      "Epoch: 148, Loss: 951.69366, Residuals: -1.11693, Convergence: 0.001507\n",
      "Epoch: 149, Loss: 950.31751, Residuals: -1.11335, Convergence: 0.001448\n",
      "Epoch: 150, Loss: 948.99534, Residuals: -1.10993, Convergence: 0.001393\n",
      "Epoch: 151, Loss: 947.72233, Residuals: -1.10668, Convergence: 0.001343\n",
      "Epoch: 152, Loss: 946.49503, Residuals: -1.10359, Convergence: 0.001297\n",
      "Epoch: 153, Loss: 945.30979, Residuals: -1.10063, Convergence: 0.001254\n",
      "Epoch: 154, Loss: 944.16321, Residuals: -1.09781, Convergence: 0.001214\n",
      "Epoch: 155, Loss: 943.05332, Residuals: -1.09511, Convergence: 0.001177\n",
      "Epoch: 156, Loss: 941.97720, Residuals: -1.09253, Convergence: 0.001142\n",
      "Epoch: 157, Loss: 940.93279, Residuals: -1.09006, Convergence: 0.001110\n",
      "Epoch: 158, Loss: 939.91836, Residuals: -1.08768, Convergence: 0.001079\n",
      "Epoch: 159, Loss: 938.93180, Residuals: -1.08540, Convergence: 0.001051\n",
      "Epoch: 160, Loss: 937.97183, Residuals: -1.08320, Convergence: 0.001023\n",
      "Epoch: 161, Loss: 937.03668, Residuals: -1.08108, Convergence: 0.000998\n",
      "Evidence 11181.212\n",
      "\n",
      "Epoch: 161, Evidence: 11181.21191, Convergence: 1.016306\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 5.76e-01\n",
      "Epoch: 161, Loss: 2352.24913, Residuals: -1.08108, Convergence:   inf\n",
      "Epoch: 162, Loss: 2311.89592, Residuals: -1.09212, Convergence: 0.017455\n",
      "Epoch: 163, Loss: 2284.01788, Residuals: -1.09202, Convergence: 0.012206\n",
      "Epoch: 164, Loss: 2260.64626, Residuals: -1.09068, Convergence: 0.010338\n",
      "Epoch: 165, Loss: 2240.73579, Residuals: -1.08883, Convergence: 0.008886\n",
      "Epoch: 166, Loss: 2223.61338, Residuals: -1.08664, Convergence: 0.007700\n",
      "Epoch: 167, Loss: 2208.76362, Residuals: -1.08415, Convergence: 0.006723\n",
      "Epoch: 168, Loss: 2195.77149, Residuals: -1.08142, Convergence: 0.005917\n",
      "Epoch: 169, Loss: 2184.29612, Residuals: -1.07848, Convergence: 0.005254\n",
      "Epoch: 170, Loss: 2174.05671, Residuals: -1.07535, Convergence: 0.004710\n",
      "Epoch: 171, Loss: 2164.81896, Residuals: -1.07206, Convergence: 0.004267\n",
      "Epoch: 172, Loss: 2156.38838, Residuals: -1.06862, Convergence: 0.003910\n",
      "Epoch: 173, Loss: 2148.61077, Residuals: -1.06503, Convergence: 0.003620\n",
      "Epoch: 174, Loss: 2141.37794, Residuals: -1.06131, Convergence: 0.003378\n",
      "Epoch: 175, Loss: 2134.62636, Residuals: -1.05749, Convergence: 0.003163\n",
      "Epoch: 176, Loss: 2128.32791, Residuals: -1.05360, Convergence: 0.002959\n",
      "Epoch: 177, Loss: 2122.46963, Residuals: -1.04970, Convergence: 0.002760\n",
      "Epoch: 178, Loss: 2117.04073, Residuals: -1.04584, Convergence: 0.002564\n",
      "Epoch: 179, Loss: 2112.02384, Residuals: -1.04206, Convergence: 0.002375\n",
      "Epoch: 180, Loss: 2107.39477, Residuals: -1.03839, Convergence: 0.002197\n",
      "Epoch: 181, Loss: 2103.12406, Residuals: -1.03486, Convergence: 0.002031\n",
      "Epoch: 182, Loss: 2099.18091, Residuals: -1.03148, Convergence: 0.001878\n",
      "Epoch: 183, Loss: 2095.53426, Residuals: -1.02825, Convergence: 0.001740\n",
      "Epoch: 184, Loss: 2092.15427, Residuals: -1.02517, Convergence: 0.001616\n",
      "Epoch: 185, Loss: 2089.01179, Residuals: -1.02224, Convergence: 0.001504\n",
      "Epoch: 186, Loss: 2086.08145, Residuals: -1.01946, Convergence: 0.001405\n",
      "Epoch: 187, Loss: 2083.33928, Residuals: -1.01683, Convergence: 0.001316\n",
      "Epoch: 188, Loss: 2080.76450, Residuals: -1.01433, Convergence: 0.001237\n",
      "Epoch: 189, Loss: 2078.33811, Residuals: -1.01195, Convergence: 0.001167\n",
      "Epoch: 190, Loss: 2076.04496, Residuals: -1.00971, Convergence: 0.001105\n",
      "Epoch: 191, Loss: 2073.87082, Residuals: -1.00757, Convergence: 0.001048\n",
      "Epoch: 192, Loss: 2071.80497, Residuals: -1.00555, Convergence: 0.000997\n",
      "Evidence 14349.104\n",
      "\n",
      "Epoch: 192, Evidence: 14349.10352, Convergence: 0.220773\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 4.40e-01\n",
      "Epoch: 192, Loss: 2477.38322, Residuals: -1.00555, Convergence:   inf\n",
      "Epoch: 193, Loss: 2462.67567, Residuals: -1.00227, Convergence: 0.005972\n",
      "Epoch: 194, Loss: 2450.77422, Residuals: -0.99829, Convergence: 0.004856\n",
      "Epoch: 195, Loss: 2440.56266, Residuals: -0.99448, Convergence: 0.004184\n",
      "Epoch: 196, Loss: 2431.73459, Residuals: -0.99093, Convergence: 0.003630\n",
      "Epoch: 197, Loss: 2424.05274, Residuals: -0.98766, Convergence: 0.003169\n",
      "Epoch: 198, Loss: 2417.32796, Residuals: -0.98464, Convergence: 0.002782\n",
      "Epoch: 199, Loss: 2411.40660, Residuals: -0.98187, Convergence: 0.002456\n",
      "Epoch: 200, Loss: 2406.15867, Residuals: -0.97931, Convergence: 0.002181\n",
      "Epoch: 201, Loss: 2401.47963, Residuals: -0.97695, Convergence: 0.001948\n",
      "Epoch: 202, Loss: 2397.28258, Residuals: -0.97476, Convergence: 0.001751\n",
      "Epoch: 203, Loss: 2393.49473, Residuals: -0.97274, Convergence: 0.001583\n",
      "Epoch: 204, Loss: 2390.05676, Residuals: -0.97086, Convergence: 0.001438\n",
      "Epoch: 205, Loss: 2386.92032, Residuals: -0.96912, Convergence: 0.001314\n",
      "Epoch: 206, Loss: 2384.04424, Residuals: -0.96750, Convergence: 0.001206\n",
      "Epoch: 207, Loss: 2381.39534, Residuals: -0.96598, Convergence: 0.001112\n",
      "Epoch: 208, Loss: 2378.94572, Residuals: -0.96457, Convergence: 0.001030\n",
      "Epoch: 209, Loss: 2376.67220, Residuals: -0.96325, Convergence: 0.000957\n",
      "Evidence 14769.332\n",
      "\n",
      "Epoch: 209, Evidence: 14769.33203, Convergence: 0.028453\n",
      "Updating hyper-parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 183, Updated regularization: 3.36e-01\n",
      "Epoch: 209, Loss: 2482.65352, Residuals: -0.96325, Convergence:   inf\n",
      "Epoch: 210, Loss: 2476.05999, Residuals: -0.95983, Convergence: 0.002663\n",
      "Epoch: 211, Loss: 2470.60330, Residuals: -0.95684, Convergence: 0.002209\n",
      "Epoch: 212, Loss: 2465.95360, Residuals: -0.95430, Convergence: 0.001886\n",
      "Epoch: 213, Loss: 2461.93025, Residuals: -0.95211, Convergence: 0.001634\n",
      "Epoch: 214, Loss: 2458.40333, Residuals: -0.95022, Convergence: 0.001435\n",
      "Epoch: 215, Loss: 2455.27381, Residuals: -0.94856, Convergence: 0.001275\n",
      "Epoch: 216, Loss: 2452.46853, Residuals: -0.94712, Convergence: 0.001144\n",
      "Epoch: 217, Loss: 2449.92939, Residuals: -0.94586, Convergence: 0.001036\n",
      "Epoch: 218, Loss: 2447.61174, Residuals: -0.94476, Convergence: 0.000947\n",
      "Evidence 14858.014\n",
      "\n",
      "Epoch: 218, Evidence: 14858.01367, Convergence: 0.005969\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.64e-01\n",
      "Epoch: 218, Loss: 2484.47027, Residuals: -0.94476, Convergence:   inf\n",
      "Epoch: 219, Loss: 2480.67119, Residuals: -0.94217, Convergence: 0.001531\n",
      "Epoch: 220, Loss: 2477.48191, Residuals: -0.94012, Convergence: 0.001287\n",
      "Epoch: 221, Loss: 2474.71763, Residuals: -0.93845, Convergence: 0.001117\n",
      "Epoch: 222, Loss: 2472.27554, Residuals: -0.93708, Convergence: 0.000988\n",
      "Evidence 14886.350\n",
      "\n",
      "Epoch: 222, Evidence: 14886.34961, Convergence: 0.001903\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.13e-01\n",
      "Epoch: 222, Loss: 2485.64829, Residuals: -0.93708, Convergence:   inf\n",
      "Epoch: 223, Loss: 2482.80429, Residuals: -0.93502, Convergence: 0.001145\n",
      "Epoch: 224, Loss: 2480.38681, Residuals: -0.93341, Convergence: 0.000975\n",
      "Evidence 14898.141\n",
      "\n",
      "Epoch: 224, Evidence: 14898.14062, Convergence: 0.000791\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.77e-01\n",
      "Epoch: 224, Loss: 2486.47689, Residuals: -0.93341, Convergence:   inf\n",
      "Epoch: 225, Loss: 2481.93074, Residuals: -0.93055, Convergence: 0.001832\n",
      "Epoch: 226, Loss: 2478.41685, Residuals: -0.92860, Convergence: 0.001418\n",
      "Epoch: 227, Loss: 2475.52951, Residuals: -0.92722, Convergence: 0.001166\n",
      "Epoch: 228, Loss: 2473.06107, Residuals: -0.92639, Convergence: 0.000998\n",
      "Evidence 14916.092\n",
      "\n",
      "Epoch: 228, Evidence: 14916.09180, Convergence: 0.001994\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.47e-01\n",
      "Epoch: 228, Loss: 2486.75840, Residuals: -0.92639, Convergence:   inf\n",
      "Epoch: 229, Loss: 2483.75105, Residuals: -0.92388, Convergence: 0.001211\n",
      "Epoch: 230, Loss: 2481.32871, Residuals: -0.92254, Convergence: 0.000976\n",
      "Evidence 14926.836\n",
      "\n",
      "Epoch: 230, Evidence: 14926.83594, Convergence: 0.000720\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.25e-01\n",
      "Epoch: 230, Loss: 2487.01442, Residuals: -0.92254, Convergence:   inf\n",
      "Epoch: 231, Loss: 2482.57495, Residuals: -0.91887, Convergence: 0.001788\n",
      "Epoch: 232, Loss: 2479.34895, Residuals: -0.91964, Convergence: 0.001301\n",
      "Epoch: 233, Loss: 2476.70087, Residuals: -0.92028, Convergence: 0.001069\n",
      "Epoch: 234, Loss: 2474.36959, Residuals: -0.92274, Convergence: 0.000942\n",
      "Evidence 14942.682\n",
      "\n",
      "Epoch: 234, Evidence: 14942.68164, Convergence: 0.001779\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.11e-01\n",
      "Epoch: 234, Loss: 2486.49234, Residuals: -0.92274, Convergence:   inf\n",
      "Epoch: 235, Loss: 2484.89918, Residuals: -0.92003, Convergence: 0.000641\n",
      "Evidence 14948.981\n",
      "\n",
      "Epoch: 235, Evidence: 14948.98145, Convergence: 0.000421\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 9.10e-02\n",
      "Epoch: 235, Loss: 2487.41329, Residuals: -0.92003, Convergence:   inf\n",
      "Epoch: 236, Loss: 2531.88570, Residuals: -0.95947, Convergence: -0.017565\n",
      "Epoch: 236, Loss: 2485.19657, Residuals: -0.91743, Convergence: 0.000892\n",
      "Evidence 14953.153\n",
      "\n",
      "Epoch: 236, Evidence: 14953.15332, Convergence: 0.000700\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 8.39e-02\n",
      "Epoch: 236, Loss: 2486.91679, Residuals: -0.91743, Convergence:   inf\n",
      "Epoch: 237, Loss: 2492.01065, Residuals: -0.92078, Convergence: -0.002044\n",
      "Epoch: 237, Loss: 2487.02194, Residuals: -0.91650, Convergence: -0.000042\n",
      "Evidence 14954.564\n",
      "\n",
      "Epoch: 237, Evidence: 14954.56445, Convergence: 0.000795\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 381.32652, Residuals: -4.52697, Convergence:   inf\n",
      "Epoch: 1, Loss: 355.63134, Residuals: -4.40667, Convergence: 0.072252\n",
      "Epoch: 2, Loss: 334.57694, Residuals: -4.24202, Convergence: 0.062928\n",
      "Epoch: 3, Loss: 318.55217, Residuals: -4.07762, Convergence: 0.050305\n",
      "Epoch: 4, Loss: 306.33068, Residuals: -3.93282, Convergence: 0.039896\n",
      "Epoch: 5, Loss: 296.63516, Residuals: -3.80479, Convergence: 0.032685\n",
      "Epoch: 6, Loss: 288.75976, Residuals: -3.69328, Convergence: 0.027273\n",
      "Epoch: 7, Loss: 282.21593, Residuals: -3.59781, Convergence: 0.023187\n",
      "Epoch: 8, Loss: 276.64275, Residuals: -3.51635, Convergence: 0.020146\n",
      "Epoch: 9, Loss: 271.78555, Residuals: -3.44658, Convergence: 0.017871\n",
      "Epoch: 10, Loss: 267.46345, Residuals: -3.38641, Convergence: 0.016160\n",
      "Epoch: 11, Loss: 263.54555, Residuals: -3.33405, Convergence: 0.014866\n",
      "Epoch: 12, Loss: 259.93640, Residuals: -3.28796, Convergence: 0.013885\n",
      "Epoch: 13, Loss: 256.56660, Residuals: -3.24679, Convergence: 0.013134\n",
      "Epoch: 14, Loss: 253.38656, Residuals: -3.20937, Convergence: 0.012550\n",
      "Epoch: 15, Loss: 250.36331, Residuals: -3.17474, Convergence: 0.012075\n",
      "Epoch: 16, Loss: 247.47858, Residuals: -3.14220, Convergence: 0.011656\n",
      "Epoch: 17, Loss: 244.72060, Residuals: -3.11136, Convergence: 0.011270\n",
      "Epoch: 18, Loss: 242.07059, Residuals: -3.08181, Convergence: 0.010947\n",
      "Epoch: 19, Loss: 239.49822, Residuals: -3.05304, Convergence: 0.010741\n",
      "Epoch: 20, Loss: 236.96776, Residuals: -3.02446, Convergence: 0.010678\n",
      "Epoch: 21, Loss: 234.44656, Residuals: -2.99554, Convergence: 0.010754\n",
      "Epoch: 22, Loss: 231.90993, Residuals: -2.96591, Convergence: 0.010938\n",
      "Epoch: 23, Loss: 229.33447, Residuals: -2.93530, Convergence: 0.011230\n",
      "Epoch: 24, Loss: 226.68053, Residuals: -2.90326, Convergence: 0.011708\n",
      "Epoch: 25, Loss: 223.88910, Residuals: -2.86911, Convergence: 0.012468\n",
      "Epoch: 26, Loss: 220.93757, Residuals: -2.83251, Convergence: 0.013359\n",
      "Epoch: 27, Loss: 217.93101, Residuals: -2.79448, Convergence: 0.013796\n",
      "Epoch: 28, Loss: 215.00018, Residuals: -2.75650, Convergence: 0.013632\n",
      "Epoch: 29, Loss: 212.17787, Residuals: -2.71907, Convergence: 0.013302\n",
      "Epoch: 30, Loss: 209.45125, Residuals: -2.68216, Convergence: 0.013018\n",
      "Epoch: 31, Loss: 206.80363, Residuals: -2.64565, Convergence: 0.012803\n",
      "Epoch: 32, Loss: 204.22317, Residuals: -2.60945, Convergence: 0.012636\n",
      "Epoch: 33, Loss: 201.70289, Residuals: -2.57351, Convergence: 0.012495\n",
      "Epoch: 34, Loss: 199.23942, Residuals: -2.53781, Convergence: 0.012364\n",
      "Epoch: 35, Loss: 196.83176, Residuals: -2.50234, Convergence: 0.012232\n",
      "Epoch: 36, Loss: 194.48026, Residuals: -2.46710, Convergence: 0.012091\n",
      "Epoch: 37, Loss: 192.18600, Residuals: -2.43210, Convergence: 0.011938\n",
      "Epoch: 38, Loss: 189.95021, Residuals: -2.39735, Convergence: 0.011770\n",
      "Epoch: 39, Loss: 187.77403, Residuals: -2.36286, Convergence: 0.011589\n",
      "Epoch: 40, Loss: 185.65831, Residuals: -2.32864, Convergence: 0.011396\n",
      "Epoch: 41, Loss: 183.60357, Residuals: -2.29471, Convergence: 0.011191\n",
      "Epoch: 42, Loss: 181.60998, Residuals: -2.26108, Convergence: 0.010977\n",
      "Epoch: 43, Loss: 179.67753, Residuals: -2.22776, Convergence: 0.010755\n",
      "Epoch: 44, Loss: 177.80619, Residuals: -2.19478, Convergence: 0.010525\n",
      "Epoch: 45, Loss: 175.99621, Residuals: -2.16215, Convergence: 0.010284\n",
      "Epoch: 46, Loss: 174.24820, Residuals: -2.12991, Convergence: 0.010032\n",
      "Epoch: 47, Loss: 172.56324, Residuals: -2.09809, Convergence: 0.009764\n",
      "Epoch: 48, Loss: 170.94247, Residuals: -2.06672, Convergence: 0.009481\n",
      "Epoch: 49, Loss: 169.38676, Residuals: -2.03585, Convergence: 0.009184\n",
      "Epoch: 50, Loss: 167.89639, Residuals: -2.00551, Convergence: 0.008877\n",
      "Epoch: 51, Loss: 166.47086, Residuals: -1.97572, Convergence: 0.008563\n",
      "Epoch: 52, Loss: 165.10909, Residuals: -1.94651, Convergence: 0.008248\n",
      "Epoch: 53, Loss: 163.80951, Residuals: -1.91789, Convergence: 0.007934\n",
      "Epoch: 54, Loss: 162.57027, Residuals: -1.88988, Convergence: 0.007623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55, Loss: 161.38938, Residuals: -1.86250, Convergence: 0.007317\n",
      "Epoch: 56, Loss: 160.26475, Residuals: -1.83576, Convergence: 0.007017\n",
      "Epoch: 57, Loss: 159.19430, Residuals: -1.80968, Convergence: 0.006724\n",
      "Epoch: 58, Loss: 158.17596, Residuals: -1.78427, Convergence: 0.006438\n",
      "Epoch: 59, Loss: 157.20766, Residuals: -1.75953, Convergence: 0.006159\n",
      "Epoch: 60, Loss: 156.28740, Residuals: -1.73548, Convergence: 0.005888\n",
      "Epoch: 61, Loss: 155.41322, Residuals: -1.71212, Convergence: 0.005625\n",
      "Epoch: 62, Loss: 154.58320, Residuals: -1.68947, Convergence: 0.005369\n",
      "Epoch: 63, Loss: 153.79547, Residuals: -1.66752, Convergence: 0.005122\n",
      "Epoch: 64, Loss: 153.04817, Residuals: -1.64628, Convergence: 0.004883\n",
      "Epoch: 65, Loss: 152.33946, Residuals: -1.62574, Convergence: 0.004652\n",
      "Epoch: 66, Loss: 151.66744, Residuals: -1.60592, Convergence: 0.004431\n",
      "Epoch: 67, Loss: 151.03019, Residuals: -1.58679, Convergence: 0.004219\n",
      "Epoch: 68, Loss: 150.42571, Residuals: -1.56836, Convergence: 0.004018\n",
      "Epoch: 69, Loss: 149.85195, Residuals: -1.55061, Convergence: 0.003829\n",
      "Epoch: 70, Loss: 149.30681, Residuals: -1.53353, Convergence: 0.003651\n",
      "Epoch: 71, Loss: 148.78821, Residuals: -1.51709, Convergence: 0.003486\n",
      "Epoch: 72, Loss: 148.29411, Residuals: -1.50126, Convergence: 0.003332\n",
      "Epoch: 73, Loss: 147.82260, Residuals: -1.48603, Convergence: 0.003190\n",
      "Epoch: 74, Loss: 147.37198, Residuals: -1.47135, Convergence: 0.003058\n",
      "Epoch: 75, Loss: 146.94075, Residuals: -1.45721, Convergence: 0.002935\n",
      "Epoch: 76, Loss: 146.52763, Residuals: -1.44357, Convergence: 0.002819\n",
      "Epoch: 77, Loss: 146.13160, Residuals: -1.43043, Convergence: 0.002710\n",
      "Epoch: 78, Loss: 145.75178, Residuals: -1.41775, Convergence: 0.002606\n",
      "Epoch: 79, Loss: 145.38747, Residuals: -1.40553, Convergence: 0.002506\n",
      "Epoch: 80, Loss: 145.03806, Residuals: -1.39375, Convergence: 0.002409\n",
      "Epoch: 81, Loss: 144.70305, Residuals: -1.38240, Convergence: 0.002315\n",
      "Epoch: 82, Loss: 144.38197, Residuals: -1.37146, Convergence: 0.002224\n",
      "Epoch: 83, Loss: 144.07436, Residuals: -1.36092, Convergence: 0.002135\n",
      "Epoch: 84, Loss: 143.77982, Residuals: -1.35078, Convergence: 0.002049\n",
      "Epoch: 85, Loss: 143.49793, Residuals: -1.34102, Convergence: 0.001964\n",
      "Epoch: 86, Loss: 143.22828, Residuals: -1.33164, Convergence: 0.001883\n",
      "Epoch: 87, Loss: 142.97047, Residuals: -1.32261, Convergence: 0.001803\n",
      "Epoch: 88, Loss: 142.72409, Residuals: -1.31393, Convergence: 0.001726\n",
      "Epoch: 89, Loss: 142.48875, Residuals: -1.30560, Convergence: 0.001652\n",
      "Epoch: 90, Loss: 142.26407, Residuals: -1.29759, Convergence: 0.001579\n",
      "Epoch: 91, Loss: 142.04967, Residuals: -1.28989, Convergence: 0.001509\n",
      "Epoch: 92, Loss: 141.84519, Residuals: -1.28251, Convergence: 0.001442\n",
      "Epoch: 93, Loss: 141.65030, Residuals: -1.27542, Convergence: 0.001376\n",
      "Epoch: 94, Loss: 141.46468, Residuals: -1.26861, Convergence: 0.001312\n",
      "Epoch: 95, Loss: 141.28804, Residuals: -1.26208, Convergence: 0.001250\n",
      "Epoch: 96, Loss: 141.12009, Residuals: -1.25582, Convergence: 0.001190\n",
      "Epoch: 97, Loss: 140.96059, Residuals: -1.24982, Convergence: 0.001132\n",
      "Epoch: 98, Loss: 140.80931, Residuals: -1.24406, Convergence: 0.001074\n",
      "Epoch: 99, Loss: 140.66604, Residuals: -1.23855, Convergence: 0.001018\n",
      "Epoch: 100, Loss: 140.53060, Residuals: -1.23327, Convergence: 0.000964\n",
      "Evidence -181.091\n",
      "\n",
      "Epoch: 100, Evidence: -181.09129, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.23e-01\n",
      "Epoch: 100, Loss: 1379.88024, Residuals: -1.23327, Convergence:   inf\n",
      "Epoch: 101, Loss: 1318.61798, Residuals: -1.26369, Convergence: 0.046459\n",
      "Epoch: 102, Loss: 1272.00995, Residuals: -1.28733, Convergence: 0.036641\n",
      "Epoch: 103, Loss: 1236.81799, Residuals: -1.30377, Convergence: 0.028454\n",
      "Epoch: 104, Loss: 1209.41240, Residuals: -1.31468, Convergence: 0.022660\n",
      "Epoch: 105, Loss: 1187.17146, Residuals: -1.32224, Convergence: 0.018734\n",
      "Epoch: 106, Loss: 1168.61611, Residuals: -1.32762, Convergence: 0.015878\n",
      "Epoch: 107, Loss: 1152.87822, Residuals: -1.33134, Convergence: 0.013651\n",
      "Epoch: 108, Loss: 1139.37700, Residuals: -1.33369, Convergence: 0.011850\n",
      "Epoch: 109, Loss: 1127.68574, Residuals: -1.33486, Convergence: 0.010367\n",
      "Epoch: 110, Loss: 1117.47220, Residuals: -1.33499, Convergence: 0.009140\n",
      "Epoch: 111, Loss: 1108.47257, Residuals: -1.33421, Convergence: 0.008119\n",
      "Epoch: 112, Loss: 1100.46971, Residuals: -1.33265, Convergence: 0.007272\n",
      "Epoch: 113, Loss: 1093.28521, Residuals: -1.33039, Convergence: 0.006571\n",
      "Epoch: 114, Loss: 1086.76808, Residuals: -1.32752, Convergence: 0.005997\n",
      "Epoch: 115, Loss: 1080.79000, Residuals: -1.32411, Convergence: 0.005531\n",
      "Epoch: 116, Loss: 1075.23723, Residuals: -1.32021, Convergence: 0.005164\n",
      "Epoch: 117, Loss: 1070.00861, Residuals: -1.31587, Convergence: 0.004887\n",
      "Epoch: 118, Loss: 1065.00933, Residuals: -1.31110, Convergence: 0.004694\n",
      "Epoch: 119, Loss: 1060.14893, Residuals: -1.30591, Convergence: 0.004585\n",
      "Epoch: 120, Loss: 1055.34097, Residuals: -1.30029, Convergence: 0.004556\n",
      "Epoch: 121, Loss: 1050.50537, Residuals: -1.29425, Convergence: 0.004603\n",
      "Epoch: 122, Loss: 1045.57996, Residuals: -1.28780, Convergence: 0.004711\n",
      "Epoch: 123, Loss: 1040.53523, Residuals: -1.28096, Convergence: 0.004848\n",
      "Epoch: 124, Loss: 1035.39465, Residuals: -1.27379, Convergence: 0.004965\n",
      "Epoch: 125, Loss: 1030.23693, Residuals: -1.26638, Convergence: 0.005006\n",
      "Epoch: 126, Loss: 1025.17376, Residuals: -1.25880, Convergence: 0.004939\n",
      "Epoch: 127, Loss: 1020.31014, Residuals: -1.25110, Convergence: 0.004767\n",
      "Epoch: 128, Loss: 1015.71461, Residuals: -1.24336, Convergence: 0.004524\n",
      "Epoch: 129, Loss: 1011.41355, Residuals: -1.23562, Convergence: 0.004253\n",
      "Epoch: 130, Loss: 1007.40389, Residuals: -1.22794, Convergence: 0.003980\n",
      "Epoch: 131, Loss: 1003.66598, Residuals: -1.22036, Convergence: 0.003724\n",
      "Epoch: 132, Loss: 1000.17371, Residuals: -1.21290, Convergence: 0.003492\n",
      "Epoch: 133, Loss: 996.90121, Residuals: -1.20560, Convergence: 0.003283\n",
      "Epoch: 134, Loss: 993.82413, Residuals: -1.19847, Convergence: 0.003096\n",
      "Epoch: 135, Loss: 990.92157, Residuals: -1.19154, Convergence: 0.002929\n",
      "Epoch: 136, Loss: 988.17544, Residuals: -1.18481, Convergence: 0.002779\n",
      "Epoch: 137, Loss: 985.57107, Residuals: -1.17830, Convergence: 0.002642\n",
      "Epoch: 138, Loss: 983.09521, Residuals: -1.17200, Convergence: 0.002518\n",
      "Epoch: 139, Loss: 980.73813, Residuals: -1.16594, Convergence: 0.002403\n",
      "Epoch: 140, Loss: 978.49061, Residuals: -1.16011, Convergence: 0.002297\n",
      "Epoch: 141, Loss: 976.34520, Residuals: -1.15450, Convergence: 0.002197\n",
      "Epoch: 142, Loss: 974.29543, Residuals: -1.14913, Convergence: 0.002104\n",
      "Epoch: 143, Loss: 972.33503, Residuals: -1.14398, Convergence: 0.002016\n",
      "Epoch: 144, Loss: 970.45933, Residuals: -1.13905, Convergence: 0.001933\n",
      "Epoch: 145, Loss: 968.66326, Residuals: -1.13434, Convergence: 0.001854\n",
      "Epoch: 146, Loss: 966.94223, Residuals: -1.12984, Convergence: 0.001780\n",
      "Epoch: 147, Loss: 965.29184, Residuals: -1.12554, Convergence: 0.001710\n",
      "Epoch: 148, Loss: 963.70810, Residuals: -1.12144, Convergence: 0.001643\n",
      "Epoch: 149, Loss: 962.18669, Residuals: -1.11752, Convergence: 0.001581\n",
      "Epoch: 150, Loss: 960.72388, Residuals: -1.11378, Convergence: 0.001523\n",
      "Epoch: 151, Loss: 959.31604, Residuals: -1.11021, Convergence: 0.001468\n",
      "Epoch: 152, Loss: 957.95980, Residuals: -1.10680, Convergence: 0.001416\n",
      "Epoch: 153, Loss: 956.65158, Residuals: -1.10354, Convergence: 0.001368\n",
      "Epoch: 154, Loss: 955.38777, Residuals: -1.10043, Convergence: 0.001323\n",
      "Epoch: 155, Loss: 954.16589, Residuals: -1.09744, Convergence: 0.001281\n",
      "Epoch: 156, Loss: 952.98202, Residuals: -1.09459, Convergence: 0.001242\n",
      "Epoch: 157, Loss: 951.83332, Residuals: -1.09184, Convergence: 0.001207\n",
      "Epoch: 158, Loss: 950.71641, Residuals: -1.08921, Convergence: 0.001175\n",
      "Epoch: 159, Loss: 949.62772, Residuals: -1.08667, Convergence: 0.001146\n",
      "Epoch: 160, Loss: 948.56371, Residuals: -1.08423, Convergence: 0.001122\n",
      "Epoch: 161, Loss: 947.52072, Residuals: -1.08186, Convergence: 0.001101\n",
      "Epoch: 162, Loss: 946.49469, Residuals: -1.07957, Convergence: 0.001084\n",
      "Epoch: 163, Loss: 945.48177, Residuals: -1.07734, Convergence: 0.001071\n",
      "Epoch: 164, Loss: 944.47817, Residuals: -1.07515, Convergence: 0.001063\n",
      "Epoch: 165, Loss: 943.48049, Residuals: -1.07302, Convergence: 0.001057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 166, Loss: 942.48587, Residuals: -1.07091, Convergence: 0.001055\n",
      "Epoch: 167, Loss: 941.49301, Residuals: -1.06883, Convergence: 0.001055\n",
      "Epoch: 168, Loss: 940.50161, Residuals: -1.06678, Convergence: 0.001054\n",
      "Epoch: 169, Loss: 939.51328, Residuals: -1.06475, Convergence: 0.001052\n",
      "Epoch: 170, Loss: 938.53108, Residuals: -1.06275, Convergence: 0.001047\n",
      "Epoch: 171, Loss: 937.55959, Residuals: -1.06078, Convergence: 0.001036\n",
      "Epoch: 172, Loss: 936.60365, Residuals: -1.05884, Convergence: 0.001021\n",
      "Epoch: 173, Loss: 935.66839, Residuals: -1.05695, Convergence: 0.001000\n",
      "Evidence 11227.383\n",
      "\n",
      "Epoch: 173, Evidence: 11227.38281, Convergence: 1.016129\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.73e-01\n",
      "Epoch: 173, Loss: 2350.00771, Residuals: -1.05695, Convergence:   inf\n",
      "Epoch: 174, Loss: 2307.69180, Residuals: -1.06730, Convergence: 0.018337\n",
      "Epoch: 175, Loss: 2279.99821, Residuals: -1.06596, Convergence: 0.012146\n",
      "Epoch: 176, Loss: 2257.19919, Residuals: -1.06378, Convergence: 0.010101\n",
      "Epoch: 177, Loss: 2238.12112, Residuals: -1.06137, Convergence: 0.008524\n",
      "Epoch: 178, Loss: 2222.00010, Residuals: -1.05886, Convergence: 0.007255\n",
      "Epoch: 179, Loss: 2208.25930, Residuals: -1.05631, Convergence: 0.006222\n",
      "Epoch: 180, Loss: 2196.43935, Residuals: -1.05375, Convergence: 0.005381\n",
      "Epoch: 181, Loss: 2186.16755, Residuals: -1.05118, Convergence: 0.004699\n",
      "Epoch: 182, Loss: 2177.13529, Residuals: -1.04860, Convergence: 0.004149\n",
      "Epoch: 183, Loss: 2169.08629, Residuals: -1.04598, Convergence: 0.003711\n",
      "Epoch: 184, Loss: 2161.81439, Residuals: -1.04329, Convergence: 0.003364\n",
      "Epoch: 185, Loss: 2155.16186, Residuals: -1.04052, Convergence: 0.003087\n",
      "Epoch: 186, Loss: 2149.02004, Residuals: -1.03765, Convergence: 0.002858\n",
      "Epoch: 187, Loss: 2143.32479, Residuals: -1.03472, Convergence: 0.002657\n",
      "Epoch: 188, Loss: 2138.03824, Residuals: -1.03176, Convergence: 0.002473\n",
      "Epoch: 189, Loss: 2133.13583, Residuals: -1.02881, Convergence: 0.002298\n",
      "Epoch: 190, Loss: 2128.59247, Residuals: -1.02590, Convergence: 0.002134\n",
      "Epoch: 191, Loss: 2124.38271, Residuals: -1.02306, Convergence: 0.001982\n",
      "Epoch: 192, Loss: 2120.47688, Residuals: -1.02033, Convergence: 0.001842\n",
      "Epoch: 193, Loss: 2116.84429, Residuals: -1.01769, Convergence: 0.001716\n",
      "Epoch: 194, Loss: 2113.45673, Residuals: -1.01517, Convergence: 0.001603\n",
      "Epoch: 195, Loss: 2110.28583, Residuals: -1.01276, Convergence: 0.001503\n",
      "Epoch: 196, Loss: 2107.30718, Residuals: -1.01045, Convergence: 0.001413\n",
      "Epoch: 197, Loss: 2104.49937, Residuals: -1.00825, Convergence: 0.001334\n",
      "Epoch: 198, Loss: 2101.84528, Residuals: -1.00614, Convergence: 0.001263\n",
      "Epoch: 199, Loss: 2099.33143, Residuals: -1.00411, Convergence: 0.001197\n",
      "Epoch: 200, Loss: 2096.94651, Residuals: -1.00218, Convergence: 0.001137\n",
      "Epoch: 201, Loss: 2094.68224, Residuals: -1.00032, Convergence: 0.001081\n",
      "Epoch: 202, Loss: 2092.53160, Residuals: -0.99854, Convergence: 0.001028\n",
      "Epoch: 203, Loss: 2090.49047, Residuals: -0.99683, Convergence: 0.000976\n",
      "Evidence 14376.902\n",
      "\n",
      "Epoch: 203, Evidence: 14376.90234, Convergence: 0.219068\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.36e-01\n",
      "Epoch: 203, Loss: 2470.31670, Residuals: -0.99683, Convergence:   inf\n",
      "Epoch: 204, Loss: 2456.49436, Residuals: -0.99486, Convergence: 0.005627\n",
      "Epoch: 205, Loss: 2445.21709, Residuals: -0.99196, Convergence: 0.004612\n",
      "Epoch: 206, Loss: 2435.35528, Residuals: -0.98898, Convergence: 0.004049\n",
      "Epoch: 207, Loss: 2426.67366, Residuals: -0.98614, Convergence: 0.003578\n",
      "Epoch: 208, Loss: 2419.01356, Residuals: -0.98346, Convergence: 0.003167\n",
      "Epoch: 209, Loss: 2412.24427, Residuals: -0.98095, Convergence: 0.002806\n",
      "Epoch: 210, Loss: 2406.24978, Residuals: -0.97861, Convergence: 0.002491\n",
      "Epoch: 211, Loss: 2400.92690, Residuals: -0.97644, Convergence: 0.002217\n",
      "Epoch: 212, Loss: 2396.18474, Residuals: -0.97441, Convergence: 0.001979\n",
      "Epoch: 213, Loss: 2391.94266, Residuals: -0.97253, Convergence: 0.001773\n",
      "Epoch: 214, Loss: 2388.13142, Residuals: -0.97079, Convergence: 0.001596\n",
      "Epoch: 215, Loss: 2384.69131, Residuals: -0.96917, Convergence: 0.001443\n",
      "Epoch: 216, Loss: 2381.57144, Residuals: -0.96767, Convergence: 0.001310\n",
      "Epoch: 217, Loss: 2378.72691, Residuals: -0.96627, Convergence: 0.001196\n",
      "Epoch: 218, Loss: 2376.12195, Residuals: -0.96498, Convergence: 0.001096\n",
      "Epoch: 219, Loss: 2373.72651, Residuals: -0.96379, Convergence: 0.001009\n",
      "Epoch: 220, Loss: 2371.51174, Residuals: -0.96268, Convergence: 0.000934\n",
      "Evidence 14765.932\n",
      "\n",
      "Epoch: 220, Evidence: 14765.93164, Convergence: 0.026346\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.34e-01\n",
      "Epoch: 220, Loss: 2474.38400, Residuals: -0.96268, Convergence:   inf\n",
      "Epoch: 221, Loss: 2467.55198, Residuals: -0.95966, Convergence: 0.002769\n",
      "Epoch: 222, Loss: 2461.85649, Residuals: -0.95700, Convergence: 0.002313\n",
      "Epoch: 223, Loss: 2456.98691, Residuals: -0.95474, Convergence: 0.001982\n",
      "Epoch: 224, Loss: 2452.78162, Residuals: -0.95282, Convergence: 0.001715\n",
      "Epoch: 225, Loss: 2449.11346, Residuals: -0.95118, Convergence: 0.001498\n",
      "Epoch: 226, Loss: 2445.88262, Residuals: -0.94978, Convergence: 0.001321\n",
      "Epoch: 227, Loss: 2443.00874, Residuals: -0.94858, Convergence: 0.001176\n",
      "Epoch: 228, Loss: 2440.42740, Residuals: -0.94756, Convergence: 0.001058\n",
      "Epoch: 229, Loss: 2438.08955, Residuals: -0.94669, Convergence: 0.000959\n",
      "Evidence 14857.326\n",
      "\n",
      "Epoch: 229, Evidence: 14857.32617, Convergence: 0.006151\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.63e-01\n",
      "Epoch: 229, Loss: 2475.84964, Residuals: -0.94669, Convergence:   inf\n",
      "Epoch: 230, Loss: 2471.77854, Residuals: -0.94444, Convergence: 0.001647\n",
      "Epoch: 231, Loss: 2468.36011, Residuals: -0.94270, Convergence: 0.001385\n",
      "Epoch: 232, Loss: 2465.42666, Residuals: -0.94134, Convergence: 0.001190\n",
      "Epoch: 233, Loss: 2462.86996, Residuals: -0.94027, Convergence: 0.001038\n",
      "Epoch: 234, Loss: 2460.61045, Residuals: -0.93942, Convergence: 0.000918\n",
      "Evidence 14890.013\n",
      "\n",
      "Epoch: 234, Evidence: 14890.01270, Convergence: 0.002195\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.12e-01\n",
      "Epoch: 234, Loss: 2476.80465, Residuals: -0.93942, Convergence:   inf\n",
      "Epoch: 235, Loss: 2473.88021, Residuals: -0.93788, Convergence: 0.001182\n",
      "Epoch: 236, Loss: 2471.41836, Residuals: -0.93677, Convergence: 0.000996\n",
      "Evidence 14903.057\n",
      "\n",
      "Epoch: 236, Evidence: 14903.05664, Convergence: 0.000875\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.77e-01\n",
      "Epoch: 236, Loss: 2477.53878, Residuals: -0.93677, Convergence:   inf\n",
      "Epoch: 237, Loss: 2472.97185, Residuals: -0.93514, Convergence: 0.001847\n",
      "Epoch: 238, Loss: 2469.45875, Residuals: -0.93416, Convergence: 0.001423\n",
      "Epoch: 239, Loss: 2466.63576, Residuals: -0.93360, Convergence: 0.001144\n",
      "Epoch: 240, Loss: 2464.25900, Residuals: -0.93339, Convergence: 0.000964\n",
      "Evidence 14920.948\n",
      "\n",
      "Epoch: 240, Evidence: 14920.94824, Convergence: 0.002073\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.47e-01\n",
      "Epoch: 240, Loss: 2477.80011, Residuals: -0.93339, Convergence:   inf\n",
      "Epoch: 241, Loss: 2474.77270, Residuals: -0.93200, Convergence: 0.001223\n",
      "Epoch: 242, Loss: 2472.37853, Residuals: -0.93151, Convergence: 0.000968\n",
      "Evidence 14931.990\n",
      "\n",
      "Epoch: 242, Evidence: 14931.99023, Convergence: 0.000739\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.24e-01\n",
      "Epoch: 242, Loss: 2478.02423, Residuals: -0.93151, Convergence:   inf\n",
      "Epoch: 243, Loss: 2473.61182, Residuals: -0.92990, Convergence: 0.001784\n",
      "Epoch: 244, Loss: 2470.44670, Residuals: -0.93170, Convergence: 0.001281\n",
      "Epoch: 245, Loss: 2467.86054, Residuals: -0.93220, Convergence: 0.001048\n",
      "Epoch: 246, Loss: 2465.60709, Residuals: -0.93455, Convergence: 0.000914\n",
      "Evidence 14947.549\n",
      "\n",
      "Epoch: 246, Evidence: 14947.54883, Convergence: 0.001780\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.10e-01\n",
      "Epoch: 246, Loss: 2477.37839, Residuals: -0.93455, Convergence:   inf\n",
      "Epoch: 247, Loss: 2475.61180, Residuals: -0.93355, Convergence: 0.000714\n",
      "Evidence 14954.080\n",
      "\n",
      "Epoch: 247, Evidence: 14954.08008, Convergence: 0.000437\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 9.03e-02\n",
      "Epoch: 247, Loss: 2478.32461, Residuals: -0.93355, Convergence:   inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 248, Loss: 2528.31439, Residuals: -0.97511, Convergence: -0.019772\n",
      "Epoch: 248, Loss: 2475.85204, Residuals: -0.93225, Convergence: 0.000999\n",
      "Evidence 14958.482\n",
      "\n",
      "Epoch: 248, Evidence: 14958.48242, Convergence: 0.000731\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.26e-02\n",
      "Epoch: 248, Loss: 2477.80180, Residuals: -0.93225, Convergence:   inf\n",
      "Epoch: 249, Loss: 2479.61958, Residuals: -0.93529, Convergence: -0.000733\n",
      "Evidence 14958.207\n",
      "\n",
      "Epoch: 249, Evidence: 14958.20703, Convergence: 0.000713\n",
      "Total samples: 183, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 382.02594, Residuals: -4.51148, Convergence:   inf\n",
      "Epoch: 1, Loss: 356.25097, Residuals: -4.39173, Convergence: 0.072351\n",
      "Epoch: 2, Loss: 335.16147, Residuals: -4.22823, Convergence: 0.062923\n",
      "Epoch: 3, Loss: 319.09877, Residuals: -4.06484, Convergence: 0.050338\n",
      "Epoch: 4, Loss: 306.85501, Residuals: -3.92111, Convergence: 0.039901\n",
      "Epoch: 5, Loss: 297.14639, Residuals: -3.79409, Convergence: 0.032673\n",
      "Epoch: 6, Loss: 289.26397, Residuals: -3.68340, Convergence: 0.027250\n",
      "Epoch: 7, Loss: 282.71929, Residuals: -3.58855, Convergence: 0.023149\n",
      "Epoch: 8, Loss: 277.15071, Residuals: -3.50752, Convergence: 0.020092\n",
      "Epoch: 9, Loss: 272.30264, Residuals: -3.43799, Convergence: 0.017804\n",
      "Epoch: 10, Loss: 267.99314, Residuals: -3.37789, Convergence: 0.016081\n",
      "Epoch: 11, Loss: 264.09008, Residuals: -3.32545, Convergence: 0.014779\n",
      "Epoch: 12, Loss: 260.49651, Residuals: -3.27916, Convergence: 0.013795\n",
      "Epoch: 13, Loss: 257.14155, Residuals: -3.23771, Convergence: 0.013047\n",
      "Epoch: 14, Loss: 253.97478, Residuals: -3.19996, Convergence: 0.012469\n",
      "Epoch: 15, Loss: 250.96381, Residuals: -3.16498, Convergence: 0.011998\n",
      "Epoch: 16, Loss: 248.09189, Residuals: -3.13215, Convergence: 0.011576\n",
      "Epoch: 17, Loss: 245.34762, Residuals: -3.10106, Convergence: 0.011185\n",
      "Epoch: 18, Loss: 242.71107, Residuals: -3.07129, Convergence: 0.010863\n",
      "Epoch: 19, Loss: 240.15083, Residuals: -3.04230, Convergence: 0.010661\n",
      "Epoch: 20, Loss: 237.63085, Residuals: -3.01347, Convergence: 0.010605\n",
      "Epoch: 21, Loss: 235.11818, Residuals: -2.98423, Convergence: 0.010687\n",
      "Epoch: 22, Loss: 232.58735, Residuals: -2.95416, Convergence: 0.010881\n",
      "Epoch: 23, Loss: 230.01496, Residuals: -2.92296, Convergence: 0.011184\n",
      "Epoch: 24, Loss: 227.36424, Residuals: -2.89018, Convergence: 0.011658\n",
      "Epoch: 25, Loss: 224.58275, Residuals: -2.85524, Convergence: 0.012385\n",
      "Epoch: 26, Loss: 221.65764, Residuals: -2.81792, Convergence: 0.013197\n",
      "Epoch: 27, Loss: 218.69594, Residuals: -2.77932, Convergence: 0.013543\n",
      "Epoch: 28, Loss: 215.81991, Residuals: -2.74092, Convergence: 0.013326\n",
      "Epoch: 29, Loss: 213.05908, Residuals: -2.70323, Convergence: 0.012958\n",
      "Epoch: 30, Loss: 210.40054, Residuals: -2.66625, Convergence: 0.012636\n",
      "Epoch: 31, Loss: 207.82692, Residuals: -2.62989, Convergence: 0.012383\n",
      "Epoch: 32, Loss: 205.32474, Residuals: -2.59403, Convergence: 0.012186\n",
      "Epoch: 33, Loss: 202.88486, Residuals: -2.55861, Convergence: 0.012026\n",
      "Epoch: 34, Loss: 200.50153, Residuals: -2.52354, Convergence: 0.011887\n",
      "Epoch: 35, Loss: 198.17146, Residuals: -2.48879, Convergence: 0.011758\n",
      "Epoch: 36, Loss: 195.89294, Residuals: -2.45432, Convergence: 0.011631\n",
      "Epoch: 37, Loss: 193.66525, Residuals: -2.42011, Convergence: 0.011503\n",
      "Epoch: 38, Loss: 191.48825, Residuals: -2.38614, Convergence: 0.011369\n",
      "Epoch: 39, Loss: 189.36207, Residuals: -2.35239, Convergence: 0.011228\n",
      "Epoch: 40, Loss: 187.28692, Residuals: -2.31888, Convergence: 0.011080\n",
      "Epoch: 41, Loss: 185.26296, Residuals: -2.28559, Convergence: 0.010925\n",
      "Epoch: 42, Loss: 183.29023, Residuals: -2.25252, Convergence: 0.010763\n",
      "Epoch: 43, Loss: 181.36863, Residuals: -2.21970, Convergence: 0.010595\n",
      "Epoch: 44, Loss: 179.49808, Residuals: -2.18711, Convergence: 0.010421\n",
      "Epoch: 45, Loss: 177.67874, Residuals: -2.15476, Convergence: 0.010240\n",
      "Epoch: 46, Loss: 175.91132, Residuals: -2.12268, Convergence: 0.010047\n",
      "Epoch: 47, Loss: 174.19733, Residuals: -2.09088, Convergence: 0.009839\n",
      "Epoch: 48, Loss: 172.53894, Residuals: -2.05939, Convergence: 0.009612\n",
      "Epoch: 49, Loss: 170.93848, Residuals: -2.02826, Convergence: 0.009363\n",
      "Epoch: 50, Loss: 169.39785, Residuals: -1.99752, Convergence: 0.009095\n",
      "Epoch: 51, Loss: 167.91812, Residuals: -1.96722, Convergence: 0.008812\n",
      "Epoch: 52, Loss: 166.49940, Residuals: -1.93740, Convergence: 0.008521\n",
      "Epoch: 53, Loss: 165.14102, Residuals: -1.90809, Convergence: 0.008226\n",
      "Epoch: 54, Loss: 163.84175, Residuals: -1.87932, Convergence: 0.007930\n",
      "Epoch: 55, Loss: 162.60003, Residuals: -1.85112, Convergence: 0.007637\n",
      "Epoch: 56, Loss: 161.41410, Residuals: -1.82351, Convergence: 0.007347\n",
      "Epoch: 57, Loss: 160.28212, Residuals: -1.79651, Convergence: 0.007062\n",
      "Epoch: 58, Loss: 159.20226, Residuals: -1.77014, Convergence: 0.006783\n",
      "Epoch: 59, Loss: 158.17269, Residuals: -1.74442, Convergence: 0.006509\n",
      "Epoch: 60, Loss: 157.19164, Residuals: -1.71935, Convergence: 0.006241\n",
      "Epoch: 61, Loss: 156.25741, Residuals: -1.69496, Convergence: 0.005979\n",
      "Epoch: 62, Loss: 155.36834, Residuals: -1.67124, Convergence: 0.005722\n",
      "Epoch: 63, Loss: 154.52284, Residuals: -1.64822, Convergence: 0.005472\n",
      "Epoch: 64, Loss: 153.71937, Residuals: -1.62589, Convergence: 0.005227\n",
      "Epoch: 65, Loss: 152.95644, Residuals: -1.60426, Convergence: 0.004988\n",
      "Epoch: 66, Loss: 152.23265, Residuals: -1.58335, Convergence: 0.004755\n",
      "Epoch: 67, Loss: 151.54661, Residuals: -1.56314, Convergence: 0.004527\n",
      "Epoch: 68, Loss: 150.89702, Residuals: -1.54366, Convergence: 0.004305\n",
      "Epoch: 69, Loss: 150.28264, Residuals: -1.52488, Convergence: 0.004088\n",
      "Epoch: 70, Loss: 149.70227, Residuals: -1.50682, Convergence: 0.003877\n",
      "Epoch: 71, Loss: 149.15480, Residuals: -1.48948, Convergence: 0.003671\n",
      "Epoch: 72, Loss: 148.63911, Residuals: -1.47284, Convergence: 0.003469\n",
      "Epoch: 73, Loss: 148.15413, Residuals: -1.45692, Convergence: 0.003273\n",
      "Epoch: 74, Loss: 147.69880, Residuals: -1.44169, Convergence: 0.003083\n",
      "Epoch: 75, Loss: 147.27198, Residuals: -1.42715, Convergence: 0.002898\n",
      "Epoch: 76, Loss: 146.87246, Residuals: -1.41330, Convergence: 0.002720\n",
      "Epoch: 77, Loss: 146.49892, Residuals: -1.40012, Convergence: 0.002550\n",
      "Epoch: 78, Loss: 146.14988, Residuals: -1.38759, Convergence: 0.002388\n",
      "Epoch: 79, Loss: 145.82371, Residuals: -1.37568, Convergence: 0.002237\n",
      "Epoch: 80, Loss: 145.51861, Residuals: -1.36438, Convergence: 0.002097\n",
      "Epoch: 81, Loss: 145.23271, Residuals: -1.35365, Convergence: 0.001969\n",
      "Epoch: 82, Loss: 144.96407, Residuals: -1.34345, Convergence: 0.001853\n",
      "Epoch: 83, Loss: 144.71081, Residuals: -1.33375, Convergence: 0.001750\n",
      "Epoch: 84, Loss: 144.47120, Residuals: -1.32451, Convergence: 0.001659\n",
      "Epoch: 85, Loss: 144.24372, Residuals: -1.31568, Convergence: 0.001577\n",
      "Epoch: 86, Loss: 144.02710, Residuals: -1.30724, Convergence: 0.001504\n",
      "Epoch: 87, Loss: 143.82032, Residuals: -1.29916, Convergence: 0.001438\n",
      "Epoch: 88, Loss: 143.62259, Residuals: -1.29142, Convergence: 0.001377\n",
      "Epoch: 89, Loss: 143.43333, Residuals: -1.28398, Convergence: 0.001320\n",
      "Epoch: 90, Loss: 143.25211, Residuals: -1.27684, Convergence: 0.001265\n",
      "Epoch: 91, Loss: 143.07863, Residuals: -1.26998, Convergence: 0.001212\n",
      "Epoch: 92, Loss: 142.91268, Residuals: -1.26340, Convergence: 0.001161\n",
      "Epoch: 93, Loss: 142.75410, Residuals: -1.25707, Convergence: 0.001111\n",
      "Epoch: 94, Loss: 142.60277, Residuals: -1.25099, Convergence: 0.001061\n",
      "Epoch: 95, Loss: 142.45859, Residuals: -1.24517, Convergence: 0.001012\n",
      "Epoch: 96, Loss: 142.32149, Residuals: -1.23957, Convergence: 0.000963\n",
      "Evidence -184.064\n",
      "\n",
      "Epoch: 96, Evidence: -184.06438, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.24e-01\n",
      "Epoch: 96, Loss: 1366.23604, Residuals: -1.23957, Convergence:   inf\n",
      "Epoch: 97, Loss: 1302.82388, Residuals: -1.27349, Convergence: 0.048673\n",
      "Epoch: 98, Loss: 1254.75750, Residuals: -1.29889, Convergence: 0.038307\n",
      "Epoch: 99, Loss: 1218.42952, Residuals: -1.31614, Convergence: 0.029815\n",
      "Epoch: 100, Loss: 1190.04154, Residuals: -1.32757, Convergence: 0.023855\n",
      "Epoch: 101, Loss: 1166.93714, Residuals: -1.33552, Convergence: 0.019799\n",
      "Epoch: 102, Loss: 1147.62274, Residuals: -1.34124, Convergence: 0.016830\n",
      "Epoch: 103, Loss: 1131.20288, Residuals: -1.34525, Convergence: 0.014515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 104, Loss: 1117.06520, Residuals: -1.34783, Convergence: 0.012656\n",
      "Epoch: 105, Loss: 1104.74932, Residuals: -1.34916, Convergence: 0.011148\n",
      "Epoch: 106, Loss: 1093.89192, Residuals: -1.34940, Convergence: 0.009925\n",
      "Epoch: 107, Loss: 1084.19607, Residuals: -1.34869, Convergence: 0.008943\n",
      "Epoch: 108, Loss: 1075.41735, Residuals: -1.34717, Convergence: 0.008163\n",
      "Epoch: 109, Loss: 1067.35631, Residuals: -1.34496, Convergence: 0.007552\n",
      "Epoch: 110, Loss: 1059.86426, Residuals: -1.34217, Convergence: 0.007069\n",
      "Epoch: 111, Loss: 1052.84683, Residuals: -1.33882, Convergence: 0.006665\n",
      "Epoch: 112, Loss: 1046.25091, Residuals: -1.33482, Convergence: 0.006304\n",
      "Epoch: 113, Loss: 1040.03074, Residuals: -1.33012, Convergence: 0.005981\n",
      "Epoch: 114, Loss: 1034.12826, Residuals: -1.32470, Convergence: 0.005708\n",
      "Epoch: 115, Loss: 1028.47169, Residuals: -1.31861, Convergence: 0.005500\n",
      "Epoch: 116, Loss: 1022.98732, Residuals: -1.31191, Convergence: 0.005361\n",
      "Epoch: 117, Loss: 1017.60991, Residuals: -1.30469, Convergence: 0.005284\n",
      "Epoch: 118, Loss: 1012.30048, Residuals: -1.29702, Convergence: 0.005245\n",
      "Epoch: 119, Loss: 1007.05919, Residuals: -1.28900, Convergence: 0.005205\n",
      "Epoch: 120, Loss: 1001.92828, Residuals: -1.28073, Convergence: 0.005121\n",
      "Epoch: 121, Loss: 996.97485, Residuals: -1.27231, Convergence: 0.004968\n",
      "Epoch: 122, Loss: 992.26336, Residuals: -1.26383, Convergence: 0.004748\n",
      "Epoch: 123, Loss: 987.83399, Residuals: -1.25534, Convergence: 0.004484\n",
      "Epoch: 124, Loss: 983.70020, Residuals: -1.24690, Convergence: 0.004202\n",
      "Epoch: 125, Loss: 979.85300, Residuals: -1.23856, Convergence: 0.003926\n",
      "Epoch: 126, Loss: 976.27163, Residuals: -1.23036, Convergence: 0.003668\n",
      "Epoch: 127, Loss: 972.93109, Residuals: -1.22234, Convergence: 0.003433\n",
      "Epoch: 128, Loss: 969.80626, Residuals: -1.21450, Convergence: 0.003222\n",
      "Epoch: 129, Loss: 966.87528, Residuals: -1.20689, Convergence: 0.003031\n",
      "Epoch: 130, Loss: 964.11841, Residuals: -1.19950, Convergence: 0.002859\n",
      "Epoch: 131, Loss: 961.51989, Residuals: -1.19236, Convergence: 0.002703\n",
      "Epoch: 132, Loss: 959.06646, Residuals: -1.18547, Convergence: 0.002558\n",
      "Epoch: 133, Loss: 956.74705, Residuals: -1.17883, Convergence: 0.002424\n",
      "Epoch: 134, Loss: 954.55261, Residuals: -1.17246, Convergence: 0.002299\n",
      "Epoch: 135, Loss: 952.47476, Residuals: -1.16636, Convergence: 0.002182\n",
      "Epoch: 136, Loss: 950.50659, Residuals: -1.16052, Convergence: 0.002071\n",
      "Epoch: 137, Loss: 948.64182, Residuals: -1.15494, Convergence: 0.001966\n",
      "Epoch: 138, Loss: 946.87399, Residuals: -1.14962, Convergence: 0.001867\n",
      "Epoch: 139, Loss: 945.19699, Residuals: -1.14455, Convergence: 0.001774\n",
      "Epoch: 140, Loss: 943.60523, Residuals: -1.13973, Convergence: 0.001687\n",
      "Epoch: 141, Loss: 942.09292, Residuals: -1.13514, Convergence: 0.001605\n",
      "Epoch: 142, Loss: 940.65418, Residuals: -1.13078, Convergence: 0.001530\n",
      "Epoch: 143, Loss: 939.28364, Residuals: -1.12663, Convergence: 0.001459\n",
      "Epoch: 144, Loss: 937.97600, Residuals: -1.12268, Convergence: 0.001394\n",
      "Epoch: 145, Loss: 936.72615, Residuals: -1.11893, Convergence: 0.001334\n",
      "Epoch: 146, Loss: 935.52940, Residuals: -1.11536, Convergence: 0.001279\n",
      "Epoch: 147, Loss: 934.38073, Residuals: -1.11196, Convergence: 0.001229\n",
      "Epoch: 148, Loss: 933.27625, Residuals: -1.10872, Convergence: 0.001183\n",
      "Epoch: 149, Loss: 932.21129, Residuals: -1.10563, Convergence: 0.001142\n",
      "Epoch: 150, Loss: 931.18269, Residuals: -1.10268, Convergence: 0.001105\n",
      "Epoch: 151, Loss: 930.18642, Residuals: -1.09986, Convergence: 0.001071\n",
      "Epoch: 152, Loss: 929.21945, Residuals: -1.09716, Convergence: 0.001041\n",
      "Epoch: 153, Loss: 928.27867, Residuals: -1.09457, Convergence: 0.001013\n",
      "Epoch: 154, Loss: 927.36142, Residuals: -1.09207, Convergence: 0.000989\n",
      "Evidence 11077.575\n",
      "\n",
      "Epoch: 154, Evidence: 11077.57520, Convergence: 1.016616\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 5.84e-01\n",
      "Epoch: 154, Loss: 2344.66137, Residuals: -1.09207, Convergence:   inf\n",
      "Epoch: 155, Loss: 2303.99802, Residuals: -1.10472, Convergence: 0.017649\n",
      "Epoch: 156, Loss: 2276.14331, Residuals: -1.10456, Convergence: 0.012238\n",
      "Epoch: 157, Loss: 2252.91991, Residuals: -1.10277, Convergence: 0.010308\n",
      "Epoch: 158, Loss: 2233.02266, Residuals: -1.10044, Convergence: 0.008910\n",
      "Epoch: 159, Loss: 2215.72201, Residuals: -1.09779, Convergence: 0.007808\n",
      "Epoch: 160, Loss: 2200.52812, Residuals: -1.09490, Convergence: 0.006905\n",
      "Epoch: 161, Loss: 2187.08031, Residuals: -1.09182, Convergence: 0.006149\n",
      "Epoch: 162, Loss: 2175.09429, Residuals: -1.08857, Convergence: 0.005511\n",
      "Epoch: 163, Loss: 2164.33416, Residuals: -1.08518, Convergence: 0.004972\n",
      "Epoch: 164, Loss: 2154.59320, Residuals: -1.08163, Convergence: 0.004521\n",
      "Epoch: 165, Loss: 2145.69319, Residuals: -1.07794, Convergence: 0.004148\n",
      "Epoch: 166, Loss: 2137.47955, Residuals: -1.07411, Convergence: 0.003843\n",
      "Epoch: 167, Loss: 2129.82619, Residuals: -1.07012, Convergence: 0.003593\n",
      "Epoch: 168, Loss: 2122.64142, Residuals: -1.06599, Convergence: 0.003385\n",
      "Epoch: 169, Loss: 2115.87085, Residuals: -1.06175, Convergence: 0.003200\n",
      "Epoch: 170, Loss: 2109.49258, Residuals: -1.05743, Convergence: 0.003024\n",
      "Epoch: 171, Loss: 2103.50255, Residuals: -1.05309, Convergence: 0.002848\n",
      "Epoch: 172, Loss: 2097.90161, Residuals: -1.04880, Convergence: 0.002670\n",
      "Epoch: 173, Loss: 2092.68700, Residuals: -1.04459, Convergence: 0.002492\n",
      "Epoch: 174, Loss: 2087.84861, Residuals: -1.04053, Convergence: 0.002317\n",
      "Epoch: 175, Loss: 2083.37070, Residuals: -1.03662, Convergence: 0.002149\n",
      "Epoch: 176, Loss: 2079.23410, Residuals: -1.03289, Convergence: 0.001989\n",
      "Epoch: 177, Loss: 2075.41587, Residuals: -1.02935, Convergence: 0.001840\n",
      "Epoch: 178, Loss: 2071.89263, Residuals: -1.02601, Convergence: 0.001700\n",
      "Epoch: 179, Loss: 2068.64155, Residuals: -1.02286, Convergence: 0.001572\n",
      "Epoch: 180, Loss: 2065.63990, Residuals: -1.01989, Convergence: 0.001453\n",
      "Epoch: 181, Loss: 2062.86515, Residuals: -1.01710, Convergence: 0.001345\n",
      "Epoch: 182, Loss: 2060.29585, Residuals: -1.01447, Convergence: 0.001247\n",
      "Epoch: 183, Loss: 2057.91274, Residuals: -1.01199, Convergence: 0.001158\n",
      "Epoch: 184, Loss: 2055.69828, Residuals: -1.00965, Convergence: 0.001077\n",
      "Epoch: 185, Loss: 2053.63507, Residuals: -1.00744, Convergence: 0.001005\n",
      "Epoch: 186, Loss: 2051.70844, Residuals: -1.00535, Convergence: 0.000939\n",
      "Evidence 14301.275\n",
      "\n",
      "Epoch: 186, Evidence: 14301.27539, Convergence: 0.225413\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 4.46e-01\n",
      "Epoch: 186, Loss: 2474.00192, Residuals: -1.00535, Convergence:   inf\n",
      "Epoch: 187, Loss: 2459.08295, Residuals: -1.00378, Convergence: 0.006067\n",
      "Epoch: 188, Loss: 2447.15907, Residuals: -1.00051, Convergence: 0.004873\n",
      "Epoch: 189, Loss: 2436.96094, Residuals: -0.99711, Convergence: 0.004185\n",
      "Epoch: 190, Loss: 2428.14783, Residuals: -0.99380, Convergence: 0.003630\n",
      "Epoch: 191, Loss: 2420.48294, Residuals: -0.99065, Convergence: 0.003167\n",
      "Epoch: 192, Loss: 2413.78154, Residuals: -0.98768, Convergence: 0.002776\n",
      "Epoch: 193, Loss: 2407.89072, Residuals: -0.98489, Convergence: 0.002446\n",
      "Epoch: 194, Loss: 2402.68393, Residuals: -0.98227, Convergence: 0.002167\n",
      "Epoch: 195, Loss: 2398.05352, Residuals: -0.97981, Convergence: 0.001931\n",
      "Epoch: 196, Loss: 2393.91000, Residuals: -0.97750, Convergence: 0.001731\n",
      "Epoch: 197, Loss: 2390.17876, Residuals: -0.97534, Convergence: 0.001561\n",
      "Epoch: 198, Loss: 2386.79775, Residuals: -0.97332, Convergence: 0.001417\n",
      "Epoch: 199, Loss: 2383.71539, Residuals: -0.97142, Convergence: 0.001293\n",
      "Epoch: 200, Loss: 2380.89025, Residuals: -0.96965, Convergence: 0.001187\n",
      "Epoch: 201, Loss: 2378.28638, Residuals: -0.96799, Convergence: 0.001095\n",
      "Epoch: 202, Loss: 2375.87562, Residuals: -0.96644, Convergence: 0.001015\n",
      "Epoch: 203, Loss: 2373.63451, Residuals: -0.96499, Convergence: 0.000944\n",
      "Evidence 14749.951\n",
      "\n",
      "Epoch: 203, Evidence: 14749.95117, Convergence: 0.030419\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 3.41e-01\n",
      "Epoch: 203, Loss: 2479.88359, Residuals: -0.96499, Convergence:   inf\n",
      "Epoch: 204, Loss: 2473.32853, Residuals: -0.96200, Convergence: 0.002650\n",
      "Epoch: 205, Loss: 2467.91024, Residuals: -0.95895, Convergence: 0.002195\n",
      "Epoch: 206, Loss: 2463.28127, Residuals: -0.95621, Convergence: 0.001879\n",
      "Epoch: 207, Loss: 2459.27338, Residuals: -0.95377, Convergence: 0.001630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 208, Loss: 2455.76469, Residuals: -0.95160, Convergence: 0.001429\n",
      "Epoch: 209, Loss: 2452.65584, Residuals: -0.94967, Convergence: 0.001268\n",
      "Epoch: 210, Loss: 2449.87317, Residuals: -0.94796, Convergence: 0.001136\n",
      "Epoch: 211, Loss: 2447.35576, Residuals: -0.94646, Convergence: 0.001029\n",
      "Epoch: 212, Loss: 2445.05837, Residuals: -0.94512, Convergence: 0.000940\n",
      "Evidence 14838.895\n",
      "\n",
      "Epoch: 212, Evidence: 14838.89453, Convergence: 0.005994\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.68e-01\n",
      "Epoch: 212, Loss: 2481.54218, Residuals: -0.94512, Convergence:   inf\n",
      "Epoch: 213, Loss: 2477.70352, Residuals: -0.94259, Convergence: 0.001549\n",
      "Epoch: 214, Loss: 2474.48441, Residuals: -0.94035, Convergence: 0.001301\n",
      "Epoch: 215, Loss: 2471.70345, Residuals: -0.93844, Convergence: 0.001125\n",
      "Epoch: 216, Loss: 2469.25926, Residuals: -0.93681, Convergence: 0.000990\n",
      "Evidence 14867.457\n",
      "\n",
      "Epoch: 216, Evidence: 14867.45703, Convergence: 0.001921\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.18e-01\n",
      "Epoch: 216, Loss: 2482.60344, Residuals: -0.93681, Convergence:   inf\n",
      "Epoch: 217, Loss: 2479.71834, Residuals: -0.93466, Convergence: 0.001163\n",
      "Epoch: 218, Loss: 2477.27355, Residuals: -0.93282, Convergence: 0.000987\n",
      "Evidence 14879.463\n",
      "\n",
      "Epoch: 218, Evidence: 14879.46289, Convergence: 0.000807\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.81e-01\n",
      "Epoch: 218, Loss: 2483.37627, Residuals: -0.93282, Convergence:   inf\n",
      "Epoch: 219, Loss: 2478.83529, Residuals: -0.92974, Convergence: 0.001832\n",
      "Epoch: 220, Loss: 2475.27200, Residuals: -0.92715, Convergence: 0.001440\n",
      "Epoch: 221, Loss: 2472.36240, Residuals: -0.92521, Convergence: 0.001177\n",
      "Epoch: 222, Loss: 2469.87667, Residuals: -0.92390, Convergence: 0.001006\n",
      "Epoch: 223, Loss: 2467.68381, Residuals: -0.92300, Convergence: 0.000889\n",
      "Evidence 14900.029\n",
      "\n",
      "Epoch: 223, Evidence: 14900.02930, Convergence: 0.002186\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.50e-01\n",
      "Epoch: 223, Loss: 2483.48900, Residuals: -0.92300, Convergence:   inf\n",
      "Epoch: 224, Loss: 2480.63034, Residuals: -0.92010, Convergence: 0.001152\n",
      "Epoch: 225, Loss: 2478.33042, Residuals: -0.91830, Convergence: 0.000928\n",
      "Evidence 14911.344\n",
      "\n",
      "Epoch: 225, Evidence: 14911.34375, Convergence: 0.000759\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.27e-01\n",
      "Epoch: 225, Loss: 2483.76767, Residuals: -0.91830, Convergence:   inf\n",
      "Epoch: 226, Loss: 2479.56171, Residuals: -0.91386, Convergence: 0.001696\n",
      "Epoch: 227, Loss: 2476.46391, Residuals: -0.91394, Convergence: 0.001251\n",
      "Epoch: 228, Loss: 2473.82223, Residuals: -0.91484, Convergence: 0.001068\n",
      "Epoch: 229, Loss: 2471.48527, Residuals: -0.91707, Convergence: 0.000946\n",
      "Evidence 14927.095\n",
      "\n",
      "Epoch: 229, Evidence: 14927.09473, Convergence: 0.001813\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.13e-01\n",
      "Epoch: 229, Loss: 2483.24443, Residuals: -0.91707, Convergence:   inf\n",
      "Epoch: 230, Loss: 2481.89807, Residuals: -0.91691, Convergence: 0.000542\n",
      "Evidence 14933.059\n",
      "\n",
      "Epoch: 230, Evidence: 14933.05859, Convergence: 0.000399\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 9.15e-02\n",
      "Epoch: 230, Loss: 2484.23963, Residuals: -0.91691, Convergence:   inf\n",
      "Epoch: 231, Loss: 2533.00697, Residuals: -0.96317, Convergence: -0.019253\n",
      "Epoch: 231, Loss: 2481.93751, Residuals: -0.91269, Convergence: 0.000928\n",
      "Evidence 14937.518\n",
      "\n",
      "Epoch: 231, Evidence: 14937.51758, Convergence: 0.000698\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 8.36e-02\n",
      "Epoch: 231, Loss: 2483.74855, Residuals: -0.91269, Convergence:   inf\n",
      "Epoch: 232, Loss: 2486.23459, Residuals: -0.91502, Convergence: -0.001000\n",
      "Evidence 14936.711\n",
      "\n",
      "Epoch: 232, Evidence: 14936.71094, Convergence: 0.000644\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 381.80336, Residuals: -4.52780, Convergence:   inf\n",
      "Epoch: 1, Loss: 356.05626, Residuals: -4.40664, Convergence: 0.072312\n",
      "Epoch: 2, Loss: 334.98855, Residuals: -4.24159, Convergence: 0.062891\n",
      "Epoch: 3, Loss: 318.90661, Residuals: -4.07632, Convergence: 0.050428\n",
      "Epoch: 4, Loss: 306.63548, Residuals: -3.93062, Convergence: 0.040019\n",
      "Epoch: 5, Loss: 296.89616, Residuals: -3.80175, Convergence: 0.032804\n",
      "Epoch: 6, Loss: 288.98306, Residuals: -3.68944, Convergence: 0.027383\n",
      "Epoch: 7, Loss: 282.40853, Residuals: -3.59325, Convergence: 0.023280\n",
      "Epoch: 8, Loss: 276.81136, Residuals: -3.51110, Convergence: 0.020220\n",
      "Epoch: 9, Loss: 271.93654, Residuals: -3.44066, Convergence: 0.017926\n",
      "Epoch: 10, Loss: 267.60290, Residuals: -3.37981, Convergence: 0.016194\n",
      "Epoch: 11, Loss: 263.67956, Residuals: -3.32678, Convergence: 0.014879\n",
      "Epoch: 12, Loss: 260.07155, Residuals: -3.28007, Convergence: 0.013873\n",
      "Epoch: 13, Loss: 256.71039, Residuals: -3.23838, Convergence: 0.013093\n",
      "Epoch: 14, Loss: 253.54683, Residuals: -3.20057, Convergence: 0.012477\n",
      "Epoch: 15, Loss: 250.54576, Residuals: -3.16563, Convergence: 0.011978\n",
      "Epoch: 16, Loss: 247.68356, Residuals: -3.13279, Convergence: 0.011556\n",
      "Epoch: 17, Loss: 244.94398, Residuals: -3.10153, Convergence: 0.011184\n",
      "Epoch: 18, Loss: 242.30855, Residuals: -3.07144, Convergence: 0.010876\n",
      "Epoch: 19, Loss: 239.74981, Residuals: -3.04206, Convergence: 0.010673\n",
      "Epoch: 20, Loss: 237.23523, Residuals: -3.01288, Convergence: 0.010600\n",
      "Epoch: 21, Loss: 234.73681, Residuals: -2.98343, Convergence: 0.010643\n",
      "Epoch: 22, Loss: 232.23724, Residuals: -2.95345, Convergence: 0.010763\n",
      "Epoch: 23, Loss: 229.72179, Residuals: -2.92276, Convergence: 0.010950\n",
      "Epoch: 24, Loss: 227.16010, Residuals: -2.89107, Convergence: 0.011277\n",
      "Epoch: 25, Loss: 224.49908, Residuals: -2.85775, Convergence: 0.011853\n",
      "Epoch: 26, Loss: 221.68880, Residuals: -2.82219, Convergence: 0.012677\n",
      "Epoch: 27, Loss: 218.76914, Residuals: -2.78471, Convergence: 0.013346\n",
      "Epoch: 28, Loss: 215.87948, Residuals: -2.74682, Convergence: 0.013386\n",
      "Epoch: 29, Loss: 213.09813, Residuals: -2.70951, Convergence: 0.013052\n",
      "Epoch: 30, Loss: 210.42615, Residuals: -2.67294, Convergence: 0.012698\n",
      "Epoch: 31, Loss: 207.84567, Residuals: -2.63700, Convergence: 0.012415\n",
      "Epoch: 32, Loss: 205.34036, Residuals: -2.60157, Convergence: 0.012201\n",
      "Epoch: 33, Loss: 202.89857, Residuals: -2.56652, Convergence: 0.012035\n",
      "Epoch: 34, Loss: 200.51297, Residuals: -2.53178, Convergence: 0.011898\n",
      "Epoch: 35, Loss: 198.17952, Residuals: -2.49728, Convergence: 0.011774\n",
      "Epoch: 36, Loss: 195.89664, Residuals: -2.46297, Convergence: 0.011653\n",
      "Epoch: 37, Loss: 193.66439, Residuals: -2.42883, Convergence: 0.011526\n",
      "Epoch: 38, Loss: 191.48389, Residuals: -2.39485, Convergence: 0.011387\n",
      "Epoch: 39, Loss: 189.35685, Residuals: -2.36103, Convergence: 0.011233\n",
      "Epoch: 40, Loss: 187.28517, Residuals: -2.32740, Convergence: 0.011062\n",
      "Epoch: 41, Loss: 185.27078, Residuals: -2.29396, Convergence: 0.010873\n",
      "Epoch: 42, Loss: 183.31539, Residuals: -2.26076, Convergence: 0.010667\n",
      "Epoch: 43, Loss: 181.42039, Residuals: -2.22783, Convergence: 0.010445\n",
      "Epoch: 44, Loss: 179.58692, Residuals: -2.19521, Convergence: 0.010209\n",
      "Epoch: 45, Loss: 177.81577, Residuals: -2.16294, Convergence: 0.009961\n",
      "Epoch: 46, Loss: 176.10749, Residuals: -2.13105, Convergence: 0.009700\n",
      "Epoch: 47, Loss: 174.46252, Residuals: -2.09960, Convergence: 0.009429\n",
      "Epoch: 48, Loss: 172.88109, Residuals: -2.06862, Convergence: 0.009148\n",
      "Epoch: 49, Loss: 171.36325, Residuals: -2.03817, Convergence: 0.008857\n",
      "Epoch: 50, Loss: 169.90864, Residuals: -2.00829, Convergence: 0.008561\n",
      "Epoch: 51, Loss: 168.51636, Residuals: -1.97900, Convergence: 0.008262\n",
      "Epoch: 52, Loss: 167.18478, Residuals: -1.95035, Convergence: 0.007965\n",
      "Epoch: 53, Loss: 165.91157, Residuals: -1.92234, Convergence: 0.007674\n",
      "Epoch: 54, Loss: 164.69381, Residuals: -1.89498, Convergence: 0.007394\n",
      "Epoch: 55, Loss: 163.52824, Residuals: -1.86826, Convergence: 0.007128\n",
      "Epoch: 56, Loss: 162.41149, Residuals: -1.84216, Convergence: 0.006876\n",
      "Epoch: 57, Loss: 161.34038, Residuals: -1.81666, Convergence: 0.006639\n",
      "Epoch: 58, Loss: 160.31208, Residuals: -1.79176, Convergence: 0.006414\n",
      "Epoch: 59, Loss: 159.32416, Residuals: -1.76742, Convergence: 0.006201\n",
      "Epoch: 60, Loss: 158.37463, Residuals: -1.74365, Convergence: 0.005995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61, Loss: 157.46189, Residuals: -1.72044, Convergence: 0.005797\n",
      "Epoch: 62, Loss: 156.58462, Residuals: -1.69780, Convergence: 0.005603\n",
      "Epoch: 63, Loss: 155.74174, Residuals: -1.67572, Convergence: 0.005412\n",
      "Epoch: 64, Loss: 154.93229, Residuals: -1.65421, Convergence: 0.005225\n",
      "Epoch: 65, Loss: 154.15541, Residuals: -1.63327, Convergence: 0.005040\n",
      "Epoch: 66, Loss: 153.41028, Residuals: -1.61292, Convergence: 0.004857\n",
      "Epoch: 67, Loss: 152.69612, Residuals: -1.59316, Convergence: 0.004677\n",
      "Epoch: 68, Loss: 152.01214, Residuals: -1.57398, Convergence: 0.004500\n",
      "Epoch: 69, Loss: 151.35750, Residuals: -1.55540, Convergence: 0.004325\n",
      "Epoch: 70, Loss: 150.73142, Residuals: -1.53741, Convergence: 0.004154\n",
      "Epoch: 71, Loss: 150.13302, Residuals: -1.52000, Convergence: 0.003986\n",
      "Epoch: 72, Loss: 149.56148, Residuals: -1.50318, Convergence: 0.003821\n",
      "Epoch: 73, Loss: 149.01590, Residuals: -1.48694, Convergence: 0.003661\n",
      "Epoch: 74, Loss: 148.49540, Residuals: -1.47128, Convergence: 0.003505\n",
      "Epoch: 75, Loss: 147.99908, Residuals: -1.45618, Convergence: 0.003354\n",
      "Epoch: 76, Loss: 147.52603, Residuals: -1.44164, Convergence: 0.003207\n",
      "Epoch: 77, Loss: 147.07536, Residuals: -1.42764, Convergence: 0.003064\n",
      "Epoch: 78, Loss: 146.64615, Residuals: -1.41417, Convergence: 0.002927\n",
      "Epoch: 79, Loss: 146.23750, Residuals: -1.40123, Convergence: 0.002794\n",
      "Epoch: 80, Loss: 145.84854, Residuals: -1.38879, Convergence: 0.002667\n",
      "Epoch: 81, Loss: 145.47839, Residuals: -1.37684, Convergence: 0.002544\n",
      "Epoch: 82, Loss: 145.12620, Residuals: -1.36538, Convergence: 0.002427\n",
      "Epoch: 83, Loss: 144.79116, Residuals: -1.35437, Convergence: 0.002314\n",
      "Epoch: 84, Loss: 144.47247, Residuals: -1.34382, Convergence: 0.002206\n",
      "Epoch: 85, Loss: 144.16935, Residuals: -1.33370, Convergence: 0.002102\n",
      "Epoch: 86, Loss: 143.88110, Residuals: -1.32400, Convergence: 0.002003\n",
      "Epoch: 87, Loss: 143.60702, Residuals: -1.31469, Convergence: 0.001909\n",
      "Epoch: 88, Loss: 143.34645, Residuals: -1.30578, Convergence: 0.001818\n",
      "Epoch: 89, Loss: 143.09875, Residuals: -1.29724, Convergence: 0.001731\n",
      "Epoch: 90, Loss: 142.86338, Residuals: -1.28905, Convergence: 0.001648\n",
      "Epoch: 91, Loss: 142.63977, Residuals: -1.28121, Convergence: 0.001568\n",
      "Epoch: 92, Loss: 142.42742, Residuals: -1.27370, Convergence: 0.001491\n",
      "Epoch: 93, Loss: 142.22585, Residuals: -1.26650, Convergence: 0.001417\n",
      "Epoch: 94, Loss: 142.03463, Residuals: -1.25961, Convergence: 0.001346\n",
      "Epoch: 95, Loss: 141.85335, Residuals: -1.25301, Convergence: 0.001278\n",
      "Epoch: 96, Loss: 141.68165, Residuals: -1.24669, Convergence: 0.001212\n",
      "Epoch: 97, Loss: 141.51918, Residuals: -1.24064, Convergence: 0.001148\n",
      "Epoch: 98, Loss: 141.36562, Residuals: -1.23485, Convergence: 0.001086\n",
      "Epoch: 99, Loss: 141.22069, Residuals: -1.22931, Convergence: 0.001026\n",
      "Epoch: 100, Loss: 141.08410, Residuals: -1.22401, Convergence: 0.000968\n",
      "Evidence -182.694\n",
      "\n",
      "Epoch: 100, Evidence: -182.69426, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.25e-01\n",
      "Epoch: 100, Loss: 1359.65852, Residuals: -1.22401, Convergence:   inf\n",
      "Epoch: 101, Loss: 1298.76923, Residuals: -1.25435, Convergence: 0.046882\n",
      "Epoch: 102, Loss: 1252.25549, Residuals: -1.27835, Convergence: 0.037144\n",
      "Epoch: 103, Loss: 1216.96442, Residuals: -1.29608, Convergence: 0.028999\n",
      "Epoch: 104, Loss: 1189.53903, Residuals: -1.30893, Convergence: 0.023055\n",
      "Epoch: 105, Loss: 1167.46115, Residuals: -1.31859, Convergence: 0.018911\n",
      "Epoch: 106, Loss: 1149.20804, Residuals: -1.32608, Convergence: 0.015883\n",
      "Epoch: 107, Loss: 1133.85190, Residuals: -1.33187, Convergence: 0.013543\n",
      "Epoch: 108, Loss: 1120.76554, Residuals: -1.33621, Convergence: 0.011676\n",
      "Epoch: 109, Loss: 1109.48749, Residuals: -1.33926, Convergence: 0.010165\n",
      "Epoch: 110, Loss: 1099.65921, Residuals: -1.34114, Convergence: 0.008938\n",
      "Epoch: 111, Loss: 1090.99346, Residuals: -1.34197, Convergence: 0.007943\n",
      "Epoch: 112, Loss: 1083.25359, Residuals: -1.34183, Convergence: 0.007145\n",
      "Epoch: 113, Loss: 1076.24218, Residuals: -1.34081, Convergence: 0.006515\n",
      "Epoch: 114, Loss: 1069.79114, Residuals: -1.33896, Convergence: 0.006030\n",
      "Epoch: 115, Loss: 1063.75649, Residuals: -1.33632, Convergence: 0.005673\n",
      "Epoch: 116, Loss: 1058.01358, Residuals: -1.33294, Convergence: 0.005428\n",
      "Epoch: 117, Loss: 1052.45525, Residuals: -1.32884, Convergence: 0.005281\n",
      "Epoch: 118, Loss: 1046.99037, Residuals: -1.32403, Convergence: 0.005220\n",
      "Epoch: 119, Loss: 1041.54614, Residuals: -1.31853, Convergence: 0.005227\n",
      "Epoch: 120, Loss: 1036.07299, Residuals: -1.31240, Convergence: 0.005283\n",
      "Epoch: 121, Loss: 1030.55856, Residuals: -1.30569, Convergence: 0.005351\n",
      "Epoch: 122, Loss: 1025.03569, Residuals: -1.29848, Convergence: 0.005388\n",
      "Epoch: 123, Loss: 1019.58267, Residuals: -1.29089, Convergence: 0.005348\n",
      "Epoch: 124, Loss: 1014.29656, Residuals: -1.28301, Convergence: 0.005212\n",
      "Epoch: 125, Loss: 1009.26046, Residuals: -1.27495, Convergence: 0.004990\n",
      "Epoch: 126, Loss: 1004.52077, Residuals: -1.26677, Convergence: 0.004718\n",
      "Epoch: 127, Loss: 1000.08808, Residuals: -1.25856, Convergence: 0.004432\n",
      "Epoch: 128, Loss: 995.94919, Residuals: -1.25039, Convergence: 0.004156\n",
      "Epoch: 129, Loss: 992.07856, Residuals: -1.24228, Convergence: 0.003902\n",
      "Epoch: 130, Loss: 988.44911, Residuals: -1.23430, Convergence: 0.003672\n",
      "Epoch: 131, Loss: 985.03465, Residuals: -1.22647, Convergence: 0.003466\n",
      "Epoch: 132, Loss: 981.81192, Residuals: -1.21882, Convergence: 0.003282\n",
      "Epoch: 133, Loss: 978.76221, Residuals: -1.21137, Convergence: 0.003116\n",
      "Epoch: 134, Loss: 975.87068, Residuals: -1.20413, Convergence: 0.002963\n",
      "Epoch: 135, Loss: 973.12397, Residuals: -1.19712, Convergence: 0.002823\n",
      "Epoch: 136, Loss: 970.51235, Residuals: -1.19036, Convergence: 0.002691\n",
      "Epoch: 137, Loss: 968.02737, Residuals: -1.18384, Convergence: 0.002567\n",
      "Epoch: 138, Loss: 965.66133, Residuals: -1.17758, Convergence: 0.002450\n",
      "Epoch: 139, Loss: 963.40754, Residuals: -1.17156, Convergence: 0.002339\n",
      "Epoch: 140, Loss: 961.25994, Residuals: -1.16581, Convergence: 0.002234\n",
      "Epoch: 141, Loss: 959.21257, Residuals: -1.16030, Convergence: 0.002134\n",
      "Epoch: 142, Loss: 957.25970, Residuals: -1.15504, Convergence: 0.002040\n",
      "Epoch: 143, Loss: 955.39631, Residuals: -1.15002, Convergence: 0.001950\n",
      "Epoch: 144, Loss: 953.61621, Residuals: -1.14523, Convergence: 0.001867\n",
      "Epoch: 145, Loss: 951.91471, Residuals: -1.14066, Convergence: 0.001787\n",
      "Epoch: 146, Loss: 950.28668, Residuals: -1.13631, Convergence: 0.001713\n",
      "Epoch: 147, Loss: 948.72709, Residuals: -1.13215, Convergence: 0.001644\n",
      "Epoch: 148, Loss: 947.23136, Residuals: -1.12819, Convergence: 0.001579\n",
      "Epoch: 149, Loss: 945.79455, Residuals: -1.12440, Convergence: 0.001519\n",
      "Epoch: 150, Loss: 944.41338, Residuals: -1.12079, Convergence: 0.001462\n",
      "Epoch: 151, Loss: 943.08291, Residuals: -1.11734, Convergence: 0.001411\n",
      "Epoch: 152, Loss: 941.79945, Residuals: -1.11403, Convergence: 0.001363\n",
      "Epoch: 153, Loss: 940.55896, Residuals: -1.11087, Convergence: 0.001319\n",
      "Epoch: 154, Loss: 939.35774, Residuals: -1.10783, Convergence: 0.001279\n",
      "Epoch: 155, Loss: 938.19158, Residuals: -1.10491, Convergence: 0.001243\n",
      "Epoch: 156, Loss: 937.05683, Residuals: -1.10209, Convergence: 0.001211\n",
      "Epoch: 157, Loss: 935.94910, Residuals: -1.09938, Convergence: 0.001184\n",
      "Epoch: 158, Loss: 934.86494, Residuals: -1.09674, Convergence: 0.001160\n",
      "Epoch: 159, Loss: 933.79954, Residuals: -1.09419, Convergence: 0.001141\n",
      "Epoch: 160, Loss: 932.74900, Residuals: -1.09169, Convergence: 0.001126\n",
      "Epoch: 161, Loss: 931.71021, Residuals: -1.08925, Convergence: 0.001115\n",
      "Epoch: 162, Loss: 930.67897, Residuals: -1.08685, Convergence: 0.001108\n",
      "Epoch: 163, Loss: 929.65314, Residuals: -1.08448, Convergence: 0.001103\n",
      "Epoch: 164, Loss: 928.63145, Residuals: -1.08214, Convergence: 0.001100\n",
      "Epoch: 165, Loss: 927.61352, Residuals: -1.07982, Convergence: 0.001097\n",
      "Epoch: 166, Loss: 926.60115, Residuals: -1.07753, Convergence: 0.001093\n",
      "Epoch: 167, Loss: 925.59815, Residuals: -1.07526, Convergence: 0.001084\n",
      "Epoch: 168, Loss: 924.60833, Residuals: -1.07303, Convergence: 0.001071\n",
      "Epoch: 169, Loss: 923.63688, Residuals: -1.07083, Convergence: 0.001052\n",
      "Epoch: 170, Loss: 922.68921, Residuals: -1.06870, Convergence: 0.001027\n",
      "Epoch: 171, Loss: 921.76985, Residuals: -1.06662, Convergence: 0.000997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence 11173.489\n",
      "\n",
      "Epoch: 171, Evidence: 11173.48926, Convergence: 1.016351\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.75e-01\n",
      "Epoch: 171, Loss: 2339.93720, Residuals: -1.06662, Convergence:   inf\n",
      "Epoch: 172, Loss: 2303.91443, Residuals: -1.07361, Convergence: 0.015635\n",
      "Epoch: 173, Loss: 2277.77452, Residuals: -1.07228, Convergence: 0.011476\n",
      "Epoch: 174, Loss: 2255.95610, Residuals: -1.07004, Convergence: 0.009671\n",
      "Epoch: 175, Loss: 2237.56615, Residuals: -1.06759, Convergence: 0.008219\n",
      "Epoch: 176, Loss: 2221.93008, Residuals: -1.06505, Convergence: 0.007037\n",
      "Epoch: 177, Loss: 2208.52387, Residuals: -1.06246, Convergence: 0.006070\n",
      "Epoch: 178, Loss: 2196.91570, Residuals: -1.05980, Convergence: 0.005284\n",
      "Epoch: 179, Loss: 2186.74537, Residuals: -1.05709, Convergence: 0.004651\n",
      "Epoch: 180, Loss: 2177.71189, Residuals: -1.05430, Convergence: 0.004148\n",
      "Epoch: 181, Loss: 2169.57201, Residuals: -1.05142, Convergence: 0.003752\n",
      "Epoch: 182, Loss: 2162.14190, Residuals: -1.04841, Convergence: 0.003436\n",
      "Epoch: 183, Loss: 2155.29994, Residuals: -1.04529, Convergence: 0.003174\n",
      "Epoch: 184, Loss: 2148.97492, Residuals: -1.04208, Convergence: 0.002943\n",
      "Epoch: 185, Loss: 2143.12826, Residuals: -1.03883, Convergence: 0.002728\n",
      "Epoch: 186, Loss: 2137.73151, Residuals: -1.03558, Convergence: 0.002525\n",
      "Epoch: 187, Loss: 2132.75857, Residuals: -1.03238, Convergence: 0.002332\n",
      "Epoch: 188, Loss: 2128.17947, Residuals: -1.02926, Convergence: 0.002152\n",
      "Epoch: 189, Loss: 2123.96279, Residuals: -1.02624, Convergence: 0.001985\n",
      "Epoch: 190, Loss: 2120.07568, Residuals: -1.02334, Convergence: 0.001833\n",
      "Epoch: 191, Loss: 2116.48641, Residuals: -1.02056, Convergence: 0.001696\n",
      "Epoch: 192, Loss: 2113.16380, Residuals: -1.01792, Convergence: 0.001572\n",
      "Epoch: 193, Loss: 2110.07946, Residuals: -1.01541, Convergence: 0.001462\n",
      "Epoch: 194, Loss: 2107.20810, Residuals: -1.01303, Convergence: 0.001363\n",
      "Epoch: 195, Loss: 2104.52494, Residuals: -1.01077, Convergence: 0.001275\n",
      "Epoch: 196, Loss: 2102.00879, Residuals: -1.00863, Convergence: 0.001197\n",
      "Epoch: 197, Loss: 2099.64181, Residuals: -1.00661, Convergence: 0.001127\n",
      "Epoch: 198, Loss: 2097.40716, Residuals: -1.00470, Convergence: 0.001065\n",
      "Epoch: 199, Loss: 2095.29101, Residuals: -1.00290, Convergence: 0.001010\n",
      "Epoch: 200, Loss: 2093.28240, Residuals: -1.00119, Convergence: 0.000960\n",
      "Evidence 14336.888\n",
      "\n",
      "Epoch: 200, Evidence: 14336.88770, Convergence: 0.220647\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.36e-01\n",
      "Epoch: 200, Loss: 2464.85746, Residuals: -1.00119, Convergence:   inf\n",
      "Epoch: 201, Loss: 2451.71706, Residuals: -0.99736, Convergence: 0.005360\n",
      "Epoch: 202, Loss: 2440.97672, Residuals: -0.99327, Convergence: 0.004400\n",
      "Epoch: 203, Loss: 2431.73779, Residuals: -0.98950, Convergence: 0.003799\n",
      "Epoch: 204, Loss: 2423.74323, Residuals: -0.98607, Convergence: 0.003298\n",
      "Epoch: 205, Loss: 2416.78566, Residuals: -0.98299, Convergence: 0.002879\n",
      "Epoch: 206, Loss: 2410.69420, Residuals: -0.98024, Convergence: 0.002527\n",
      "Epoch: 207, Loss: 2405.32687, Residuals: -0.97780, Convergence: 0.002231\n",
      "Epoch: 208, Loss: 2400.56866, Residuals: -0.97563, Convergence: 0.001982\n",
      "Epoch: 209, Loss: 2396.32363, Residuals: -0.97370, Convergence: 0.001771\n",
      "Epoch: 210, Loss: 2392.51305, Residuals: -0.97199, Convergence: 0.001593\n",
      "Epoch: 211, Loss: 2389.07313, Residuals: -0.97047, Convergence: 0.001440\n",
      "Epoch: 212, Loss: 2385.95039, Residuals: -0.96912, Convergence: 0.001309\n",
      "Epoch: 213, Loss: 2383.10150, Residuals: -0.96791, Convergence: 0.001195\n",
      "Epoch: 214, Loss: 2380.49098, Residuals: -0.96683, Convergence: 0.001097\n",
      "Epoch: 215, Loss: 2378.08751, Residuals: -0.96585, Convergence: 0.001011\n",
      "Epoch: 216, Loss: 2375.86799, Residuals: -0.96498, Convergence: 0.000934\n",
      "Evidence 14697.821\n",
      "\n",
      "Epoch: 216, Evidence: 14697.82129, Convergence: 0.024557\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.31e-01\n",
      "Epoch: 216, Loss: 2470.40164, Residuals: -0.96498, Convergence:   inf\n",
      "Epoch: 217, Loss: 2464.11487, Residuals: -0.96159, Convergence: 0.002551\n",
      "Epoch: 218, Loss: 2458.92216, Residuals: -0.95876, Convergence: 0.002112\n",
      "Epoch: 219, Loss: 2454.51981, Residuals: -0.95645, Convergence: 0.001794\n",
      "Epoch: 220, Loss: 2450.72527, Residuals: -0.95455, Convergence: 0.001548\n",
      "Epoch: 221, Loss: 2447.40655, Residuals: -0.95298, Convergence: 0.001356\n",
      "Epoch: 222, Loss: 2444.46599, Residuals: -0.95169, Convergence: 0.001203\n",
      "Epoch: 223, Loss: 2441.83154, Residuals: -0.95061, Convergence: 0.001079\n",
      "Epoch: 224, Loss: 2439.44792, Residuals: -0.94971, Convergence: 0.000977\n",
      "Evidence 14776.872\n",
      "\n",
      "Epoch: 224, Evidence: 14776.87207, Convergence: 0.005350\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.58e-01\n",
      "Epoch: 224, Loss: 2472.30424, Residuals: -0.94971, Convergence:   inf\n",
      "Epoch: 225, Loss: 2468.53479, Residuals: -0.94718, Convergence: 0.001527\n",
      "Epoch: 226, Loss: 2465.38932, Residuals: -0.94524, Convergence: 0.001276\n",
      "Epoch: 227, Loss: 2462.67481, Residuals: -0.94373, Convergence: 0.001102\n",
      "Epoch: 228, Loss: 2460.28306, Residuals: -0.94253, Convergence: 0.000972\n",
      "Evidence 14803.844\n",
      "\n",
      "Epoch: 228, Evidence: 14803.84375, Convergence: 0.001822\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.07e-01\n",
      "Epoch: 228, Loss: 2473.42455, Residuals: -0.94253, Convergence:   inf\n",
      "Epoch: 229, Loss: 2470.62102, Residuals: -0.94052, Convergence: 0.001135\n",
      "Epoch: 230, Loss: 2468.25187, Residuals: -0.93902, Convergence: 0.000960\n",
      "Evidence 14815.539\n",
      "\n",
      "Epoch: 230, Evidence: 14815.53906, Convergence: 0.000789\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.71e-01\n",
      "Epoch: 230, Loss: 2474.20699, Residuals: -0.93902, Convergence:   inf\n",
      "Epoch: 231, Loss: 2469.73762, Residuals: -0.93616, Convergence: 0.001810\n",
      "Epoch: 232, Loss: 2466.32081, Residuals: -0.93444, Convergence: 0.001385\n",
      "Epoch: 233, Loss: 2463.52044, Residuals: -0.93329, Convergence: 0.001137\n",
      "Epoch: 234, Loss: 2461.12744, Residuals: -0.93264, Convergence: 0.000972\n",
      "Evidence 14832.916\n",
      "\n",
      "Epoch: 234, Evidence: 14832.91602, Convergence: 0.001960\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.42e-01\n",
      "Epoch: 234, Loss: 2474.37850, Residuals: -0.93264, Convergence:   inf\n",
      "Epoch: 235, Loss: 2471.44485, Residuals: -0.93019, Convergence: 0.001187\n",
      "Epoch: 236, Loss: 2469.09114, Residuals: -0.92898, Convergence: 0.000953\n",
      "Evidence 14843.452\n",
      "\n",
      "Epoch: 236, Evidence: 14843.45215, Convergence: 0.000710\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.20e-01\n",
      "Epoch: 236, Loss: 2474.57879, Residuals: -0.92898, Convergence:   inf\n",
      "Epoch: 237, Loss: 2470.27045, Residuals: -0.92558, Convergence: 0.001744\n",
      "Epoch: 238, Loss: 2467.10568, Residuals: -0.92641, Convergence: 0.001283\n",
      "Epoch: 239, Loss: 2464.51056, Residuals: -0.92698, Convergence: 0.001053\n",
      "Epoch: 240, Loss: 2462.16321, Residuals: -0.92953, Convergence: 0.000953\n",
      "Evidence 14858.866\n",
      "\n",
      "Epoch: 240, Evidence: 14858.86621, Convergence: 0.001746\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.07e-01\n",
      "Epoch: 240, Loss: 2473.88222, Residuals: -0.92953, Convergence:   inf\n",
      "Epoch: 241, Loss: 2472.00112, Residuals: -0.92663, Convergence: 0.000761\n",
      "Evidence 14865.487\n",
      "\n",
      "Epoch: 241, Evidence: 14865.48730, Convergence: 0.000445\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.76e-02\n",
      "Epoch: 241, Loss: 2474.80660, Residuals: -0.92663, Convergence:   inf\n",
      "Epoch: 242, Loss: 2513.71401, Residuals: -0.96932, Convergence: -0.015478\n",
      "Epoch: 242, Loss: 2472.65464, Residuals: -0.92575, Convergence: 0.000870\n",
      "Evidence 14869.471\n",
      "\n",
      "Epoch: 242, Evidence: 14869.47070, Convergence: 0.000713\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.13e-02\n",
      "Epoch: 242, Loss: 2474.21270, Residuals: -0.92575, Convergence:   inf\n",
      "Epoch: 243, Loss: 2478.92255, Residuals: -0.92881, Convergence: -0.001900\n",
      "Epoch: 243, Loss: 2474.07613, Residuals: -0.92477, Convergence: 0.000055\n",
      "Evidence 14871.252\n",
      "\n",
      "Epoch: 243, Evidence: 14871.25195, Convergence: 0.000833\n",
      "Total samples: 181, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 379.29316, Residuals: -4.52503, Convergence:   inf\n",
      "Epoch: 1, Loss: 353.45575, Residuals: -4.40313, Convergence: 0.073099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: 332.43916, Residuals: -4.23748, Convergence: 0.063219\n",
      "Epoch: 3, Loss: 316.43389, Residuals: -4.07327, Convergence: 0.050580\n",
      "Epoch: 4, Loss: 304.21825, Residuals: -3.92892, Convergence: 0.040154\n",
      "Epoch: 5, Loss: 294.53660, Residuals: -3.80183, Convergence: 0.032871\n",
      "Epoch: 6, Loss: 286.68528, Residuals: -3.69148, Convergence: 0.027387\n",
      "Epoch: 7, Loss: 280.17622, Residuals: -3.59718, Convergence: 0.023232\n",
      "Epoch: 8, Loss: 274.64691, Residuals: -3.51669, Convergence: 0.020132\n",
      "Epoch: 9, Loss: 269.84033, Residuals: -3.44758, Convergence: 0.017813\n",
      "Epoch: 10, Loss: 265.57270, Residuals: -3.38773, Convergence: 0.016070\n",
      "Epoch: 11, Loss: 261.70992, Residuals: -3.33536, Convergence: 0.014760\n",
      "Epoch: 12, Loss: 258.15317, Residuals: -3.28896, Convergence: 0.013778\n",
      "Epoch: 13, Loss: 254.83010, Residuals: -3.24725, Convergence: 0.013040\n",
      "Epoch: 14, Loss: 251.68922, Residuals: -3.20912, Convergence: 0.012479\n",
      "Epoch: 15, Loss: 248.69736, Residuals: -3.17369, Convergence: 0.012030\n",
      "Epoch: 16, Loss: 245.83777, Residuals: -3.14036, Convergence: 0.011632\n",
      "Epoch: 17, Loss: 243.10183, Residuals: -3.10878, Convergence: 0.011254\n",
      "Epoch: 18, Loss: 240.47465, Residuals: -3.07865, Convergence: 0.010925\n",
      "Epoch: 19, Loss: 237.92906, Residuals: -3.04951, Convergence: 0.010699\n",
      "Epoch: 20, Loss: 235.43066, Residuals: -3.02079, Convergence: 0.010612\n",
      "Epoch: 21, Loss: 232.94545, Residuals: -2.99192, Convergence: 0.010669\n",
      "Epoch: 22, Loss: 230.44457, Residuals: -2.96242, Convergence: 0.010852\n",
      "Epoch: 23, Loss: 227.90125, Residuals: -2.93191, Convergence: 0.011160\n",
      "Epoch: 24, Loss: 225.27801, Residuals: -2.89991, Convergence: 0.011644\n",
      "Epoch: 25, Loss: 222.52249, Residuals: -2.86578, Convergence: 0.012383\n",
      "Epoch: 26, Loss: 219.61667, Residuals: -2.82923, Convergence: 0.013231\n",
      "Epoch: 27, Loss: 216.65895, Residuals: -2.79125, Convergence: 0.013651\n",
      "Epoch: 28, Loss: 213.77284, Residuals: -2.75324, Convergence: 0.013501\n",
      "Epoch: 29, Loss: 210.99183, Residuals: -2.71572, Convergence: 0.013181\n",
      "Epoch: 30, Loss: 208.30474, Residuals: -2.67866, Convergence: 0.012900\n",
      "Epoch: 31, Loss: 205.69561, Residuals: -2.64195, Convergence: 0.012684\n",
      "Epoch: 32, Loss: 203.15280, Residuals: -2.60550, Convergence: 0.012517\n",
      "Epoch: 33, Loss: 200.66925, Residuals: -2.56926, Convergence: 0.012376\n",
      "Epoch: 34, Loss: 198.24134, Residuals: -2.53322, Convergence: 0.012247\n",
      "Epoch: 35, Loss: 195.86760, Residuals: -2.49738, Convergence: 0.012119\n",
      "Epoch: 36, Loss: 193.54785, Residuals: -2.46175, Convergence: 0.011985\n",
      "Epoch: 37, Loss: 191.28249, Residuals: -2.42635, Convergence: 0.011843\n",
      "Epoch: 38, Loss: 189.07216, Residuals: -2.39117, Convergence: 0.011690\n",
      "Epoch: 39, Loss: 186.91744, Residuals: -2.35624, Convergence: 0.011528\n",
      "Epoch: 40, Loss: 184.81871, Residuals: -2.32154, Convergence: 0.011356\n",
      "Epoch: 41, Loss: 182.77610, Residuals: -2.28709, Convergence: 0.011176\n",
      "Epoch: 42, Loss: 180.78948, Residuals: -2.25288, Convergence: 0.010989\n",
      "Epoch: 43, Loss: 178.85866, Residuals: -2.21891, Convergence: 0.010795\n",
      "Epoch: 44, Loss: 176.98358, Residuals: -2.18519, Convergence: 0.010595\n",
      "Epoch: 45, Loss: 175.16460, Residuals: -2.15174, Convergence: 0.010384\n",
      "Epoch: 46, Loss: 173.40271, Residuals: -2.11858, Convergence: 0.010161\n",
      "Epoch: 47, Loss: 171.69955, Residuals: -2.08576, Convergence: 0.009919\n",
      "Epoch: 48, Loss: 170.05712, Residuals: -2.05333, Convergence: 0.009658\n",
      "Epoch: 49, Loss: 168.47745, Residuals: -2.02133, Convergence: 0.009376\n",
      "Epoch: 50, Loss: 166.96204, Residuals: -1.98984, Convergence: 0.009076\n",
      "Epoch: 51, Loss: 165.51171, Residuals: -1.95890, Convergence: 0.008763\n",
      "Epoch: 52, Loss: 164.12646, Residuals: -1.92856, Convergence: 0.008440\n",
      "Epoch: 53, Loss: 162.80553, Residuals: -1.89885, Convergence: 0.008114\n",
      "Epoch: 54, Loss: 161.54761, Residuals: -1.86981, Convergence: 0.007787\n",
      "Epoch: 55, Loss: 160.35094, Residuals: -1.84146, Convergence: 0.007463\n",
      "Epoch: 56, Loss: 159.21350, Residuals: -1.81382, Convergence: 0.007144\n",
      "Epoch: 57, Loss: 158.13308, Residuals: -1.78690, Convergence: 0.006832\n",
      "Epoch: 58, Loss: 157.10742, Residuals: -1.76071, Convergence: 0.006528\n",
      "Epoch: 59, Loss: 156.13425, Residuals: -1.73524, Convergence: 0.006233\n",
      "Epoch: 60, Loss: 155.21135, Residuals: -1.71051, Convergence: 0.005946\n",
      "Epoch: 61, Loss: 154.33660, Residuals: -1.68652, Convergence: 0.005668\n",
      "Epoch: 62, Loss: 153.50800, Residuals: -1.66326, Convergence: 0.005398\n",
      "Epoch: 63, Loss: 152.72367, Residuals: -1.64073, Convergence: 0.005136\n",
      "Epoch: 64, Loss: 151.98190, Residuals: -1.61892, Convergence: 0.004881\n",
      "Epoch: 65, Loss: 151.28112, Residuals: -1.59784, Convergence: 0.004632\n",
      "Epoch: 66, Loss: 150.61992, Residuals: -1.57748, Convergence: 0.004390\n",
      "Epoch: 67, Loss: 149.99703, Residuals: -1.55784, Convergence: 0.004153\n",
      "Epoch: 68, Loss: 149.41131, Residuals: -1.53892, Convergence: 0.003920\n",
      "Epoch: 69, Loss: 148.86173, Residuals: -1.52071, Convergence: 0.003692\n",
      "Epoch: 70, Loss: 148.34732, Residuals: -1.50324, Convergence: 0.003468\n",
      "Epoch: 71, Loss: 147.86718, Residuals: -1.48648, Convergence: 0.003247\n",
      "Epoch: 72, Loss: 147.42034, Residuals: -1.47046, Convergence: 0.003031\n",
      "Epoch: 73, Loss: 147.00580, Residuals: -1.45518, Convergence: 0.002820\n",
      "Epoch: 74, Loss: 146.62234, Residuals: -1.44063, Convergence: 0.002615\n",
      "Epoch: 75, Loss: 146.26852, Residuals: -1.42683, Convergence: 0.002419\n",
      "Epoch: 76, Loss: 145.94257, Residuals: -1.41376, Convergence: 0.002233\n",
      "Epoch: 77, Loss: 145.64230, Residuals: -1.40141, Convergence: 0.002062\n",
      "Epoch: 78, Loss: 145.36509, Residuals: -1.38977, Convergence: 0.001907\n",
      "Epoch: 79, Loss: 145.10793, Residuals: -1.37880, Convergence: 0.001772\n",
      "Epoch: 80, Loss: 144.86749, Residuals: -1.36845, Convergence: 0.001660\n",
      "Epoch: 81, Loss: 144.64031, Residuals: -1.35866, Convergence: 0.001571\n",
      "Epoch: 82, Loss: 144.42300, Residuals: -1.34937, Convergence: 0.001505\n",
      "Epoch: 83, Loss: 144.21245, Residuals: -1.34050, Convergence: 0.001460\n",
      "Epoch: 84, Loss: 144.00604, Residuals: -1.33201, Convergence: 0.001433\n",
      "Epoch: 85, Loss: 143.80170, Residuals: -1.32382, Convergence: 0.001421\n",
      "Epoch: 86, Loss: 143.59800, Residuals: -1.31589, Convergence: 0.001419\n",
      "Epoch: 87, Loss: 143.39408, Residuals: -1.30818, Convergence: 0.001422\n",
      "Epoch: 88, Loss: 143.18954, Residuals: -1.30068, Convergence: 0.001428\n",
      "Epoch: 89, Loss: 142.98440, Residuals: -1.29335, Convergence: 0.001435\n",
      "Epoch: 90, Loss: 142.77892, Residuals: -1.28619, Convergence: 0.001439\n",
      "Epoch: 91, Loss: 142.57355, Residuals: -1.27919, Convergence: 0.001440\n",
      "Epoch: 92, Loss: 142.36881, Residuals: -1.27236, Convergence: 0.001438\n",
      "Epoch: 93, Loss: 142.16529, Residuals: -1.26569, Convergence: 0.001432\n",
      "Epoch: 94, Loss: 141.96353, Residuals: -1.25918, Convergence: 0.001421\n",
      "Epoch: 95, Loss: 141.76404, Residuals: -1.25285, Convergence: 0.001407\n",
      "Epoch: 96, Loss: 141.56732, Residuals: -1.24668, Convergence: 0.001390\n",
      "Epoch: 97, Loss: 141.37375, Residuals: -1.24068, Convergence: 0.001369\n",
      "Epoch: 98, Loss: 141.18366, Residuals: -1.23485, Convergence: 0.001346\n",
      "Epoch: 99, Loss: 140.99737, Residuals: -1.22920, Convergence: 0.001321\n",
      "Epoch: 100, Loss: 140.81507, Residuals: -1.22371, Convergence: 0.001295\n",
      "Epoch: 101, Loss: 140.63696, Residuals: -1.21840, Convergence: 0.001266\n",
      "Epoch: 102, Loss: 140.46315, Residuals: -1.21325, Convergence: 0.001237\n",
      "Epoch: 103, Loss: 140.29373, Residuals: -1.20827, Convergence: 0.001208\n",
      "Epoch: 104, Loss: 140.12877, Residuals: -1.20345, Convergence: 0.001177\n",
      "Epoch: 105, Loss: 139.96828, Residuals: -1.19879, Convergence: 0.001147\n",
      "Epoch: 106, Loss: 139.81227, Residuals: -1.19429, Convergence: 0.001116\n",
      "Epoch: 107, Loss: 139.66070, Residuals: -1.18994, Convergence: 0.001085\n",
      "Epoch: 108, Loss: 139.51354, Residuals: -1.18573, Convergence: 0.001055\n",
      "Epoch: 109, Loss: 139.37074, Residuals: -1.18167, Convergence: 0.001025\n",
      "Epoch: 110, Loss: 139.23222, Residuals: -1.17774, Convergence: 0.000995\n",
      "Evidence -179.119\n",
      "\n",
      "Epoch: 110, Evidence: -179.11877, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 7.23e-01\n",
      "Epoch: 110, Loss: 1383.70099, Residuals: -1.17774, Convergence:   inf\n",
      "Epoch: 111, Loss: 1328.81881, Residuals: -1.20260, Convergence: 0.041301\n",
      "Epoch: 112, Loss: 1286.28810, Residuals: -1.22360, Convergence: 0.033065\n",
      "Epoch: 113, Loss: 1253.50708, Residuals: -1.24003, Convergence: 0.026151\n",
      "Epoch: 114, Loss: 1227.54900, Residuals: -1.25259, Convergence: 0.021146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 115, Loss: 1206.29787, Residuals: -1.26247, Convergence: 0.017617\n",
      "Epoch: 116, Loss: 1188.53300, Residuals: -1.27036, Convergence: 0.014947\n",
      "Epoch: 117, Loss: 1173.49065, Residuals: -1.27656, Convergence: 0.012818\n",
      "Epoch: 118, Loss: 1160.62628, Residuals: -1.28128, Convergence: 0.011084\n",
      "Epoch: 119, Loss: 1149.52021, Residuals: -1.28467, Convergence: 0.009661\n",
      "Epoch: 120, Loss: 1139.83610, Residuals: -1.28688, Convergence: 0.008496\n",
      "Epoch: 121, Loss: 1131.29809, Residuals: -1.28803, Convergence: 0.007547\n",
      "Epoch: 122, Loss: 1123.67693, Residuals: -1.28823, Convergence: 0.006782\n",
      "Epoch: 123, Loss: 1116.77624, Residuals: -1.28757, Convergence: 0.006179\n",
      "Epoch: 124, Loss: 1110.42566, Residuals: -1.28612, Convergence: 0.005719\n",
      "Epoch: 125, Loss: 1104.47243, Residuals: -1.28393, Convergence: 0.005390\n",
      "Epoch: 126, Loss: 1098.77830, Residuals: -1.28103, Convergence: 0.005182\n",
      "Epoch: 127, Loss: 1093.21734, Residuals: -1.27743, Convergence: 0.005087\n",
      "Epoch: 128, Loss: 1087.67923, Residuals: -1.27314, Convergence: 0.005092\n",
      "Epoch: 129, Loss: 1082.07564, Residuals: -1.26814, Convergence: 0.005179\n",
      "Epoch: 130, Loss: 1076.34976, Residuals: -1.26247, Convergence: 0.005320\n",
      "Epoch: 131, Loss: 1070.48311, Residuals: -1.25616, Convergence: 0.005480\n",
      "Epoch: 132, Loss: 1064.50546, Residuals: -1.24929, Convergence: 0.005615\n",
      "Epoch: 133, Loss: 1058.49888, Residuals: -1.24197, Convergence: 0.005675\n",
      "Epoch: 134, Loss: 1052.58290, Residuals: -1.23433, Convergence: 0.005620\n",
      "Epoch: 135, Loss: 1046.87995, Residuals: -1.22646, Convergence: 0.005448\n",
      "Epoch: 136, Loss: 1041.47749, Residuals: -1.21848, Convergence: 0.005187\n",
      "Epoch: 137, Loss: 1036.41583, Residuals: -1.21047, Convergence: 0.004884\n",
      "Epoch: 138, Loss: 1031.69733, Residuals: -1.20250, Convergence: 0.004574\n",
      "Epoch: 139, Loss: 1027.30266, Residuals: -1.19463, Convergence: 0.004278\n",
      "Epoch: 140, Loss: 1023.20385, Residuals: -1.18692, Convergence: 0.004006\n",
      "Epoch: 141, Loss: 1019.37221, Residuals: -1.17939, Convergence: 0.003759\n",
      "Epoch: 142, Loss: 1015.78184, Residuals: -1.17208, Convergence: 0.003535\n",
      "Epoch: 143, Loss: 1012.41021, Residuals: -1.16501, Convergence: 0.003330\n",
      "Epoch: 144, Loss: 1009.23798, Residuals: -1.15819, Convergence: 0.003143\n",
      "Epoch: 145, Loss: 1006.24887, Residuals: -1.15164, Convergence: 0.002971\n",
      "Epoch: 146, Loss: 1003.42813, Residuals: -1.14536, Convergence: 0.002811\n",
      "Epoch: 147, Loss: 1000.76368, Residuals: -1.13936, Convergence: 0.002662\n",
      "Epoch: 148, Loss: 998.24381, Residuals: -1.13362, Convergence: 0.002524\n",
      "Epoch: 149, Loss: 995.85889, Residuals: -1.12816, Convergence: 0.002395\n",
      "Epoch: 150, Loss: 993.59960, Residuals: -1.12295, Convergence: 0.002274\n",
      "Epoch: 151, Loss: 991.45770, Residuals: -1.11800, Convergence: 0.002160\n",
      "Epoch: 152, Loss: 989.42583, Residuals: -1.11330, Convergence: 0.002054\n",
      "Epoch: 153, Loss: 987.49704, Residuals: -1.10883, Convergence: 0.001953\n",
      "Epoch: 154, Loss: 985.66505, Residuals: -1.10459, Convergence: 0.001859\n",
      "Epoch: 155, Loss: 983.92378, Residuals: -1.10056, Convergence: 0.001770\n",
      "Epoch: 156, Loss: 982.26762, Residuals: -1.09674, Convergence: 0.001686\n",
      "Epoch: 157, Loss: 980.69138, Residuals: -1.09311, Convergence: 0.001607\n",
      "Epoch: 158, Loss: 979.18935, Residuals: -1.08967, Convergence: 0.001534\n",
      "Epoch: 159, Loss: 977.75634, Residuals: -1.08639, Convergence: 0.001466\n",
      "Epoch: 160, Loss: 976.38692, Residuals: -1.08328, Convergence: 0.001403\n",
      "Epoch: 161, Loss: 975.07576, Residuals: -1.08031, Convergence: 0.001345\n",
      "Epoch: 162, Loss: 973.81730, Residuals: -1.07748, Convergence: 0.001292\n",
      "Epoch: 163, Loss: 972.60636, Residuals: -1.07478, Convergence: 0.001245\n",
      "Epoch: 164, Loss: 971.43728, Residuals: -1.07219, Convergence: 0.001203\n",
      "Epoch: 165, Loss: 970.30435, Residuals: -1.06970, Convergence: 0.001168\n",
      "Epoch: 166, Loss: 969.20189, Residuals: -1.06731, Convergence: 0.001137\n",
      "Epoch: 167, Loss: 968.12388, Residuals: -1.06499, Convergence: 0.001114\n",
      "Epoch: 168, Loss: 967.06466, Residuals: -1.06273, Convergence: 0.001095\n",
      "Epoch: 169, Loss: 966.01871, Residuals: -1.06053, Convergence: 0.001083\n",
      "Epoch: 170, Loss: 964.98054, Residuals: -1.05836, Convergence: 0.001076\n",
      "Epoch: 171, Loss: 963.94553, Residuals: -1.05623, Convergence: 0.001074\n",
      "Epoch: 172, Loss: 962.91025, Residuals: -1.05411, Convergence: 0.001075\n",
      "Epoch: 173, Loss: 961.87311, Residuals: -1.05200, Convergence: 0.001078\n",
      "Epoch: 174, Loss: 960.83422, Residuals: -1.04989, Convergence: 0.001081\n",
      "Epoch: 175, Loss: 959.79620, Residuals: -1.04780, Convergence: 0.001081\n",
      "Epoch: 176, Loss: 958.76314, Residuals: -1.04573, Convergence: 0.001077\n",
      "Epoch: 177, Loss: 957.74076, Residuals: -1.04368, Convergence: 0.001067\n",
      "Epoch: 178, Loss: 956.73575, Residuals: -1.04167, Convergence: 0.001050\n",
      "Epoch: 179, Loss: 955.75440, Residuals: -1.03971, Convergence: 0.001027\n",
      "Epoch: 180, Loss: 954.80172, Residuals: -1.03781, Convergence: 0.000998\n",
      "Evidence 11397.457\n",
      "\n",
      "Epoch: 180, Evidence: 11397.45703, Convergence: 1.015716\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 5.68e-01\n",
      "Epoch: 180, Loss: 2350.07779, Residuals: -1.03781, Convergence:   inf\n",
      "Epoch: 181, Loss: 2313.23342, Residuals: -1.04563, Convergence: 0.015928\n",
      "Epoch: 182, Loss: 2289.05781, Residuals: -1.04416, Convergence: 0.010561\n",
      "Epoch: 183, Loss: 2268.75187, Residuals: -1.04243, Convergence: 0.008950\n",
      "Epoch: 184, Loss: 2251.57504, Residuals: -1.04063, Convergence: 0.007629\n",
      "Epoch: 185, Loss: 2236.92652, Residuals: -1.03878, Convergence: 0.006548\n",
      "Epoch: 186, Loss: 2224.29819, Residuals: -1.03686, Convergence: 0.005677\n",
      "Epoch: 187, Loss: 2213.26708, Residuals: -1.03486, Convergence: 0.004984\n",
      "Epoch: 188, Loss: 2203.49064, Residuals: -1.03276, Convergence: 0.004437\n",
      "Epoch: 189, Loss: 2194.71740, Residuals: -1.03055, Convergence: 0.003997\n",
      "Epoch: 190, Loss: 2186.77766, Residuals: -1.02822, Convergence: 0.003631\n",
      "Epoch: 191, Loss: 2179.57122, Residuals: -1.02582, Convergence: 0.003306\n",
      "Epoch: 192, Loss: 2173.03161, Residuals: -1.02340, Convergence: 0.003009\n",
      "Epoch: 193, Loss: 2167.10571, Residuals: -1.02099, Convergence: 0.002734\n",
      "Epoch: 194, Loss: 2161.73789, Residuals: -1.01865, Convergence: 0.002483\n",
      "Epoch: 195, Loss: 2156.87071, Residuals: -1.01638, Convergence: 0.002257\n",
      "Epoch: 196, Loss: 2152.44731, Residuals: -1.01421, Convergence: 0.002055\n",
      "Epoch: 197, Loss: 2148.41153, Residuals: -1.01214, Convergence: 0.001878\n",
      "Epoch: 198, Loss: 2144.71163, Residuals: -1.01017, Convergence: 0.001725\n",
      "Epoch: 199, Loss: 2141.29999, Residuals: -1.00829, Convergence: 0.001593\n",
      "Epoch: 200, Loss: 2138.13465, Residuals: -1.00650, Convergence: 0.001480\n",
      "Epoch: 201, Loss: 2135.17915, Residuals: -1.00479, Convergence: 0.001384\n",
      "Epoch: 202, Loss: 2132.40178, Residuals: -1.00315, Convergence: 0.001302\n",
      "Epoch: 203, Loss: 2129.77742, Residuals: -1.00157, Convergence: 0.001232\n",
      "Epoch: 204, Loss: 2127.28340, Residuals: -1.00004, Convergence: 0.001172\n",
      "Epoch: 205, Loss: 2124.90505, Residuals: -0.99857, Convergence: 0.001119\n",
      "Epoch: 206, Loss: 2122.62780, Residuals: -0.99714, Convergence: 0.001073\n",
      "Epoch: 207, Loss: 2120.44445, Residuals: -0.99575, Convergence: 0.001030\n",
      "Epoch: 208, Loss: 2118.34768, Residuals: -0.99440, Convergence: 0.000990\n",
      "Evidence 14302.797\n",
      "\n",
      "Epoch: 208, Evidence: 14302.79688, Convergence: 0.203131\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 4.31e-01\n",
      "Epoch: 208, Loss: 2460.65154, Residuals: -0.99440, Convergence:   inf\n",
      "Epoch: 209, Loss: 2447.72024, Residuals: -0.99209, Convergence: 0.005283\n",
      "Epoch: 210, Loss: 2436.96397, Residuals: -0.98950, Convergence: 0.004414\n",
      "Epoch: 211, Loss: 2427.53511, Residuals: -0.98697, Convergence: 0.003884\n",
      "Epoch: 212, Loss: 2419.22642, Residuals: -0.98457, Convergence: 0.003434\n",
      "Epoch: 213, Loss: 2411.87726, Residuals: -0.98233, Convergence: 0.003047\n",
      "Epoch: 214, Loss: 2405.35887, Residuals: -0.98027, Convergence: 0.002710\n",
      "Epoch: 215, Loss: 2399.56201, Residuals: -0.97839, Convergence: 0.002416\n",
      "Epoch: 216, Loss: 2394.39396, Residuals: -0.97669, Convergence: 0.002158\n",
      "Epoch: 217, Loss: 2389.77262, Residuals: -0.97514, Convergence: 0.001934\n",
      "Epoch: 218, Loss: 2385.62805, Residuals: -0.97372, Convergence: 0.001737\n",
      "Epoch: 219, Loss: 2381.89731, Residuals: -0.97243, Convergence: 0.001566\n",
      "Epoch: 220, Loss: 2378.52579, Residuals: -0.97125, Convergence: 0.001417\n",
      "Epoch: 221, Loss: 2375.46723, Residuals: -0.97017, Convergence: 0.001288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 222, Loss: 2372.68052, Residuals: -0.96918, Convergence: 0.001174\n",
      "Epoch: 223, Loss: 2370.13070, Residuals: -0.96826, Convergence: 0.001076\n",
      "Epoch: 224, Loss: 2367.78854, Residuals: -0.96741, Convergence: 0.000989\n",
      "Evidence 14637.236\n",
      "\n",
      "Epoch: 224, Evidence: 14637.23633, Convergence: 0.022849\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 3.30e-01\n",
      "Epoch: 224, Loss: 2465.17718, Residuals: -0.96741, Convergence:   inf\n",
      "Epoch: 225, Loss: 2458.09251, Residuals: -0.96475, Convergence: 0.002882\n",
      "Epoch: 226, Loss: 2452.21351, Residuals: -0.96244, Convergence: 0.002397\n",
      "Epoch: 227, Loss: 2447.21948, Residuals: -0.96046, Convergence: 0.002041\n",
      "Epoch: 228, Loss: 2442.92242, Residuals: -0.95876, Convergence: 0.001759\n",
      "Epoch: 229, Loss: 2439.18595, Residuals: -0.95733, Convergence: 0.001532\n",
      "Epoch: 230, Loss: 2435.90331, Residuals: -0.95610, Convergence: 0.001348\n",
      "Epoch: 231, Loss: 2432.99095, Residuals: -0.95506, Convergence: 0.001197\n",
      "Epoch: 232, Loss: 2430.38364, Residuals: -0.95416, Convergence: 0.001073\n",
      "Epoch: 233, Loss: 2428.02801, Residuals: -0.95339, Convergence: 0.000970\n",
      "Evidence 14726.194\n",
      "\n",
      "Epoch: 233, Evidence: 14726.19434, Convergence: 0.006041\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 2.59e-01\n",
      "Epoch: 233, Loss: 2466.55752, Residuals: -0.95339, Convergence:   inf\n",
      "Epoch: 234, Loss: 2462.33793, Residuals: -0.95137, Convergence: 0.001714\n",
      "Epoch: 235, Loss: 2458.83251, Residuals: -0.94971, Convergence: 0.001426\n",
      "Epoch: 236, Loss: 2455.84610, Residuals: -0.94835, Convergence: 0.001216\n",
      "Epoch: 237, Loss: 2453.25787, Residuals: -0.94725, Convergence: 0.001055\n",
      "Epoch: 238, Loss: 2450.98161, Residuals: -0.94634, Convergence: 0.000929\n",
      "Evidence 14759.712\n",
      "\n",
      "Epoch: 238, Evidence: 14759.71191, Convergence: 0.002271\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 2.08e-01\n",
      "Epoch: 238, Loss: 2467.42215, Residuals: -0.94634, Convergence:   inf\n",
      "Epoch: 239, Loss: 2464.47026, Residuals: -0.94488, Convergence: 0.001198\n",
      "Epoch: 240, Loss: 2462.01226, Residuals: -0.94368, Convergence: 0.000998\n",
      "Evidence 14772.945\n",
      "\n",
      "Epoch: 240, Evidence: 14772.94531, Convergence: 0.000896\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.72e-01\n",
      "Epoch: 240, Loss: 2468.10234, Residuals: -0.94368, Convergence:   inf\n",
      "Epoch: 241, Loss: 2463.73366, Residuals: -0.94229, Convergence: 0.001773\n",
      "Epoch: 242, Loss: 2460.27415, Residuals: -0.94053, Convergence: 0.001406\n",
      "Epoch: 243, Loss: 2457.48618, Residuals: -0.93918, Convergence: 0.001134\n",
      "Epoch: 244, Loss: 2455.13000, Residuals: -0.93835, Convergence: 0.000960\n",
      "Evidence 14790.593\n",
      "\n",
      "Epoch: 244, Evidence: 14790.59277, Convergence: 0.002088\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.43e-01\n",
      "Epoch: 244, Loss: 2468.25461, Residuals: -0.93835, Convergence:   inf\n",
      "Epoch: 245, Loss: 2465.38311, Residuals: -0.93618, Convergence: 0.001165\n",
      "Epoch: 246, Loss: 2463.08948, Residuals: -0.93477, Convergence: 0.000931\n",
      "Evidence 14801.320\n",
      "\n",
      "Epoch: 246, Evidence: 14801.32031, Convergence: 0.000725\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.21e-01\n",
      "Epoch: 246, Loss: 2468.44658, Residuals: -0.93477, Convergence:   inf\n",
      "Epoch: 247, Loss: 2464.29555, Residuals: -0.93113, Convergence: 0.001684\n",
      "Epoch: 248, Loss: 2461.28303, Residuals: -0.93184, Convergence: 0.001224\n",
      "Epoch: 249, Loss: 2458.73343, Residuals: -0.93295, Convergence: 0.001037\n",
      "Epoch: 250, Loss: 2456.55121, Residuals: -0.93591, Convergence: 0.000888\n",
      "Evidence 14816.445\n",
      "\n",
      "Epoch: 250, Evidence: 14816.44531, Convergence: 0.001745\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.08e-01\n",
      "Epoch: 250, Loss: 2467.80464, Residuals: -0.93591, Convergence:   inf\n",
      "Epoch: 251, Loss: 2466.47272, Residuals: -0.93476, Convergence: 0.000540\n",
      "Evidence 14822.283\n",
      "\n",
      "Epoch: 251, Evidence: 14822.28320, Convergence: 0.000394\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 8.79e-02\n",
      "Epoch: 251, Loss: 2468.84472, Residuals: -0.93476, Convergence:   inf\n",
      "Epoch: 252, Loss: 2521.06551, Residuals: -0.97529, Convergence: -0.020714\n",
      "Epoch: 252, Loss: 2466.61541, Residuals: -0.93181, Convergence: 0.000904\n",
      "Evidence 14826.613\n",
      "\n",
      "Epoch: 252, Evidence: 14826.61328, Convergence: 0.000686\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 8.08e-02\n",
      "Epoch: 252, Loss: 2468.29187, Residuals: -0.93181, Convergence:   inf\n",
      "Epoch: 253, Loss: 2472.65671, Residuals: -0.93250, Convergence: -0.001765\n",
      "Epoch: 253, Loss: 2468.26773, Residuals: -0.92999, Convergence: 0.000010\n",
      "Evidence 14828.178\n",
      "\n",
      "Epoch: 253, Evidence: 14828.17773, Convergence: 0.000791\n",
      "Total samples: 183, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 384.56974, Residuals: -4.53355, Convergence:   inf\n",
      "Epoch: 1, Loss: 358.74275, Residuals: -4.41221, Convergence: 0.071993\n",
      "Epoch: 2, Loss: 337.57518, Residuals: -4.24721, Convergence: 0.062705\n",
      "Epoch: 3, Loss: 321.41200, Residuals: -4.08221, Convergence: 0.050288\n",
      "Epoch: 4, Loss: 309.07555, Residuals: -3.93681, Convergence: 0.039914\n",
      "Epoch: 5, Loss: 299.28667, Residuals: -3.80832, Convergence: 0.032707\n",
      "Epoch: 6, Loss: 291.33634, Residuals: -3.69650, Convergence: 0.027289\n",
      "Epoch: 7, Loss: 284.73298, Residuals: -3.60082, Convergence: 0.023191\n",
      "Epoch: 8, Loss: 279.11270, Residuals: -3.51919, Convergence: 0.020136\n",
      "Epoch: 9, Loss: 274.21883, Residuals: -3.44922, Convergence: 0.017847\n",
      "Epoch: 10, Loss: 269.86896, Residuals: -3.38880, Convergence: 0.016118\n",
      "Epoch: 11, Loss: 265.93101, Residuals: -3.33610, Convergence: 0.014808\n",
      "Epoch: 12, Loss: 262.30868, Residuals: -3.28958, Convergence: 0.013809\n",
      "Epoch: 13, Loss: 258.93206, Residuals: -3.24791, Convergence: 0.013041\n",
      "Epoch: 14, Loss: 255.75117, Residuals: -3.20994, Convergence: 0.012437\n",
      "Epoch: 15, Loss: 252.73221, Residuals: -3.17469, Convergence: 0.011945\n",
      "Epoch: 16, Loss: 249.85500, Residuals: -3.14150, Convergence: 0.011516\n",
      "Epoch: 17, Loss: 247.10547, Residuals: -3.10992, Convergence: 0.011127\n",
      "Epoch: 18, Loss: 244.46347, Residuals: -3.07958, Convergence: 0.010807\n",
      "Epoch: 19, Loss: 241.89839, Residuals: -3.04997, Convergence: 0.010604\n",
      "Epoch: 20, Loss: 239.37528, Residuals: -3.02054, Convergence: 0.010540\n",
      "Epoch: 21, Loss: 236.86384, Residuals: -2.99079, Convergence: 0.010603\n",
      "Epoch: 22, Loss: 234.34245, Residuals: -2.96040, Convergence: 0.010759\n",
      "Epoch: 23, Loss: 231.78846, Residuals: -2.92912, Convergence: 0.011019\n",
      "Epoch: 24, Loss: 229.16110, Residuals: -2.89648, Convergence: 0.011465\n",
      "Epoch: 25, Loss: 226.40399, Residuals: -2.86184, Convergence: 0.012178\n",
      "Epoch: 26, Loss: 223.50590, Residuals: -2.82497, Convergence: 0.012966\n",
      "Epoch: 27, Loss: 220.57628, Residuals: -2.78702, Convergence: 0.013282\n",
      "Epoch: 28, Loss: 217.73097, Residuals: -2.74932, Convergence: 0.013068\n",
      "Epoch: 29, Loss: 214.99360, Residuals: -2.71232, Convergence: 0.012732\n",
      "Epoch: 30, Loss: 212.34927, Residuals: -2.67597, Convergence: 0.012453\n",
      "Epoch: 31, Loss: 209.78037, Residuals: -2.64014, Convergence: 0.012246\n",
      "Epoch: 32, Loss: 207.27409, Residuals: -2.60470, Convergence: 0.012092\n",
      "Epoch: 33, Loss: 204.82255, Residuals: -2.56958, Convergence: 0.011969\n",
      "Epoch: 34, Loss: 202.42176, Residuals: -2.53470, Convergence: 0.011860\n",
      "Epoch: 35, Loss: 200.07040, Residuals: -2.50002, Convergence: 0.011753\n",
      "Epoch: 36, Loss: 197.76894, Residuals: -2.46553, Convergence: 0.011637\n",
      "Epoch: 37, Loss: 195.51882, Residuals: -2.43120, Convergence: 0.011508\n",
      "Epoch: 38, Loss: 193.32197, Residuals: -2.39703, Convergence: 0.011364\n",
      "Epoch: 39, Loss: 191.18042, Residuals: -2.36305, Convergence: 0.011202\n",
      "Epoch: 40, Loss: 189.09607, Residuals: -2.32926, Convergence: 0.011023\n",
      "Epoch: 41, Loss: 187.07054, Residuals: -2.29569, Convergence: 0.010828\n",
      "Epoch: 42, Loss: 185.10510, Residuals: -2.26236, Convergence: 0.010618\n",
      "Epoch: 43, Loss: 183.20063, Residuals: -2.22931, Convergence: 0.010396\n",
      "Epoch: 44, Loss: 181.35766, Residuals: -2.19657, Convergence: 0.010162\n",
      "Epoch: 45, Loss: 179.57641, Residuals: -2.16417, Convergence: 0.009919\n",
      "Epoch: 46, Loss: 177.85701, Residuals: -2.13214, Convergence: 0.009667\n",
      "Epoch: 47, Loss: 176.19952, Residuals: -2.10052, Convergence: 0.009407\n",
      "Epoch: 48, Loss: 174.60406, Residuals: -2.06935, Convergence: 0.009138\n",
      "Epoch: 49, Loss: 173.07078, Residuals: -2.03867, Convergence: 0.008859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50, Loss: 171.59959, Residuals: -2.00853, Convergence: 0.008573\n",
      "Epoch: 51, Loss: 170.18999, Residuals: -1.97895, Convergence: 0.008282\n",
      "Epoch: 52, Loss: 168.84081, Residuals: -1.94999, Convergence: 0.007991\n",
      "Epoch: 53, Loss: 167.55014, Residuals: -1.92164, Convergence: 0.007703\n",
      "Epoch: 54, Loss: 166.31545, Residuals: -1.89394, Convergence: 0.007424\n",
      "Epoch: 55, Loss: 165.13376, Residuals: -1.86688, Convergence: 0.007156\n",
      "Epoch: 56, Loss: 164.00190, Residuals: -1.84046, Convergence: 0.006901\n",
      "Epoch: 57, Loss: 162.91679, Residuals: -1.81466, Convergence: 0.006661\n",
      "Epoch: 58, Loss: 161.87554, Residuals: -1.78948, Convergence: 0.006432\n",
      "Epoch: 59, Loss: 160.87569, Residuals: -1.76490, Convergence: 0.006215\n",
      "Epoch: 60, Loss: 159.91516, Residuals: -1.74091, Convergence: 0.006006\n",
      "Epoch: 61, Loss: 158.99223, Residuals: -1.71751, Convergence: 0.005805\n",
      "Epoch: 62, Loss: 158.10547, Residuals: -1.69470, Convergence: 0.005609\n",
      "Epoch: 63, Loss: 157.25369, Residuals: -1.67248, Convergence: 0.005417\n",
      "Epoch: 64, Loss: 156.43583, Residuals: -1.65085, Convergence: 0.005228\n",
      "Epoch: 65, Loss: 155.65095, Residuals: -1.62982, Convergence: 0.005043\n",
      "Epoch: 66, Loss: 154.89818, Residuals: -1.60940, Convergence: 0.004860\n",
      "Epoch: 67, Loss: 154.17666, Residuals: -1.58957, Convergence: 0.004680\n",
      "Epoch: 68, Loss: 153.48557, Residuals: -1.57036, Convergence: 0.004503\n",
      "Epoch: 69, Loss: 152.82409, Residuals: -1.55175, Convergence: 0.004328\n",
      "Epoch: 70, Loss: 152.19140, Residuals: -1.53375, Convergence: 0.004157\n",
      "Epoch: 71, Loss: 151.58669, Residuals: -1.51635, Convergence: 0.003989\n",
      "Epoch: 72, Loss: 151.00913, Residuals: -1.49955, Convergence: 0.003825\n",
      "Epoch: 73, Loss: 150.45788, Residuals: -1.48334, Convergence: 0.003664\n",
      "Epoch: 74, Loss: 149.93209, Residuals: -1.46772, Convergence: 0.003507\n",
      "Epoch: 75, Loss: 149.43088, Residuals: -1.45269, Convergence: 0.003354\n",
      "Epoch: 76, Loss: 148.95338, Residuals: -1.43822, Convergence: 0.003206\n",
      "Epoch: 77, Loss: 148.49870, Residuals: -1.42431, Convergence: 0.003062\n",
      "Epoch: 78, Loss: 148.06592, Residuals: -1.41095, Convergence: 0.002923\n",
      "Epoch: 79, Loss: 147.65415, Residuals: -1.39813, Convergence: 0.002789\n",
      "Epoch: 80, Loss: 147.26246, Residuals: -1.38583, Convergence: 0.002660\n",
      "Epoch: 81, Loss: 146.88999, Residuals: -1.37403, Convergence: 0.002536\n",
      "Epoch: 82, Loss: 146.53584, Residuals: -1.36273, Convergence: 0.002417\n",
      "Epoch: 83, Loss: 146.19914, Residuals: -1.35190, Convergence: 0.002303\n",
      "Epoch: 84, Loss: 145.87906, Residuals: -1.34154, Convergence: 0.002194\n",
      "Epoch: 85, Loss: 145.57479, Residuals: -1.33161, Convergence: 0.002090\n",
      "Epoch: 86, Loss: 145.28555, Residuals: -1.32212, Convergence: 0.001991\n",
      "Epoch: 87, Loss: 145.01062, Residuals: -1.31303, Convergence: 0.001896\n",
      "Epoch: 88, Loss: 144.74931, Residuals: -1.30434, Convergence: 0.001805\n",
      "Epoch: 89, Loss: 144.50094, Residuals: -1.29603, Convergence: 0.001719\n",
      "Epoch: 90, Loss: 144.26491, Residuals: -1.28809, Convergence: 0.001636\n",
      "Epoch: 91, Loss: 144.04064, Residuals: -1.28049, Convergence: 0.001557\n",
      "Epoch: 92, Loss: 143.82759, Residuals: -1.27322, Convergence: 0.001481\n",
      "Epoch: 93, Loss: 143.62526, Residuals: -1.26628, Convergence: 0.001409\n",
      "Epoch: 94, Loss: 143.43318, Residuals: -1.25964, Convergence: 0.001339\n",
      "Epoch: 95, Loss: 143.25093, Residuals: -1.25329, Convergence: 0.001272\n",
      "Epoch: 96, Loss: 143.07811, Residuals: -1.24723, Convergence: 0.001208\n",
      "Epoch: 97, Loss: 142.91435, Residuals: -1.24143, Convergence: 0.001146\n",
      "Epoch: 98, Loss: 142.75932, Residuals: -1.23588, Convergence: 0.001086\n",
      "Epoch: 99, Loss: 142.61270, Residuals: -1.23058, Convergence: 0.001028\n",
      "Epoch: 100, Loss: 142.47421, Residuals: -1.22552, Convergence: 0.000972\n",
      "Evidence -184.717\n",
      "\n",
      "Epoch: 100, Evidence: -184.71664, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.25e-01\n",
      "Epoch: 100, Loss: 1382.69231, Residuals: -1.22552, Convergence:   inf\n",
      "Epoch: 101, Loss: 1319.15594, Residuals: -1.25634, Convergence: 0.048164\n",
      "Epoch: 102, Loss: 1270.80037, Residuals: -1.28049, Convergence: 0.038051\n",
      "Epoch: 103, Loss: 1234.32300, Residuals: -1.29777, Convergence: 0.029553\n",
      "Epoch: 104, Loss: 1206.01731, Residuals: -1.30991, Convergence: 0.023470\n",
      "Epoch: 105, Loss: 1183.18080, Residuals: -1.31893, Convergence: 0.019301\n",
      "Epoch: 106, Loss: 1164.25538, Residuals: -1.32589, Convergence: 0.016255\n",
      "Epoch: 107, Loss: 1148.30528, Residuals: -1.33125, Convergence: 0.013890\n",
      "Epoch: 108, Loss: 1134.69485, Residuals: -1.33524, Convergence: 0.011995\n",
      "Epoch: 109, Loss: 1122.95409, Residuals: -1.33800, Convergence: 0.010455\n",
      "Epoch: 110, Loss: 1112.71585, Residuals: -1.33965, Convergence: 0.009201\n",
      "Epoch: 111, Loss: 1103.68589, Residuals: -1.34030, Convergence: 0.008182\n",
      "Epoch: 112, Loss: 1095.62166, Residuals: -1.34004, Convergence: 0.007360\n",
      "Epoch: 113, Loss: 1088.31964, Residuals: -1.33892, Convergence: 0.006709\n",
      "Epoch: 114, Loss: 1081.60522, Residuals: -1.33700, Convergence: 0.006208\n",
      "Epoch: 115, Loss: 1075.32488, Residuals: -1.33432, Convergence: 0.005840\n",
      "Epoch: 116, Loss: 1069.34193, Residuals: -1.33089, Convergence: 0.005595\n",
      "Epoch: 117, Loss: 1063.53583, Residuals: -1.32674, Convergence: 0.005459\n",
      "Epoch: 118, Loss: 1057.80654, Residuals: -1.32187, Convergence: 0.005416\n",
      "Epoch: 119, Loss: 1052.08699, Residuals: -1.31634, Convergence: 0.005436\n",
      "Epoch: 120, Loss: 1046.35749, Residuals: -1.31020, Convergence: 0.005476\n",
      "Epoch: 121, Loss: 1040.65563, Residuals: -1.30355, Convergence: 0.005479\n",
      "Epoch: 122, Loss: 1035.06541, Residuals: -1.29648, Convergence: 0.005401\n",
      "Epoch: 123, Loss: 1029.68268, Residuals: -1.28910, Convergence: 0.005228\n",
      "Epoch: 124, Loss: 1024.58203, Residuals: -1.28150, Convergence: 0.004978\n",
      "Epoch: 125, Loss: 1019.79982, Residuals: -1.27376, Convergence: 0.004689\n",
      "Epoch: 126, Loss: 1015.33877, Residuals: -1.26595, Convergence: 0.004394\n",
      "Epoch: 127, Loss: 1011.18090, Residuals: -1.25813, Convergence: 0.004112\n",
      "Epoch: 128, Loss: 1007.29856, Residuals: -1.25036, Convergence: 0.003854\n",
      "Epoch: 129, Loss: 1003.66284, Residuals: -1.24268, Convergence: 0.003622\n",
      "Epoch: 130, Loss: 1000.24608, Residuals: -1.23512, Convergence: 0.003416\n",
      "Epoch: 131, Loss: 997.02428, Residuals: -1.22770, Convergence: 0.003231\n",
      "Epoch: 132, Loss: 993.97637, Residuals: -1.22046, Convergence: 0.003066\n",
      "Epoch: 133, Loss: 991.08574, Residuals: -1.21340, Convergence: 0.002917\n",
      "Epoch: 134, Loss: 988.33736, Residuals: -1.20655, Convergence: 0.002781\n",
      "Epoch: 135, Loss: 985.71975, Residuals: -1.19991, Convergence: 0.002656\n",
      "Epoch: 136, Loss: 983.22310, Residuals: -1.19349, Convergence: 0.002539\n",
      "Epoch: 137, Loss: 980.83875, Residuals: -1.18729, Convergence: 0.002431\n",
      "Epoch: 138, Loss: 978.55925, Residuals: -1.18131, Convergence: 0.002329\n",
      "Epoch: 139, Loss: 976.37838, Residuals: -1.17556, Convergence: 0.002234\n",
      "Epoch: 140, Loss: 974.29041, Residuals: -1.17004, Convergence: 0.002143\n",
      "Epoch: 141, Loss: 972.28945, Residuals: -1.16474, Convergence: 0.002058\n",
      "Epoch: 142, Loss: 970.37035, Residuals: -1.15966, Convergence: 0.001978\n",
      "Epoch: 143, Loss: 968.52833, Residuals: -1.15479, Convergence: 0.001902\n",
      "Epoch: 144, Loss: 966.75861, Residuals: -1.15012, Convergence: 0.001831\n",
      "Epoch: 145, Loss: 965.05681, Residuals: -1.14565, Convergence: 0.001763\n",
      "Epoch: 146, Loss: 963.41842, Residuals: -1.14137, Convergence: 0.001701\n",
      "Epoch: 147, Loss: 961.83938, Residuals: -1.13727, Convergence: 0.001642\n",
      "Epoch: 148, Loss: 960.31543, Residuals: -1.13334, Convergence: 0.001587\n",
      "Epoch: 149, Loss: 958.84242, Residuals: -1.12958, Convergence: 0.001536\n",
      "Epoch: 150, Loss: 957.41669, Residuals: -1.12596, Convergence: 0.001489\n",
      "Epoch: 151, Loss: 956.03413, Residuals: -1.12249, Convergence: 0.001446\n",
      "Epoch: 152, Loss: 954.69000, Residuals: -1.11914, Convergence: 0.001408\n",
      "Epoch: 153, Loss: 953.38011, Residuals: -1.11592, Convergence: 0.001374\n",
      "Epoch: 154, Loss: 952.09995, Residuals: -1.11281, Convergence: 0.001345\n",
      "Epoch: 155, Loss: 950.84508, Residuals: -1.10979, Convergence: 0.001320\n",
      "Epoch: 156, Loss: 949.61013, Residuals: -1.10686, Convergence: 0.001300\n",
      "Epoch: 157, Loss: 948.39113, Residuals: -1.10400, Convergence: 0.001285\n",
      "Epoch: 158, Loss: 947.18347, Residuals: -1.10121, Convergence: 0.001275\n",
      "Epoch: 159, Loss: 945.98468, Residuals: -1.09846, Convergence: 0.001267\n",
      "Epoch: 160, Loss: 944.79291, Residuals: -1.09577, Convergence: 0.001261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 161, Loss: 943.60845, Residuals: -1.09311, Convergence: 0.001255\n",
      "Epoch: 162, Loss: 942.43422, Residuals: -1.09049, Convergence: 0.001246\n",
      "Epoch: 163, Loss: 941.27385, Residuals: -1.08792, Convergence: 0.001233\n",
      "Epoch: 164, Loss: 940.13258, Residuals: -1.08540, Convergence: 0.001214\n",
      "Epoch: 165, Loss: 939.01566, Residuals: -1.08294, Convergence: 0.001189\n",
      "Epoch: 166, Loss: 937.92831, Residuals: -1.08055, Convergence: 0.001159\n",
      "Epoch: 167, Loss: 936.87363, Residuals: -1.07823, Convergence: 0.001126\n",
      "Epoch: 168, Loss: 935.85468, Residuals: -1.07598, Convergence: 0.001089\n",
      "Epoch: 169, Loss: 934.87271, Residuals: -1.07381, Convergence: 0.001050\n",
      "Epoch: 170, Loss: 933.92809, Residuals: -1.07172, Convergence: 0.001011\n",
      "Epoch: 171, Loss: 933.02028, Residuals: -1.06970, Convergence: 0.000973\n",
      "Evidence 11227.997\n",
      "\n",
      "Epoch: 171, Evidence: 11227.99707, Convergence: 1.016451\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 5.75e-01\n",
      "Epoch: 171, Loss: 2368.54865, Residuals: -1.06970, Convergence:   inf\n",
      "Epoch: 172, Loss: 2328.94877, Residuals: -1.07719, Convergence: 0.017003\n",
      "Epoch: 173, Loss: 2301.82537, Residuals: -1.07571, Convergence: 0.011783\n",
      "Epoch: 174, Loss: 2279.30593, Residuals: -1.07340, Convergence: 0.009880\n",
      "Epoch: 175, Loss: 2260.32469, Residuals: -1.07089, Convergence: 0.008398\n",
      "Epoch: 176, Loss: 2244.17996, Residuals: -1.06827, Convergence: 0.007194\n",
      "Epoch: 177, Loss: 2230.32784, Residuals: -1.06558, Convergence: 0.006211\n",
      "Epoch: 178, Loss: 2218.31954, Residuals: -1.06282, Convergence: 0.005413\n",
      "Epoch: 179, Loss: 2207.78425, Residuals: -1.05999, Convergence: 0.004772\n",
      "Epoch: 180, Loss: 2198.41662, Residuals: -1.05707, Convergence: 0.004261\n",
      "Epoch: 181, Loss: 2189.97658, Residuals: -1.05405, Convergence: 0.003854\n",
      "Epoch: 182, Loss: 2182.28997, Residuals: -1.05093, Convergence: 0.003522\n",
      "Epoch: 183, Loss: 2175.24082, Residuals: -1.04771, Convergence: 0.003241\n",
      "Epoch: 184, Loss: 2168.75706, Residuals: -1.04446, Convergence: 0.002990\n",
      "Epoch: 185, Loss: 2162.79060, Residuals: -1.04120, Convergence: 0.002759\n",
      "Epoch: 186, Loss: 2157.29920, Residuals: -1.03799, Convergence: 0.002545\n",
      "Epoch: 187, Loss: 2152.24295, Residuals: -1.03486, Convergence: 0.002349\n",
      "Epoch: 188, Loss: 2147.58160, Residuals: -1.03184, Convergence: 0.002171\n",
      "Epoch: 189, Loss: 2143.27560, Residuals: -1.02894, Convergence: 0.002009\n",
      "Epoch: 190, Loss: 2139.28829, Residuals: -1.02617, Convergence: 0.001864\n",
      "Epoch: 191, Loss: 2135.58754, Residuals: -1.02353, Convergence: 0.001733\n",
      "Epoch: 192, Loss: 2132.14441, Residuals: -1.02101, Convergence: 0.001615\n",
      "Epoch: 193, Loss: 2128.93567, Residuals: -1.01861, Convergence: 0.001507\n",
      "Epoch: 194, Loss: 2125.94154, Residuals: -1.01634, Convergence: 0.001408\n",
      "Epoch: 195, Loss: 2123.14493, Residuals: -1.01418, Convergence: 0.001317\n",
      "Epoch: 196, Loss: 2120.53086, Residuals: -1.01213, Convergence: 0.001233\n",
      "Epoch: 197, Loss: 2118.08681, Residuals: -1.01018, Convergence: 0.001154\n",
      "Epoch: 198, Loss: 2115.79997, Residuals: -1.00833, Convergence: 0.001081\n",
      "Epoch: 199, Loss: 2113.65974, Residuals: -1.00656, Convergence: 0.001013\n",
      "Epoch: 200, Loss: 2111.65525, Residuals: -1.00488, Convergence: 0.000949\n",
      "Evidence 14402.509\n",
      "\n",
      "Epoch: 200, Evidence: 14402.50879, Convergence: 0.220414\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 4.36e-01\n",
      "Epoch: 200, Loss: 2493.05854, Residuals: -1.00488, Convergence:   inf\n",
      "Epoch: 201, Loss: 2479.89414, Residuals: -1.00199, Convergence: 0.005308\n",
      "Epoch: 202, Loss: 2469.03705, Residuals: -0.99870, Convergence: 0.004397\n",
      "Epoch: 203, Loss: 2459.64438, Residuals: -0.99553, Convergence: 0.003819\n",
      "Epoch: 204, Loss: 2451.48209, Residuals: -0.99256, Convergence: 0.003330\n",
      "Epoch: 205, Loss: 2444.36690, Residuals: -0.98982, Convergence: 0.002911\n",
      "Epoch: 206, Loss: 2438.14191, Residuals: -0.98732, Convergence: 0.002553\n",
      "Epoch: 207, Loss: 2432.67359, Residuals: -0.98504, Convergence: 0.002248\n",
      "Epoch: 208, Loss: 2427.84803, Residuals: -0.98296, Convergence: 0.001988\n",
      "Epoch: 209, Loss: 2423.56813, Residuals: -0.98107, Convergence: 0.001766\n",
      "Epoch: 210, Loss: 2419.75141, Residuals: -0.97935, Convergence: 0.001577\n",
      "Epoch: 211, Loss: 2416.32855, Residuals: -0.97778, Convergence: 0.001417\n",
      "Epoch: 212, Loss: 2413.24229, Residuals: -0.97635, Convergence: 0.001279\n",
      "Epoch: 213, Loss: 2410.44370, Residuals: -0.97504, Convergence: 0.001161\n",
      "Epoch: 214, Loss: 2407.89397, Residuals: -0.97385, Convergence: 0.001059\n",
      "Epoch: 215, Loss: 2405.55791, Residuals: -0.97276, Convergence: 0.000971\n",
      "Evidence 14775.448\n",
      "\n",
      "Epoch: 215, Evidence: 14775.44824, Convergence: 0.025240\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 3.34e-01\n",
      "Epoch: 215, Loss: 2498.89266, Residuals: -0.97276, Convergence:   inf\n",
      "Epoch: 216, Loss: 2492.43447, Residuals: -0.96961, Convergence: 0.002591\n",
      "Epoch: 217, Loss: 2487.07053, Residuals: -0.96689, Convergence: 0.002157\n",
      "Epoch: 218, Loss: 2482.52319, Residuals: -0.96461, Convergence: 0.001832\n",
      "Epoch: 219, Loss: 2478.62428, Residuals: -0.96269, Convergence: 0.001573\n",
      "Epoch: 220, Loss: 2475.24129, Residuals: -0.96109, Convergence: 0.001367\n",
      "Epoch: 221, Loss: 2472.27123, Residuals: -0.95974, Convergence: 0.001201\n",
      "Epoch: 222, Loss: 2469.63297, Residuals: -0.95862, Convergence: 0.001068\n",
      "Epoch: 223, Loss: 2467.26480, Residuals: -0.95768, Convergence: 0.000960\n",
      "Evidence 14854.786\n",
      "\n",
      "Epoch: 223, Evidence: 14854.78613, Convergence: 0.005341\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.62e-01\n",
      "Epoch: 223, Loss: 2500.64684, Residuals: -0.95768, Convergence:   inf\n",
      "Epoch: 224, Loss: 2496.71553, Residuals: -0.95543, Convergence: 0.001575\n",
      "Epoch: 225, Loss: 2493.45958, Residuals: -0.95366, Convergence: 0.001306\n",
      "Epoch: 226, Loss: 2490.69183, Residuals: -0.95227, Convergence: 0.001111\n",
      "Epoch: 227, Loss: 2488.29185, Residuals: -0.95118, Convergence: 0.000965\n",
      "Evidence 14882.757\n",
      "\n",
      "Epoch: 227, Evidence: 14882.75684, Convergence: 0.001879\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.12e-01\n",
      "Epoch: 227, Loss: 2501.66444, Residuals: -0.95118, Convergence:   inf\n",
      "Epoch: 228, Loss: 2498.75805, Residuals: -0.94947, Convergence: 0.001163\n",
      "Epoch: 229, Loss: 2496.33774, Residuals: -0.94816, Convergence: 0.000970\n",
      "Evidence 14894.965\n",
      "\n",
      "Epoch: 229, Evidence: 14894.96484, Convergence: 0.000820\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.76e-01\n",
      "Epoch: 229, Loss: 2502.37493, Residuals: -0.94816, Convergence:   inf\n",
      "Epoch: 230, Loss: 2497.85928, Residuals: -0.94605, Convergence: 0.001808\n",
      "Epoch: 231, Loss: 2494.41931, Residuals: -0.94455, Convergence: 0.001379\n",
      "Epoch: 232, Loss: 2491.64343, Residuals: -0.94367, Convergence: 0.001114\n",
      "Epoch: 233, Loss: 2489.29740, Residuals: -0.94328, Convergence: 0.000942\n",
      "Evidence 14912.746\n",
      "\n",
      "Epoch: 233, Evidence: 14912.74609, Convergence: 0.002011\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.46e-01\n",
      "Epoch: 233, Loss: 2502.60684, Residuals: -0.94328, Convergence:   inf\n",
      "Epoch: 234, Loss: 2499.60115, Residuals: -0.94124, Convergence: 0.001202\n",
      "Epoch: 235, Loss: 2497.23906, Residuals: -0.94035, Convergence: 0.000946\n",
      "Evidence 14923.753\n",
      "\n",
      "Epoch: 235, Evidence: 14923.75293, Convergence: 0.000738\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.23e-01\n",
      "Epoch: 235, Loss: 2502.79551, Residuals: -0.94035, Convergence:   inf\n",
      "Epoch: 236, Loss: 2498.40265, Residuals: -0.93775, Convergence: 0.001758\n",
      "Epoch: 237, Loss: 2495.31427, Residuals: -0.93915, Convergence: 0.001238\n",
      "Epoch: 238, Loss: 2492.76197, Residuals: -0.93972, Convergence: 0.001024\n",
      "Epoch: 239, Loss: 2490.53969, Residuals: -0.94234, Convergence: 0.000892\n",
      "Evidence 14939.368\n",
      "\n",
      "Epoch: 239, Evidence: 14939.36816, Convergence: 0.001782\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.09e-01\n",
      "Epoch: 239, Loss: 2502.14879, Residuals: -0.94234, Convergence:   inf\n",
      "Epoch: 240, Loss: 2500.30458, Residuals: -0.93987, Convergence: 0.000738\n",
      "Evidence 14946.188\n",
      "\n",
      "Epoch: 240, Evidence: 14946.18750, Convergence: 0.000456\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 8.97e-02\n",
      "Epoch: 240, Loss: 2503.04289, Residuals: -0.93987, Convergence:   inf\n",
      "Epoch: 241, Loss: 2544.75208, Residuals: -0.97865, Convergence: -0.016390\n",
      "Epoch: 241, Loss: 2500.88893, Residuals: -0.93879, Convergence: 0.000861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence 14950.375\n",
      "\n",
      "Epoch: 241, Evidence: 14950.37500, Convergence: 0.000736\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 8.25e-02\n",
      "Epoch: 241, Loss: 2502.50036, Residuals: -0.93879, Convergence:   inf\n",
      "Epoch: 242, Loss: 2507.82186, Residuals: -0.94137, Convergence: -0.002122\n",
      "Epoch: 242, Loss: 2502.58389, Residuals: -0.93749, Convergence: -0.000033\n",
      "Evidence 14951.980\n",
      "\n",
      "Epoch: 242, Evidence: 14951.98047, Convergence: 0.000844\n",
      "Total samples: 184, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 383.37948, Residuals: -4.51548, Convergence:   inf\n",
      "Epoch: 1, Loss: 357.55997, Residuals: -4.39473, Convergence: 0.072210\n",
      "Epoch: 2, Loss: 336.43127, Residuals: -4.23069, Convergence: 0.062802\n",
      "Epoch: 3, Loss: 320.27197, Residuals: -4.06573, Convergence: 0.050455\n",
      "Epoch: 4, Loss: 307.96278, Residuals: -3.92070, Convergence: 0.039970\n",
      "Epoch: 5, Loss: 298.20502, Residuals: -3.79264, Convergence: 0.032722\n",
      "Epoch: 6, Loss: 290.28294, Residuals: -3.68110, Convergence: 0.027291\n",
      "Epoch: 7, Loss: 283.70319, Residuals: -3.58552, Convergence: 0.023192\n",
      "Epoch: 8, Loss: 278.10195, Residuals: -3.50383, Convergence: 0.020141\n",
      "Epoch: 9, Loss: 273.22215, Residuals: -3.43371, Convergence: 0.017860\n",
      "Epoch: 10, Loss: 268.88075, Residuals: -3.37306, Convergence: 0.016146\n",
      "Epoch: 11, Loss: 264.94497, Residuals: -3.32009, Convergence: 0.014855\n",
      "Epoch: 12, Loss: 261.31786, Residuals: -3.27329, Convergence: 0.013880\n",
      "Epoch: 13, Loss: 257.92925, Residuals: -3.23133, Convergence: 0.013138\n",
      "Epoch: 14, Loss: 254.73012, Residuals: -3.19306, Convergence: 0.012559\n",
      "Epoch: 15, Loss: 251.68971, Residuals: -3.15756, Convergence: 0.012080\n",
      "Epoch: 16, Loss: 248.79257, Residuals: -3.12419, Convergence: 0.011645\n",
      "Epoch: 17, Loss: 246.02799, Residuals: -3.09252, Convergence: 0.011237\n",
      "Epoch: 18, Loss: 243.37622, Residuals: -3.06214, Convergence: 0.010896\n",
      "Epoch: 19, Loss: 240.80617, Residuals: -3.03253, Convergence: 0.010673\n",
      "Epoch: 20, Loss: 238.28319, Residuals: -3.00312, Convergence: 0.010588\n",
      "Epoch: 21, Loss: 235.77819, Residuals: -2.97344, Convergence: 0.010624\n",
      "Epoch: 22, Loss: 233.27040, Residuals: -2.94318, Convergence: 0.010751\n",
      "Epoch: 23, Loss: 230.73577, Residuals: -2.91212, Convergence: 0.010985\n",
      "Epoch: 24, Loss: 228.12969, Residuals: -2.87975, Convergence: 0.011424\n",
      "Epoch: 25, Loss: 225.39407, Residuals: -2.84542, Convergence: 0.012137\n",
      "Epoch: 26, Loss: 222.52839, Residuals: -2.80902, Convergence: 0.012878\n",
      "Epoch: 27, Loss: 219.65487, Residuals: -2.77181, Convergence: 0.013082\n",
      "Epoch: 28, Loss: 216.87994, Residuals: -2.73504, Convergence: 0.012795\n",
      "Epoch: 29, Loss: 214.21756, Residuals: -2.69902, Convergence: 0.012428\n",
      "Epoch: 30, Loss: 211.64961, Residuals: -2.66367, Convergence: 0.012133\n",
      "Epoch: 31, Loss: 209.15718, Residuals: -2.62885, Convergence: 0.011917\n",
      "Epoch: 32, Loss: 206.72623, Residuals: -2.59443, Convergence: 0.011759\n",
      "Epoch: 33, Loss: 204.34742, Residuals: -2.56033, Convergence: 0.011641\n",
      "Epoch: 34, Loss: 202.01498, Residuals: -2.52647, Convergence: 0.011546\n",
      "Epoch: 35, Loss: 199.72567, Residuals: -2.49282, Convergence: 0.011462\n",
      "Epoch: 36, Loss: 197.47796, Residuals: -2.45932, Convergence: 0.011382\n",
      "Epoch: 37, Loss: 195.27149, Residuals: -2.42596, Convergence: 0.011300\n",
      "Epoch: 38, Loss: 193.10659, Residuals: -2.39272, Convergence: 0.011211\n",
      "Epoch: 39, Loss: 190.98406, Residuals: -2.35957, Convergence: 0.011114\n",
      "Epoch: 40, Loss: 188.90501, Residuals: -2.32653, Convergence: 0.011006\n",
      "Epoch: 41, Loss: 186.87070, Residuals: -2.29359, Convergence: 0.010886\n",
      "Epoch: 42, Loss: 184.88243, Residuals: -2.26076, Convergence: 0.010754\n",
      "Epoch: 43, Loss: 182.94145, Residuals: -2.22808, Convergence: 0.010610\n",
      "Epoch: 44, Loss: 181.04895, Residuals: -2.19557, Convergence: 0.010453\n",
      "Epoch: 45, Loss: 179.20603, Residuals: -2.16325, Convergence: 0.010284\n",
      "Epoch: 46, Loss: 177.41375, Residuals: -2.13117, Convergence: 0.010102\n",
      "Epoch: 47, Loss: 175.67332, Residuals: -2.09934, Convergence: 0.009907\n",
      "Epoch: 48, Loss: 173.98607, Residuals: -2.06781, Convergence: 0.009698\n",
      "Epoch: 49, Loss: 172.35346, Residuals: -2.03662, Convergence: 0.009472\n",
      "Epoch: 50, Loss: 170.77691, Residuals: -2.00580, Convergence: 0.009232\n",
      "Epoch: 51, Loss: 169.25763, Residuals: -1.97540, Convergence: 0.008976\n",
      "Epoch: 52, Loss: 167.79645, Residuals: -1.94546, Convergence: 0.008708\n",
      "Epoch: 53, Loss: 166.39374, Residuals: -1.91603, Convergence: 0.008430\n",
      "Epoch: 54, Loss: 165.04932, Residuals: -1.88715, Convergence: 0.008146\n",
      "Epoch: 55, Loss: 163.76261, Residuals: -1.85885, Convergence: 0.007857\n",
      "Epoch: 56, Loss: 162.53258, Residuals: -1.83116, Convergence: 0.007568\n",
      "Epoch: 57, Loss: 161.35788, Residuals: -1.80413, Convergence: 0.007280\n",
      "Epoch: 58, Loss: 160.23693, Residuals: -1.77777, Convergence: 0.006996\n",
      "Epoch: 59, Loss: 159.16797, Residuals: -1.75209, Convergence: 0.006716\n",
      "Epoch: 60, Loss: 158.14910, Residuals: -1.72712, Convergence: 0.006442\n",
      "Epoch: 61, Loss: 157.17838, Residuals: -1.70286, Convergence: 0.006176\n",
      "Epoch: 62, Loss: 156.25382, Residuals: -1.67932, Convergence: 0.005917\n",
      "Epoch: 63, Loss: 155.37348, Residuals: -1.65650, Convergence: 0.005666\n",
      "Epoch: 64, Loss: 154.53543, Residuals: -1.63440, Convergence: 0.005423\n",
      "Epoch: 65, Loss: 153.73777, Residuals: -1.61302, Convergence: 0.005188\n",
      "Epoch: 66, Loss: 152.97870, Residuals: -1.59234, Convergence: 0.004962\n",
      "Epoch: 67, Loss: 152.25649, Residuals: -1.57238, Convergence: 0.004743\n",
      "Epoch: 68, Loss: 151.56945, Residuals: -1.55310, Convergence: 0.004533\n",
      "Epoch: 69, Loss: 150.91600, Residuals: -1.53451, Convergence: 0.004330\n",
      "Epoch: 70, Loss: 150.29461, Residuals: -1.51660, Convergence: 0.004134\n",
      "Epoch: 71, Loss: 149.70382, Residuals: -1.49935, Convergence: 0.003946\n",
      "Epoch: 72, Loss: 149.14223, Residuals: -1.48275, Convergence: 0.003765\n",
      "Epoch: 73, Loss: 148.60849, Residuals: -1.46679, Convergence: 0.003592\n",
      "Epoch: 74, Loss: 148.10132, Residuals: -1.45145, Convergence: 0.003424\n",
      "Epoch: 75, Loss: 147.61946, Residuals: -1.43671, Convergence: 0.003264\n",
      "Epoch: 76, Loss: 147.16170, Residuals: -1.42257, Convergence: 0.003111\n",
      "Epoch: 77, Loss: 146.72687, Residuals: -1.40901, Convergence: 0.002964\n",
      "Epoch: 78, Loss: 146.31385, Residuals: -1.39601, Convergence: 0.002823\n",
      "Epoch: 79, Loss: 145.92155, Residuals: -1.38354, Convergence: 0.002688\n",
      "Epoch: 80, Loss: 145.54893, Residuals: -1.37161, Convergence: 0.002560\n",
      "Epoch: 81, Loss: 145.19497, Residuals: -1.36018, Convergence: 0.002438\n",
      "Epoch: 82, Loss: 144.85870, Residuals: -1.34924, Convergence: 0.002321\n",
      "Epoch: 83, Loss: 144.53920, Residuals: -1.33877, Convergence: 0.002210\n",
      "Epoch: 84, Loss: 144.23561, Residuals: -1.32876, Convergence: 0.002105\n",
      "Epoch: 85, Loss: 143.94708, Residuals: -1.31919, Convergence: 0.002004\n",
      "Epoch: 86, Loss: 143.67281, Residuals: -1.31003, Convergence: 0.001909\n",
      "Epoch: 87, Loss: 143.41209, Residuals: -1.30128, Convergence: 0.001818\n",
      "Epoch: 88, Loss: 143.16419, Residuals: -1.29291, Convergence: 0.001732\n",
      "Epoch: 89, Loss: 142.92845, Residuals: -1.28491, Convergence: 0.001649\n",
      "Epoch: 90, Loss: 142.70427, Residuals: -1.27727, Convergence: 0.001571\n",
      "Epoch: 91, Loss: 142.49105, Residuals: -1.26996, Convergence: 0.001496\n",
      "Epoch: 92, Loss: 142.28826, Residuals: -1.26297, Convergence: 0.001425\n",
      "Epoch: 93, Loss: 142.09540, Residuals: -1.25629, Convergence: 0.001357\n",
      "Epoch: 94, Loss: 141.91199, Residuals: -1.24991, Convergence: 0.001292\n",
      "Epoch: 95, Loss: 141.73758, Residuals: -1.24380, Convergence: 0.001230\n",
      "Epoch: 96, Loss: 141.57179, Residuals: -1.23796, Convergence: 0.001171\n",
      "Epoch: 97, Loss: 141.41425, Residuals: -1.23237, Convergence: 0.001114\n",
      "Epoch: 98, Loss: 141.26459, Residuals: -1.22702, Convergence: 0.001059\n",
      "Epoch: 99, Loss: 141.12251, Residuals: -1.22191, Convergence: 0.001007\n",
      "Epoch: 100, Loss: 140.98772, Residuals: -1.21701, Convergence: 0.000956\n",
      "Evidence -182.854\n",
      "\n",
      "Epoch: 100, Evidence: -182.85402, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 7.25e-01\n",
      "Epoch: 100, Loss: 1376.95129, Residuals: -1.21701, Convergence:   inf\n",
      "Epoch: 101, Loss: 1314.32082, Residuals: -1.24779, Convergence: 0.047652\n",
      "Epoch: 102, Loss: 1266.85210, Residuals: -1.27155, Convergence: 0.037470\n",
      "Epoch: 103, Loss: 1231.20788, Residuals: -1.28815, Convergence: 0.028951\n",
      "Epoch: 104, Loss: 1203.65266, Residuals: -1.29947, Convergence: 0.022893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 105, Loss: 1181.47111, Residuals: -1.30767, Convergence: 0.018775\n",
      "Epoch: 106, Loss: 1163.11325, Residuals: -1.31388, Convergence: 0.015783\n",
      "Epoch: 107, Loss: 1147.65767, Residuals: -1.31855, Convergence: 0.013467\n",
      "Epoch: 108, Loss: 1134.48452, Residuals: -1.32192, Convergence: 0.011612\n",
      "Epoch: 109, Loss: 1123.13672, Residuals: -1.32414, Convergence: 0.010104\n",
      "Epoch: 110, Loss: 1113.25933, Residuals: -1.32532, Convergence: 0.008873\n",
      "Epoch: 111, Loss: 1104.56819, Residuals: -1.32557, Convergence: 0.007868\n",
      "Epoch: 112, Loss: 1096.83108, Residuals: -1.32498, Convergence: 0.007054\n",
      "Epoch: 113, Loss: 1089.85494, Residuals: -1.32361, Convergence: 0.006401\n",
      "Epoch: 114, Loss: 1083.47907, Residuals: -1.32154, Convergence: 0.005885\n",
      "Epoch: 115, Loss: 1077.56835, Residuals: -1.31881, Convergence: 0.005485\n",
      "Epoch: 116, Loss: 1072.01112, Residuals: -1.31547, Convergence: 0.005184\n",
      "Epoch: 117, Loss: 1066.71551, Residuals: -1.31154, Convergence: 0.004964\n",
      "Epoch: 118, Loss: 1061.60741, Residuals: -1.30706, Convergence: 0.004812\n",
      "Epoch: 119, Loss: 1056.62753, Residuals: -1.30206, Convergence: 0.004713\n",
      "Epoch: 120, Loss: 1051.72681, Residuals: -1.29654, Convergence: 0.004660\n",
      "Epoch: 121, Loss: 1046.86642, Residuals: -1.29055, Convergence: 0.004643\n",
      "Epoch: 122, Loss: 1042.01917, Residuals: -1.28412, Convergence: 0.004652\n",
      "Epoch: 123, Loss: 1037.17839, Residuals: -1.27730, Convergence: 0.004667\n",
      "Epoch: 124, Loss: 1032.36402, Residuals: -1.27018, Convergence: 0.004663\n",
      "Epoch: 125, Loss: 1027.62331, Residuals: -1.26283, Convergence: 0.004613\n",
      "Epoch: 126, Loss: 1023.02006, Residuals: -1.25534, Convergence: 0.004500\n",
      "Epoch: 127, Loss: 1018.61162, Residuals: -1.24778, Convergence: 0.004328\n",
      "Epoch: 128, Loss: 1014.43461, Residuals: -1.24021, Convergence: 0.004118\n",
      "Epoch: 129, Loss: 1010.50191, Residuals: -1.23266, Convergence: 0.003892\n",
      "Epoch: 130, Loss: 1006.80704, Residuals: -1.22518, Convergence: 0.003670\n",
      "Epoch: 131, Loss: 1003.33394, Residuals: -1.21780, Convergence: 0.003462\n",
      "Epoch: 132, Loss: 1000.06143, Residuals: -1.21054, Convergence: 0.003272\n",
      "Epoch: 133, Loss: 996.97012, Residuals: -1.20342, Convergence: 0.003101\n",
      "Epoch: 134, Loss: 994.04218, Residuals: -1.19648, Convergence: 0.002945\n",
      "Epoch: 135, Loss: 991.26185, Residuals: -1.18971, Convergence: 0.002805\n",
      "Epoch: 136, Loss: 988.61680, Residuals: -1.18313, Convergence: 0.002676\n",
      "Epoch: 137, Loss: 986.09556, Residuals: -1.17676, Convergence: 0.002557\n",
      "Epoch: 138, Loss: 983.68924, Residuals: -1.17060, Convergence: 0.002446\n",
      "Epoch: 139, Loss: 981.39025, Residuals: -1.16465, Convergence: 0.002343\n",
      "Epoch: 140, Loss: 979.19120, Residuals: -1.15891, Convergence: 0.002246\n",
      "Epoch: 141, Loss: 977.08574, Residuals: -1.15339, Convergence: 0.002155\n",
      "Epoch: 142, Loss: 975.06824, Residuals: -1.14807, Convergence: 0.002069\n",
      "Epoch: 143, Loss: 973.13379, Residuals: -1.14297, Convergence: 0.001988\n",
      "Epoch: 144, Loss: 971.27760, Residuals: -1.13806, Convergence: 0.001911\n",
      "Epoch: 145, Loss: 969.49486, Residuals: -1.13335, Convergence: 0.001839\n",
      "Epoch: 146, Loss: 967.78213, Residuals: -1.12883, Convergence: 0.001770\n",
      "Epoch: 147, Loss: 966.13596, Residuals: -1.12450, Convergence: 0.001704\n",
      "Epoch: 148, Loss: 964.55286, Residuals: -1.12033, Convergence: 0.001641\n",
      "Epoch: 149, Loss: 963.02983, Residuals: -1.11634, Convergence: 0.001581\n",
      "Epoch: 150, Loss: 961.56453, Residuals: -1.11251, Convergence: 0.001524\n",
      "Epoch: 151, Loss: 960.15358, Residuals: -1.10884, Convergence: 0.001470\n",
      "Epoch: 152, Loss: 958.79494, Residuals: -1.10531, Convergence: 0.001417\n",
      "Epoch: 153, Loss: 957.48574, Residuals: -1.10193, Convergence: 0.001367\n",
      "Epoch: 154, Loss: 956.22346, Residuals: -1.09868, Convergence: 0.001320\n",
      "Epoch: 155, Loss: 955.00534, Residuals: -1.09556, Convergence: 0.001276\n",
      "Epoch: 156, Loss: 953.82830, Residuals: -1.09256, Convergence: 0.001234\n",
      "Epoch: 157, Loss: 952.68944, Residuals: -1.08968, Convergence: 0.001195\n",
      "Epoch: 158, Loss: 951.58549, Residuals: -1.08690, Convergence: 0.001160\n",
      "Epoch: 159, Loss: 950.51311, Residuals: -1.08422, Convergence: 0.001128\n",
      "Epoch: 160, Loss: 949.46825, Residuals: -1.08163, Convergence: 0.001100\n",
      "Epoch: 161, Loss: 948.44760, Residuals: -1.07913, Convergence: 0.001076\n",
      "Epoch: 162, Loss: 947.44669, Residuals: -1.07670, Convergence: 0.001056\n",
      "Epoch: 163, Loss: 946.46170, Residuals: -1.07433, Convergence: 0.001041\n",
      "Epoch: 164, Loss: 945.48826, Residuals: -1.07202, Convergence: 0.001030\n",
      "Epoch: 165, Loss: 944.52318, Residuals: -1.06976, Convergence: 0.001022\n",
      "Epoch: 166, Loss: 943.56359, Residuals: -1.06753, Convergence: 0.001017\n",
      "Epoch: 167, Loss: 942.60692, Residuals: -1.06533, Convergence: 0.001015\n",
      "Epoch: 168, Loss: 941.65302, Residuals: -1.06316, Convergence: 0.001013\n",
      "Epoch: 169, Loss: 940.70272, Residuals: -1.06101, Convergence: 0.001010\n",
      "Epoch: 170, Loss: 939.75907, Residuals: -1.05889, Convergence: 0.001004\n",
      "Epoch: 171, Loss: 938.82549, Residuals: -1.05680, Convergence: 0.000994\n",
      "Evidence 11304.798\n",
      "\n",
      "Epoch: 171, Evidence: 11304.79785, Convergence: 1.016175\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 5.74e-01\n",
      "Epoch: 171, Loss: 2361.44051, Residuals: -1.05680, Convergence:   inf\n",
      "Epoch: 172, Loss: 2317.56658, Residuals: -1.06616, Convergence: 0.018931\n",
      "Epoch: 173, Loss: 2288.06569, Residuals: -1.06464, Convergence: 0.012893\n",
      "Epoch: 174, Loss: 2263.59398, Residuals: -1.06216, Convergence: 0.010811\n",
      "Epoch: 175, Loss: 2242.97145, Residuals: -1.05944, Convergence: 0.009194\n",
      "Epoch: 176, Loss: 2225.47181, Residuals: -1.05660, Convergence: 0.007863\n",
      "Epoch: 177, Loss: 2210.53878, Residuals: -1.05370, Convergence: 0.006755\n",
      "Epoch: 178, Loss: 2197.71426, Residuals: -1.05078, Convergence: 0.005835\n",
      "Epoch: 179, Loss: 2186.61636, Residuals: -1.04785, Convergence: 0.005075\n",
      "Epoch: 180, Loss: 2176.92217, Residuals: -1.04493, Convergence: 0.004453\n",
      "Epoch: 181, Loss: 2168.35801, Residuals: -1.04200, Convergence: 0.003950\n",
      "Epoch: 182, Loss: 2160.69286, Residuals: -1.03906, Convergence: 0.003548\n",
      "Epoch: 183, Loss: 2153.73750, Residuals: -1.03608, Convergence: 0.003229\n",
      "Epoch: 184, Loss: 2147.34318, Residuals: -1.03305, Convergence: 0.002978\n",
      "Epoch: 185, Loss: 2141.40364, Residuals: -1.02996, Convergence: 0.002774\n",
      "Epoch: 186, Loss: 2135.85283, Residuals: -1.02681, Convergence: 0.002599\n",
      "Epoch: 187, Loss: 2130.65307, Residuals: -1.02363, Convergence: 0.002440\n",
      "Epoch: 188, Loss: 2125.78369, Residuals: -1.02046, Convergence: 0.002291\n",
      "Epoch: 189, Loss: 2121.22697, Residuals: -1.01732, Convergence: 0.002148\n",
      "Epoch: 190, Loss: 2116.96586, Residuals: -1.01425, Convergence: 0.002013\n",
      "Epoch: 191, Loss: 2112.98128, Residuals: -1.01126, Convergence: 0.001886\n",
      "Epoch: 192, Loss: 2109.25258, Residuals: -1.00839, Convergence: 0.001768\n",
      "Epoch: 193, Loss: 2105.75893, Residuals: -1.00563, Convergence: 0.001659\n",
      "Epoch: 194, Loss: 2102.48147, Residuals: -1.00300, Convergence: 0.001559\n",
      "Epoch: 195, Loss: 2099.40153, Residuals: -1.00049, Convergence: 0.001467\n",
      "Epoch: 196, Loss: 2096.50401, Residuals: -0.99811, Convergence: 0.001382\n",
      "Epoch: 197, Loss: 2093.77501, Residuals: -0.99587, Convergence: 0.001303\n",
      "Epoch: 198, Loss: 2091.20322, Residuals: -0.99374, Convergence: 0.001230\n",
      "Epoch: 199, Loss: 2088.77880, Residuals: -0.99175, Convergence: 0.001161\n",
      "Epoch: 200, Loss: 2086.49288, Residuals: -0.98987, Convergence: 0.001096\n",
      "Epoch: 201, Loss: 2084.33851, Residuals: -0.98810, Convergence: 0.001034\n",
      "Epoch: 202, Loss: 2082.30830, Residuals: -0.98643, Convergence: 0.000975\n",
      "Evidence 14579.062\n",
      "\n",
      "Epoch: 202, Evidence: 14579.06250, Convergence: 0.224587\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 4.34e-01\n",
      "Epoch: 202, Loss: 2475.02389, Residuals: -0.98643, Convergence:   inf\n",
      "Epoch: 203, Loss: 2460.28325, Residuals: -0.98301, Convergence: 0.005991\n",
      "Epoch: 204, Loss: 2448.22619, Residuals: -0.97912, Convergence: 0.004925\n",
      "Epoch: 205, Loss: 2437.84468, Residuals: -0.97542, Convergence: 0.004258\n",
      "Epoch: 206, Loss: 2428.86581, Residuals: -0.97196, Convergence: 0.003697\n",
      "Epoch: 207, Loss: 2421.06974, Residuals: -0.96876, Convergence: 0.003220\n",
      "Epoch: 208, Loss: 2414.27269, Residuals: -0.96583, Convergence: 0.002815\n",
      "Epoch: 209, Loss: 2408.31984, Residuals: -0.96316, Convergence: 0.002472\n",
      "Epoch: 210, Loss: 2403.08043, Residuals: -0.96072, Convergence: 0.002180\n",
      "Epoch: 211, Loss: 2398.44543, Residuals: -0.95851, Convergence: 0.001933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 212, Loss: 2394.32167, Residuals: -0.95649, Convergence: 0.001722\n",
      "Epoch: 213, Loss: 2390.63205, Residuals: -0.95465, Convergence: 0.001543\n",
      "Epoch: 214, Loss: 2387.31357, Residuals: -0.95296, Convergence: 0.001390\n",
      "Epoch: 215, Loss: 2384.31149, Residuals: -0.95142, Convergence: 0.001259\n",
      "Epoch: 216, Loss: 2381.58193, Residuals: -0.95001, Convergence: 0.001146\n",
      "Epoch: 217, Loss: 2379.08767, Residuals: -0.94872, Convergence: 0.001048\n",
      "Epoch: 218, Loss: 2376.79813, Residuals: -0.94754, Convergence: 0.000963\n",
      "Evidence 15027.341\n",
      "\n",
      "Epoch: 218, Evidence: 15027.34082, Convergence: 0.029831\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 3.31e-01\n",
      "Epoch: 218, Loss: 2479.11690, Residuals: -0.94754, Convergence:   inf\n",
      "Epoch: 219, Loss: 2472.37905, Residuals: -0.94425, Convergence: 0.002725\n",
      "Epoch: 220, Loss: 2466.77822, Residuals: -0.94140, Convergence: 0.002271\n",
      "Epoch: 221, Loss: 2462.03372, Residuals: -0.93896, Convergence: 0.001927\n",
      "Epoch: 222, Loss: 2457.96826, Residuals: -0.93690, Convergence: 0.001654\n",
      "Epoch: 223, Loss: 2454.44166, Residuals: -0.93514, Convergence: 0.001437\n",
      "Epoch: 224, Loss: 2451.34447, Residuals: -0.93366, Convergence: 0.001263\n",
      "Epoch: 225, Loss: 2448.59309, Residuals: -0.93242, Convergence: 0.001124\n",
      "Epoch: 226, Loss: 2446.12346, Residuals: -0.93137, Convergence: 0.001010\n",
      "Epoch: 227, Loss: 2443.88444, Residuals: -0.93050, Convergence: 0.000916\n",
      "Evidence 15116.986\n",
      "\n",
      "Epoch: 227, Evidence: 15116.98633, Convergence: 0.005930\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 2.58e-01\n",
      "Epoch: 227, Loss: 2480.60181, Residuals: -0.93050, Convergence:   inf\n",
      "Epoch: 228, Loss: 2476.68296, Residuals: -0.92813, Convergence: 0.001582\n",
      "Epoch: 229, Loss: 2473.41356, Residuals: -0.92626, Convergence: 0.001322\n",
      "Epoch: 230, Loss: 2470.61805, Residuals: -0.92477, Convergence: 0.001132\n",
      "Epoch: 231, Loss: 2468.18119, Residuals: -0.92359, Convergence: 0.000987\n",
      "Evidence 15146.582\n",
      "\n",
      "Epoch: 231, Evidence: 15146.58203, Convergence: 0.001954\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 2.08e-01\n",
      "Epoch: 231, Loss: 2481.62754, Residuals: -0.92359, Convergence:   inf\n",
      "Epoch: 232, Loss: 2478.67717, Residuals: -0.92167, Convergence: 0.001190\n",
      "Epoch: 233, Loss: 2476.19870, Residuals: -0.92019, Convergence: 0.001001\n",
      "Epoch: 234, Loss: 2474.05375, Residuals: -0.91905, Convergence: 0.000867\n",
      "Evidence 15161.235\n",
      "\n",
      "Epoch: 234, Evidence: 15161.23535, Convergence: 0.000966\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.72e-01\n",
      "Epoch: 234, Loss: 2482.29816, Residuals: -0.91905, Convergence:   inf\n",
      "Epoch: 235, Loss: 2477.84009, Residuals: -0.91635, Convergence: 0.001799\n",
      "Epoch: 236, Loss: 2474.47352, Residuals: -0.91483, Convergence: 0.001361\n",
      "Epoch: 237, Loss: 2471.74934, Residuals: -0.91406, Convergence: 0.001102\n",
      "Epoch: 238, Loss: 2469.43936, Residuals: -0.91385, Convergence: 0.000935\n",
      "Evidence 15179.174\n",
      "\n",
      "Epoch: 238, Evidence: 15179.17383, Convergence: 0.002147\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.42e-01\n",
      "Epoch: 238, Loss: 2482.37930, Residuals: -0.91385, Convergence:   inf\n",
      "Epoch: 239, Loss: 2479.42908, Residuals: -0.91148, Convergence: 0.001190\n",
      "Epoch: 240, Loss: 2477.12892, Residuals: -0.91053, Convergence: 0.000929\n",
      "Evidence 15190.131\n",
      "\n",
      "Epoch: 240, Evidence: 15190.13086, Convergence: 0.000721\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.20e-01\n",
      "Epoch: 240, Loss: 2482.57408, Residuals: -0.91053, Convergence:   inf\n",
      "Epoch: 241, Loss: 2478.43865, Residuals: -0.90715, Convergence: 0.001669\n",
      "Epoch: 242, Loss: 2475.45983, Residuals: -0.90901, Convergence: 0.001203\n",
      "Epoch: 243, Loss: 2472.83825, Residuals: -0.91048, Convergence: 0.001060\n",
      "Epoch: 244, Loss: 2470.61573, Residuals: -0.91349, Convergence: 0.000900\n",
      "Evidence 15205.406\n",
      "\n",
      "Epoch: 244, Evidence: 15205.40625, Convergence: 0.001725\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.06e-01\n",
      "Epoch: 244, Loss: 2481.84027, Residuals: -0.91349, Convergence:   inf\n",
      "Epoch: 245, Loss: 2480.39123, Residuals: -0.91054, Convergence: 0.000584\n",
      "Evidence 15212.203\n",
      "\n",
      "Epoch: 245, Evidence: 15212.20312, Convergence: 0.000447\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 8.67e-02\n",
      "Epoch: 245, Loss: 2482.69110, Residuals: -0.91054, Convergence:   inf\n",
      "Epoch: 246, Loss: 2523.32069, Residuals: -0.94732, Convergence: -0.016102\n",
      "Epoch: 246, Loss: 2481.26386, Residuals: -0.90780, Convergence: 0.000575\n",
      "Evidence 15215.680\n",
      "\n",
      "Epoch: 246, Evidence: 15215.67969, Convergence: 0.000675\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 7.93e-02\n",
      "Epoch: 246, Loss: 2482.13530, Residuals: -0.90780, Convergence:   inf\n",
      "Epoch: 247, Loss: 2489.44721, Residuals: -0.90967, Convergence: -0.002937\n",
      "Epoch: 247, Loss: 2483.01426, Residuals: -0.90604, Convergence: -0.000354\n",
      "Evidence 15216.596\n",
      "\n",
      "Epoch: 247, Evidence: 15216.59570, Convergence: 0.000735\n",
      "Total samples: 184, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 384.17317, Residuals: -4.54523, Convergence:   inf\n",
      "Epoch: 1, Loss: 358.46333, Residuals: -4.42455, Convergence: 0.071722\n",
      "Epoch: 2, Loss: 337.32208, Residuals: -4.25963, Convergence: 0.062674\n",
      "Epoch: 3, Loss: 321.13363, Residuals: -4.09373, Convergence: 0.050410\n",
      "Epoch: 4, Loss: 308.81876, Residuals: -3.94861, Convergence: 0.039877\n",
      "Epoch: 5, Loss: 299.06138, Residuals: -3.82079, Convergence: 0.032627\n",
      "Epoch: 6, Loss: 291.14410, Residuals: -3.70948, Convergence: 0.027194\n",
      "Epoch: 7, Loss: 284.57523, Residuals: -3.61403, Convergence: 0.023083\n",
      "Epoch: 8, Loss: 278.99212, Residuals: -3.53242, Convergence: 0.020012\n",
      "Epoch: 9, Loss: 274.13720, Residuals: -3.46232, Convergence: 0.017710\n",
      "Epoch: 10, Loss: 269.82601, Residuals: -3.40159, Convergence: 0.015978\n",
      "Epoch: 11, Loss: 265.92386, Residuals: -3.34844, Convergence: 0.014674\n",
      "Epoch: 12, Loss: 262.33157, Residuals: -3.30130, Convergence: 0.013694\n",
      "Epoch: 13, Loss: 258.97687, Residuals: -3.25886, Convergence: 0.012954\n",
      "Epoch: 14, Loss: 255.80904, Residuals: -3.22000, Convergence: 0.012384\n",
      "Epoch: 15, Loss: 252.79654, Residuals: -3.18383, Convergence: 0.011917\n",
      "Epoch: 16, Loss: 249.92439, Residuals: -3.14977, Convergence: 0.011492\n",
      "Epoch: 17, Loss: 247.18351, Residuals: -3.11749, Convergence: 0.011088\n",
      "Epoch: 18, Loss: 244.55602, Residuals: -3.08663, Convergence: 0.010744\n",
      "Epoch: 19, Loss: 242.01200, Residuals: -3.05671, Convergence: 0.010512\n",
      "Epoch: 20, Loss: 239.51684, Residuals: -3.02715, Convergence: 0.010417\n",
      "Epoch: 21, Loss: 237.03968, Residuals: -2.99744, Convergence: 0.010450\n",
      "Epoch: 22, Loss: 234.55692, Residuals: -2.96722, Convergence: 0.010585\n",
      "Epoch: 23, Loss: 232.04338, Residuals: -2.93617, Convergence: 0.010832\n",
      "Epoch: 24, Loss: 229.45587, Residuals: -2.90381, Convergence: 0.011277\n",
      "Epoch: 25, Loss: 226.73814, Residuals: -2.86948, Convergence: 0.011986\n",
      "Epoch: 26, Loss: 223.89171, Residuals: -2.83311, Convergence: 0.012713\n",
      "Epoch: 27, Loss: 221.04177, Residuals: -2.79601, Convergence: 0.012893\n",
      "Epoch: 28, Loss: 218.29517, Residuals: -2.75946, Convergence: 0.012582\n",
      "Epoch: 29, Loss: 215.66316, Residuals: -2.72375, Convergence: 0.012204\n",
      "Epoch: 30, Loss: 213.12396, Residuals: -2.68877, Convergence: 0.011914\n",
      "Epoch: 31, Loss: 210.65496, Residuals: -2.65433, Convergence: 0.011721\n",
      "Epoch: 32, Loss: 208.23898, Residuals: -2.62027, Convergence: 0.011602\n",
      "Epoch: 33, Loss: 205.86440, Residuals: -2.58645, Convergence: 0.011535\n",
      "Epoch: 34, Loss: 203.52433, Residuals: -2.55276, Convergence: 0.011498\n",
      "Epoch: 35, Loss: 201.21549, Residuals: -2.51913, Convergence: 0.011474\n",
      "Epoch: 36, Loss: 198.93722, Residuals: -2.48551, Convergence: 0.011452\n",
      "Epoch: 37, Loss: 196.69052, Residuals: -2.45186, Convergence: 0.011423\n",
      "Epoch: 38, Loss: 194.47727, Residuals: -2.41819, Convergence: 0.011381\n",
      "Epoch: 39, Loss: 192.29968, Residuals: -2.38449, Convergence: 0.011324\n",
      "Epoch: 40, Loss: 190.15999, Residuals: -2.35077, Convergence: 0.011252\n",
      "Epoch: 41, Loss: 188.06024, Residuals: -2.31704, Convergence: 0.011165\n",
      "Epoch: 42, Loss: 186.00233, Residuals: -2.28332, Convergence: 0.011064\n",
      "Epoch: 43, Loss: 183.98805, Residuals: -2.24964, Convergence: 0.010948\n",
      "Epoch: 44, Loss: 182.01928, Residuals: -2.21602, Convergence: 0.010816\n",
      "Epoch: 45, Loss: 180.09803, Residuals: -2.18250, Convergence: 0.010668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46, Loss: 178.22664, Residuals: -2.14911, Convergence: 0.010500\n",
      "Epoch: 47, Loss: 176.40767, Residuals: -2.11592, Convergence: 0.010311\n",
      "Epoch: 48, Loss: 174.64386, Residuals: -2.08297, Convergence: 0.010099\n",
      "Epoch: 49, Loss: 172.93786, Residuals: -2.05033, Convergence: 0.009865\n",
      "Epoch: 50, Loss: 171.29201, Residuals: -2.01807, Convergence: 0.009608\n",
      "Epoch: 51, Loss: 169.70808, Residuals: -1.98624, Convergence: 0.009333\n",
      "Epoch: 52, Loss: 168.18720, Residuals: -1.95491, Convergence: 0.009043\n",
      "Epoch: 53, Loss: 166.72975, Residuals: -1.92412, Convergence: 0.008741\n",
      "Epoch: 54, Loss: 165.33548, Residuals: -1.89394, Convergence: 0.008433\n",
      "Epoch: 55, Loss: 164.00363, Residuals: -1.86438, Convergence: 0.008121\n",
      "Epoch: 56, Loss: 162.73299, Residuals: -1.83550, Convergence: 0.007808\n",
      "Epoch: 57, Loss: 161.52202, Residuals: -1.80731, Convergence: 0.007497\n",
      "Epoch: 58, Loss: 160.36899, Residuals: -1.77984, Convergence: 0.007190\n",
      "Epoch: 59, Loss: 159.27202, Residuals: -1.75310, Convergence: 0.006887\n",
      "Epoch: 60, Loss: 158.22915, Residuals: -1.72711, Convergence: 0.006591\n",
      "Epoch: 61, Loss: 157.23841, Residuals: -1.70188, Convergence: 0.006301\n",
      "Epoch: 62, Loss: 156.29785, Residuals: -1.67741, Convergence: 0.006018\n",
      "Epoch: 63, Loss: 155.40554, Residuals: -1.65371, Convergence: 0.005742\n",
      "Epoch: 64, Loss: 154.55963, Residuals: -1.63077, Convergence: 0.005473\n",
      "Epoch: 65, Loss: 153.75834, Residuals: -1.60860, Convergence: 0.005211\n",
      "Epoch: 66, Loss: 152.99997, Residuals: -1.58720, Convergence: 0.004957\n",
      "Epoch: 67, Loss: 152.28293, Residuals: -1.56656, Convergence: 0.004709\n",
      "Epoch: 68, Loss: 151.60568, Residuals: -1.54668, Convergence: 0.004467\n",
      "Epoch: 69, Loss: 150.96679, Residuals: -1.52756, Convergence: 0.004232\n",
      "Epoch: 70, Loss: 150.36487, Residuals: -1.50918, Convergence: 0.004003\n",
      "Epoch: 71, Loss: 149.79862, Residuals: -1.49156, Convergence: 0.003780\n",
      "Epoch: 72, Loss: 149.26679, Residuals: -1.47468, Convergence: 0.003563\n",
      "Epoch: 73, Loss: 148.76814, Residuals: -1.45853, Convergence: 0.003352\n",
      "Epoch: 74, Loss: 148.30148, Residuals: -1.44312, Convergence: 0.003147\n",
      "Epoch: 75, Loss: 147.86558, Residuals: -1.42843, Convergence: 0.002948\n",
      "Epoch: 76, Loss: 147.45918, Residuals: -1.41447, Convergence: 0.002756\n",
      "Epoch: 77, Loss: 147.08096, Residuals: -1.40121, Convergence: 0.002572\n",
      "Epoch: 78, Loss: 146.72946, Residuals: -1.38866, Convergence: 0.002396\n",
      "Epoch: 79, Loss: 146.40310, Residuals: -1.37680, Convergence: 0.002229\n",
      "Epoch: 80, Loss: 146.10011, Residuals: -1.36561, Convergence: 0.002074\n",
      "Epoch: 81, Loss: 145.81854, Residuals: -1.35507, Convergence: 0.001931\n",
      "Epoch: 82, Loss: 145.55626, Residuals: -1.34516, Convergence: 0.001802\n",
      "Epoch: 83, Loss: 145.31102, Residuals: -1.33583, Convergence: 0.001688\n",
      "Epoch: 84, Loss: 145.08048, Residuals: -1.32706, Convergence: 0.001589\n",
      "Epoch: 85, Loss: 144.86232, Residuals: -1.31881, Convergence: 0.001506\n",
      "Epoch: 86, Loss: 144.65432, Residuals: -1.31101, Convergence: 0.001438\n",
      "Epoch: 87, Loss: 144.45439, Residuals: -1.30364, Convergence: 0.001384\n",
      "Epoch: 88, Loss: 144.26071, Residuals: -1.29663, Convergence: 0.001343\n",
      "Epoch: 89, Loss: 144.07171, Residuals: -1.28995, Convergence: 0.001312\n",
      "Epoch: 90, Loss: 143.88617, Residuals: -1.28354, Convergence: 0.001290\n",
      "Epoch: 91, Loss: 143.70315, Residuals: -1.27738, Convergence: 0.001274\n",
      "Epoch: 92, Loss: 143.52208, Residuals: -1.27143, Convergence: 0.001262\n",
      "Epoch: 93, Loss: 143.34265, Residuals: -1.26566, Convergence: 0.001252\n",
      "Epoch: 94, Loss: 143.16478, Residuals: -1.26005, Convergence: 0.001242\n",
      "Epoch: 95, Loss: 142.98856, Residuals: -1.25460, Convergence: 0.001232\n",
      "Epoch: 96, Loss: 142.81420, Residuals: -1.24929, Convergence: 0.001221\n",
      "Epoch: 97, Loss: 142.64196, Residuals: -1.24411, Convergence: 0.001207\n",
      "Epoch: 98, Loss: 142.47214, Residuals: -1.23906, Convergence: 0.001192\n",
      "Epoch: 99, Loss: 142.30501, Residuals: -1.23414, Convergence: 0.001174\n",
      "Epoch: 100, Loss: 142.14086, Residuals: -1.22935, Convergence: 0.001155\n",
      "Epoch: 101, Loss: 141.97990, Residuals: -1.22469, Convergence: 0.001134\n",
      "Epoch: 102, Loss: 141.82232, Residuals: -1.22015, Convergence: 0.001111\n",
      "Epoch: 103, Loss: 141.66828, Residuals: -1.21573, Convergence: 0.001087\n",
      "Epoch: 104, Loss: 141.51789, Residuals: -1.21144, Convergence: 0.001063\n",
      "Epoch: 105, Loss: 141.37121, Residuals: -1.20727, Convergence: 0.001038\n",
      "Epoch: 106, Loss: 141.22827, Residuals: -1.20321, Convergence: 0.001012\n",
      "Epoch: 107, Loss: 141.08911, Residuals: -1.19928, Convergence: 0.000986\n",
      "Evidence -181.711\n",
      "\n",
      "Epoch: 107, Evidence: -181.71080, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 7.23e-01\n",
      "Epoch: 107, Loss: 1391.04132, Residuals: -1.19928, Convergence:   inf\n",
      "Epoch: 108, Loss: 1334.06943, Residuals: -1.22439, Convergence: 0.042705\n",
      "Epoch: 109, Loss: 1290.35024, Residuals: -1.24544, Convergence: 0.033882\n",
      "Epoch: 110, Loss: 1256.94842, Residuals: -1.26172, Convergence: 0.026574\n",
      "Epoch: 111, Loss: 1230.58322, Residuals: -1.27412, Convergence: 0.021425\n",
      "Epoch: 112, Loss: 1209.01936, Residuals: -1.28390, Convergence: 0.017836\n",
      "Epoch: 113, Loss: 1191.01600, Residuals: -1.29171, Convergence: 0.015116\n",
      "Epoch: 114, Loss: 1175.79904, Residuals: -1.29783, Convergence: 0.012942\n",
      "Epoch: 115, Loss: 1162.81346, Residuals: -1.30248, Convergence: 0.011167\n",
      "Epoch: 116, Loss: 1151.63267, Residuals: -1.30583, Convergence: 0.009709\n",
      "Epoch: 117, Loss: 1141.91547, Residuals: -1.30803, Convergence: 0.008510\n",
      "Epoch: 118, Loss: 1133.38576, Residuals: -1.30920, Convergence: 0.007526\n",
      "Epoch: 119, Loss: 1125.81455, Residuals: -1.30948, Convergence: 0.006725\n",
      "Epoch: 120, Loss: 1119.01154, Residuals: -1.30896, Convergence: 0.006079\n",
      "Epoch: 121, Loss: 1112.81591, Residuals: -1.30772, Convergence: 0.005568\n",
      "Epoch: 122, Loss: 1107.09155, Residuals: -1.30583, Convergence: 0.005171\n",
      "Epoch: 123, Loss: 1101.72510, Residuals: -1.30334, Convergence: 0.004871\n",
      "Epoch: 124, Loss: 1096.62345, Residuals: -1.30030, Convergence: 0.004652\n",
      "Epoch: 125, Loss: 1091.71350, Residuals: -1.29673, Convergence: 0.004497\n",
      "Epoch: 126, Loss: 1086.94067, Residuals: -1.29268, Convergence: 0.004391\n",
      "Epoch: 127, Loss: 1082.26194, Residuals: -1.28815, Convergence: 0.004323\n",
      "Epoch: 128, Loss: 1077.64057, Residuals: -1.28317, Convergence: 0.004288\n",
      "Epoch: 129, Loss: 1073.03670, Residuals: -1.27773, Convergence: 0.004291\n",
      "Epoch: 130, Loss: 1068.40320, Residuals: -1.27184, Convergence: 0.004337\n",
      "Epoch: 131, Loss: 1063.69078, Residuals: -1.26549, Convergence: 0.004430\n",
      "Epoch: 132, Loss: 1058.86297, Residuals: -1.25873, Convergence: 0.004559\n",
      "Epoch: 133, Loss: 1053.92101, Residuals: -1.25161, Convergence: 0.004689\n",
      "Epoch: 134, Loss: 1048.92003, Residuals: -1.24422, Convergence: 0.004768\n",
      "Epoch: 135, Loss: 1043.95890, Residuals: -1.23664, Convergence: 0.004752\n",
      "Epoch: 136, Loss: 1039.14578, Residuals: -1.22898, Convergence: 0.004632\n",
      "Epoch: 137, Loss: 1034.56271, Residuals: -1.22129, Convergence: 0.004430\n",
      "Epoch: 138, Loss: 1030.25181, Residuals: -1.21364, Convergence: 0.004184\n",
      "Epoch: 139, Loss: 1026.22217, Residuals: -1.20608, Convergence: 0.003927\n",
      "Epoch: 140, Loss: 1022.46215, Residuals: -1.19867, Convergence: 0.003677\n",
      "Epoch: 141, Loss: 1018.95275, Residuals: -1.19143, Convergence: 0.003444\n",
      "Epoch: 142, Loss: 1015.67262, Residuals: -1.18441, Convergence: 0.003230\n",
      "Epoch: 143, Loss: 1012.60162, Residuals: -1.17762, Convergence: 0.003033\n",
      "Epoch: 144, Loss: 1009.72156, Residuals: -1.17110, Convergence: 0.002852\n",
      "Epoch: 145, Loss: 1007.01704, Residuals: -1.16484, Convergence: 0.002686\n",
      "Epoch: 146, Loss: 1004.47436, Residuals: -1.15886, Convergence: 0.002531\n",
      "Epoch: 147, Loss: 1002.08082, Residuals: -1.15317, Convergence: 0.002389\n",
      "Epoch: 148, Loss: 999.82478, Residuals: -1.14776, Convergence: 0.002256\n",
      "Epoch: 149, Loss: 997.69615, Residuals: -1.14262, Convergence: 0.002134\n",
      "Epoch: 150, Loss: 995.68459, Residuals: -1.13775, Convergence: 0.002020\n",
      "Epoch: 151, Loss: 993.78093, Residuals: -1.13315, Convergence: 0.001916\n",
      "Epoch: 152, Loss: 991.97591, Residuals: -1.12879, Convergence: 0.001820\n",
      "Epoch: 153, Loss: 990.26089, Residuals: -1.12467, Convergence: 0.001732\n",
      "Epoch: 154, Loss: 988.62783, Residuals: -1.12078, Convergence: 0.001652\n",
      "Epoch: 155, Loss: 987.06894, Residuals: -1.11710, Convergence: 0.001579\n",
      "Epoch: 156, Loss: 985.57719, Residuals: -1.11361, Convergence: 0.001514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 157, Loss: 984.14614, Residuals: -1.11030, Convergence: 0.001454\n",
      "Epoch: 158, Loss: 982.76943, Residuals: -1.10716, Convergence: 0.001401\n",
      "Epoch: 159, Loss: 981.44158, Residuals: -1.10417, Convergence: 0.001353\n",
      "Epoch: 160, Loss: 980.15776, Residuals: -1.10132, Convergence: 0.001310\n",
      "Epoch: 161, Loss: 978.91361, Residuals: -1.09860, Convergence: 0.001271\n",
      "Epoch: 162, Loss: 977.70473, Residuals: -1.09601, Convergence: 0.001236\n",
      "Epoch: 163, Loss: 976.52802, Residuals: -1.09351, Convergence: 0.001205\n",
      "Epoch: 164, Loss: 975.38028, Residuals: -1.09112, Convergence: 0.001177\n",
      "Epoch: 165, Loss: 974.25945, Residuals: -1.08882, Convergence: 0.001150\n",
      "Epoch: 166, Loss: 973.16376, Residuals: -1.08660, Convergence: 0.001126\n",
      "Epoch: 167, Loss: 972.09221, Residuals: -1.08446, Convergence: 0.001102\n",
      "Epoch: 168, Loss: 971.04399, Residuals: -1.08239, Convergence: 0.001079\n",
      "Epoch: 169, Loss: 970.01944, Residuals: -1.08039, Convergence: 0.001056\n",
      "Epoch: 170, Loss: 969.01789, Residuals: -1.07846, Convergence: 0.001034\n",
      "Epoch: 171, Loss: 968.03947, Residuals: -1.07659, Convergence: 0.001011\n",
      "Epoch: 172, Loss: 967.08316, Residuals: -1.07478, Convergence: 0.000989\n",
      "Evidence 11460.253\n",
      "\n",
      "Epoch: 172, Evidence: 11460.25293, Convergence: 1.015856\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 5.70e-01\n",
      "Epoch: 172, Loss: 2377.52287, Residuals: -1.07478, Convergence:   inf\n",
      "Epoch: 173, Loss: 2340.39676, Residuals: -1.08316, Convergence: 0.015863\n",
      "Epoch: 174, Loss: 2315.02158, Residuals: -1.08131, Convergence: 0.010961\n",
      "Epoch: 175, Loss: 2293.57795, Residuals: -1.07910, Convergence: 0.009349\n",
      "Epoch: 176, Loss: 2275.33897, Residuals: -1.07673, Convergence: 0.008016\n",
      "Epoch: 177, Loss: 2259.71416, Residuals: -1.07424, Convergence: 0.006915\n",
      "Epoch: 178, Loss: 2246.21787, Residuals: -1.07168, Convergence: 0.006008\n",
      "Epoch: 179, Loss: 2234.44813, Residuals: -1.06905, Convergence: 0.005267\n",
      "Epoch: 180, Loss: 2224.07035, Residuals: -1.06637, Convergence: 0.004666\n",
      "Epoch: 181, Loss: 2214.80395, Residuals: -1.06363, Convergence: 0.004184\n",
      "Epoch: 182, Loss: 2206.41720, Residuals: -1.06083, Convergence: 0.003801\n",
      "Epoch: 183, Loss: 2198.73179, Residuals: -1.05792, Convergence: 0.003495\n",
      "Epoch: 184, Loss: 2191.62641, Residuals: -1.05493, Convergence: 0.003242\n",
      "Epoch: 185, Loss: 2185.02753, Residuals: -1.05185, Convergence: 0.003020\n",
      "Epoch: 186, Loss: 2178.89922, Residuals: -1.04873, Convergence: 0.002813\n",
      "Epoch: 187, Loss: 2173.21882, Residuals: -1.04562, Convergence: 0.002614\n",
      "Epoch: 188, Loss: 2167.96461, Residuals: -1.04256, Convergence: 0.002424\n",
      "Epoch: 189, Loss: 2163.11169, Residuals: -1.03957, Convergence: 0.002243\n",
      "Epoch: 190, Loss: 2158.63167, Residuals: -1.03670, Convergence: 0.002075\n",
      "Epoch: 191, Loss: 2154.49421, Residuals: -1.03394, Convergence: 0.001920\n",
      "Epoch: 192, Loss: 2150.66976, Residuals: -1.03131, Convergence: 0.001778\n",
      "Epoch: 193, Loss: 2147.12871, Residuals: -1.02882, Convergence: 0.001649\n",
      "Epoch: 194, Loss: 2143.84481, Residuals: -1.02647, Convergence: 0.001532\n",
      "Epoch: 195, Loss: 2140.79332, Residuals: -1.02424, Convergence: 0.001425\n",
      "Epoch: 196, Loss: 2137.95164, Residuals: -1.02215, Convergence: 0.001329\n",
      "Epoch: 197, Loss: 2135.30047, Residuals: -1.02018, Convergence: 0.001242\n",
      "Epoch: 198, Loss: 2132.82279, Residuals: -1.01833, Convergence: 0.001162\n",
      "Epoch: 199, Loss: 2130.50349, Residuals: -1.01659, Convergence: 0.001089\n",
      "Epoch: 200, Loss: 2128.32870, Residuals: -1.01495, Convergence: 0.001022\n",
      "Epoch: 201, Loss: 2126.28651, Residuals: -1.01341, Convergence: 0.000960\n",
      "Evidence 14499.994\n",
      "\n",
      "Epoch: 201, Evidence: 14499.99414, Convergence: 0.209637\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 4.31e-01\n",
      "Epoch: 201, Loss: 2493.71762, Residuals: -1.01341, Convergence:   inf\n",
      "Epoch: 202, Loss: 2479.77251, Residuals: -1.00982, Convergence: 0.005624\n",
      "Epoch: 203, Loss: 2468.43667, Residuals: -1.00605, Convergence: 0.004592\n",
      "Epoch: 204, Loss: 2458.75893, Residuals: -1.00259, Convergence: 0.003936\n",
      "Epoch: 205, Loss: 2450.43833, Residuals: -0.99947, Convergence: 0.003396\n",
      "Epoch: 206, Loss: 2443.23699, Residuals: -0.99668, Convergence: 0.002947\n",
      "Epoch: 207, Loss: 2436.96140, Residuals: -0.99420, Convergence: 0.002575\n",
      "Epoch: 208, Loss: 2431.45481, Residuals: -0.99197, Convergence: 0.002265\n",
      "Epoch: 209, Loss: 2426.59138, Residuals: -0.98998, Convergence: 0.002004\n",
      "Epoch: 210, Loss: 2422.26624, Residuals: -0.98818, Convergence: 0.001786\n",
      "Epoch: 211, Loss: 2418.39496, Residuals: -0.98656, Convergence: 0.001601\n",
      "Epoch: 212, Loss: 2414.90747, Residuals: -0.98508, Convergence: 0.001444\n",
      "Epoch: 213, Loss: 2411.74640, Residuals: -0.98374, Convergence: 0.001311\n",
      "Epoch: 214, Loss: 2408.86545, Residuals: -0.98251, Convergence: 0.001196\n",
      "Epoch: 215, Loss: 2406.22397, Residuals: -0.98138, Convergence: 0.001098\n",
      "Epoch: 216, Loss: 2403.79088, Residuals: -0.98034, Convergence: 0.001012\n",
      "Epoch: 217, Loss: 2401.53992, Residuals: -0.97937, Convergence: 0.000937\n",
      "Evidence 14868.767\n",
      "\n",
      "Epoch: 217, Evidence: 14868.76660, Convergence: 0.024802\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 3.29e-01\n",
      "Epoch: 217, Loss: 2498.59730, Residuals: -0.97937, Convergence:   inf\n",
      "Epoch: 218, Loss: 2491.91921, Residuals: -0.97626, Convergence: 0.002680\n",
      "Epoch: 219, Loss: 2486.43255, Residuals: -0.97356, Convergence: 0.002207\n",
      "Epoch: 220, Loss: 2481.79443, Residuals: -0.97129, Convergence: 0.001869\n",
      "Epoch: 221, Loss: 2477.80622, Residuals: -0.96939, Convergence: 0.001610\n",
      "Epoch: 222, Loss: 2474.32767, Residuals: -0.96776, Convergence: 0.001406\n",
      "Epoch: 223, Loss: 2471.25607, Residuals: -0.96638, Convergence: 0.001243\n",
      "Epoch: 224, Loss: 2468.51210, Residuals: -0.96518, Convergence: 0.001112\n",
      "Epoch: 225, Loss: 2466.03553, Residuals: -0.96415, Convergence: 0.001004\n",
      "Epoch: 226, Loss: 2463.78020, Residuals: -0.96325, Convergence: 0.000915\n",
      "Evidence 14953.489\n",
      "\n",
      "Epoch: 226, Evidence: 14953.48926, Convergence: 0.005666\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 2.57e-01\n",
      "Epoch: 226, Loss: 2499.93724, Residuals: -0.96325, Convergence:   inf\n",
      "Epoch: 227, Loss: 2496.01080, Residuals: -0.96105, Convergence: 0.001573\n",
      "Epoch: 228, Loss: 2492.75256, Residuals: -0.95928, Convergence: 0.001307\n",
      "Epoch: 229, Loss: 2489.95465, Residuals: -0.95785, Convergence: 0.001124\n",
      "Epoch: 230, Loss: 2487.50198, Residuals: -0.95668, Convergence: 0.000986\n",
      "Evidence 14982.434\n",
      "\n",
      "Epoch: 230, Evidence: 14982.43359, Convergence: 0.001932\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 2.07e-01\n",
      "Epoch: 230, Loss: 2500.85522, Residuals: -0.95668, Convergence:   inf\n",
      "Epoch: 231, Loss: 2497.91262, Residuals: -0.95497, Convergence: 0.001178\n",
      "Epoch: 232, Loss: 2495.43955, Residuals: -0.95361, Convergence: 0.000991\n",
      "Evidence 14994.529\n",
      "\n",
      "Epoch: 232, Evidence: 14994.52930, Convergence: 0.000807\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.72e-01\n",
      "Epoch: 232, Loss: 2501.52620, Residuals: -0.95361, Convergence:   inf\n",
      "Epoch: 233, Loss: 2496.88977, Residuals: -0.95144, Convergence: 0.001857\n",
      "Epoch: 234, Loss: 2493.31771, Residuals: -0.94978, Convergence: 0.001433\n",
      "Epoch: 235, Loss: 2490.41130, Residuals: -0.94862, Convergence: 0.001167\n",
      "Epoch: 236, Loss: 2487.94109, Residuals: -0.94792, Convergence: 0.000993\n",
      "Evidence 15012.594\n",
      "\n",
      "Epoch: 236, Evidence: 15012.59375, Convergence: 0.002009\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.42e-01\n",
      "Epoch: 236, Loss: 2501.61428, Residuals: -0.94792, Convergence:   inf\n",
      "Epoch: 237, Loss: 2498.52117, Residuals: -0.94589, Convergence: 0.001238\n",
      "Epoch: 238, Loss: 2496.05100, Residuals: -0.94482, Convergence: 0.000990\n",
      "Evidence 15023.688\n",
      "\n",
      "Epoch: 238, Evidence: 15023.68750, Convergence: 0.000738\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.21e-01\n",
      "Epoch: 238, Loss: 2501.79033, Residuals: -0.94482, Convergence:   inf\n",
      "Epoch: 239, Loss: 2497.26361, Residuals: -0.94197, Convergence: 0.001813\n",
      "Epoch: 240, Loss: 2493.95012, Residuals: -0.94303, Convergence: 0.001329\n",
      "Epoch: 241, Loss: 2491.21220, Residuals: -0.94379, Convergence: 0.001099\n",
      "Epoch: 242, Loss: 2488.78435, Residuals: -0.94617, Convergence: 0.000976\n",
      "Evidence 15039.835\n",
      "\n",
      "Epoch: 242, Evidence: 15039.83496, Convergence: 0.001811\n",
      "Updating hyper-parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 184, Updated regularization: 1.07e-01\n",
      "Epoch: 242, Loss: 2501.05908, Residuals: -0.94617, Convergence:   inf\n",
      "Epoch: 243, Loss: 2499.19382, Residuals: -0.94527, Convergence: 0.000746\n",
      "Evidence 15046.525\n",
      "\n",
      "Epoch: 243, Evidence: 15046.52539, Convergence: 0.000445\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 8.76e-02\n",
      "Epoch: 243, Loss: 2502.02178, Residuals: -0.94527, Convergence:   inf\n",
      "Epoch: 244, Loss: 2542.10112, Residuals: -0.99063, Convergence: -0.015766\n",
      "Epoch: 244, Loss: 2499.55992, Residuals: -0.94303, Convergence: 0.000985\n",
      "Evidence 15050.985\n",
      "\n",
      "Epoch: 244, Evidence: 15050.98535, Convergence: 0.000741\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 8.08e-02\n",
      "Epoch: 244, Loss: 2501.44840, Residuals: -0.94303, Convergence:   inf\n",
      "Epoch: 245, Loss: 2504.62158, Residuals: -0.94745, Convergence: -0.001267\n",
      "Epoch: 245, Loss: 2500.77425, Residuals: -0.94287, Convergence: 0.000270\n",
      "Evidence 15053.130\n",
      "\n",
      "Epoch: 245, Evidence: 15053.12988, Convergence: 0.000883\n",
      "Total samples: 185, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 383.70184, Residuals: -4.51234, Convergence:   inf\n",
      "Epoch: 1, Loss: 357.96431, Residuals: -4.39220, Convergence: 0.071900\n",
      "Epoch: 2, Loss: 336.88241, Residuals: -4.22879, Convergence: 0.062579\n",
      "Epoch: 3, Loss: 320.74813, Residuals: -4.06416, Convergence: 0.050302\n",
      "Epoch: 4, Loss: 308.45284, Residuals: -3.91931, Convergence: 0.039861\n",
      "Epoch: 5, Loss: 298.70552, Residuals: -3.79128, Convergence: 0.032632\n",
      "Epoch: 6, Loss: 290.79499, Residuals: -3.67974, Convergence: 0.027203\n",
      "Epoch: 7, Loss: 284.22850, Residuals: -3.58419, Convergence: 0.023103\n",
      "Epoch: 8, Loss: 278.64164, Residuals: -3.50260, Convergence: 0.020050\n",
      "Epoch: 9, Loss: 273.77727, Residuals: -3.43264, Convergence: 0.017768\n",
      "Epoch: 10, Loss: 269.45273, Residuals: -3.37220, Convergence: 0.016049\n",
      "Epoch: 11, Loss: 265.53590, Residuals: -3.31948, Convergence: 0.014751\n",
      "Epoch: 12, Loss: 261.93061, Residuals: -3.27297, Convergence: 0.013764\n",
      "Epoch: 13, Loss: 258.56736, Residuals: -3.23132, Convergence: 0.013007\n",
      "Epoch: 14, Loss: 255.39701, Residuals: -3.19339, Convergence: 0.012413\n",
      "Epoch: 15, Loss: 252.38710, Residuals: -3.15820, Convergence: 0.011926\n",
      "Epoch: 16, Loss: 249.51933, Residuals: -3.12507, Convergence: 0.011493\n",
      "Epoch: 17, Loss: 246.78182, Residuals: -3.09355, Convergence: 0.011093\n",
      "Epoch: 18, Loss: 244.15636, Residuals: -3.06328, Convergence: 0.010753\n",
      "Epoch: 19, Loss: 241.61389, Residuals: -3.03376, Convergence: 0.010523\n",
      "Epoch: 20, Loss: 239.12067, Residuals: -3.00447, Convergence: 0.010427\n",
      "Epoch: 21, Loss: 236.64744, Residuals: -2.97494, Convergence: 0.010451\n",
      "Epoch: 22, Loss: 234.17401, Residuals: -2.94489, Convergence: 0.010562\n",
      "Epoch: 23, Loss: 231.67980, Residuals: -2.91409, Convergence: 0.010766\n",
      "Epoch: 24, Loss: 229.12554, Residuals: -2.88214, Convergence: 0.011148\n",
      "Epoch: 25, Loss: 226.45229, Residuals: -2.84838, Convergence: 0.011805\n",
      "Epoch: 26, Loss: 223.63717, Residuals: -2.81249, Convergence: 0.012588\n",
      "Epoch: 27, Loss: 220.78207, Residuals: -2.77550, Convergence: 0.012932\n",
      "Epoch: 28, Loss: 218.01368, Residuals: -2.73886, Convergence: 0.012698\n",
      "Epoch: 29, Loss: 215.36122, Residuals: -2.70305, Convergence: 0.012316\n",
      "Epoch: 30, Loss: 212.80758, Residuals: -2.66802, Convergence: 0.012000\n",
      "Epoch: 31, Loss: 210.33119, Residuals: -2.63359, Convergence: 0.011774\n",
      "Epoch: 32, Loss: 207.91537, Residuals: -2.59962, Convergence: 0.011619\n",
      "Epoch: 33, Loss: 205.54888, Residuals: -2.56596, Convergence: 0.011513\n",
      "Epoch: 34, Loss: 203.22483, Residuals: -2.53252, Convergence: 0.011436\n",
      "Epoch: 35, Loss: 200.93954, Residuals: -2.49922, Convergence: 0.011373\n",
      "Epoch: 36, Loss: 198.69141, Residuals: -2.46600, Convergence: 0.011315\n",
      "Epoch: 37, Loss: 196.48016, Residuals: -2.43283, Convergence: 0.011254\n",
      "Epoch: 38, Loss: 194.30630, Residuals: -2.39968, Convergence: 0.011188\n",
      "Epoch: 39, Loss: 192.17074, Residuals: -2.36652, Convergence: 0.011113\n",
      "Epoch: 40, Loss: 190.07468, Residuals: -2.33336, Convergence: 0.011028\n",
      "Epoch: 41, Loss: 188.01957, Residuals: -2.30020, Convergence: 0.010930\n",
      "Epoch: 42, Loss: 186.00703, Residuals: -2.26705, Convergence: 0.010820\n",
      "Epoch: 43, Loss: 184.03895, Residuals: -2.23392, Convergence: 0.010694\n",
      "Epoch: 44, Loss: 182.11747, Residuals: -2.20086, Convergence: 0.010551\n",
      "Epoch: 45, Loss: 180.24508, Residuals: -2.16791, Convergence: 0.010388\n",
      "Epoch: 46, Loss: 178.42446, Residuals: -2.13513, Convergence: 0.010204\n",
      "Epoch: 47, Loss: 176.65851, Residuals: -2.10256, Convergence: 0.009996\n",
      "Epoch: 48, Loss: 174.95003, Residuals: -2.07028, Convergence: 0.009766\n",
      "Epoch: 49, Loss: 173.30149, Residuals: -2.03836, Convergence: 0.009513\n",
      "Epoch: 50, Loss: 171.71480, Residuals: -2.00687, Convergence: 0.009240\n",
      "Epoch: 51, Loss: 170.19115, Residuals: -1.97586, Convergence: 0.008953\n",
      "Epoch: 52, Loss: 168.73102, Residuals: -1.94539, Convergence: 0.008654\n",
      "Epoch: 53, Loss: 167.33416, Residuals: -1.91551, Convergence: 0.008348\n",
      "Epoch: 54, Loss: 165.99976, Residuals: -1.88626, Convergence: 0.008039\n",
      "Epoch: 55, Loss: 164.72659, Residuals: -1.85767, Convergence: 0.007729\n",
      "Epoch: 56, Loss: 163.51305, Residuals: -1.82979, Convergence: 0.007422\n",
      "Epoch: 57, Loss: 162.35736, Residuals: -1.80262, Convergence: 0.007118\n",
      "Epoch: 58, Loss: 161.25756, Residuals: -1.77620, Convergence: 0.006820\n",
      "Epoch: 59, Loss: 160.21166, Residuals: -1.75052, Convergence: 0.006528\n",
      "Epoch: 60, Loss: 159.21760, Residuals: -1.72562, Convergence: 0.006243\n",
      "Epoch: 61, Loss: 158.27333, Residuals: -1.70149, Convergence: 0.005966\n",
      "Epoch: 62, Loss: 157.37678, Residuals: -1.67815, Convergence: 0.005697\n",
      "Epoch: 63, Loss: 156.52589, Residuals: -1.65559, Convergence: 0.005436\n",
      "Epoch: 64, Loss: 155.71857, Residuals: -1.63382, Convergence: 0.005185\n",
      "Epoch: 65, Loss: 154.95268, Residuals: -1.61283, Convergence: 0.004943\n",
      "Epoch: 66, Loss: 154.22602, Residuals: -1.59262, Convergence: 0.004712\n",
      "Epoch: 67, Loss: 153.53632, Residuals: -1.57316, Convergence: 0.004492\n",
      "Epoch: 68, Loss: 152.88126, Residuals: -1.55445, Convergence: 0.004285\n",
      "Epoch: 69, Loss: 152.25852, Residuals: -1.53645, Convergence: 0.004090\n",
      "Epoch: 70, Loss: 151.66578, Residuals: -1.51915, Convergence: 0.003908\n",
      "Epoch: 71, Loss: 151.10091, Residuals: -1.50250, Convergence: 0.003738\n",
      "Epoch: 72, Loss: 150.56197, Residuals: -1.48648, Convergence: 0.003580\n",
      "Epoch: 73, Loss: 150.04724, Residuals: -1.47106, Convergence: 0.003430\n",
      "Epoch: 74, Loss: 149.55528, Residuals: -1.45622, Convergence: 0.003289\n",
      "Epoch: 75, Loss: 149.08488, Residuals: -1.44192, Convergence: 0.003155\n",
      "Epoch: 76, Loss: 148.63503, Residuals: -1.42815, Convergence: 0.003027\n",
      "Epoch: 77, Loss: 148.20486, Residuals: -1.41489, Convergence: 0.002903\n",
      "Epoch: 78, Loss: 147.79361, Residuals: -1.40213, Convergence: 0.002783\n",
      "Epoch: 79, Loss: 147.40060, Residuals: -1.38985, Convergence: 0.002666\n",
      "Epoch: 80, Loss: 147.02519, Residuals: -1.37804, Convergence: 0.002553\n",
      "Epoch: 81, Loss: 146.66673, Residuals: -1.36668, Convergence: 0.002444\n",
      "Epoch: 82, Loss: 146.32462, Residuals: -1.35578, Convergence: 0.002338\n",
      "Epoch: 83, Loss: 145.99825, Residuals: -1.34530, Convergence: 0.002235\n",
      "Epoch: 84, Loss: 145.68703, Residuals: -1.33524, Convergence: 0.002136\n",
      "Epoch: 85, Loss: 145.39035, Residuals: -1.32558, Convergence: 0.002041\n",
      "Epoch: 86, Loss: 145.10767, Residuals: -1.31632, Convergence: 0.001948\n",
      "Epoch: 87, Loss: 144.83840, Residuals: -1.30744, Convergence: 0.001859\n",
      "Epoch: 88, Loss: 144.58203, Residuals: -1.29892, Convergence: 0.001773\n",
      "Epoch: 89, Loss: 144.33804, Residuals: -1.29076, Convergence: 0.001690\n",
      "Epoch: 90, Loss: 144.10593, Residuals: -1.28293, Convergence: 0.001611\n",
      "Epoch: 91, Loss: 143.88528, Residuals: -1.27544, Convergence: 0.001534\n",
      "Epoch: 92, Loss: 143.67563, Residuals: -1.26826, Convergence: 0.001459\n",
      "Epoch: 93, Loss: 143.47660, Residuals: -1.26139, Convergence: 0.001387\n",
      "Epoch: 94, Loss: 143.28782, Residuals: -1.25481, Convergence: 0.001318\n",
      "Epoch: 95, Loss: 143.10894, Residuals: -1.24852, Convergence: 0.001250\n",
      "Epoch: 96, Loss: 142.93966, Residuals: -1.24250, Convergence: 0.001184\n",
      "Epoch: 97, Loss: 142.77967, Residuals: -1.23676, Convergence: 0.001120\n",
      "Epoch: 98, Loss: 142.62871, Residuals: -1.23127, Convergence: 0.001058\n",
      "Epoch: 99, Loss: 142.48649, Residuals: -1.22604, Convergence: 0.000998\n",
      "Evidence -184.248\n",
      "\n",
      "Epoch: 99, Evidence: -184.24803, Convergence:   inf\n",
      "Updating hyper-parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 185, Updated regularization: 7.25e-01\n",
      "Epoch: 99, Loss: 1390.32690, Residuals: -1.22604, Convergence:   inf\n",
      "Epoch: 100, Loss: 1326.83508, Residuals: -1.25556, Convergence: 0.047852\n",
      "Epoch: 101, Loss: 1278.94777, Residuals: -1.27853, Convergence: 0.037443\n",
      "Epoch: 102, Loss: 1243.07171, Residuals: -1.29456, Convergence: 0.028861\n",
      "Epoch: 103, Loss: 1215.24406, Residuals: -1.30541, Convergence: 0.022899\n",
      "Epoch: 104, Loss: 1192.73907, Residuals: -1.31320, Convergence: 0.018868\n",
      "Epoch: 105, Loss: 1174.04934, Residuals: -1.31898, Convergence: 0.015919\n",
      "Epoch: 106, Loss: 1158.27645, Residuals: -1.32322, Convergence: 0.013618\n",
      "Epoch: 107, Loss: 1144.80552, Residuals: -1.32615, Convergence: 0.011767\n",
      "Epoch: 108, Loss: 1133.17896, Residuals: -1.32794, Convergence: 0.010260\n",
      "Epoch: 109, Loss: 1123.03819, Residuals: -1.32871, Convergence: 0.009030\n",
      "Epoch: 110, Loss: 1114.09444, Residuals: -1.32857, Convergence: 0.008028\n",
      "Epoch: 111, Loss: 1106.11133, Residuals: -1.32761, Convergence: 0.007217\n",
      "Epoch: 112, Loss: 1098.89199, Residuals: -1.32590, Convergence: 0.006570\n",
      "Epoch: 113, Loss: 1092.27037, Residuals: -1.32350, Convergence: 0.006062\n",
      "Epoch: 114, Loss: 1086.10625, Residuals: -1.32044, Convergence: 0.005675\n",
      "Epoch: 115, Loss: 1080.27894, Residuals: -1.31676, Convergence: 0.005394\n",
      "Epoch: 116, Loss: 1074.68591, Residuals: -1.31248, Convergence: 0.005204\n",
      "Epoch: 117, Loss: 1069.23828, Residuals: -1.30760, Convergence: 0.005095\n",
      "Epoch: 118, Loss: 1063.86048, Residuals: -1.30215, Convergence: 0.005055\n",
      "Epoch: 119, Loss: 1058.49256, Residuals: -1.29613, Convergence: 0.005071\n",
      "Epoch: 120, Loss: 1053.09816, Residuals: -1.28959, Convergence: 0.005122\n",
      "Epoch: 121, Loss: 1047.67784, Residuals: -1.28261, Convergence: 0.005174\n",
      "Epoch: 122, Loss: 1042.27948, Residuals: -1.27528, Convergence: 0.005179\n",
      "Epoch: 123, Loss: 1036.98759, Residuals: -1.26769, Convergence: 0.005103\n",
      "Epoch: 124, Loss: 1031.89645, Residuals: -1.25994, Convergence: 0.004934\n",
      "Epoch: 125, Loss: 1027.07699, Residuals: -1.25210, Convergence: 0.004692\n",
      "Epoch: 126, Loss: 1022.56292, Residuals: -1.24423, Convergence: 0.004414\n",
      "Epoch: 127, Loss: 1018.35494, Residuals: -1.23640, Convergence: 0.004132\n",
      "Epoch: 128, Loss: 1014.43489, Residuals: -1.22863, Convergence: 0.003864\n",
      "Epoch: 129, Loss: 1010.77679, Residuals: -1.22099, Convergence: 0.003619\n",
      "Epoch: 130, Loss: 1007.35245, Residuals: -1.21350, Convergence: 0.003399\n",
      "Epoch: 131, Loss: 1004.13744, Residuals: -1.20618, Convergence: 0.003202\n",
      "Epoch: 132, Loss: 1001.11062, Residuals: -1.19907, Convergence: 0.003023\n",
      "Epoch: 133, Loss: 998.25404, Residuals: -1.19217, Convergence: 0.002862\n",
      "Epoch: 134, Loss: 995.55324, Residuals: -1.18551, Convergence: 0.002713\n",
      "Epoch: 135, Loss: 992.99544, Residuals: -1.17910, Convergence: 0.002576\n",
      "Epoch: 136, Loss: 990.57072, Residuals: -1.17293, Convergence: 0.002448\n",
      "Epoch: 137, Loss: 988.26938, Residuals: -1.16703, Convergence: 0.002329\n",
      "Epoch: 138, Loss: 986.08299, Residuals: -1.16137, Convergence: 0.002217\n",
      "Epoch: 139, Loss: 984.00397, Residuals: -1.15597, Convergence: 0.002113\n",
      "Epoch: 140, Loss: 982.02415, Residuals: -1.15082, Convergence: 0.002016\n",
      "Epoch: 141, Loss: 980.13726, Residuals: -1.14591, Convergence: 0.001925\n",
      "Epoch: 142, Loss: 978.33628, Residuals: -1.14124, Convergence: 0.001841\n",
      "Epoch: 143, Loss: 976.61512, Residuals: -1.13679, Convergence: 0.001762\n",
      "Epoch: 144, Loss: 974.96803, Residuals: -1.13255, Convergence: 0.001689\n",
      "Epoch: 145, Loss: 973.38966, Residuals: -1.12852, Convergence: 0.001622\n",
      "Epoch: 146, Loss: 971.87491, Residuals: -1.12467, Convergence: 0.001559\n",
      "Epoch: 147, Loss: 970.41984, Residuals: -1.12101, Convergence: 0.001499\n",
      "Epoch: 148, Loss: 969.01996, Residuals: -1.11752, Convergence: 0.001445\n",
      "Epoch: 149, Loss: 967.67179, Residuals: -1.11418, Convergence: 0.001393\n",
      "Epoch: 150, Loss: 966.37255, Residuals: -1.11100, Convergence: 0.001344\n",
      "Epoch: 151, Loss: 965.11846, Residuals: -1.10795, Convergence: 0.001299\n",
      "Epoch: 152, Loss: 963.90708, Residuals: -1.10504, Convergence: 0.001257\n",
      "Epoch: 153, Loss: 962.73540, Residuals: -1.10225, Convergence: 0.001217\n",
      "Epoch: 154, Loss: 961.60039, Residuals: -1.09957, Convergence: 0.001180\n",
      "Epoch: 155, Loss: 960.49947, Residuals: -1.09699, Convergence: 0.001146\n",
      "Epoch: 156, Loss: 959.42946, Residuals: -1.09451, Convergence: 0.001115\n",
      "Epoch: 157, Loss: 958.38690, Residuals: -1.09211, Convergence: 0.001088\n",
      "Epoch: 158, Loss: 957.36841, Residuals: -1.08979, Convergence: 0.001064\n",
      "Epoch: 159, Loss: 956.37060, Residuals: -1.08754, Convergence: 0.001043\n",
      "Epoch: 160, Loss: 955.38965, Residuals: -1.08535, Convergence: 0.001027\n",
      "Epoch: 161, Loss: 954.42187, Residuals: -1.08321, Convergence: 0.001014\n",
      "Epoch: 162, Loss: 953.46322, Residuals: -1.08111, Convergence: 0.001005\n",
      "Epoch: 163, Loss: 952.51031, Residuals: -1.07904, Convergence: 0.001000\n",
      "Epoch: 164, Loss: 951.56080, Residuals: -1.07700, Convergence: 0.000998\n",
      "Evidence 11322.291\n",
      "\n",
      "Epoch: 164, Evidence: 11322.29102, Convergence: 1.016273\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 5.75e-01\n",
      "Epoch: 164, Loss: 2384.65920, Residuals: -1.07700, Convergence:   inf\n",
      "Epoch: 165, Loss: 2345.30435, Residuals: -1.08500, Convergence: 0.016780\n",
      "Epoch: 166, Loss: 2317.55130, Residuals: -1.08334, Convergence: 0.011975\n",
      "Epoch: 167, Loss: 2294.29506, Residuals: -1.08079, Convergence: 0.010137\n",
      "Epoch: 168, Loss: 2274.55589, Residuals: -1.07794, Convergence: 0.008678\n",
      "Epoch: 169, Loss: 2257.66767, Residuals: -1.07494, Convergence: 0.007480\n",
      "Epoch: 170, Loss: 2243.12029, Residuals: -1.07186, Convergence: 0.006485\n",
      "Epoch: 171, Loss: 2230.49810, Residuals: -1.06874, Convergence: 0.005659\n",
      "Epoch: 172, Loss: 2219.45143, Residuals: -1.06559, Convergence: 0.004977\n",
      "Epoch: 173, Loss: 2209.68097, Residuals: -1.06242, Convergence: 0.004422\n",
      "Epoch: 174, Loss: 2200.93364, Residuals: -1.05921, Convergence: 0.003974\n",
      "Epoch: 175, Loss: 2193.00039, Residuals: -1.05594, Convergence: 0.003618\n",
      "Epoch: 176, Loss: 2185.71680, Residuals: -1.05259, Convergence: 0.003332\n",
      "Epoch: 177, Loss: 2178.97028, Residuals: -1.04918, Convergence: 0.003096\n",
      "Epoch: 178, Loss: 2172.69254, Residuals: -1.04571, Convergence: 0.002889\n",
      "Epoch: 179, Loss: 2166.84620, Residuals: -1.04222, Convergence: 0.002698\n",
      "Epoch: 180, Loss: 2161.40810, Residuals: -1.03876, Convergence: 0.002516\n",
      "Epoch: 181, Loss: 2156.35744, Residuals: -1.03537, Convergence: 0.002342\n",
      "Epoch: 182, Loss: 2151.67316, Residuals: -1.03208, Convergence: 0.002177\n",
      "Epoch: 183, Loss: 2147.33295, Residuals: -1.02892, Convergence: 0.002021\n",
      "Epoch: 184, Loss: 2143.31201, Residuals: -1.02590, Convergence: 0.001876\n",
      "Epoch: 185, Loss: 2139.58776, Residuals: -1.02304, Convergence: 0.001741\n",
      "Epoch: 186, Loss: 2136.13503, Residuals: -1.02034, Convergence: 0.001616\n",
      "Epoch: 187, Loss: 2132.93320, Residuals: -1.01779, Convergence: 0.001501\n",
      "Epoch: 188, Loss: 2129.96234, Residuals: -1.01539, Convergence: 0.001395\n",
      "Epoch: 189, Loss: 2127.20283, Residuals: -1.01315, Convergence: 0.001297\n",
      "Epoch: 190, Loss: 2124.63670, Residuals: -1.01103, Convergence: 0.001208\n",
      "Epoch: 191, Loss: 2122.24805, Residuals: -1.00905, Convergence: 0.001126\n",
      "Epoch: 192, Loss: 2120.02081, Residuals: -1.00719, Convergence: 0.001051\n",
      "Epoch: 193, Loss: 2117.94154, Residuals: -1.00543, Convergence: 0.000982\n",
      "Evidence 14494.738\n",
      "\n",
      "Epoch: 193, Evidence: 14494.73828, Convergence: 0.218869\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 4.37e-01\n",
      "Epoch: 193, Loss: 2510.08288, Residuals: -1.00543, Convergence:   inf\n",
      "Epoch: 194, Loss: 2496.12817, Residuals: -1.00205, Convergence: 0.005591\n",
      "Epoch: 195, Loss: 2484.72529, Residuals: -0.99832, Convergence: 0.004589\n",
      "Epoch: 196, Loss: 2474.93391, Residuals: -0.99478, Convergence: 0.003956\n",
      "Epoch: 197, Loss: 2466.46727, Residuals: -0.99148, Convergence: 0.003433\n",
      "Epoch: 198, Loss: 2459.10616, Residuals: -0.98846, Convergence: 0.002993\n",
      "Epoch: 199, Loss: 2452.67079, Residuals: -0.98570, Convergence: 0.002624\n",
      "Epoch: 200, Loss: 2447.01461, Residuals: -0.98318, Convergence: 0.002311\n",
      "Epoch: 201, Loss: 2442.01445, Residuals: -0.98088, Convergence: 0.002048\n",
      "Epoch: 202, Loss: 2437.56831, Residuals: -0.97878, Convergence: 0.001824\n",
      "Epoch: 203, Loss: 2433.59037, Residuals: -0.97686, Convergence: 0.001635\n",
      "Epoch: 204, Loss: 2430.01059, Residuals: -0.97510, Convergence: 0.001473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 205, Loss: 2426.76951, Residuals: -0.97348, Convergence: 0.001336\n",
      "Epoch: 206, Loss: 2423.81864, Residuals: -0.97199, Convergence: 0.001217\n",
      "Epoch: 207, Loss: 2421.11833, Residuals: -0.97062, Convergence: 0.001115\n",
      "Epoch: 208, Loss: 2418.63527, Residuals: -0.96936, Convergence: 0.001027\n",
      "Epoch: 209, Loss: 2416.34115, Residuals: -0.96820, Convergence: 0.000949\n",
      "Evidence 14890.687\n",
      "\n",
      "Epoch: 209, Evidence: 14890.68652, Convergence: 0.026590\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 3.33e-01\n",
      "Epoch: 209, Loss: 2514.96967, Residuals: -0.96820, Convergence:   inf\n",
      "Epoch: 210, Loss: 2508.43576, Residuals: -0.96466, Convergence: 0.002605\n",
      "Epoch: 211, Loss: 2503.03349, Residuals: -0.96161, Convergence: 0.002158\n",
      "Epoch: 212, Loss: 2498.45274, Residuals: -0.95904, Convergence: 0.001833\n",
      "Epoch: 213, Loss: 2494.51119, Residuals: -0.95688, Convergence: 0.001580\n",
      "Epoch: 214, Loss: 2491.07355, Residuals: -0.95505, Convergence: 0.001380\n",
      "Epoch: 215, Loss: 2488.03727, Residuals: -0.95349, Convergence: 0.001220\n",
      "Epoch: 216, Loss: 2485.32435, Residuals: -0.95218, Convergence: 0.001092\n",
      "Epoch: 217, Loss: 2482.87438, Residuals: -0.95105, Convergence: 0.000987\n",
      "Evidence 14973.108\n",
      "\n",
      "Epoch: 217, Evidence: 14973.10840, Convergence: 0.005505\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 2.61e-01\n",
      "Epoch: 217, Loss: 2516.59172, Residuals: -0.95105, Convergence:   inf\n",
      "Epoch: 218, Loss: 2512.60238, Residuals: -0.94844, Convergence: 0.001588\n",
      "Epoch: 219, Loss: 2509.28813, Residuals: -0.94639, Convergence: 0.001321\n",
      "Epoch: 220, Loss: 2506.44725, Residuals: -0.94474, Convergence: 0.001133\n",
      "Epoch: 221, Loss: 2503.96244, Residuals: -0.94342, Convergence: 0.000992\n",
      "Evidence 15001.230\n",
      "\n",
      "Epoch: 221, Evidence: 15001.23047, Convergence: 0.001875\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 2.10e-01\n",
      "Epoch: 221, Loss: 2517.56847, Residuals: -0.94342, Convergence:   inf\n",
      "Epoch: 222, Loss: 2514.57996, Residuals: -0.94140, Convergence: 0.001188\n",
      "Epoch: 223, Loss: 2512.07344, Residuals: -0.93984, Convergence: 0.000998\n",
      "Evidence 15013.548\n",
      "\n",
      "Epoch: 223, Evidence: 15013.54785, Convergence: 0.000820\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 1.74e-01\n",
      "Epoch: 223, Loss: 2518.28011, Residuals: -0.93984, Convergence:   inf\n",
      "Epoch: 224, Loss: 2513.55405, Residuals: -0.93723, Convergence: 0.001880\n",
      "Epoch: 225, Loss: 2509.95107, Residuals: -0.93546, Convergence: 0.001435\n",
      "Epoch: 226, Loss: 2507.02382, Residuals: -0.93430, Convergence: 0.001168\n",
      "Epoch: 227, Loss: 2504.53903, Residuals: -0.93362, Convergence: 0.000992\n",
      "Evidence 15031.871\n",
      "\n",
      "Epoch: 227, Evidence: 15031.87109, Convergence: 0.002038\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 1.44e-01\n",
      "Epoch: 227, Loss: 2518.42418, Residuals: -0.93362, Convergence:   inf\n",
      "Epoch: 228, Loss: 2515.29036, Residuals: -0.93133, Convergence: 0.001246\n",
      "Epoch: 229, Loss: 2512.80783, Residuals: -0.93022, Convergence: 0.000988\n",
      "Evidence 15043.095\n",
      "\n",
      "Epoch: 229, Evidence: 15043.09473, Convergence: 0.000746\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 1.21e-01\n",
      "Epoch: 229, Loss: 2518.62170, Residuals: -0.93022, Convergence:   inf\n",
      "Epoch: 230, Loss: 2514.07458, Residuals: -0.92726, Convergence: 0.001809\n",
      "Epoch: 231, Loss: 2510.77031, Residuals: -0.92841, Convergence: 0.001316\n",
      "Epoch: 232, Loss: 2508.01538, Residuals: -0.92859, Convergence: 0.001098\n",
      "Epoch: 233, Loss: 2505.63692, Residuals: -0.93123, Convergence: 0.000949\n",
      "Evidence 15059.232\n",
      "\n",
      "Epoch: 233, Evidence: 15059.23242, Convergence: 0.001817\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 1.08e-01\n",
      "Epoch: 233, Loss: 2517.86026, Residuals: -0.93123, Convergence:   inf\n",
      "Epoch: 234, Loss: 2515.99563, Residuals: -0.92889, Convergence: 0.000741\n",
      "Evidence 15066.016\n",
      "\n",
      "Epoch: 234, Evidence: 15066.01562, Convergence: 0.000450\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 8.77e-02\n",
      "Epoch: 234, Loss: 2518.87096, Residuals: -0.92889, Convergence:   inf\n",
      "Epoch: 235, Loss: 2561.60082, Residuals: -0.96850, Convergence: -0.016681\n",
      "Epoch: 235, Loss: 2516.51357, Residuals: -0.92708, Convergence: 0.000937\n",
      "Evidence 15070.400\n",
      "\n",
      "Epoch: 235, Evidence: 15070.40039, Convergence: 0.000741\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 8.07e-02\n",
      "Epoch: 235, Loss: 2518.27029, Residuals: -0.92708, Convergence:   inf\n",
      "Epoch: 236, Loss: 2522.32846, Residuals: -0.92834, Convergence: -0.001609\n",
      "Epoch: 236, Loss: 2517.97017, Residuals: -0.92551, Convergence: 0.000119\n",
      "Evidence 15072.262\n",
      "\n",
      "Epoch: 236, Evidence: 15072.26172, Convergence: 0.000864\n",
      "Total samples: 184, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 384.84350, Residuals: -4.53878, Convergence:   inf\n",
      "Epoch: 1, Loss: 358.96235, Residuals: -4.41707, Convergence: 0.072100\n",
      "Epoch: 2, Loss: 337.78218, Residuals: -4.25198, Convergence: 0.062704\n",
      "Epoch: 3, Loss: 321.60041, Residuals: -4.08662, Convergence: 0.050316\n",
      "Epoch: 4, Loss: 309.25241, Residuals: -3.94089, Convergence: 0.039929\n",
      "Epoch: 5, Loss: 299.45443, Residuals: -3.81213, Convergence: 0.032719\n",
      "Epoch: 6, Loss: 291.49831, Residuals: -3.70016, Convergence: 0.027294\n",
      "Epoch: 7, Loss: 284.89262, Residuals: -3.60440, Convergence: 0.023187\n",
      "Epoch: 8, Loss: 279.27317, Residuals: -3.52269, Convergence: 0.020122\n",
      "Epoch: 9, Loss: 274.38258, Residuals: -3.45264, Convergence: 0.017824\n",
      "Epoch: 10, Loss: 270.03724, Residuals: -3.39210, Convergence: 0.016092\n",
      "Epoch: 11, Loss: 266.10348, Residuals: -3.33925, Convergence: 0.014783\n",
      "Epoch: 12, Loss: 262.48325, Residuals: -3.29255, Convergence: 0.013792\n",
      "Epoch: 13, Loss: 259.10500, Residuals: -3.25063, Convergence: 0.013038\n",
      "Epoch: 14, Loss: 255.91781, Residuals: -3.21236, Convergence: 0.012454\n",
      "Epoch: 15, Loss: 252.88868, Residuals: -3.17677, Convergence: 0.011978\n",
      "Epoch: 16, Loss: 250.00026, Residuals: -3.14325, Convergence: 0.011554\n",
      "Epoch: 17, Loss: 247.24101, Residuals: -3.11144, Convergence: 0.011160\n",
      "Epoch: 18, Loss: 244.59059, Residuals: -3.08095, Convergence: 0.010836\n",
      "Epoch: 19, Loss: 242.01623, Residuals: -3.05128, Convergence: 0.010637\n",
      "Epoch: 20, Loss: 239.48020, Residuals: -3.02186, Convergence: 0.010590\n",
      "Epoch: 21, Loss: 236.94902, Residuals: -2.99215, Convergence: 0.010682\n",
      "Epoch: 22, Loss: 234.39727, Residuals: -2.96176, Convergence: 0.010886\n",
      "Epoch: 23, Loss: 231.79761, Residuals: -2.93039, Convergence: 0.011215\n",
      "Epoch: 24, Loss: 229.10290, Residuals: -2.89748, Convergence: 0.011762\n",
      "Epoch: 25, Loss: 226.25375, Residuals: -2.86234, Convergence: 0.012593\n",
      "Epoch: 26, Loss: 223.25793, Residuals: -2.82493, Convergence: 0.013419\n",
      "Epoch: 27, Loss: 220.25144, Residuals: -2.78661, Convergence: 0.013650\n",
      "Epoch: 28, Loss: 217.34084, Residuals: -2.74865, Convergence: 0.013392\n",
      "Epoch: 29, Loss: 214.53768, Residuals: -2.71132, Convergence: 0.013066\n",
      "Epoch: 30, Loss: 211.82459, Residuals: -2.67453, Convergence: 0.012808\n",
      "Epoch: 31, Loss: 209.18482, Residuals: -2.63815, Convergence: 0.012619\n",
      "Epoch: 32, Loss: 206.60702, Residuals: -2.60207, Convergence: 0.012477\n",
      "Epoch: 33, Loss: 204.08478, Residuals: -2.56621, Convergence: 0.012359\n",
      "Epoch: 34, Loss: 201.61530, Residuals: -2.53052, Convergence: 0.012249\n",
      "Epoch: 35, Loss: 199.19815, Residuals: -2.49499, Convergence: 0.012134\n",
      "Epoch: 36, Loss: 196.83442, Residuals: -2.45961, Convergence: 0.012009\n",
      "Epoch: 37, Loss: 194.52594, Residuals: -2.42438, Convergence: 0.011867\n",
      "Epoch: 38, Loss: 192.27484, Residuals: -2.38931, Convergence: 0.011708\n",
      "Epoch: 39, Loss: 190.08316, Residuals: -2.35443, Convergence: 0.011530\n",
      "Epoch: 40, Loss: 187.95272, Residuals: -2.31977, Convergence: 0.011335\n",
      "Epoch: 41, Loss: 185.88496, Residuals: -2.28534, Convergence: 0.011124\n",
      "Epoch: 42, Loss: 183.88095, Residuals: -2.25118, Convergence: 0.010898\n",
      "Epoch: 43, Loss: 181.94146, Residuals: -2.21732, Convergence: 0.010660\n",
      "Epoch: 44, Loss: 180.06703, Residuals: -2.18379, Convergence: 0.010410\n",
      "Epoch: 45, Loss: 178.25828, Residuals: -2.15062, Convergence: 0.010147\n",
      "Epoch: 46, Loss: 176.51593, Residuals: -2.11785, Convergence: 0.009871\n",
      "Epoch: 47, Loss: 174.84101, Residuals: -2.08552, Convergence: 0.009580\n",
      "Epoch: 48, Loss: 173.23470, Residuals: -2.05369, Convergence: 0.009272\n",
      "Epoch: 49, Loss: 171.69805, Residuals: -2.02240, Convergence: 0.008950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50, Loss: 170.23177, Residuals: -1.99171, Convergence: 0.008613\n",
      "Epoch: 51, Loss: 168.83588, Residuals: -1.96167, Convergence: 0.008268\n",
      "Epoch: 52, Loss: 167.50972, Residuals: -1.93231, Convergence: 0.007917\n",
      "Epoch: 53, Loss: 166.25187, Residuals: -1.90368, Convergence: 0.007566\n",
      "Epoch: 54, Loss: 165.06029, Residuals: -1.87580, Convergence: 0.007219\n",
      "Epoch: 55, Loss: 163.93242, Residuals: -1.84870, Convergence: 0.006880\n",
      "Epoch: 56, Loss: 162.86532, Residuals: -1.82238, Convergence: 0.006552\n",
      "Epoch: 57, Loss: 161.85574, Residuals: -1.79684, Convergence: 0.006238\n",
      "Epoch: 58, Loss: 160.90023, Residuals: -1.77210, Convergence: 0.005939\n",
      "Epoch: 59, Loss: 159.99525, Residuals: -1.74813, Convergence: 0.005656\n",
      "Epoch: 60, Loss: 159.13723, Residuals: -1.72492, Convergence: 0.005392\n",
      "Epoch: 61, Loss: 158.32271, Residuals: -1.70246, Convergence: 0.005145\n",
      "Epoch: 62, Loss: 157.54839, Residuals: -1.68072, Convergence: 0.004915\n",
      "Epoch: 63, Loss: 156.81117, Residuals: -1.65969, Convergence: 0.004701\n",
      "Epoch: 64, Loss: 156.10821, Residuals: -1.63933, Convergence: 0.004503\n",
      "Epoch: 65, Loss: 155.43688, Residuals: -1.61964, Convergence: 0.004319\n",
      "Epoch: 66, Loss: 154.79471, Residuals: -1.60059, Convergence: 0.004148\n",
      "Epoch: 67, Loss: 154.17944, Residuals: -1.58216, Convergence: 0.003991\n",
      "Epoch: 68, Loss: 153.58888, Residuals: -1.56433, Convergence: 0.003845\n",
      "Epoch: 69, Loss: 153.02101, Residuals: -1.54707, Convergence: 0.003711\n",
      "Epoch: 70, Loss: 152.47400, Residuals: -1.53037, Convergence: 0.003588\n",
      "Epoch: 71, Loss: 151.94623, Residuals: -1.51420, Convergence: 0.003473\n",
      "Epoch: 72, Loss: 151.43633, Residuals: -1.49853, Convergence: 0.003367\n",
      "Epoch: 73, Loss: 150.94328, Residuals: -1.48334, Convergence: 0.003266\n",
      "Epoch: 74, Loss: 150.46631, Residuals: -1.46862, Convergence: 0.003170\n",
      "Epoch: 75, Loss: 150.00490, Residuals: -1.45436, Convergence: 0.003076\n",
      "Epoch: 76, Loss: 149.55877, Residuals: -1.44053, Convergence: 0.002983\n",
      "Epoch: 77, Loss: 149.12771, Residuals: -1.42715, Convergence: 0.002891\n",
      "Epoch: 78, Loss: 148.71166, Residuals: -1.41419, Convergence: 0.002798\n",
      "Epoch: 79, Loss: 148.31054, Residuals: -1.40166, Convergence: 0.002705\n",
      "Epoch: 80, Loss: 147.92431, Residuals: -1.38956, Convergence: 0.002611\n",
      "Epoch: 81, Loss: 147.55288, Residuals: -1.37786, Convergence: 0.002517\n",
      "Epoch: 82, Loss: 147.19615, Residuals: -1.36658, Convergence: 0.002424\n",
      "Epoch: 83, Loss: 146.85393, Residuals: -1.35571, Convergence: 0.002330\n",
      "Epoch: 84, Loss: 146.52603, Residuals: -1.34523, Convergence: 0.002238\n",
      "Epoch: 85, Loss: 146.21216, Residuals: -1.33515, Convergence: 0.002147\n",
      "Epoch: 86, Loss: 145.91205, Residuals: -1.32545, Convergence: 0.002057\n",
      "Epoch: 87, Loss: 145.62534, Residuals: -1.31613, Convergence: 0.001969\n",
      "Epoch: 88, Loss: 145.35166, Residuals: -1.30717, Convergence: 0.001883\n",
      "Epoch: 89, Loss: 145.09063, Residuals: -1.29858, Convergence: 0.001799\n",
      "Epoch: 90, Loss: 144.84184, Residuals: -1.29033, Convergence: 0.001718\n",
      "Epoch: 91, Loss: 144.60489, Residuals: -1.28242, Convergence: 0.001639\n",
      "Epoch: 92, Loss: 144.37937, Residuals: -1.27483, Convergence: 0.001562\n",
      "Epoch: 93, Loss: 144.16488, Residuals: -1.26757, Convergence: 0.001488\n",
      "Epoch: 94, Loss: 143.96105, Residuals: -1.26062, Convergence: 0.001416\n",
      "Epoch: 95, Loss: 143.76748, Residuals: -1.25396, Convergence: 0.001346\n",
      "Epoch: 96, Loss: 143.58384, Residuals: -1.24759, Convergence: 0.001279\n",
      "Epoch: 97, Loss: 143.40978, Residuals: -1.24151, Convergence: 0.001214\n",
      "Epoch: 98, Loss: 143.24498, Residuals: -1.23570, Convergence: 0.001150\n",
      "Epoch: 99, Loss: 143.08914, Residuals: -1.23016, Convergence: 0.001089\n",
      "Epoch: 100, Loss: 142.94196, Residuals: -1.22488, Convergence: 0.001030\n",
      "Epoch: 101, Loss: 142.80317, Residuals: -1.21985, Convergence: 0.000972\n",
      "Evidence -184.829\n",
      "\n",
      "Epoch: 101, Evidence: -184.82932, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 7.25e-01\n",
      "Epoch: 101, Loss: 1392.03906, Residuals: -1.21985, Convergence:   inf\n",
      "Epoch: 102, Loss: 1327.97474, Residuals: -1.25002, Convergence: 0.048242\n",
      "Epoch: 103, Loss: 1279.20136, Residuals: -1.27393, Convergence: 0.038128\n",
      "Epoch: 104, Loss: 1242.38350, Residuals: -1.29119, Convergence: 0.029635\n",
      "Epoch: 105, Loss: 1213.75759, Residuals: -1.30337, Convergence: 0.023585\n",
      "Epoch: 106, Loss: 1190.61632, Residuals: -1.31243, Convergence: 0.019436\n",
      "Epoch: 107, Loss: 1171.41235, Residuals: -1.31943, Convergence: 0.016394\n",
      "Epoch: 108, Loss: 1155.21080, Residuals: -1.32482, Convergence: 0.014025\n",
      "Epoch: 109, Loss: 1141.37054, Residuals: -1.32883, Convergence: 0.012126\n",
      "Epoch: 110, Loss: 1129.41338, Residuals: -1.33161, Convergence: 0.010587\n",
      "Epoch: 111, Loss: 1118.96414, Residuals: -1.33328, Convergence: 0.009338\n",
      "Epoch: 112, Loss: 1109.71923, Residuals: -1.33393, Convergence: 0.008331\n",
      "Epoch: 113, Loss: 1101.42763, Residuals: -1.33363, Convergence: 0.007528\n",
      "Epoch: 114, Loss: 1093.87709, Residuals: -1.33246, Convergence: 0.006903\n",
      "Epoch: 115, Loss: 1086.88389, Residuals: -1.33045, Convergence: 0.006434\n",
      "Epoch: 116, Loss: 1080.28674, Residuals: -1.32763, Convergence: 0.006107\n",
      "Epoch: 117, Loss: 1073.94392, Residuals: -1.32402, Convergence: 0.005906\n",
      "Epoch: 118, Loss: 1067.73612, Residuals: -1.31964, Convergence: 0.005814\n",
      "Epoch: 119, Loss: 1061.57690, Residuals: -1.31453, Convergence: 0.005802\n",
      "Epoch: 120, Loss: 1055.42798, Residuals: -1.30877, Convergence: 0.005826\n",
      "Epoch: 121, Loss: 1049.31251, Residuals: -1.30244, Convergence: 0.005828\n",
      "Epoch: 122, Loss: 1043.30909, Residuals: -1.29567, Convergence: 0.005754\n",
      "Epoch: 123, Loss: 1037.51958, Residuals: -1.28855, Convergence: 0.005580\n",
      "Epoch: 124, Loss: 1032.02927, Residuals: -1.28117, Convergence: 0.005320\n",
      "Epoch: 125, Loss: 1026.88469, Residuals: -1.27363, Convergence: 0.005010\n",
      "Epoch: 126, Loss: 1022.09395, Residuals: -1.26598, Convergence: 0.004687\n",
      "Epoch: 127, Loss: 1017.63940, Residuals: -1.25831, Convergence: 0.004377\n",
      "Epoch: 128, Loss: 1013.49142, Residuals: -1.25065, Convergence: 0.004093\n",
      "Epoch: 129, Loss: 1009.61670, Residuals: -1.24306, Convergence: 0.003838\n",
      "Epoch: 130, Loss: 1005.98409, Residuals: -1.23556, Convergence: 0.003611\n",
      "Epoch: 131, Loss: 1002.56504, Residuals: -1.22820, Convergence: 0.003410\n",
      "Epoch: 132, Loss: 999.33567, Residuals: -1.22098, Convergence: 0.003232\n",
      "Epoch: 133, Loss: 996.27581, Residuals: -1.21394, Convergence: 0.003071\n",
      "Epoch: 134, Loss: 993.36846, Residuals: -1.20708, Convergence: 0.002927\n",
      "Epoch: 135, Loss: 990.60046, Residuals: -1.20042, Convergence: 0.002794\n",
      "Epoch: 136, Loss: 987.95994, Residuals: -1.19397, Convergence: 0.002673\n",
      "Epoch: 137, Loss: 985.43817, Residuals: -1.18773, Convergence: 0.002559\n",
      "Epoch: 138, Loss: 983.02712, Residuals: -1.18170, Convergence: 0.002453\n",
      "Epoch: 139, Loss: 980.72003, Residuals: -1.17589, Convergence: 0.002352\n",
      "Epoch: 140, Loss: 978.51095, Residuals: -1.17030, Convergence: 0.002258\n",
      "Epoch: 141, Loss: 976.39508, Residuals: -1.16493, Convergence: 0.002167\n",
      "Epoch: 142, Loss: 974.36743, Residuals: -1.15977, Convergence: 0.002081\n",
      "Epoch: 143, Loss: 972.42403, Residuals: -1.15481, Convergence: 0.001999\n",
      "Epoch: 144, Loss: 970.56077, Residuals: -1.15007, Convergence: 0.001920\n",
      "Epoch: 145, Loss: 968.77367, Residuals: -1.14552, Convergence: 0.001845\n",
      "Epoch: 146, Loss: 967.05914, Residuals: -1.14116, Convergence: 0.001773\n",
      "Epoch: 147, Loss: 965.41380, Residuals: -1.13699, Convergence: 0.001704\n",
      "Epoch: 148, Loss: 963.83361, Residuals: -1.13299, Convergence: 0.001639\n",
      "Epoch: 149, Loss: 962.31485, Residuals: -1.12917, Convergence: 0.001578\n",
      "Epoch: 150, Loss: 960.85382, Residuals: -1.12550, Convergence: 0.001521\n",
      "Epoch: 151, Loss: 959.44655, Residuals: -1.12199, Convergence: 0.001467\n",
      "Epoch: 152, Loss: 958.08869, Residuals: -1.11862, Convergence: 0.001417\n",
      "Epoch: 153, Loss: 956.77571, Residuals: -1.11538, Convergence: 0.001372\n",
      "Epoch: 154, Loss: 955.50299, Residuals: -1.11226, Convergence: 0.001332\n",
      "Epoch: 155, Loss: 954.26527, Residuals: -1.10925, Convergence: 0.001297\n",
      "Epoch: 156, Loss: 953.05795, Residuals: -1.10634, Convergence: 0.001267\n",
      "Epoch: 157, Loss: 951.87522, Residuals: -1.10352, Convergence: 0.001243\n",
      "Epoch: 158, Loss: 950.71229, Residuals: -1.10077, Convergence: 0.001223\n",
      "Epoch: 159, Loss: 949.56400, Residuals: -1.09809, Convergence: 0.001209\n",
      "Epoch: 160, Loss: 948.42629, Residuals: -1.09545, Convergence: 0.001200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 161, Loss: 947.29596, Residuals: -1.09286, Convergence: 0.001193\n",
      "Epoch: 162, Loss: 946.17204, Residuals: -1.09030, Convergence: 0.001188\n",
      "Epoch: 163, Loss: 945.05421, Residuals: -1.08777, Convergence: 0.001183\n",
      "Epoch: 164, Loss: 943.94516, Residuals: -1.08528, Convergence: 0.001175\n",
      "Epoch: 165, Loss: 942.84846, Residuals: -1.08283, Convergence: 0.001163\n",
      "Epoch: 166, Loss: 941.76864, Residuals: -1.08042, Convergence: 0.001147\n",
      "Epoch: 167, Loss: 940.71101, Residuals: -1.07807, Convergence: 0.001124\n",
      "Epoch: 168, Loss: 939.67992, Residuals: -1.07577, Convergence: 0.001097\n",
      "Epoch: 169, Loss: 938.67926, Residuals: -1.07354, Convergence: 0.001066\n",
      "Epoch: 170, Loss: 937.71207, Residuals: -1.07139, Convergence: 0.001031\n",
      "Epoch: 171, Loss: 936.77970, Residuals: -1.06930, Convergence: 0.000995\n",
      "Evidence 11291.348\n",
      "\n",
      "Epoch: 171, Evidence: 11291.34766, Convergence: 1.016369\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 5.75e-01\n",
      "Epoch: 171, Loss: 2372.41403, Residuals: -1.06930, Convergence:   inf\n",
      "Epoch: 172, Loss: 2333.82732, Residuals: -1.07728, Convergence: 0.016534\n",
      "Epoch: 173, Loss: 2307.31606, Residuals: -1.07560, Convergence: 0.011490\n",
      "Epoch: 174, Loss: 2285.29407, Residuals: -1.07321, Convergence: 0.009636\n",
      "Epoch: 175, Loss: 2266.77112, Residuals: -1.07062, Convergence: 0.008172\n",
      "Epoch: 176, Loss: 2251.03235, Residuals: -1.06792, Convergence: 0.006992\n",
      "Epoch: 177, Loss: 2237.51622, Residuals: -1.06513, Convergence: 0.006041\n",
      "Epoch: 178, Loss: 2225.76667, Residuals: -1.06227, Convergence: 0.005279\n",
      "Epoch: 179, Loss: 2215.41108, Residuals: -1.05932, Convergence: 0.004674\n",
      "Epoch: 180, Loss: 2206.15308, Residuals: -1.05627, Convergence: 0.004196\n",
      "Epoch: 181, Loss: 2197.77113, Residuals: -1.05311, Convergence: 0.003814\n",
      "Epoch: 182, Loss: 2190.11557, Residuals: -1.04984, Convergence: 0.003496\n",
      "Epoch: 183, Loss: 2183.09405, Residuals: -1.04650, Convergence: 0.003216\n",
      "Epoch: 184, Loss: 2176.64959, Residuals: -1.04314, Convergence: 0.002961\n",
      "Epoch: 185, Loss: 2170.73613, Residuals: -1.03981, Convergence: 0.002724\n",
      "Epoch: 186, Loss: 2165.31183, Residuals: -1.03656, Convergence: 0.002505\n",
      "Epoch: 187, Loss: 2160.33300, Residuals: -1.03341, Convergence: 0.002305\n",
      "Epoch: 188, Loss: 2155.75475, Residuals: -1.03038, Convergence: 0.002124\n",
      "Epoch: 189, Loss: 2151.53658, Residuals: -1.02748, Convergence: 0.001961\n",
      "Epoch: 190, Loss: 2147.63927, Residuals: -1.02471, Convergence: 0.001815\n",
      "Epoch: 191, Loss: 2144.02764, Residuals: -1.02206, Convergence: 0.001685\n",
      "Epoch: 192, Loss: 2140.67243, Residuals: -1.01953, Convergence: 0.001567\n",
      "Epoch: 193, Loss: 2137.54885, Residuals: -1.01713, Convergence: 0.001461\n",
      "Epoch: 194, Loss: 2134.63567, Residuals: -1.01484, Convergence: 0.001365\n",
      "Epoch: 195, Loss: 2131.91483, Residuals: -1.01265, Convergence: 0.001276\n",
      "Epoch: 196, Loss: 2129.37061, Residuals: -1.01058, Convergence: 0.001195\n",
      "Epoch: 197, Loss: 2126.98927, Residuals: -1.00860, Convergence: 0.001120\n",
      "Epoch: 198, Loss: 2124.75955, Residuals: -1.00671, Convergence: 0.001049\n",
      "Epoch: 199, Loss: 2122.66889, Residuals: -1.00492, Convergence: 0.000985\n",
      "Evidence 14417.923\n",
      "\n",
      "Epoch: 199, Evidence: 14417.92285, Convergence: 0.216853\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 4.36e-01\n",
      "Epoch: 199, Loss: 2495.19589, Residuals: -1.00492, Convergence:   inf\n",
      "Epoch: 200, Loss: 2481.90754, Residuals: -1.00192, Convergence: 0.005354\n",
      "Epoch: 201, Loss: 2470.98278, Residuals: -0.99856, Convergence: 0.004421\n",
      "Epoch: 202, Loss: 2461.54452, Residuals: -0.99534, Convergence: 0.003834\n",
      "Epoch: 203, Loss: 2453.34491, Residuals: -0.99236, Convergence: 0.003342\n",
      "Epoch: 204, Loss: 2446.19151, Residuals: -0.98963, Convergence: 0.002924\n",
      "Epoch: 205, Loss: 2439.92362, Residuals: -0.98714, Convergence: 0.002569\n",
      "Epoch: 206, Loss: 2434.40882, Residuals: -0.98487, Convergence: 0.002265\n",
      "Epoch: 207, Loss: 2429.53318, Residuals: -0.98282, Convergence: 0.002007\n",
      "Epoch: 208, Loss: 2425.20047, Residuals: -0.98095, Convergence: 0.001787\n",
      "Epoch: 209, Loss: 2421.33099, Residuals: -0.97926, Convergence: 0.001598\n",
      "Epoch: 210, Loss: 2417.85691, Residuals: -0.97772, Convergence: 0.001437\n",
      "Epoch: 211, Loss: 2414.72071, Residuals: -0.97632, Convergence: 0.001299\n",
      "Epoch: 212, Loss: 2411.87528, Residuals: -0.97504, Convergence: 0.001180\n",
      "Epoch: 213, Loss: 2409.28126, Residuals: -0.97389, Convergence: 0.001077\n",
      "Epoch: 214, Loss: 2406.90512, Residuals: -0.97284, Convergence: 0.000987\n",
      "Evidence 14781.321\n",
      "\n",
      "Epoch: 214, Evidence: 14781.32129, Convergence: 0.024585\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 3.33e-01\n",
      "Epoch: 214, Loss: 2500.66662, Residuals: -0.97284, Convergence:   inf\n",
      "Epoch: 215, Loss: 2493.93497, Residuals: -0.96965, Convergence: 0.002699\n",
      "Epoch: 216, Loss: 2488.37092, Residuals: -0.96699, Convergence: 0.002236\n",
      "Epoch: 217, Loss: 2483.65993, Residuals: -0.96480, Convergence: 0.001897\n",
      "Epoch: 218, Loss: 2479.61877, Residuals: -0.96299, Convergence: 0.001630\n",
      "Epoch: 219, Loss: 2476.10711, Residuals: -0.96148, Convergence: 0.001418\n",
      "Epoch: 220, Loss: 2473.02051, Residuals: -0.96023, Convergence: 0.001248\n",
      "Epoch: 221, Loss: 2470.27560, Residuals: -0.95918, Convergence: 0.001111\n",
      "Epoch: 222, Loss: 2467.81118, Residuals: -0.95831, Convergence: 0.000999\n",
      "Evidence 14863.251\n",
      "\n",
      "Epoch: 222, Evidence: 14863.25098, Convergence: 0.005512\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 2.60e-01\n",
      "Epoch: 222, Loss: 2502.33796, Residuals: -0.95831, Convergence:   inf\n",
      "Epoch: 223, Loss: 2498.20680, Residuals: -0.95602, Convergence: 0.001654\n",
      "Epoch: 224, Loss: 2494.79070, Residuals: -0.95428, Convergence: 0.001369\n",
      "Epoch: 225, Loss: 2491.88136, Residuals: -0.95293, Convergence: 0.001168\n",
      "Epoch: 226, Loss: 2489.35473, Residuals: -0.95186, Convergence: 0.001015\n",
      "Epoch: 227, Loss: 2487.12396, Residuals: -0.95102, Convergence: 0.000897\n",
      "Evidence 14894.788\n",
      "\n",
      "Epoch: 227, Evidence: 14894.78809, Convergence: 0.002117\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 2.08e-01\n",
      "Epoch: 227, Loss: 2503.21931, Residuals: -0.95102, Convergence:   inf\n",
      "Epoch: 228, Loss: 2500.30791, Residuals: -0.94935, Convergence: 0.001164\n",
      "Epoch: 229, Loss: 2497.88036, Residuals: -0.94811, Convergence: 0.000972\n",
      "Evidence 14907.887\n",
      "\n",
      "Epoch: 229, Evidence: 14907.88672, Convergence: 0.000879\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.72e-01\n",
      "Epoch: 229, Loss: 2503.90748, Residuals: -0.94811, Convergence:   inf\n",
      "Epoch: 230, Loss: 2499.37413, Residuals: -0.94607, Convergence: 0.001814\n",
      "Epoch: 231, Loss: 2495.91538, Residuals: -0.94458, Convergence: 0.001386\n",
      "Epoch: 232, Loss: 2493.11964, Residuals: -0.94356, Convergence: 0.001121\n",
      "Epoch: 233, Loss: 2490.75273, Residuals: -0.94299, Convergence: 0.000950\n",
      "Evidence 14925.675\n",
      "\n",
      "Epoch: 233, Evidence: 14925.67480, Convergence: 0.002069\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.42e-01\n",
      "Epoch: 233, Loss: 2504.01833, Residuals: -0.94299, Convergence:   inf\n",
      "Epoch: 234, Loss: 2501.01062, Residuals: -0.94077, Convergence: 0.001203\n",
      "Epoch: 235, Loss: 2498.63527, Residuals: -0.93960, Convergence: 0.000951\n",
      "Evidence 14936.701\n",
      "\n",
      "Epoch: 235, Evidence: 14936.70117, Convergence: 0.000738\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.20e-01\n",
      "Epoch: 235, Loss: 2504.19311, Residuals: -0.93960, Convergence:   inf\n",
      "Epoch: 236, Loss: 2499.80114, Residuals: -0.93660, Convergence: 0.001757\n",
      "Epoch: 237, Loss: 2496.66539, Residuals: -0.93727, Convergence: 0.001256\n",
      "Epoch: 238, Loss: 2494.04098, Residuals: -0.93757, Convergence: 0.001052\n",
      "Epoch: 239, Loss: 2491.73980, Residuals: -0.94003, Convergence: 0.000924\n",
      "Evidence 14952.441\n",
      "\n",
      "Epoch: 239, Evidence: 14952.44141, Convergence: 0.001790\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.07e-01\n",
      "Epoch: 239, Loss: 2503.45705, Residuals: -0.94003, Convergence:   inf\n",
      "Epoch: 240, Loss: 2501.52987, Residuals: -0.93697, Convergence: 0.000770\n",
      "Evidence 14959.287\n",
      "\n",
      "Epoch: 240, Evidence: 14959.28711, Convergence: 0.000458\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 8.76e-02\n",
      "Epoch: 240, Loss: 2504.43365, Residuals: -0.93697, Convergence:   inf\n",
      "Epoch: 241, Loss: 2545.67037, Residuals: -0.97810, Convergence: -0.016199\n",
      "Epoch: 241, Loss: 2502.26322, Residuals: -0.93619, Convergence: 0.000867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence 14963.494\n",
      "\n",
      "Epoch: 241, Evidence: 14963.49414, Convergence: 0.000739\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 8.09e-02\n",
      "Epoch: 241, Loss: 2503.86321, Residuals: -0.93619, Convergence:   inf\n",
      "Epoch: 242, Loss: 2507.85590, Residuals: -0.93689, Convergence: -0.001592\n",
      "Epoch: 242, Loss: 2503.55129, Residuals: -0.93412, Convergence: 0.000125\n",
      "Evidence 14965.504\n",
      "\n",
      "Epoch: 242, Evidence: 14965.50391, Convergence: 0.000873\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 382.02974, Residuals: -4.53552, Convergence:   inf\n",
      "Epoch: 1, Loss: 356.19207, Residuals: -4.41362, Convergence: 0.072539\n",
      "Epoch: 2, Loss: 334.99805, Residuals: -4.24742, Convergence: 0.063266\n",
      "Epoch: 3, Loss: 318.80282, Residuals: -4.08087, Convergence: 0.050800\n",
      "Epoch: 4, Loss: 306.44992, Residuals: -3.93407, Convergence: 0.040310\n",
      "Epoch: 5, Loss: 296.65176, Residuals: -3.80426, Convergence: 0.033029\n",
      "Epoch: 6, Loss: 288.69892, Residuals: -3.69121, Convergence: 0.027547\n",
      "Epoch: 7, Loss: 282.09961, Residuals: -3.59446, Convergence: 0.023394\n",
      "Epoch: 8, Loss: 276.48979, Residuals: -3.51193, Convergence: 0.020289\n",
      "Epoch: 9, Loss: 271.61284, Residuals: -3.44122, Convergence: 0.017956\n",
      "Epoch: 10, Loss: 267.28654, Residuals: -3.38023, Convergence: 0.016186\n",
      "Epoch: 11, Loss: 263.37914, Residuals: -3.32715, Convergence: 0.014836\n",
      "Epoch: 12, Loss: 259.79490, Residuals: -3.28045, Convergence: 0.013796\n",
      "Epoch: 13, Loss: 256.46450, Residuals: -3.23884, Convergence: 0.012986\n",
      "Epoch: 14, Loss: 253.33823, Residuals: -3.20117, Convergence: 0.012340\n",
      "Epoch: 15, Loss: 250.38140, Residuals: -3.16646, Convergence: 0.011809\n",
      "Epoch: 16, Loss: 247.57159, Residuals: -3.13398, Convergence: 0.011349\n",
      "Epoch: 17, Loss: 244.89300, Residuals: -3.10325, Convergence: 0.010938\n",
      "Epoch: 18, Loss: 242.32631, Residuals: -3.07387, Convergence: 0.010592\n",
      "Epoch: 19, Loss: 239.84300, Residuals: -3.04538, Convergence: 0.010354\n",
      "Epoch: 20, Loss: 237.40862, Residuals: -3.01721, Convergence: 0.010254\n",
      "Epoch: 21, Loss: 234.98929, Residuals: -2.98885, Convergence: 0.010295\n",
      "Epoch: 22, Loss: 232.55679, Residuals: -2.95983, Convergence: 0.010460\n",
      "Epoch: 23, Loss: 230.08711, Residuals: -2.92982, Convergence: 0.010734\n",
      "Epoch: 24, Loss: 227.54800, Residuals: -2.89845, Convergence: 0.011159\n",
      "Epoch: 25, Loss: 224.88909, Residuals: -2.86514, Convergence: 0.011823\n",
      "Epoch: 26, Loss: 222.07664, Residuals: -2.82946, Convergence: 0.012664\n",
      "Epoch: 27, Loss: 219.18622, Residuals: -2.79214, Convergence: 0.013187\n",
      "Epoch: 28, Loss: 216.35584, Residuals: -2.75474, Convergence: 0.013082\n",
      "Epoch: 29, Loss: 213.63703, Residuals: -2.71799, Convergence: 0.012726\n",
      "Epoch: 30, Loss: 211.02102, Residuals: -2.68193, Convergence: 0.012397\n",
      "Epoch: 31, Loss: 208.48890, Residuals: -2.64646, Convergence: 0.012145\n",
      "Epoch: 32, Loss: 206.02510, Residuals: -2.61144, Convergence: 0.011959\n",
      "Epoch: 33, Loss: 203.61884, Residuals: -2.57678, Convergence: 0.011817\n",
      "Epoch: 34, Loss: 201.26350, Residuals: -2.54239, Convergence: 0.011703\n",
      "Epoch: 35, Loss: 198.95552, Residuals: -2.50822, Convergence: 0.011600\n",
      "Epoch: 36, Loss: 196.69363, Residuals: -2.47421, Convergence: 0.011500\n",
      "Epoch: 37, Loss: 194.47811, Residuals: -2.44036, Convergence: 0.011392\n",
      "Epoch: 38, Loss: 192.31021, Residuals: -2.40664, Convergence: 0.011273\n",
      "Epoch: 39, Loss: 190.19169, Residuals: -2.37306, Convergence: 0.011139\n",
      "Epoch: 40, Loss: 188.12449, Residuals: -2.33965, Convergence: 0.010988\n",
      "Epoch: 41, Loss: 186.11049, Residuals: -2.30642, Convergence: 0.010822\n",
      "Epoch: 42, Loss: 184.15129, Residuals: -2.27340, Convergence: 0.010639\n",
      "Epoch: 43, Loss: 182.24808, Residuals: -2.24062, Convergence: 0.010443\n",
      "Epoch: 44, Loss: 180.40161, Residuals: -2.20812, Convergence: 0.010235\n",
      "Epoch: 45, Loss: 178.61214, Residuals: -2.17592, Convergence: 0.010019\n",
      "Epoch: 46, Loss: 176.87961, Residuals: -2.14405, Convergence: 0.009795\n",
      "Epoch: 47, Loss: 175.20373, Residuals: -2.11254, Convergence: 0.009565\n",
      "Epoch: 48, Loss: 173.58425, Residuals: -2.08139, Convergence: 0.009330\n",
      "Epoch: 49, Loss: 172.02104, Residuals: -2.05065, Convergence: 0.009087\n",
      "Epoch: 50, Loss: 170.51419, Residuals: -2.02033, Convergence: 0.008837\n",
      "Epoch: 51, Loss: 169.06387, Residuals: -1.99047, Convergence: 0.008579\n",
      "Epoch: 52, Loss: 167.67021, Residuals: -1.96110, Convergence: 0.008312\n",
      "Epoch: 53, Loss: 166.33308, Residuals: -1.93225, Convergence: 0.008039\n",
      "Epoch: 54, Loss: 165.05198, Residuals: -1.90396, Convergence: 0.007762\n",
      "Epoch: 55, Loss: 163.82605, Residuals: -1.87626, Convergence: 0.007483\n",
      "Epoch: 56, Loss: 162.65402, Residuals: -1.84917, Convergence: 0.007206\n",
      "Epoch: 57, Loss: 161.53426, Residuals: -1.82273, Convergence: 0.006932\n",
      "Epoch: 58, Loss: 160.46488, Residuals: -1.79695, Convergence: 0.006664\n",
      "Epoch: 59, Loss: 159.44373, Residuals: -1.77184, Convergence: 0.006404\n",
      "Epoch: 60, Loss: 158.46849, Residuals: -1.74741, Convergence: 0.006154\n",
      "Epoch: 61, Loss: 157.53671, Residuals: -1.72367, Convergence: 0.005915\n",
      "Epoch: 62, Loss: 156.64592, Residuals: -1.70060, Convergence: 0.005687\n",
      "Epoch: 63, Loss: 155.79371, Residuals: -1.67820, Convergence: 0.005470\n",
      "Epoch: 64, Loss: 154.97787, Residuals: -1.65644, Convergence: 0.005264\n",
      "Epoch: 65, Loss: 154.19640, Residuals: -1.63533, Convergence: 0.005068\n",
      "Epoch: 66, Loss: 153.44764, Residuals: -1.61483, Convergence: 0.004880\n",
      "Epoch: 67, Loss: 152.73018, Residuals: -1.59495, Convergence: 0.004698\n",
      "Epoch: 68, Loss: 152.04289, Residuals: -1.57567, Convergence: 0.004520\n",
      "Epoch: 69, Loss: 151.38481, Residuals: -1.55699, Convergence: 0.004347\n",
      "Epoch: 70, Loss: 150.75513, Residuals: -1.53890, Convergence: 0.004177\n",
      "Epoch: 71, Loss: 150.15311, Residuals: -1.52141, Convergence: 0.004009\n",
      "Epoch: 72, Loss: 149.57803, Residuals: -1.50450, Convergence: 0.003845\n",
      "Epoch: 73, Loss: 149.02918, Residuals: -1.48818, Convergence: 0.003683\n",
      "Epoch: 74, Loss: 148.50579, Residuals: -1.47244, Convergence: 0.003524\n",
      "Epoch: 75, Loss: 148.00710, Residuals: -1.45727, Convergence: 0.003369\n",
      "Epoch: 76, Loss: 147.53229, Residuals: -1.44267, Convergence: 0.003218\n",
      "Epoch: 77, Loss: 147.08050, Residuals: -1.42862, Convergence: 0.003072\n",
      "Epoch: 78, Loss: 146.65087, Residuals: -1.41511, Convergence: 0.002930\n",
      "Epoch: 79, Loss: 146.24251, Residuals: -1.40214, Convergence: 0.002792\n",
      "Epoch: 80, Loss: 145.85452, Residuals: -1.38969, Convergence: 0.002660\n",
      "Epoch: 81, Loss: 145.48602, Residuals: -1.37774, Convergence: 0.002533\n",
      "Epoch: 82, Loss: 145.13612, Residuals: -1.36628, Convergence: 0.002411\n",
      "Epoch: 83, Loss: 144.80398, Residuals: -1.35529, Convergence: 0.002294\n",
      "Epoch: 84, Loss: 144.48876, Residuals: -1.34476, Convergence: 0.002182\n",
      "Epoch: 85, Loss: 144.18966, Residuals: -1.33468, Convergence: 0.002074\n",
      "Epoch: 86, Loss: 143.90594, Residuals: -1.32501, Convergence: 0.001972\n",
      "Epoch: 87, Loss: 143.63684, Residuals: -1.31576, Convergence: 0.001873\n",
      "Epoch: 88, Loss: 143.38172, Residuals: -1.30690, Convergence: 0.001779\n",
      "Epoch: 89, Loss: 143.13991, Residuals: -1.29841, Convergence: 0.001689\n",
      "Epoch: 90, Loss: 142.91080, Residuals: -1.29029, Convergence: 0.001603\n",
      "Epoch: 91, Loss: 142.69384, Residuals: -1.28251, Convergence: 0.001520\n",
      "Epoch: 92, Loss: 142.48851, Residuals: -1.27507, Convergence: 0.001441\n",
      "Epoch: 93, Loss: 142.29431, Residuals: -1.26794, Convergence: 0.001365\n",
      "Epoch: 94, Loss: 142.11079, Residuals: -1.26113, Convergence: 0.001291\n",
      "Epoch: 95, Loss: 141.93754, Residuals: -1.25461, Convergence: 0.001221\n",
      "Epoch: 96, Loss: 141.77417, Residuals: -1.24837, Convergence: 0.001152\n",
      "Epoch: 97, Loss: 141.62034, Residuals: -1.24241, Convergence: 0.001086\n",
      "Epoch: 98, Loss: 141.47569, Residuals: -1.23672, Convergence: 0.001022\n",
      "Epoch: 99, Loss: 141.33990, Residuals: -1.23129, Convergence: 0.000961\n",
      "Evidence -183.191\n",
      "\n",
      "Epoch: 99, Evidence: -183.19101, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.25e-01\n",
      "Epoch: 99, Loss: 1360.94744, Residuals: -1.23129, Convergence:   inf\n",
      "Epoch: 100, Loss: 1298.51282, Residuals: -1.26157, Convergence: 0.048082\n",
      "Epoch: 101, Loss: 1251.44998, Residuals: -1.28510, Convergence: 0.037607\n",
      "Epoch: 102, Loss: 1216.14856, Residuals: -1.30169, Convergence: 0.029027\n",
      "Epoch: 103, Loss: 1188.74675, Residuals: -1.31311, Convergence: 0.023051\n",
      "Epoch: 104, Loss: 1166.58336, Residuals: -1.32144, Convergence: 0.018999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 105, Loss: 1148.17247, Residuals: -1.32771, Convergence: 0.016035\n",
      "Epoch: 106, Loss: 1132.62539, Residuals: -1.33239, Convergence: 0.013727\n",
      "Epoch: 107, Loss: 1119.33681, Residuals: -1.33573, Convergence: 0.011872\n",
      "Epoch: 108, Loss: 1107.85611, Residuals: -1.33787, Convergence: 0.010363\n",
      "Epoch: 109, Loss: 1097.83027, Residuals: -1.33894, Convergence: 0.009132\n",
      "Epoch: 110, Loss: 1088.97437, Residuals: -1.33906, Convergence: 0.008132\n",
      "Epoch: 111, Loss: 1081.05390, Residuals: -1.33831, Convergence: 0.007327\n",
      "Epoch: 112, Loss: 1073.87018, Residuals: -1.33675, Convergence: 0.006690\n",
      "Epoch: 113, Loss: 1067.25485, Residuals: -1.33445, Convergence: 0.006198\n",
      "Epoch: 114, Loss: 1061.06019, Residuals: -1.33143, Convergence: 0.005838\n",
      "Epoch: 115, Loss: 1055.15677, Residuals: -1.32772, Convergence: 0.005595\n",
      "Epoch: 116, Loss: 1049.43057, Residuals: -1.32332, Convergence: 0.005456\n",
      "Epoch: 117, Loss: 1043.78523, Residuals: -1.31824, Convergence: 0.005409\n",
      "Epoch: 118, Loss: 1038.14635, Residuals: -1.31252, Convergence: 0.005432\n",
      "Epoch: 119, Loss: 1032.47508, Residuals: -1.30619, Convergence: 0.005493\n",
      "Epoch: 120, Loss: 1026.78178, Residuals: -1.29934, Convergence: 0.005545\n",
      "Epoch: 121, Loss: 1021.13190, Residuals: -1.29206, Convergence: 0.005533\n",
      "Epoch: 122, Loss: 1015.62559, Residuals: -1.28445, Convergence: 0.005422\n",
      "Epoch: 123, Loss: 1010.36159, Residuals: -1.27660, Convergence: 0.005210\n",
      "Epoch: 124, Loss: 1005.40540, Residuals: -1.26861, Convergence: 0.004930\n",
      "Epoch: 125, Loss: 1000.78179, Residuals: -1.26054, Convergence: 0.004620\n",
      "Epoch: 126, Loss: 996.48342, Residuals: -1.25247, Convergence: 0.004314\n",
      "Epoch: 127, Loss: 992.48740, Residuals: -1.24444, Convergence: 0.004026\n",
      "Epoch: 128, Loss: 988.76324, Residuals: -1.23651, Convergence: 0.003766\n",
      "Epoch: 129, Loss: 985.28126, Residuals: -1.22870, Convergence: 0.003534\n",
      "Epoch: 130, Loss: 982.01499, Residuals: -1.22106, Convergence: 0.003326\n",
      "Epoch: 131, Loss: 978.94145, Residuals: -1.21361, Convergence: 0.003140\n",
      "Epoch: 132, Loss: 976.04212, Residuals: -1.20636, Convergence: 0.002970\n",
      "Epoch: 133, Loss: 973.30116, Residuals: -1.19933, Convergence: 0.002816\n",
      "Epoch: 134, Loss: 970.70626, Residuals: -1.19254, Convergence: 0.002673\n",
      "Epoch: 135, Loss: 968.24703, Residuals: -1.18600, Convergence: 0.002540\n",
      "Epoch: 136, Loss: 965.91439, Residuals: -1.17971, Convergence: 0.002415\n",
      "Epoch: 137, Loss: 963.70066, Residuals: -1.17368, Convergence: 0.002297\n",
      "Epoch: 138, Loss: 961.59802, Residuals: -1.16791, Convergence: 0.002187\n",
      "Epoch: 139, Loss: 959.60007, Residuals: -1.16240, Convergence: 0.002082\n",
      "Epoch: 140, Loss: 957.69971, Residuals: -1.15713, Convergence: 0.001984\n",
      "Epoch: 141, Loss: 955.89036, Residuals: -1.15212, Convergence: 0.001893\n",
      "Epoch: 142, Loss: 954.16647, Residuals: -1.14734, Convergence: 0.001807\n",
      "Epoch: 143, Loss: 952.52098, Residuals: -1.14279, Convergence: 0.001728\n",
      "Epoch: 144, Loss: 950.94846, Residuals: -1.13847, Convergence: 0.001654\n",
      "Epoch: 145, Loss: 949.44360, Residuals: -1.13435, Convergence: 0.001585\n",
      "Epoch: 146, Loss: 948.00083, Residuals: -1.13043, Convergence: 0.001522\n",
      "Epoch: 147, Loss: 946.61538, Residuals: -1.12669, Convergence: 0.001464\n",
      "Epoch: 148, Loss: 945.28305, Residuals: -1.12313, Convergence: 0.001409\n",
      "Epoch: 149, Loss: 943.99960, Residuals: -1.11974, Convergence: 0.001360\n",
      "Epoch: 150, Loss: 942.76156, Residuals: -1.11650, Convergence: 0.001313\n",
      "Epoch: 151, Loss: 941.56532, Residuals: -1.11341, Convergence: 0.001270\n",
      "Epoch: 152, Loss: 940.40814, Residuals: -1.11045, Convergence: 0.001231\n",
      "Epoch: 153, Loss: 939.28727, Residuals: -1.10762, Convergence: 0.001193\n",
      "Epoch: 154, Loss: 938.20023, Residuals: -1.10490, Convergence: 0.001159\n",
      "Epoch: 155, Loss: 937.14454, Residuals: -1.10230, Convergence: 0.001126\n",
      "Epoch: 156, Loss: 936.11796, Residuals: -1.09979, Convergence: 0.001097\n",
      "Epoch: 157, Loss: 935.11813, Residuals: -1.09738, Convergence: 0.001069\n",
      "Epoch: 158, Loss: 934.14290, Residuals: -1.09505, Convergence: 0.001044\n",
      "Epoch: 159, Loss: 933.19022, Residuals: -1.09280, Convergence: 0.001021\n",
      "Epoch: 160, Loss: 932.25739, Residuals: -1.09062, Convergence: 0.001001\n",
      "Epoch: 161, Loss: 931.34193, Residuals: -1.08850, Convergence: 0.000983\n",
      "Evidence 11094.754\n",
      "\n",
      "Epoch: 161, Evidence: 11094.75391, Convergence: 1.016512\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.77e-01\n",
      "Epoch: 161, Loss: 2341.71363, Residuals: -1.08850, Convergence:   inf\n",
      "Epoch: 162, Loss: 2302.94581, Residuals: -1.09816, Convergence: 0.016834\n",
      "Epoch: 163, Loss: 2275.11525, Residuals: -1.09719, Convergence: 0.012233\n",
      "Epoch: 164, Loss: 2251.70770, Residuals: -1.09505, Convergence: 0.010395\n",
      "Epoch: 165, Loss: 2231.74108, Residuals: -1.09246, Convergence: 0.008947\n",
      "Epoch: 166, Loss: 2214.54545, Residuals: -1.08957, Convergence: 0.007765\n",
      "Epoch: 167, Loss: 2199.61356, Residuals: -1.08646, Convergence: 0.006788\n",
      "Epoch: 168, Loss: 2186.54243, Residuals: -1.08316, Convergence: 0.005978\n",
      "Epoch: 169, Loss: 2175.00082, Residuals: -1.07974, Convergence: 0.005306\n",
      "Epoch: 170, Loss: 2164.71141, Residuals: -1.07622, Convergence: 0.004753\n",
      "Epoch: 171, Loss: 2155.43834, Residuals: -1.07260, Convergence: 0.004302\n",
      "Epoch: 172, Loss: 2146.98362, Residuals: -1.06889, Convergence: 0.003938\n",
      "Epoch: 173, Loss: 2139.18947, Residuals: -1.06509, Convergence: 0.003644\n",
      "Epoch: 174, Loss: 2131.94365, Residuals: -1.06120, Convergence: 0.003399\n",
      "Epoch: 175, Loss: 2125.17764, Residuals: -1.05724, Convergence: 0.003184\n",
      "Epoch: 176, Loss: 2118.85921, Residuals: -1.05327, Convergence: 0.002982\n",
      "Epoch: 177, Loss: 2112.97442, Residuals: -1.04933, Convergence: 0.002785\n",
      "Epoch: 178, Loss: 2107.51367, Residuals: -1.04547, Convergence: 0.002591\n",
      "Epoch: 179, Loss: 2102.46653, Residuals: -1.04175, Convergence: 0.002401\n",
      "Epoch: 180, Loss: 2097.81666, Residuals: -1.03818, Convergence: 0.002217\n",
      "Epoch: 181, Loss: 2093.54185, Residuals: -1.03479, Convergence: 0.002042\n",
      "Epoch: 182, Loss: 2089.61834, Residuals: -1.03158, Convergence: 0.001878\n",
      "Epoch: 183, Loss: 2086.01958, Residuals: -1.02856, Convergence: 0.001725\n",
      "Epoch: 184, Loss: 2082.71798, Residuals: -1.02571, Convergence: 0.001585\n",
      "Epoch: 185, Loss: 2079.68716, Residuals: -1.02304, Convergence: 0.001457\n",
      "Epoch: 186, Loss: 2076.90122, Residuals: -1.02052, Convergence: 0.001341\n",
      "Epoch: 187, Loss: 2074.33584, Residuals: -1.01816, Convergence: 0.001237\n",
      "Epoch: 188, Loss: 2071.96891, Residuals: -1.01594, Convergence: 0.001142\n",
      "Epoch: 189, Loss: 2069.77871, Residuals: -1.01384, Convergence: 0.001058\n",
      "Epoch: 190, Loss: 2067.74784, Residuals: -1.01187, Convergence: 0.000982\n",
      "Evidence 14265.436\n",
      "\n",
      "Epoch: 190, Evidence: 14265.43555, Convergence: 0.222263\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.40e-01\n",
      "Epoch: 190, Loss: 2469.55116, Residuals: -1.01187, Convergence:   inf\n",
      "Epoch: 191, Loss: 2455.28598, Residuals: -1.00928, Convergence: 0.005810\n",
      "Epoch: 192, Loss: 2443.69168, Residuals: -1.00599, Convergence: 0.004745\n",
      "Epoch: 193, Loss: 2433.79915, Residuals: -1.00272, Convergence: 0.004065\n",
      "Epoch: 194, Loss: 2425.29309, Residuals: -0.99959, Convergence: 0.003507\n",
      "Epoch: 195, Loss: 2417.93253, Residuals: -0.99666, Convergence: 0.003044\n",
      "Epoch: 196, Loss: 2411.52624, Residuals: -0.99395, Convergence: 0.002657\n",
      "Epoch: 197, Loss: 2405.91367, Residuals: -0.99144, Convergence: 0.002333\n",
      "Epoch: 198, Loss: 2400.96503, Residuals: -0.98913, Convergence: 0.002061\n",
      "Epoch: 199, Loss: 2396.57342, Residuals: -0.98699, Convergence: 0.001832\n",
      "Epoch: 200, Loss: 2392.64813, Residuals: -0.98502, Convergence: 0.001641\n",
      "Epoch: 201, Loss: 2389.11713, Residuals: -0.98321, Convergence: 0.001478\n",
      "Epoch: 202, Loss: 2385.92072, Residuals: -0.98153, Convergence: 0.001340\n",
      "Epoch: 203, Loss: 2383.00882, Residuals: -0.97998, Convergence: 0.001222\n",
      "Epoch: 204, Loss: 2380.34153, Residuals: -0.97855, Convergence: 0.001121\n",
      "Epoch: 205, Loss: 2377.88658, Residuals: -0.97723, Convergence: 0.001032\n",
      "Epoch: 206, Loss: 2375.61599, Residuals: -0.97601, Convergence: 0.000956\n",
      "Evidence 14669.786\n",
      "\n",
      "Epoch: 206, Evidence: 14669.78613, Convergence: 0.027563\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.36e-01\n",
      "Epoch: 206, Loss: 2475.53698, Residuals: -0.97601, Convergence:   inf\n",
      "Epoch: 207, Loss: 2469.08120, Residuals: -0.97288, Convergence: 0.002615\n",
      "Epoch: 208, Loss: 2463.78048, Residuals: -0.97004, Convergence: 0.002151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 209, Loss: 2459.30550, Residuals: -0.96758, Convergence: 0.001820\n",
      "Epoch: 210, Loss: 2455.46300, Residuals: -0.96546, Convergence: 0.001565\n",
      "Epoch: 211, Loss: 2452.11318, Residuals: -0.96363, Convergence: 0.001366\n",
      "Epoch: 212, Loss: 2449.15240, Residuals: -0.96206, Convergence: 0.001209\n",
      "Epoch: 213, Loss: 2446.50250, Residuals: -0.96071, Convergence: 0.001083\n",
      "Epoch: 214, Loss: 2444.10401, Residuals: -0.95955, Convergence: 0.000981\n",
      "Evidence 14751.281\n",
      "\n",
      "Epoch: 214, Evidence: 14751.28125, Convergence: 0.005525\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.62e-01\n",
      "Epoch: 214, Loss: 2477.34272, Residuals: -0.95955, Convergence:   inf\n",
      "Epoch: 215, Loss: 2473.45543, Residuals: -0.95710, Convergence: 0.001572\n",
      "Epoch: 216, Loss: 2470.23529, Residuals: -0.95507, Convergence: 0.001304\n",
      "Epoch: 217, Loss: 2467.47398, Residuals: -0.95339, Convergence: 0.001119\n",
      "Epoch: 218, Loss: 2465.05219, Residuals: -0.95200, Convergence: 0.000982\n",
      "Evidence 14778.898\n",
      "\n",
      "Epoch: 218, Evidence: 14778.89844, Convergence: 0.001869\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.11e-01\n",
      "Epoch: 218, Loss: 2478.40308, Residuals: -0.95200, Convergence:   inf\n",
      "Epoch: 219, Loss: 2475.49638, Residuals: -0.94995, Convergence: 0.001174\n",
      "Epoch: 220, Loss: 2473.05387, Residuals: -0.94829, Convergence: 0.000988\n",
      "Evidence 14791.022\n",
      "\n",
      "Epoch: 220, Evidence: 14791.02246, Convergence: 0.000820\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.75e-01\n",
      "Epoch: 220, Loss: 2479.18127, Residuals: -0.94829, Convergence:   inf\n",
      "Epoch: 221, Loss: 2474.52457, Residuals: -0.94529, Convergence: 0.001882\n",
      "Epoch: 222, Loss: 2470.98554, Residuals: -0.94314, Convergence: 0.001432\n",
      "Epoch: 223, Loss: 2468.10039, Residuals: -0.94172, Convergence: 0.001169\n",
      "Epoch: 224, Loss: 2465.64583, Residuals: -0.94089, Convergence: 0.000996\n",
      "Evidence 14809.134\n",
      "\n",
      "Epoch: 224, Evidence: 14809.13379, Convergence: 0.002042\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.44e-01\n",
      "Epoch: 224, Loss: 2479.37819, Residuals: -0.94089, Convergence:   inf\n",
      "Epoch: 225, Loss: 2476.26896, Residuals: -0.93811, Convergence: 0.001256\n",
      "Epoch: 226, Loss: 2473.81173, Residuals: -0.93663, Convergence: 0.000993\n",
      "Evidence 14820.233\n",
      "\n",
      "Epoch: 226, Evidence: 14820.23340, Convergence: 0.000749\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.22e-01\n",
      "Epoch: 226, Loss: 2479.63662, Residuals: -0.93663, Convergence:   inf\n",
      "Epoch: 227, Loss: 2475.08942, Residuals: -0.93244, Convergence: 0.001837\n",
      "Epoch: 228, Loss: 2471.87346, Residuals: -0.93333, Convergence: 0.001301\n",
      "Epoch: 229, Loss: 2469.19679, Residuals: -0.93433, Convergence: 0.001084\n",
      "Epoch: 230, Loss: 2466.84730, Residuals: -0.93695, Convergence: 0.000952\n",
      "Evidence 14836.279\n",
      "\n",
      "Epoch: 230, Evidence: 14836.27930, Convergence: 0.001830\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.08e-01\n",
      "Epoch: 230, Loss: 2479.01834, Residuals: -0.93695, Convergence:   inf\n",
      "Epoch: 231, Loss: 2477.41518, Residuals: -0.93444, Convergence: 0.000647\n",
      "Evidence 14842.905\n",
      "\n",
      "Epoch: 231, Evidence: 14842.90527, Convergence: 0.000446\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.86e-02\n",
      "Epoch: 231, Loss: 2480.02243, Residuals: -0.93444, Convergence:   inf\n",
      "Epoch: 232, Loss: 2526.95717, Residuals: -0.97534, Convergence: -0.018574\n",
      "Epoch: 232, Loss: 2477.77553, Residuals: -0.93082, Convergence: 0.000907\n",
      "Evidence 14847.188\n",
      "\n",
      "Epoch: 232, Evidence: 14847.18750, Convergence: 0.000735\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.17e-02\n",
      "Epoch: 232, Loss: 2479.45066, Residuals: -0.93082, Convergence:   inf\n",
      "Epoch: 233, Loss: 2484.78803, Residuals: -0.93223, Convergence: -0.002148\n",
      "Epoch: 233, Loss: 2479.48109, Residuals: -0.92937, Convergence: -0.000012\n",
      "Evidence 14848.790\n",
      "\n",
      "Epoch: 233, Evidence: 14848.79004, Convergence: 0.000843\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 382.01199, Residuals: -4.53352, Convergence:   inf\n",
      "Epoch: 1, Loss: 356.30168, Residuals: -4.41298, Convergence: 0.072159\n",
      "Epoch: 2, Loss: 335.24483, Residuals: -4.24854, Convergence: 0.062810\n",
      "Epoch: 3, Loss: 319.13992, Residuals: -4.08335, Convergence: 0.050463\n",
      "Epoch: 4, Loss: 306.85944, Residuals: -3.93781, Convergence: 0.040020\n",
      "Epoch: 5, Loss: 297.11937, Residuals: -3.80914, Convergence: 0.032782\n",
      "Epoch: 6, Loss: 289.21028, Residuals: -3.69701, Convergence: 0.027347\n",
      "Epoch: 7, Loss: 282.64270, Residuals: -3.60096, Convergence: 0.023236\n",
      "Epoch: 8, Loss: 277.05448, Residuals: -3.51899, Convergence: 0.020170\n",
      "Epoch: 9, Loss: 272.18972, Residuals: -3.44880, Convergence: 0.017873\n",
      "Epoch: 10, Loss: 267.86651, Residuals: -3.38827, Convergence: 0.016139\n",
      "Epoch: 11, Loss: 263.95321, Residuals: -3.33562, Convergence: 0.014826\n",
      "Epoch: 12, Loss: 260.35380, Residuals: -3.28930, Convergence: 0.013825\n",
      "Epoch: 13, Loss: 256.99839, Residuals: -3.24798, Convergence: 0.013056\n",
      "Epoch: 14, Loss: 253.83646, Residuals: -3.21047, Convergence: 0.012457\n",
      "Epoch: 15, Loss: 250.83323, Residuals: -3.17578, Convergence: 0.011973\n",
      "Epoch: 16, Loss: 247.96804, Residuals: -3.14320, Convergence: 0.011555\n",
      "Epoch: 17, Loss: 245.22798, Residuals: -3.11228, Convergence: 0.011174\n",
      "Epoch: 18, Loss: 242.59501, Residuals: -3.08265, Convergence: 0.010853\n",
      "Epoch: 19, Loss: 240.03978, Residuals: -3.05382, Convergence: 0.010645\n",
      "Epoch: 20, Loss: 237.52684, Residuals: -3.02525, Convergence: 0.010580\n",
      "Epoch: 21, Loss: 235.02356, Residuals: -2.99642, Convergence: 0.010651\n",
      "Epoch: 22, Loss: 232.50575, Residuals: -2.96698, Convergence: 0.010829\n",
      "Epoch: 23, Loss: 229.95145, Residuals: -2.93666, Convergence: 0.011108\n",
      "Epoch: 24, Loss: 227.32384, Residuals: -2.90506, Convergence: 0.011559\n",
      "Epoch: 25, Loss: 224.56720, Residuals: -2.87150, Convergence: 0.012275\n",
      "Epoch: 26, Loss: 221.65393, Residuals: -2.83555, Convergence: 0.013143\n",
      "Epoch: 27, Loss: 218.67262, Residuals: -2.79806, Convergence: 0.013634\n",
      "Epoch: 28, Loss: 215.75304, Residuals: -2.76044, Convergence: 0.013532\n",
      "Epoch: 29, Loss: 212.93586, Residuals: -2.72327, Convergence: 0.013230\n",
      "Epoch: 30, Loss: 210.21220, Residuals: -2.68654, Convergence: 0.012957\n",
      "Epoch: 31, Loss: 207.56712, Residuals: -2.65015, Convergence: 0.012743\n",
      "Epoch: 32, Loss: 204.98968, Residuals: -2.61402, Convergence: 0.012574\n",
      "Epoch: 33, Loss: 202.47334, Residuals: -2.57812, Convergence: 0.012428\n",
      "Epoch: 34, Loss: 200.01489, Residuals: -2.54243, Convergence: 0.012291\n",
      "Epoch: 35, Loss: 197.61323, Residuals: -2.50695, Convergence: 0.012153\n",
      "Epoch: 36, Loss: 195.26856, Residuals: -2.47170, Convergence: 0.012007\n",
      "Epoch: 37, Loss: 192.98183, Residuals: -2.43669, Convergence: 0.011849\n",
      "Epoch: 38, Loss: 190.75431, Residuals: -2.40193, Convergence: 0.011677\n",
      "Epoch: 39, Loss: 188.58730, Residuals: -2.36744, Convergence: 0.011491\n",
      "Epoch: 40, Loss: 186.48199, Residuals: -2.33322, Convergence: 0.011290\n",
      "Epoch: 41, Loss: 184.43929, Residuals: -2.29930, Convergence: 0.011075\n",
      "Epoch: 42, Loss: 182.45984, Residuals: -2.26569, Convergence: 0.010849\n",
      "Epoch: 43, Loss: 180.54395, Residuals: -2.23240, Convergence: 0.010612\n",
      "Epoch: 44, Loss: 178.69163, Residuals: -2.19946, Convergence: 0.010366\n",
      "Epoch: 45, Loss: 176.90270, Residuals: -2.16688, Convergence: 0.010113\n",
      "Epoch: 46, Loss: 175.17689, Residuals: -2.13467, Convergence: 0.009852\n",
      "Epoch: 47, Loss: 173.51394, Residuals: -2.10286, Convergence: 0.009584\n",
      "Epoch: 48, Loss: 171.91383, Residuals: -2.07148, Convergence: 0.009308\n",
      "Epoch: 49, Loss: 170.37675, Residuals: -2.04055, Convergence: 0.009022\n",
      "Epoch: 50, Loss: 168.90309, Residuals: -2.01012, Convergence: 0.008725\n",
      "Epoch: 51, Loss: 167.49322, Residuals: -1.98021, Convergence: 0.008417\n",
      "Epoch: 52, Loss: 166.14731, Residuals: -1.95088, Convergence: 0.008101\n",
      "Epoch: 53, Loss: 164.86509, Residuals: -1.92215, Convergence: 0.007777\n",
      "Epoch: 54, Loss: 163.64581, Residuals: -1.89407, Convergence: 0.007451\n",
      "Epoch: 55, Loss: 162.48826, Residuals: -1.86667, Convergence: 0.007124\n",
      "Epoch: 56, Loss: 161.39077, Residuals: -1.83997, Convergence: 0.006800\n",
      "Epoch: 57, Loss: 160.35137, Residuals: -1.81399, Convergence: 0.006482\n",
      "Epoch: 58, Loss: 159.36784, Residuals: -1.78875, Convergence: 0.006171\n",
      "Epoch: 59, Loss: 158.43776, Residuals: -1.76427, Convergence: 0.005870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60, Loss: 157.55857, Residuals: -1.74054, Convergence: 0.005580\n",
      "Epoch: 61, Loss: 156.72756, Residuals: -1.71759, Convergence: 0.005302\n",
      "Epoch: 62, Loss: 155.94194, Residuals: -1.69539, Convergence: 0.005038\n",
      "Epoch: 63, Loss: 155.19884, Residuals: -1.67394, Convergence: 0.004788\n",
      "Epoch: 64, Loss: 154.49538, Residuals: -1.65323, Convergence: 0.004553\n",
      "Epoch: 65, Loss: 153.82875, Residuals: -1.63324, Convergence: 0.004334\n",
      "Epoch: 66, Loss: 153.19621, Residuals: -1.61394, Convergence: 0.004129\n",
      "Epoch: 67, Loss: 152.59517, Residuals: -1.59532, Convergence: 0.003939\n",
      "Epoch: 68, Loss: 152.02319, Residuals: -1.57736, Convergence: 0.003762\n",
      "Epoch: 69, Loss: 151.47797, Residuals: -1.56001, Convergence: 0.003599\n",
      "Epoch: 70, Loss: 150.95737, Residuals: -1.54327, Convergence: 0.003449\n",
      "Epoch: 71, Loss: 150.45945, Residuals: -1.52710, Convergence: 0.003309\n",
      "Epoch: 72, Loss: 149.98245, Residuals: -1.51147, Convergence: 0.003180\n",
      "Epoch: 73, Loss: 149.52484, Residuals: -1.49637, Convergence: 0.003060\n",
      "Epoch: 74, Loss: 149.08535, Residuals: -1.48178, Convergence: 0.002948\n",
      "Epoch: 75, Loss: 148.66293, Residuals: -1.46766, Convergence: 0.002841\n",
      "Epoch: 76, Loss: 148.25674, Residuals: -1.45400, Convergence: 0.002740\n",
      "Epoch: 77, Loss: 147.86616, Residuals: -1.44079, Convergence: 0.002641\n",
      "Epoch: 78, Loss: 147.49068, Residuals: -1.42802, Convergence: 0.002546\n",
      "Epoch: 79, Loss: 147.12989, Residuals: -1.41567, Convergence: 0.002452\n",
      "Epoch: 80, Loss: 146.78349, Residuals: -1.40374, Convergence: 0.002360\n",
      "Epoch: 81, Loss: 146.45117, Residuals: -1.39222, Convergence: 0.002269\n",
      "Epoch: 82, Loss: 146.13266, Residuals: -1.38110, Convergence: 0.002180\n",
      "Epoch: 83, Loss: 145.82768, Residuals: -1.37037, Convergence: 0.002091\n",
      "Epoch: 84, Loss: 145.53598, Residuals: -1.36003, Convergence: 0.002004\n",
      "Epoch: 85, Loss: 145.25725, Residuals: -1.35007, Convergence: 0.001919\n",
      "Epoch: 86, Loss: 144.99120, Residuals: -1.34048, Convergence: 0.001835\n",
      "Epoch: 87, Loss: 144.73752, Residuals: -1.33126, Convergence: 0.001753\n",
      "Epoch: 88, Loss: 144.49591, Residuals: -1.32239, Convergence: 0.001672\n",
      "Epoch: 89, Loss: 144.26605, Residuals: -1.31388, Convergence: 0.001593\n",
      "Epoch: 90, Loss: 144.04760, Residuals: -1.30570, Convergence: 0.001517\n",
      "Epoch: 91, Loss: 143.84024, Residuals: -1.29786, Convergence: 0.001442\n",
      "Epoch: 92, Loss: 143.64364, Residuals: -1.29036, Convergence: 0.001369\n",
      "Epoch: 93, Loss: 143.45746, Residuals: -1.28317, Convergence: 0.001298\n",
      "Epoch: 94, Loss: 143.28134, Residuals: -1.27631, Convergence: 0.001229\n",
      "Epoch: 95, Loss: 143.11488, Residuals: -1.26975, Convergence: 0.001163\n",
      "Epoch: 96, Loss: 142.95764, Residuals: -1.26351, Convergence: 0.001100\n",
      "Epoch: 97, Loss: 142.80913, Residuals: -1.25758, Convergence: 0.001040\n",
      "Epoch: 98, Loss: 142.66874, Residuals: -1.25194, Convergence: 0.000984\n",
      "Evidence -183.560\n",
      "\n",
      "Epoch: 98, Evidence: -183.56033, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.24e-01\n",
      "Epoch: 98, Loss: 1381.34572, Residuals: -1.25194, Convergence:   inf\n",
      "Epoch: 99, Loss: 1319.91078, Residuals: -1.28138, Convergence: 0.046545\n",
      "Epoch: 100, Loss: 1273.09895, Residuals: -1.30483, Convergence: 0.036770\n",
      "Epoch: 101, Loss: 1237.66352, Residuals: -1.32180, Convergence: 0.028631\n",
      "Epoch: 102, Loss: 1210.06818, Residuals: -1.33356, Convergence: 0.022805\n",
      "Epoch: 103, Loss: 1187.73531, Residuals: -1.34198, Convergence: 0.018803\n",
      "Epoch: 104, Loss: 1169.17073, Residuals: -1.34815, Convergence: 0.015878\n",
      "Epoch: 105, Loss: 1153.47324, Residuals: -1.35259, Convergence: 0.013609\n",
      "Epoch: 106, Loss: 1140.03360, Residuals: -1.35559, Convergence: 0.011789\n",
      "Epoch: 107, Loss: 1128.40171, Residuals: -1.35733, Convergence: 0.010308\n",
      "Epoch: 108, Loss: 1118.22778, Residuals: -1.35795, Convergence: 0.009098\n",
      "Epoch: 109, Loss: 1109.23174, Residuals: -1.35758, Convergence: 0.008110\n",
      "Epoch: 110, Loss: 1101.18343, Residuals: -1.35632, Convergence: 0.007309\n",
      "Epoch: 111, Loss: 1093.89099, Residuals: -1.35424, Convergence: 0.006667\n",
      "Epoch: 112, Loss: 1087.19095, Residuals: -1.35142, Convergence: 0.006163\n",
      "Epoch: 113, Loss: 1080.94239, Residuals: -1.34791, Convergence: 0.005781\n",
      "Epoch: 114, Loss: 1075.02249, Residuals: -1.34374, Convergence: 0.005507\n",
      "Epoch: 115, Loss: 1069.32377, Residuals: -1.33894, Convergence: 0.005329\n",
      "Epoch: 116, Loss: 1063.75344, Residuals: -1.33354, Convergence: 0.005236\n",
      "Epoch: 117, Loss: 1058.23386, Residuals: -1.32755, Convergence: 0.005216\n",
      "Epoch: 118, Loss: 1052.70814, Residuals: -1.32100, Convergence: 0.005249\n",
      "Epoch: 119, Loss: 1047.14879, Residuals: -1.31394, Convergence: 0.005309\n",
      "Epoch: 120, Loss: 1041.57072, Residuals: -1.30646, Convergence: 0.005355\n",
      "Epoch: 121, Loss: 1036.03588, Residuals: -1.29864, Convergence: 0.005342\n",
      "Epoch: 122, Loss: 1030.63810, Residuals: -1.29058, Convergence: 0.005237\n",
      "Epoch: 123, Loss: 1025.46886, Residuals: -1.28237, Convergence: 0.005041\n",
      "Epoch: 124, Loss: 1020.59085, Residuals: -1.27407, Convergence: 0.004780\n",
      "Epoch: 125, Loss: 1016.02863, Residuals: -1.26576, Convergence: 0.004490\n",
      "Epoch: 126, Loss: 1011.77657, Residuals: -1.25749, Convergence: 0.004203\n",
      "Epoch: 127, Loss: 1007.81374, Residuals: -1.24931, Convergence: 0.003932\n",
      "Epoch: 128, Loss: 1004.11253, Residuals: -1.24126, Convergence: 0.003686\n",
      "Epoch: 129, Loss: 1000.64492, Residuals: -1.23336, Convergence: 0.003465\n",
      "Epoch: 130, Loss: 997.38601, Residuals: -1.22565, Convergence: 0.003267\n",
      "Epoch: 131, Loss: 994.31309, Residuals: -1.21813, Convergence: 0.003091\n",
      "Epoch: 132, Loss: 991.40732, Residuals: -1.21084, Convergence: 0.002931\n",
      "Epoch: 133, Loss: 988.65319, Residuals: -1.20377, Convergence: 0.002786\n",
      "Epoch: 134, Loss: 986.03714, Residuals: -1.19693, Convergence: 0.002653\n",
      "Epoch: 135, Loss: 983.54803, Residuals: -1.19034, Convergence: 0.002531\n",
      "Epoch: 136, Loss: 981.17646, Residuals: -1.18399, Convergence: 0.002417\n",
      "Epoch: 137, Loss: 978.91491, Residuals: -1.17788, Convergence: 0.002310\n",
      "Epoch: 138, Loss: 976.75593, Residuals: -1.17202, Convergence: 0.002210\n",
      "Epoch: 139, Loss: 974.69407, Residuals: -1.16641, Convergence: 0.002115\n",
      "Epoch: 140, Loss: 972.72368, Residuals: -1.16104, Convergence: 0.002026\n",
      "Epoch: 141, Loss: 970.84026, Residuals: -1.15590, Convergence: 0.001940\n",
      "Epoch: 142, Loss: 969.03895, Residuals: -1.15100, Convergence: 0.001859\n",
      "Epoch: 143, Loss: 967.31561, Residuals: -1.14633, Convergence: 0.001782\n",
      "Epoch: 144, Loss: 965.66600, Residuals: -1.14187, Convergence: 0.001708\n",
      "Epoch: 145, Loss: 964.08634, Residuals: -1.13763, Convergence: 0.001639\n",
      "Epoch: 146, Loss: 962.57233, Residuals: -1.13359, Convergence: 0.001573\n",
      "Epoch: 147, Loss: 961.12031, Residuals: -1.12974, Convergence: 0.001511\n",
      "Epoch: 148, Loss: 959.72676, Residuals: -1.12608, Convergence: 0.001452\n",
      "Epoch: 149, Loss: 958.38801, Residuals: -1.12259, Convergence: 0.001397\n",
      "Epoch: 150, Loss: 957.10057, Residuals: -1.11927, Convergence: 0.001345\n",
      "Epoch: 151, Loss: 955.86112, Residuals: -1.11611, Convergence: 0.001297\n",
      "Epoch: 152, Loss: 954.66669, Residuals: -1.11310, Convergence: 0.001251\n",
      "Epoch: 153, Loss: 953.51419, Residuals: -1.11022, Convergence: 0.001209\n",
      "Epoch: 154, Loss: 952.40102, Residuals: -1.10747, Convergence: 0.001169\n",
      "Epoch: 155, Loss: 951.32414, Residuals: -1.10484, Convergence: 0.001132\n",
      "Epoch: 156, Loss: 950.28125, Residuals: -1.10233, Convergence: 0.001097\n",
      "Epoch: 157, Loss: 949.26945, Residuals: -1.09992, Convergence: 0.001066\n",
      "Epoch: 158, Loss: 948.28669, Residuals: -1.09761, Convergence: 0.001036\n",
      "Epoch: 159, Loss: 947.33006, Residuals: -1.09538, Convergence: 0.001010\n",
      "Epoch: 160, Loss: 946.39718, Residuals: -1.09324, Convergence: 0.000986\n",
      "Evidence 11109.981\n",
      "\n",
      "Epoch: 160, Evidence: 11109.98145, Convergence: 1.016522\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.77e-01\n",
      "Epoch: 160, Loss: 2346.77806, Residuals: -1.09324, Convergence:   inf\n",
      "Epoch: 161, Loss: 2307.40557, Residuals: -1.10273, Convergence: 0.017064\n",
      "Epoch: 162, Loss: 2279.62810, Residuals: -1.10162, Convergence: 0.012185\n",
      "Epoch: 163, Loss: 2256.40262, Residuals: -1.09944, Convergence: 0.010293\n",
      "Epoch: 164, Loss: 2236.74021, Residuals: -1.09686, Convergence: 0.008791\n",
      "Epoch: 165, Loss: 2219.93098, Residuals: -1.09401, Convergence: 0.007572\n",
      "Epoch: 166, Loss: 2205.43213, Residuals: -1.09096, Convergence: 0.006574\n",
      "Epoch: 167, Loss: 2192.81419, Residuals: -1.08775, Convergence: 0.005754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 168, Loss: 2181.72976, Residuals: -1.08441, Convergence: 0.005081\n",
      "Epoch: 169, Loss: 2171.89597, Residuals: -1.08097, Convergence: 0.004528\n",
      "Epoch: 170, Loss: 2163.07909, Residuals: -1.07744, Convergence: 0.004076\n",
      "Epoch: 171, Loss: 2155.08866, Residuals: -1.07382, Convergence: 0.003708\n",
      "Epoch: 172, Loss: 2147.77638, Residuals: -1.07012, Convergence: 0.003405\n",
      "Epoch: 173, Loss: 2141.03452, Residuals: -1.06633, Convergence: 0.003149\n",
      "Epoch: 174, Loss: 2134.79778, Residuals: -1.06250, Convergence: 0.002921\n",
      "Epoch: 175, Loss: 2129.02789, Residuals: -1.05865, Convergence: 0.002710\n",
      "Epoch: 176, Loss: 2123.69911, Residuals: -1.05483, Convergence: 0.002509\n",
      "Epoch: 177, Loss: 2118.78740, Residuals: -1.05108, Convergence: 0.002318\n",
      "Epoch: 178, Loss: 2114.26288, Residuals: -1.04743, Convergence: 0.002140\n",
      "Epoch: 179, Loss: 2110.09355, Residuals: -1.04391, Convergence: 0.001976\n",
      "Epoch: 180, Loss: 2106.24276, Residuals: -1.04052, Convergence: 0.001828\n",
      "Epoch: 181, Loss: 2102.67565, Residuals: -1.03728, Convergence: 0.001696\n",
      "Epoch: 182, Loss: 2099.35873, Residuals: -1.03419, Convergence: 0.001580\n",
      "Epoch: 183, Loss: 2096.26114, Residuals: -1.03124, Convergence: 0.001478\n",
      "Epoch: 184, Loss: 2093.35551, Residuals: -1.02842, Convergence: 0.001388\n",
      "Epoch: 185, Loss: 2090.61927, Residuals: -1.02575, Convergence: 0.001309\n",
      "Epoch: 186, Loss: 2088.03224, Residuals: -1.02320, Convergence: 0.001239\n",
      "Epoch: 187, Loss: 2085.57848, Residuals: -1.02078, Convergence: 0.001177\n",
      "Epoch: 188, Loss: 2083.24447, Residuals: -1.01848, Convergence: 0.001120\n",
      "Epoch: 189, Loss: 2081.02099, Residuals: -1.01629, Convergence: 0.001068\n",
      "Epoch: 190, Loss: 2078.89953, Residuals: -1.01422, Convergence: 0.001020\n",
      "Epoch: 191, Loss: 2076.87248, Residuals: -1.01225, Convergence: 0.000976\n",
      "Evidence 14216.053\n",
      "\n",
      "Epoch: 191, Evidence: 14216.05273, Convergence: 0.218490\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.39e-01\n",
      "Epoch: 191, Loss: 2469.30541, Residuals: -1.01225, Convergence:   inf\n",
      "Epoch: 192, Loss: 2455.11681, Residuals: -1.00879, Convergence: 0.005779\n",
      "Epoch: 193, Loss: 2443.52009, Residuals: -1.00467, Convergence: 0.004746\n",
      "Epoch: 194, Loss: 2433.49756, Residuals: -1.00066, Convergence: 0.004119\n",
      "Epoch: 195, Loss: 2424.75644, Residuals: -0.99689, Convergence: 0.003605\n",
      "Epoch: 196, Loss: 2417.08620, Residuals: -0.99337, Convergence: 0.003173\n",
      "Epoch: 197, Loss: 2410.32157, Residuals: -0.99011, Convergence: 0.002807\n",
      "Epoch: 198, Loss: 2404.32870, Residuals: -0.98709, Convergence: 0.002493\n",
      "Epoch: 199, Loss: 2398.99443, Residuals: -0.98430, Convergence: 0.002224\n",
      "Epoch: 200, Loss: 2394.22355, Residuals: -0.98171, Convergence: 0.001993\n",
      "Epoch: 201, Loss: 2389.93557, Residuals: -0.97931, Convergence: 0.001794\n",
      "Epoch: 202, Loss: 2386.06312, Residuals: -0.97709, Convergence: 0.001623\n",
      "Epoch: 203, Loss: 2382.54901, Residuals: -0.97502, Convergence: 0.001475\n",
      "Epoch: 204, Loss: 2379.34524, Residuals: -0.97310, Convergence: 0.001346\n",
      "Epoch: 205, Loss: 2376.41071, Residuals: -0.97131, Convergence: 0.001235\n",
      "Epoch: 206, Loss: 2373.71127, Residuals: -0.96966, Convergence: 0.001137\n",
      "Epoch: 207, Loss: 2371.21976, Residuals: -0.96811, Convergence: 0.001051\n",
      "Epoch: 208, Loss: 2368.91113, Residuals: -0.96668, Convergence: 0.000975\n",
      "Evidence 14625.269\n",
      "\n",
      "Epoch: 208, Evidence: 14625.26855, Convergence: 0.027980\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.36e-01\n",
      "Epoch: 208, Loss: 2474.41755, Residuals: -0.96668, Convergence:   inf\n",
      "Epoch: 209, Loss: 2467.47738, Residuals: -0.96285, Convergence: 0.002813\n",
      "Epoch: 210, Loss: 2461.74233, Residuals: -0.95949, Convergence: 0.002330\n",
      "Epoch: 211, Loss: 2456.86143, Residuals: -0.95662, Convergence: 0.001987\n",
      "Epoch: 212, Loss: 2452.64516, Residuals: -0.95414, Convergence: 0.001719\n",
      "Epoch: 213, Loss: 2448.95543, Residuals: -0.95200, Convergence: 0.001507\n",
      "Epoch: 214, Loss: 2445.68866, Residuals: -0.95013, Convergence: 0.001336\n",
      "Epoch: 215, Loss: 2442.76602, Residuals: -0.94849, Convergence: 0.001196\n",
      "Epoch: 216, Loss: 2440.12720, Residuals: -0.94706, Convergence: 0.001081\n",
      "Epoch: 217, Loss: 2437.72412, Residuals: -0.94581, Convergence: 0.000986\n",
      "Evidence 14716.068\n",
      "\n",
      "Epoch: 217, Evidence: 14716.06836, Convergence: 0.006170\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.64e-01\n",
      "Epoch: 217, Loss: 2476.10270, Residuals: -0.94581, Convergence:   inf\n",
      "Epoch: 218, Loss: 2472.02722, Residuals: -0.94294, Convergence: 0.001649\n",
      "Epoch: 219, Loss: 2468.62428, Residuals: -0.94068, Convergence: 0.001378\n",
      "Epoch: 220, Loss: 2465.69194, Residuals: -0.93883, Convergence: 0.001189\n",
      "Epoch: 221, Loss: 2463.11618, Residuals: -0.93730, Convergence: 0.001046\n",
      "Epoch: 222, Loss: 2460.81903, Residuals: -0.93603, Convergence: 0.000933\n",
      "Evidence 14748.286\n",
      "\n",
      "Epoch: 222, Evidence: 14748.28613, Convergence: 0.002185\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.12e-01\n",
      "Epoch: 222, Loss: 2477.11999, Residuals: -0.93603, Convergence:   inf\n",
      "Epoch: 223, Loss: 2474.21665, Residuals: -0.93393, Convergence: 0.001173\n",
      "Epoch: 224, Loss: 2471.76475, Residuals: -0.93231, Convergence: 0.000992\n",
      "Evidence 14761.077\n",
      "\n",
      "Epoch: 224, Evidence: 14761.07715, Convergence: 0.000867\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.76e-01\n",
      "Epoch: 224, Loss: 2477.88910, Residuals: -0.93231, Convergence:   inf\n",
      "Epoch: 225, Loss: 2473.31077, Residuals: -0.92956, Convergence: 0.001851\n",
      "Epoch: 226, Loss: 2469.76316, Residuals: -0.92759, Convergence: 0.001436\n",
      "Epoch: 227, Loss: 2466.85907, Residuals: -0.92615, Convergence: 0.001177\n",
      "Epoch: 228, Loss: 2464.38324, Residuals: -0.92525, Convergence: 0.001005\n",
      "Epoch: 229, Loss: 2462.20637, Residuals: -0.92471, Convergence: 0.000884\n",
      "Evidence 14781.392\n",
      "\n",
      "Epoch: 229, Evidence: 14781.39160, Convergence: 0.002240\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.45e-01\n",
      "Epoch: 229, Loss: 2478.01440, Residuals: -0.92471, Convergence:   inf\n",
      "Epoch: 230, Loss: 2475.14084, Residuals: -0.92229, Convergence: 0.001161\n",
      "Epoch: 231, Loss: 2472.83097, Residuals: -0.92103, Convergence: 0.000934\n",
      "Evidence 14792.591\n",
      "\n",
      "Epoch: 231, Evidence: 14792.59082, Convergence: 0.000757\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.23e-01\n",
      "Epoch: 231, Loss: 2478.28950, Residuals: -0.92103, Convergence:   inf\n",
      "Epoch: 232, Loss: 2474.04958, Residuals: -0.91765, Convergence: 0.001714\n",
      "Epoch: 233, Loss: 2470.93039, Residuals: -0.91868, Convergence: 0.001262\n",
      "Epoch: 234, Loss: 2468.31548, Residuals: -0.91898, Convergence: 0.001059\n",
      "Epoch: 235, Loss: 2466.07174, Residuals: -0.92162, Convergence: 0.000910\n",
      "Evidence 14807.826\n",
      "\n",
      "Epoch: 235, Evidence: 14807.82617, Convergence: 0.001785\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.09e-01\n",
      "Epoch: 235, Loss: 2477.73348, Residuals: -0.92162, Convergence:   inf\n",
      "Epoch: 236, Loss: 2476.06981, Residuals: -0.91923, Convergence: 0.000672\n",
      "Evidence 14813.992\n",
      "\n",
      "Epoch: 236, Evidence: 14813.99219, Convergence: 0.000416\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.89e-02\n",
      "Epoch: 236, Loss: 2478.77131, Residuals: -0.91923, Convergence:   inf\n",
      "Epoch: 237, Loss: 2525.74735, Residuals: -0.95692, Convergence: -0.018599\n",
      "Epoch: 237, Loss: 2476.45810, Residuals: -0.91741, Convergence: 0.000934\n",
      "Evidence 14818.306\n",
      "\n",
      "Epoch: 237, Evidence: 14818.30566, Convergence: 0.000707\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.17e-02\n",
      "Epoch: 237, Loss: 2478.22857, Residuals: -0.91741, Convergence:   inf\n",
      "Epoch: 238, Loss: 2481.42297, Residuals: -0.91766, Convergence: -0.001287\n",
      "Epoch: 238, Loss: 2477.84478, Residuals: -0.91534, Convergence: 0.000155\n",
      "Evidence 14820.159\n",
      "\n",
      "Epoch: 238, Evidence: 14820.15918, Convergence: 0.000832\n",
      "Total samples: 181, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 382.47159, Residuals: -4.54564, Convergence:   inf\n",
      "Epoch: 1, Loss: 356.67628, Residuals: -4.42441, Convergence: 0.072321\n",
      "Epoch: 2, Loss: 335.59283, Residuals: -4.25963, Convergence: 0.062824\n",
      "Epoch: 3, Loss: 319.45997, Residuals: -4.09411, Convergence: 0.050500\n",
      "Epoch: 4, Loss: 307.13968, Residuals: -3.94806, Convergence: 0.040113\n",
      "Epoch: 5, Loss: 297.35636, Residuals: -3.81880, Convergence: 0.032901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Loss: 289.40500, Residuals: -3.70610, Convergence: 0.027475\n",
      "Epoch: 7, Loss: 282.79753, Residuals: -3.60949, Convergence: 0.023365\n",
      "Epoch: 8, Loss: 277.17171, Residuals: -3.52694, Convergence: 0.020297\n",
      "Epoch: 9, Loss: 272.27136, Residuals: -3.45610, Convergence: 0.017998\n",
      "Epoch: 10, Loss: 267.91404, Residuals: -3.39485, Convergence: 0.016264\n",
      "Epoch: 11, Loss: 263.96756, Residuals: -3.34138, Convergence: 0.014951\n",
      "Epoch: 12, Loss: 260.33572, Residuals: -3.29417, Convergence: 0.013951\n",
      "Epoch: 13, Loss: 256.94913, Residuals: -3.25190, Convergence: 0.013180\n",
      "Epoch: 14, Loss: 253.75875, Residuals: -3.21342, Convergence: 0.012573\n",
      "Epoch: 15, Loss: 250.73202, Residuals: -3.17774, Convergence: 0.012072\n",
      "Epoch: 16, Loss: 247.85046, Residuals: -3.14419, Convergence: 0.011626\n",
      "Epoch: 17, Loss: 245.10169, Residuals: -3.11238, Convergence: 0.011215\n",
      "Epoch: 18, Loss: 242.46623, Residuals: -3.08194, Convergence: 0.010869\n",
      "Epoch: 19, Loss: 239.91298, Residuals: -3.05240, Convergence: 0.010642\n",
      "Epoch: 20, Loss: 237.40604, Residuals: -3.02322, Convergence: 0.010560\n",
      "Epoch: 21, Loss: 234.91436, Residuals: -2.99391, Convergence: 0.010607\n",
      "Epoch: 22, Loss: 232.41656, Residuals: -2.96417, Convergence: 0.010747\n",
      "Epoch: 23, Loss: 229.89192, Residuals: -2.93377, Convergence: 0.010982\n",
      "Epoch: 24, Loss: 227.30271, Residuals: -2.90230, Convergence: 0.011391\n",
      "Epoch: 25, Loss: 224.59019, Residuals: -2.86907, Convergence: 0.012078\n",
      "Epoch: 26, Loss: 221.71409, Residuals: -2.83351, Convergence: 0.012972\n",
      "Epoch: 27, Loss: 218.74503, Residuals: -2.79628, Convergence: 0.013573\n",
      "Epoch: 28, Loss: 215.81953, Residuals: -2.75885, Convergence: 0.013555\n",
      "Epoch: 29, Loss: 212.99052, Residuals: -2.72192, Convergence: 0.013282\n",
      "Epoch: 30, Loss: 210.25170, Residuals: -2.68550, Convergence: 0.013026\n",
      "Epoch: 31, Loss: 207.58768, Residuals: -2.64946, Convergence: 0.012833\n",
      "Epoch: 32, Loss: 204.98708, Residuals: -2.61369, Convergence: 0.012687\n",
      "Epoch: 33, Loss: 202.44346, Residuals: -2.57810, Convergence: 0.012565\n",
      "Epoch: 34, Loss: 199.95411, Residuals: -2.54265, Convergence: 0.012450\n",
      "Epoch: 35, Loss: 197.51876, Residuals: -2.50733, Convergence: 0.012330\n",
      "Epoch: 36, Loss: 195.13852, Residuals: -2.47214, Convergence: 0.012198\n",
      "Epoch: 37, Loss: 192.81522, Residuals: -2.43708, Convergence: 0.012049\n",
      "Epoch: 38, Loss: 190.55092, Residuals: -2.40218, Convergence: 0.011883\n",
      "Epoch: 39, Loss: 188.34764, Residuals: -2.36746, Convergence: 0.011698\n",
      "Epoch: 40, Loss: 186.20723, Residuals: -2.33295, Convergence: 0.011495\n",
      "Epoch: 41, Loss: 184.13123, Residuals: -2.29869, Convergence: 0.011275\n",
      "Epoch: 42, Loss: 182.12086, Residuals: -2.26471, Convergence: 0.011039\n",
      "Epoch: 43, Loss: 180.17704, Residuals: -2.23104, Convergence: 0.010788\n",
      "Epoch: 44, Loss: 178.30031, Residuals: -2.19773, Convergence: 0.010526\n",
      "Epoch: 45, Loss: 176.49097, Residuals: -2.16480, Convergence: 0.010252\n",
      "Epoch: 46, Loss: 174.74906, Residuals: -2.13230, Convergence: 0.009968\n",
      "Epoch: 47, Loss: 173.07445, Residuals: -2.10025, Convergence: 0.009676\n",
      "Epoch: 48, Loss: 171.46691, Residuals: -2.06869, Convergence: 0.009375\n",
      "Epoch: 49, Loss: 169.92616, Residuals: -2.03765, Convergence: 0.009067\n",
      "Epoch: 50, Loss: 168.45186, Residuals: -2.00718, Convergence: 0.008752\n",
      "Epoch: 51, Loss: 167.04352, Residuals: -1.97730, Convergence: 0.008431\n",
      "Epoch: 52, Loss: 165.70042, Residuals: -1.94805, Convergence: 0.008106\n",
      "Epoch: 53, Loss: 164.42145, Residuals: -1.91946, Convergence: 0.007779\n",
      "Epoch: 54, Loss: 163.20507, Residuals: -1.89155, Convergence: 0.007453\n",
      "Epoch: 55, Loss: 162.04927, Residuals: -1.86434, Convergence: 0.007132\n",
      "Epoch: 56, Loss: 160.95161, Residuals: -1.83784, Convergence: 0.006820\n",
      "Epoch: 57, Loss: 159.90936, Residuals: -1.81206, Convergence: 0.006518\n",
      "Epoch: 58, Loss: 158.91953, Residuals: -1.78700, Convergence: 0.006228\n",
      "Epoch: 59, Loss: 157.97902, Residuals: -1.76265, Convergence: 0.005953\n",
      "Epoch: 60, Loss: 157.08473, Residuals: -1.73900, Convergence: 0.005693\n",
      "Epoch: 61, Loss: 156.23353, Residuals: -1.71604, Convergence: 0.005448\n",
      "Epoch: 62, Loss: 155.42238, Residuals: -1.69376, Convergence: 0.005219\n",
      "Epoch: 63, Loss: 154.64831, Residuals: -1.67213, Convergence: 0.005005\n",
      "Epoch: 64, Loss: 153.90851, Residuals: -1.65115, Convergence: 0.004807\n",
      "Epoch: 65, Loss: 153.20035, Residuals: -1.63078, Convergence: 0.004622\n",
      "Epoch: 66, Loss: 152.52148, Residuals: -1.61101, Convergence: 0.004451\n",
      "Epoch: 67, Loss: 151.86986, Residuals: -1.59182, Convergence: 0.004291\n",
      "Epoch: 68, Loss: 151.24380, Residuals: -1.57319, Convergence: 0.004139\n",
      "Epoch: 69, Loss: 150.64193, Residuals: -1.55510, Convergence: 0.003995\n",
      "Epoch: 70, Loss: 150.06317, Residuals: -1.53754, Convergence: 0.003857\n",
      "Epoch: 71, Loss: 149.50666, Residuals: -1.52051, Convergence: 0.003722\n",
      "Epoch: 72, Loss: 148.97174, Residuals: -1.50400, Convergence: 0.003591\n",
      "Epoch: 73, Loss: 148.45781, Residuals: -1.48801, Convergence: 0.003462\n",
      "Epoch: 74, Loss: 147.96438, Residuals: -1.47252, Convergence: 0.003335\n",
      "Epoch: 75, Loss: 147.49094, Residuals: -1.45755, Convergence: 0.003210\n",
      "Epoch: 76, Loss: 147.03703, Residuals: -1.44307, Convergence: 0.003087\n",
      "Epoch: 77, Loss: 146.60216, Residuals: -1.42909, Convergence: 0.002966\n",
      "Epoch: 78, Loss: 146.18580, Residuals: -1.41561, Convergence: 0.002848\n",
      "Epoch: 79, Loss: 145.78745, Residuals: -1.40261, Convergence: 0.002732\n",
      "Epoch: 80, Loss: 145.40655, Residuals: -1.39008, Convergence: 0.002620\n",
      "Epoch: 81, Loss: 145.04255, Residuals: -1.37802, Convergence: 0.002510\n",
      "Epoch: 82, Loss: 144.69488, Residuals: -1.36642, Convergence: 0.002403\n",
      "Epoch: 83, Loss: 144.36296, Residuals: -1.35526, Convergence: 0.002299\n",
      "Epoch: 84, Loss: 144.04624, Residuals: -1.34454, Convergence: 0.002199\n",
      "Epoch: 85, Loss: 143.74414, Residuals: -1.33424, Convergence: 0.002102\n",
      "Epoch: 86, Loss: 143.45614, Residuals: -1.32435, Convergence: 0.002008\n",
      "Epoch: 87, Loss: 143.18168, Residuals: -1.31486, Convergence: 0.001917\n",
      "Epoch: 88, Loss: 142.92029, Residuals: -1.30575, Convergence: 0.001829\n",
      "Epoch: 89, Loss: 142.67144, Residuals: -1.29702, Convergence: 0.001744\n",
      "Epoch: 90, Loss: 142.43470, Residuals: -1.28864, Convergence: 0.001662\n",
      "Epoch: 91, Loss: 142.20962, Residuals: -1.28061, Convergence: 0.001583\n",
      "Epoch: 92, Loss: 141.99579, Residuals: -1.27292, Convergence: 0.001506\n",
      "Epoch: 93, Loss: 141.79282, Residuals: -1.26555, Convergence: 0.001431\n",
      "Epoch: 94, Loss: 141.60037, Residuals: -1.25849, Convergence: 0.001359\n",
      "Epoch: 95, Loss: 141.41807, Residuals: -1.25173, Convergence: 0.001289\n",
      "Epoch: 96, Loss: 141.24563, Residuals: -1.24526, Convergence: 0.001221\n",
      "Epoch: 97, Loss: 141.08274, Residuals: -1.23908, Convergence: 0.001155\n",
      "Epoch: 98, Loss: 140.92912, Residuals: -1.23317, Convergence: 0.001090\n",
      "Epoch: 99, Loss: 140.78448, Residuals: -1.22753, Convergence: 0.001027\n",
      "Epoch: 100, Loss: 140.64853, Residuals: -1.22215, Convergence: 0.000967\n",
      "Evidence -182.014\n",
      "\n",
      "Epoch: 100, Evidence: -182.01428, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 7.24e-01\n",
      "Epoch: 100, Loss: 1357.50319, Residuals: -1.22215, Convergence:   inf\n",
      "Epoch: 101, Loss: 1297.60653, Residuals: -1.25236, Convergence: 0.046159\n",
      "Epoch: 102, Loss: 1251.92289, Residuals: -1.27629, Convergence: 0.036491\n",
      "Epoch: 103, Loss: 1217.24744, Residuals: -1.29395, Convergence: 0.028487\n",
      "Epoch: 104, Loss: 1190.23752, Residuals: -1.30673, Convergence: 0.022693\n",
      "Epoch: 105, Loss: 1168.43904, Residuals: -1.31637, Convergence: 0.018656\n",
      "Epoch: 106, Loss: 1150.38850, Residuals: -1.32387, Convergence: 0.015691\n",
      "Epoch: 107, Loss: 1135.19262, Residuals: -1.32971, Convergence: 0.013386\n",
      "Epoch: 108, Loss: 1122.24269, Residuals: -1.33413, Convergence: 0.011539\n",
      "Epoch: 109, Loss: 1111.08655, Residuals: -1.33728, Convergence: 0.010041\n",
      "Epoch: 110, Loss: 1101.37110, Residuals: -1.33930, Convergence: 0.008821\n",
      "Epoch: 111, Loss: 1092.81020, Residuals: -1.34029, Convergence: 0.007834\n",
      "Epoch: 112, Loss: 1085.16773, Residuals: -1.34033, Convergence: 0.007043\n",
      "Epoch: 113, Loss: 1078.24391, Residuals: -1.33949, Convergence: 0.006421\n",
      "Epoch: 114, Loss: 1071.86611, Residuals: -1.33782, Convergence: 0.005950\n",
      "Epoch: 115, Loss: 1065.88296, Residuals: -1.33536, Convergence: 0.005613\n",
      "Epoch: 116, Loss: 1060.15870, Residuals: -1.33213, Convergence: 0.005399\n",
      "Epoch: 117, Loss: 1054.57342, Residuals: -1.32812, Convergence: 0.005296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 118, Loss: 1049.02585, Residuals: -1.32337, Convergence: 0.005288\n",
      "Epoch: 119, Loss: 1043.44265, Residuals: -1.31789, Convergence: 0.005351\n",
      "Epoch: 120, Loss: 1037.79401, Residuals: -1.31174, Convergence: 0.005443\n",
      "Epoch: 121, Loss: 1032.10738, Residuals: -1.30500, Convergence: 0.005510\n",
      "Epoch: 122, Loss: 1026.46538, Residuals: -1.29777, Convergence: 0.005497\n",
      "Epoch: 123, Loss: 1020.97866, Residuals: -1.29019, Convergence: 0.005374\n",
      "Epoch: 124, Loss: 1015.74350, Residuals: -1.28234, Convergence: 0.005154\n",
      "Epoch: 125, Loss: 1010.81778, Residuals: -1.27433, Convergence: 0.004873\n",
      "Epoch: 126, Loss: 1006.21810, Residuals: -1.26623, Convergence: 0.004571\n",
      "Epoch: 127, Loss: 1001.93298, Residuals: -1.25813, Convergence: 0.004277\n",
      "Epoch: 128, Loss: 997.93714, Residuals: -1.25007, Convergence: 0.004004\n",
      "Epoch: 129, Loss: 994.20021, Residuals: -1.24211, Convergence: 0.003759\n",
      "Epoch: 130, Loss: 990.69265, Residuals: -1.23427, Convergence: 0.003541\n",
      "Epoch: 131, Loss: 987.38829, Residuals: -1.22659, Convergence: 0.003347\n",
      "Epoch: 132, Loss: 984.26450, Residuals: -1.21909, Convergence: 0.003174\n",
      "Epoch: 133, Loss: 981.30260, Residuals: -1.21179, Convergence: 0.003018\n",
      "Epoch: 134, Loss: 978.48759, Residuals: -1.20470, Convergence: 0.002877\n",
      "Epoch: 135, Loss: 975.80665, Residuals: -1.19784, Convergence: 0.002747\n",
      "Epoch: 136, Loss: 973.24995, Residuals: -1.19121, Convergence: 0.002627\n",
      "Epoch: 137, Loss: 970.80891, Residuals: -1.18483, Convergence: 0.002514\n",
      "Epoch: 138, Loss: 968.47662, Residuals: -1.17868, Convergence: 0.002408\n",
      "Epoch: 139, Loss: 966.24699, Residuals: -1.17279, Convergence: 0.002308\n",
      "Epoch: 140, Loss: 964.11473, Residuals: -1.16714, Convergence: 0.002212\n",
      "Epoch: 141, Loss: 962.07487, Residuals: -1.16173, Convergence: 0.002120\n",
      "Epoch: 142, Loss: 960.12302, Residuals: -1.15655, Convergence: 0.002033\n",
      "Epoch: 143, Loss: 958.25473, Residuals: -1.15161, Convergence: 0.001950\n",
      "Epoch: 144, Loss: 956.46572, Residuals: -1.14689, Convergence: 0.001870\n",
      "Epoch: 145, Loss: 954.75198, Residuals: -1.14238, Convergence: 0.001795\n",
      "Epoch: 146, Loss: 953.10952, Residuals: -1.13808, Convergence: 0.001723\n",
      "Epoch: 147, Loss: 951.53476, Residuals: -1.13397, Convergence: 0.001655\n",
      "Epoch: 148, Loss: 950.02386, Residuals: -1.13005, Convergence: 0.001590\n",
      "Epoch: 149, Loss: 948.57288, Residuals: -1.12631, Convergence: 0.001530\n",
      "Epoch: 150, Loss: 947.17821, Residuals: -1.12273, Convergence: 0.001472\n",
      "Epoch: 151, Loss: 945.83684, Residuals: -1.11931, Convergence: 0.001418\n",
      "Epoch: 152, Loss: 944.54419, Residuals: -1.11604, Convergence: 0.001369\n",
      "Epoch: 153, Loss: 943.29724, Residuals: -1.11291, Convergence: 0.001322\n",
      "Epoch: 154, Loss: 942.09207, Residuals: -1.10991, Convergence: 0.001279\n",
      "Epoch: 155, Loss: 940.92472, Residuals: -1.10702, Convergence: 0.001241\n",
      "Epoch: 156, Loss: 939.79137, Residuals: -1.10425, Convergence: 0.001206\n",
      "Epoch: 157, Loss: 938.68813, Residuals: -1.10157, Convergence: 0.001175\n",
      "Epoch: 158, Loss: 937.61098, Residuals: -1.09899, Convergence: 0.001149\n",
      "Epoch: 159, Loss: 936.55572, Residuals: -1.09649, Convergence: 0.001127\n",
      "Epoch: 160, Loss: 935.51849, Residuals: -1.09406, Convergence: 0.001109\n",
      "Epoch: 161, Loss: 934.49483, Residuals: -1.09169, Convergence: 0.001095\n",
      "Epoch: 162, Loss: 933.48135, Residuals: -1.08938, Convergence: 0.001086\n",
      "Epoch: 163, Loss: 932.47440, Residuals: -1.08710, Convergence: 0.001080\n",
      "Epoch: 164, Loss: 931.47120, Residuals: -1.08486, Convergence: 0.001077\n",
      "Epoch: 165, Loss: 930.46995, Residuals: -1.08265, Convergence: 0.001076\n",
      "Epoch: 166, Loss: 929.47000, Residuals: -1.08045, Convergence: 0.001076\n",
      "Epoch: 167, Loss: 928.47267, Residuals: -1.07828, Convergence: 0.001074\n",
      "Epoch: 168, Loss: 927.48040, Residuals: -1.07612, Convergence: 0.001070\n",
      "Epoch: 169, Loss: 926.49780, Residuals: -1.07399, Convergence: 0.001061\n",
      "Epoch: 170, Loss: 925.53028, Residuals: -1.07189, Convergence: 0.001045\n",
      "Epoch: 171, Loss: 924.58332, Residuals: -1.06983, Convergence: 0.001024\n",
      "Epoch: 172, Loss: 923.66346, Residuals: -1.06783, Convergence: 0.000996\n",
      "Evidence 11102.200\n",
      "\n",
      "Epoch: 172, Evidence: 11102.20020, Convergence: 1.016394\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 5.75e-01\n",
      "Epoch: 172, Loss: 2328.39528, Residuals: -1.06783, Convergence:   inf\n",
      "Epoch: 173, Loss: 2292.66685, Residuals: -1.07571, Convergence: 0.015584\n",
      "Epoch: 174, Loss: 2267.38679, Residuals: -1.07499, Convergence: 0.011149\n",
      "Epoch: 175, Loss: 2246.31085, Residuals: -1.07344, Convergence: 0.009382\n",
      "Epoch: 176, Loss: 2228.54903, Residuals: -1.07164, Convergence: 0.007970\n",
      "Epoch: 177, Loss: 2213.45195, Residuals: -1.06973, Convergence: 0.006821\n",
      "Epoch: 178, Loss: 2200.51391, Residuals: -1.06772, Convergence: 0.005880\n",
      "Epoch: 179, Loss: 2189.31918, Residuals: -1.06564, Convergence: 0.005113\n",
      "Epoch: 180, Loss: 2179.51666, Residuals: -1.06347, Convergence: 0.004498\n",
      "Epoch: 181, Loss: 2170.81165, Residuals: -1.06122, Convergence: 0.004010\n",
      "Epoch: 182, Loss: 2162.96008, Residuals: -1.05884, Convergence: 0.003630\n",
      "Epoch: 183, Loss: 2155.77299, Residuals: -1.05633, Convergence: 0.003334\n",
      "Epoch: 184, Loss: 2149.11934, Residuals: -1.05367, Convergence: 0.003096\n",
      "Epoch: 185, Loss: 2142.92289, Residuals: -1.05088, Convergence: 0.002892\n",
      "Epoch: 186, Loss: 2137.14494, Residuals: -1.04800, Convergence: 0.002704\n",
      "Epoch: 187, Loss: 2131.76483, Residuals: -1.04507, Convergence: 0.002524\n",
      "Epoch: 188, Loss: 2126.76274, Residuals: -1.04213, Convergence: 0.002352\n",
      "Epoch: 189, Loss: 2122.11766, Residuals: -1.03923, Convergence: 0.002189\n",
      "Epoch: 190, Loss: 2117.80405, Residuals: -1.03638, Convergence: 0.002037\n",
      "Epoch: 191, Loss: 2113.79656, Residuals: -1.03362, Convergence: 0.001896\n",
      "Epoch: 192, Loss: 2110.06770, Residuals: -1.03095, Convergence: 0.001767\n",
      "Epoch: 193, Loss: 2106.59183, Residuals: -1.02838, Convergence: 0.001650\n",
      "Epoch: 194, Loss: 2103.34533, Residuals: -1.02592, Convergence: 0.001543\n",
      "Epoch: 195, Loss: 2100.30717, Residuals: -1.02357, Convergence: 0.001447\n",
      "Epoch: 196, Loss: 2097.45761, Residuals: -1.02133, Convergence: 0.001359\n",
      "Epoch: 197, Loss: 2094.78072, Residuals: -1.01920, Convergence: 0.001278\n",
      "Epoch: 198, Loss: 2092.26247, Residuals: -1.01717, Convergence: 0.001204\n",
      "Epoch: 199, Loss: 2089.89066, Residuals: -1.01525, Convergence: 0.001135\n",
      "Epoch: 200, Loss: 2087.65561, Residuals: -1.01343, Convergence: 0.001071\n",
      "Epoch: 201, Loss: 2085.54760, Residuals: -1.01170, Convergence: 0.001011\n",
      "Epoch: 202, Loss: 2083.55893, Residuals: -1.01007, Convergence: 0.000954\n",
      "Evidence 14218.244\n",
      "\n",
      "Epoch: 202, Evidence: 14218.24414, Convergence: 0.219158\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 4.37e-01\n",
      "Epoch: 202, Loss: 2452.36153, Residuals: -1.01007, Convergence:   inf\n",
      "Epoch: 203, Loss: 2439.37382, Residuals: -1.00685, Convergence: 0.005324\n",
      "Epoch: 204, Loss: 2428.81030, Residuals: -1.00337, Convergence: 0.004349\n",
      "Epoch: 205, Loss: 2419.75290, Residuals: -1.00011, Convergence: 0.003743\n",
      "Epoch: 206, Loss: 2411.93588, Residuals: -0.99712, Convergence: 0.003241\n",
      "Epoch: 207, Loss: 2405.14737, Residuals: -0.99440, Convergence: 0.002822\n",
      "Epoch: 208, Loss: 2399.21671, Residuals: -0.99191, Convergence: 0.002472\n",
      "Epoch: 209, Loss: 2394.00260, Residuals: -0.98964, Convergence: 0.002178\n",
      "Epoch: 210, Loss: 2389.39025, Residuals: -0.98756, Convergence: 0.001930\n",
      "Epoch: 211, Loss: 2385.28573, Residuals: -0.98566, Convergence: 0.001721\n",
      "Epoch: 212, Loss: 2381.61104, Residuals: -0.98391, Convergence: 0.001543\n",
      "Epoch: 213, Loss: 2378.30132, Residuals: -0.98231, Convergence: 0.001392\n",
      "Epoch: 214, Loss: 2375.30513, Residuals: -0.98083, Convergence: 0.001261\n",
      "Epoch: 215, Loss: 2372.57762, Residuals: -0.97946, Convergence: 0.001150\n",
      "Epoch: 216, Loss: 2370.08460, Residuals: -0.97820, Convergence: 0.001052\n",
      "Epoch: 217, Loss: 2367.79349, Residuals: -0.97704, Convergence: 0.000968\n",
      "Evidence 14575.828\n",
      "\n",
      "Epoch: 217, Evidence: 14575.82812, Convergence: 0.024533\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 3.34e-01\n",
      "Epoch: 217, Loss: 2458.13121, Residuals: -0.97704, Convergence:   inf\n",
      "Epoch: 218, Loss: 2451.94274, Residuals: -0.97383, Convergence: 0.002524\n",
      "Epoch: 219, Loss: 2446.83921, Residuals: -0.97113, Convergence: 0.002086\n",
      "Epoch: 220, Loss: 2442.50608, Residuals: -0.96886, Convergence: 0.001774\n",
      "Epoch: 221, Loss: 2438.76832, Residuals: -0.96693, Convergence: 0.001533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 222, Loss: 2435.50032, Residuals: -0.96527, Convergence: 0.001342\n",
      "Epoch: 223, Loss: 2432.60833, Residuals: -0.96385, Convergence: 0.001189\n",
      "Epoch: 224, Loss: 2430.02150, Residuals: -0.96263, Convergence: 0.001065\n",
      "Epoch: 225, Loss: 2427.68704, Residuals: -0.96158, Convergence: 0.000962\n",
      "Evidence 14652.115\n",
      "\n",
      "Epoch: 225, Evidence: 14652.11523, Convergence: 0.005207\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 2.62e-01\n",
      "Epoch: 225, Loss: 2459.99691, Residuals: -0.96158, Convergence:   inf\n",
      "Epoch: 226, Loss: 2456.29188, Residuals: -0.95916, Convergence: 0.001508\n",
      "Epoch: 227, Loss: 2453.19874, Residuals: -0.95726, Convergence: 0.001261\n",
      "Epoch: 228, Loss: 2450.53148, Residuals: -0.95573, Convergence: 0.001088\n",
      "Epoch: 229, Loss: 2448.18745, Residuals: -0.95448, Convergence: 0.000957\n",
      "Evidence 14678.734\n",
      "\n",
      "Epoch: 229, Evidence: 14678.73438, Convergence: 0.001813\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 2.11e-01\n",
      "Epoch: 229, Loss: 2461.10157, Residuals: -0.95448, Convergence:   inf\n",
      "Epoch: 230, Loss: 2458.34229, Residuals: -0.95255, Convergence: 0.001122\n",
      "Epoch: 231, Loss: 2456.01104, Residuals: -0.95106, Convergence: 0.000949\n",
      "Evidence 14690.298\n",
      "\n",
      "Epoch: 231, Evidence: 14690.29785, Convergence: 0.000787\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.75e-01\n",
      "Epoch: 231, Loss: 2461.85909, Residuals: -0.95106, Convergence:   inf\n",
      "Epoch: 232, Loss: 2457.47303, Residuals: -0.94845, Convergence: 0.001785\n",
      "Epoch: 233, Loss: 2454.11937, Residuals: -0.94667, Convergence: 0.001367\n",
      "Epoch: 234, Loss: 2451.38605, Residuals: -0.94551, Convergence: 0.001115\n",
      "Epoch: 235, Loss: 2449.06544, Residuals: -0.94492, Convergence: 0.000948\n",
      "Evidence 14707.560\n",
      "\n",
      "Epoch: 235, Evidence: 14707.55957, Convergence: 0.001960\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.45e-01\n",
      "Epoch: 235, Loss: 2462.07429, Residuals: -0.94492, Convergence:   inf\n",
      "Epoch: 236, Loss: 2459.20651, Residuals: -0.94256, Convergence: 0.001166\n",
      "Epoch: 237, Loss: 2456.91867, Residuals: -0.94139, Convergence: 0.000931\n",
      "Evidence 14718.034\n",
      "\n",
      "Epoch: 237, Evidence: 14718.03418, Convergence: 0.000712\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.23e-01\n",
      "Epoch: 237, Loss: 2462.26590, Residuals: -0.94139, Convergence:   inf\n",
      "Epoch: 238, Loss: 2458.08110, Residuals: -0.93804, Convergence: 0.001702\n",
      "Epoch: 239, Loss: 2455.06951, Residuals: -0.93902, Convergence: 0.001227\n",
      "Epoch: 240, Loss: 2452.62125, Residuals: -0.93981, Convergence: 0.000998\n",
      "Evidence 14731.023\n",
      "\n",
      "Epoch: 240, Evidence: 14731.02344, Convergence: 0.001593\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 1.01e-01\n",
      "Epoch: 240, Loss: 2462.34476, Residuals: -0.93981, Convergence:   inf\n",
      "Epoch: 241, Loss: 2459.74810, Residuals: -0.93731, Convergence: 0.001056\n",
      "Epoch: 242, Loss: 2458.11572, Residuals: -0.93832, Convergence: 0.000664\n",
      "Evidence 14739.533\n",
      "\n",
      "Epoch: 242, Evidence: 14739.53320, Convergence: 0.000577\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 8.30e-02\n",
      "Epoch: 242, Loss: 2462.33845, Residuals: -0.93832, Convergence:   inf\n",
      "Epoch: 243, Loss: 2502.70365, Residuals: -0.97706, Convergence: -0.016129\n",
      "Epoch: 243, Loss: 2460.45771, Residuals: -0.93569, Convergence: 0.000764\n",
      "Evidence 14743.819\n",
      "\n",
      "Epoch: 243, Evidence: 14743.81934, Convergence: 0.000868\n",
      "Updating hyper-parameters...\n",
      "Total samples: 181, Updated regularization: 7.68e-02\n",
      "Epoch: 243, Loss: 2461.97310, Residuals: -0.93569, Convergence:   inf\n",
      "Epoch: 244, Loss: 2466.21841, Residuals: -0.93677, Convergence: -0.001721\n",
      "Epoch: 244, Loss: 2461.90169, Residuals: -0.93375, Convergence: 0.000029\n",
      "Evidence 14745.415\n",
      "\n",
      "Epoch: 244, Evidence: 14745.41504, Convergence: 0.000976\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 384.64652, Residuals: -4.55836, Convergence:   inf\n",
      "Epoch: 1, Loss: 358.97517, Residuals: -4.43986, Convergence: 0.071513\n",
      "Epoch: 2, Loss: 337.89014, Residuals: -4.27601, Convergence: 0.062402\n",
      "Epoch: 3, Loss: 321.81663, Residuals: -4.11183, Convergence: 0.049946\n",
      "Epoch: 4, Loss: 309.55211, Residuals: -3.96711, Convergence: 0.039620\n",
      "Epoch: 5, Loss: 299.81516, Residuals: -3.83895, Convergence: 0.032477\n",
      "Epoch: 6, Loss: 291.90205, Residuals: -3.72721, Convergence: 0.027109\n",
      "Epoch: 7, Loss: 285.32250, Residuals: -3.63146, Convergence: 0.023060\n",
      "Epoch: 8, Loss: 279.71351, Residuals: -3.54964, Convergence: 0.020053\n",
      "Epoch: 9, Loss: 274.81927, Residuals: -3.47941, Convergence: 0.017809\n",
      "Epoch: 10, Loss: 270.45811, Residuals: -3.41867, Convergence: 0.016125\n",
      "Epoch: 11, Loss: 266.49865, Residuals: -3.36562, Convergence: 0.014857\n",
      "Epoch: 12, Loss: 262.84530, Residuals: -3.31873, Convergence: 0.013899\n",
      "Epoch: 13, Loss: 259.42907, Residuals: -3.27667, Convergence: 0.013168\n",
      "Epoch: 14, Loss: 256.20170, Residuals: -3.23827, Convergence: 0.012597\n",
      "Epoch: 15, Loss: 253.13311, Residuals: -3.20258, Convergence: 0.012122\n",
      "Epoch: 16, Loss: 250.20884, Residuals: -3.16902, Convergence: 0.011687\n",
      "Epoch: 17, Loss: 247.41890, Residuals: -3.13722, Convergence: 0.011276\n",
      "Epoch: 18, Loss: 244.74297, Residuals: -3.10681, Convergence: 0.010934\n",
      "Epoch: 19, Loss: 242.14819, Residuals: -3.07728, Convergence: 0.010716\n",
      "Epoch: 20, Loss: 239.59739, Residuals: -3.04803, Convergence: 0.010646\n",
      "Epoch: 21, Loss: 237.05808, Residuals: -3.01854, Convergence: 0.010712\n",
      "Epoch: 22, Loss: 234.50615, Residuals: -2.98845, Convergence: 0.010882\n",
      "Epoch: 23, Loss: 231.91672, Residuals: -2.95746, Convergence: 0.011165\n",
      "Epoch: 24, Loss: 229.24744, Residuals: -2.92511, Convergence: 0.011644\n",
      "Epoch: 25, Loss: 226.44120, Residuals: -2.89072, Convergence: 0.012393\n",
      "Epoch: 26, Loss: 223.48870, Residuals: -2.85410, Convergence: 0.013211\n",
      "Epoch: 27, Loss: 220.50130, Residuals: -2.81636, Convergence: 0.013548\n",
      "Epoch: 28, Loss: 217.59020, Residuals: -2.77883, Convergence: 0.013379\n",
      "Epoch: 29, Loss: 214.77459, Residuals: -2.74185, Convergence: 0.013110\n",
      "Epoch: 30, Loss: 212.03791, Residuals: -2.70535, Convergence: 0.012907\n",
      "Epoch: 31, Loss: 209.36229, Residuals: -2.66916, Convergence: 0.012780\n",
      "Epoch: 32, Loss: 206.73547, Residuals: -2.63316, Convergence: 0.012706\n",
      "Epoch: 33, Loss: 204.15072, Residuals: -2.59724, Convergence: 0.012661\n",
      "Epoch: 34, Loss: 201.60557, Residuals: -2.56135, Convergence: 0.012624\n",
      "Epoch: 35, Loss: 199.10044, Residuals: -2.52546, Convergence: 0.012582\n",
      "Epoch: 36, Loss: 196.63747, Residuals: -2.48955, Convergence: 0.012525\n",
      "Epoch: 37, Loss: 194.21967, Residuals: -2.45365, Convergence: 0.012449\n",
      "Epoch: 38, Loss: 191.85034, Residuals: -2.41777, Convergence: 0.012350\n",
      "Epoch: 39, Loss: 189.53276, Residuals: -2.38195, Convergence: 0.012228\n",
      "Epoch: 40, Loss: 187.27003, Residuals: -2.34622, Convergence: 0.012083\n",
      "Epoch: 41, Loss: 185.06515, Residuals: -2.31062, Convergence: 0.011914\n",
      "Epoch: 42, Loss: 182.92112, Residuals: -2.27520, Convergence: 0.011721\n",
      "Epoch: 43, Loss: 180.84107, Residuals: -2.24002, Convergence: 0.011502\n",
      "Epoch: 44, Loss: 178.82819, Residuals: -2.20514, Convergence: 0.011256\n",
      "Epoch: 45, Loss: 176.88569, Residuals: -2.17063, Convergence: 0.010982\n",
      "Epoch: 46, Loss: 175.01644, Residuals: -2.13658, Convergence: 0.010680\n",
      "Epoch: 47, Loss: 173.22267, Residuals: -2.10304, Convergence: 0.010355\n",
      "Epoch: 48, Loss: 171.50564, Residuals: -2.07009, Convergence: 0.010011\n",
      "Epoch: 49, Loss: 169.86558, Residuals: -2.03778, Convergence: 0.009655\n",
      "Epoch: 50, Loss: 168.30165, Residuals: -2.00617, Convergence: 0.009292\n",
      "Epoch: 51, Loss: 166.81210, Residuals: -1.97528, Convergence: 0.008930\n",
      "Epoch: 52, Loss: 165.39441, Residuals: -1.94513, Convergence: 0.008572\n",
      "Epoch: 53, Loss: 164.04559, Residuals: -1.91574, Convergence: 0.008222\n",
      "Epoch: 54, Loss: 162.76230, Residuals: -1.88709, Convergence: 0.007884\n",
      "Epoch: 55, Loss: 161.54110, Residuals: -1.85920, Convergence: 0.007560\n",
      "Epoch: 56, Loss: 160.37852, Residuals: -1.83204, Convergence: 0.007249\n",
      "Epoch: 57, Loss: 159.27115, Residuals: -1.80561, Convergence: 0.006953\n",
      "Epoch: 58, Loss: 158.21566, Residuals: -1.77991, Convergence: 0.006671\n",
      "Epoch: 59, Loss: 157.20886, Residuals: -1.75491, Convergence: 0.006404\n",
      "Epoch: 60, Loss: 156.24773, Residuals: -1.73061, Convergence: 0.006151\n",
      "Epoch: 61, Loss: 155.32945, Residuals: -1.70699, Convergence: 0.005912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62, Loss: 154.45145, Residuals: -1.68405, Convergence: 0.005685\n",
      "Epoch: 63, Loss: 153.61146, Residuals: -1.66177, Convergence: 0.005468\n",
      "Epoch: 64, Loss: 152.80749, Residuals: -1.64013, Convergence: 0.005261\n",
      "Epoch: 65, Loss: 152.03783, Residuals: -1.61914, Convergence: 0.005062\n",
      "Epoch: 66, Loss: 151.30105, Residuals: -1.59878, Convergence: 0.004870\n",
      "Epoch: 67, Loss: 150.59591, Residuals: -1.57905, Convergence: 0.004682\n",
      "Epoch: 68, Loss: 149.92135, Residuals: -1.55994, Convergence: 0.004499\n",
      "Epoch: 69, Loss: 149.27640, Residuals: -1.54145, Convergence: 0.004320\n",
      "Epoch: 70, Loss: 148.66019, Residuals: -1.52358, Convergence: 0.004145\n",
      "Epoch: 71, Loss: 148.07188, Residuals: -1.50632, Convergence: 0.003973\n",
      "Epoch: 72, Loss: 147.51065, Residuals: -1.48966, Convergence: 0.003805\n",
      "Epoch: 73, Loss: 146.97567, Residuals: -1.47360, Convergence: 0.003640\n",
      "Epoch: 74, Loss: 146.46611, Residuals: -1.45814, Convergence: 0.003479\n",
      "Epoch: 75, Loss: 145.98113, Residuals: -1.44325, Convergence: 0.003322\n",
      "Epoch: 76, Loss: 145.51985, Residuals: -1.42894, Convergence: 0.003170\n",
      "Epoch: 77, Loss: 145.08140, Residuals: -1.41518, Convergence: 0.003022\n",
      "Epoch: 78, Loss: 144.66489, Residuals: -1.40197, Convergence: 0.002879\n",
      "Epoch: 79, Loss: 144.26942, Residuals: -1.38929, Convergence: 0.002741\n",
      "Epoch: 80, Loss: 143.89411, Residuals: -1.37713, Convergence: 0.002608\n",
      "Epoch: 81, Loss: 143.53808, Residuals: -1.36546, Convergence: 0.002480\n",
      "Epoch: 82, Loss: 143.20050, Residuals: -1.35428, Convergence: 0.002357\n",
      "Epoch: 83, Loss: 142.88052, Residuals: -1.34357, Convergence: 0.002239\n",
      "Epoch: 84, Loss: 142.57735, Residuals: -1.33330, Convergence: 0.002126\n",
      "Epoch: 85, Loss: 142.29024, Residuals: -1.32347, Convergence: 0.002018\n",
      "Epoch: 86, Loss: 142.01845, Residuals: -1.31406, Convergence: 0.001914\n",
      "Epoch: 87, Loss: 141.76131, Residuals: -1.30505, Convergence: 0.001814\n",
      "Epoch: 88, Loss: 141.51817, Residuals: -1.29643, Convergence: 0.001718\n",
      "Epoch: 89, Loss: 141.28844, Residuals: -1.28818, Convergence: 0.001626\n",
      "Epoch: 90, Loss: 141.07153, Residuals: -1.28028, Convergence: 0.001538\n",
      "Epoch: 91, Loss: 140.86692, Residuals: -1.27273, Convergence: 0.001452\n",
      "Epoch: 92, Loss: 140.67414, Residuals: -1.26551, Convergence: 0.001370\n",
      "Epoch: 93, Loss: 140.49271, Residuals: -1.25861, Convergence: 0.001291\n",
      "Epoch: 94, Loss: 140.32220, Residuals: -1.25203, Convergence: 0.001215\n",
      "Epoch: 95, Loss: 140.16219, Residuals: -1.24574, Convergence: 0.001142\n",
      "Epoch: 96, Loss: 140.01226, Residuals: -1.23975, Convergence: 0.001071\n",
      "Epoch: 97, Loss: 139.87199, Residuals: -1.23404, Convergence: 0.001003\n",
      "Epoch: 98, Loss: 139.74094, Residuals: -1.22861, Convergence: 0.000938\n",
      "Evidence -181.169\n",
      "\n",
      "Epoch: 98, Evidence: -181.16872, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.24e-01\n",
      "Epoch: 98, Loss: 1358.59983, Residuals: -1.22861, Convergence:   inf\n",
      "Epoch: 99, Loss: 1298.84370, Residuals: -1.25786, Convergence: 0.046007\n",
      "Epoch: 100, Loss: 1253.43368, Residuals: -1.28047, Convergence: 0.036228\n",
      "Epoch: 101, Loss: 1219.07404, Residuals: -1.29627, Convergence: 0.028185\n",
      "Epoch: 102, Loss: 1192.26969, Residuals: -1.30689, Convergence: 0.022482\n",
      "Epoch: 103, Loss: 1170.52249, Residuals: -1.31431, Convergence: 0.018579\n",
      "Epoch: 104, Loss: 1152.40815, Residuals: -1.31963, Convergence: 0.015719\n",
      "Epoch: 105, Loss: 1137.08071, Residuals: -1.32336, Convergence: 0.013480\n",
      "Epoch: 106, Loss: 1123.97095, Residuals: -1.32580, Convergence: 0.011664\n",
      "Epoch: 107, Loss: 1112.65870, Residuals: -1.32711, Convergence: 0.010167\n",
      "Epoch: 108, Loss: 1102.81562, Residuals: -1.32744, Convergence: 0.008925\n",
      "Epoch: 109, Loss: 1094.17810, Residuals: -1.32692, Convergence: 0.007894\n",
      "Epoch: 110, Loss: 1086.53013, Residuals: -1.32564, Convergence: 0.007039\n",
      "Epoch: 111, Loss: 1079.69146, Residuals: -1.32370, Convergence: 0.006334\n",
      "Epoch: 112, Loss: 1073.50981, Residuals: -1.32116, Convergence: 0.005758\n",
      "Epoch: 113, Loss: 1067.85348, Residuals: -1.31810, Convergence: 0.005297\n",
      "Epoch: 114, Loss: 1062.60475, Residuals: -1.31454, Convergence: 0.004939\n",
      "Epoch: 115, Loss: 1057.65581, Residuals: -1.31052, Convergence: 0.004679\n",
      "Epoch: 116, Loss: 1052.90479, Residuals: -1.30603, Convergence: 0.004512\n",
      "Epoch: 117, Loss: 1048.25481, Residuals: -1.30110, Convergence: 0.004436\n",
      "Epoch: 118, Loss: 1043.61856, Residuals: -1.29571, Convergence: 0.004442\n",
      "Epoch: 119, Loss: 1038.93068, Residuals: -1.28990, Convergence: 0.004512\n",
      "Epoch: 120, Loss: 1034.16533, Residuals: -1.28369, Convergence: 0.004608\n",
      "Epoch: 121, Loss: 1029.34850, Residuals: -1.27714, Convergence: 0.004679\n",
      "Epoch: 122, Loss: 1024.55307, Residuals: -1.27031, Convergence: 0.004681\n",
      "Epoch: 123, Loss: 1019.86764, Residuals: -1.26326, Convergence: 0.004594\n",
      "Epoch: 124, Loss: 1015.36445, Residuals: -1.25605, Convergence: 0.004435\n",
      "Epoch: 125, Loss: 1011.08501, Residuals: -1.24872, Convergence: 0.004233\n",
      "Epoch: 126, Loss: 1007.04277, Residuals: -1.24135, Convergence: 0.004014\n",
      "Epoch: 127, Loss: 1003.23365, Residuals: -1.23396, Convergence: 0.003797\n",
      "Epoch: 128, Loss: 999.64537, Residuals: -1.22661, Convergence: 0.003590\n",
      "Epoch: 129, Loss: 996.26241, Residuals: -1.21934, Convergence: 0.003396\n",
      "Epoch: 130, Loss: 993.06904, Residuals: -1.21216, Convergence: 0.003216\n",
      "Epoch: 131, Loss: 990.04916, Residuals: -1.20512, Convergence: 0.003050\n",
      "Epoch: 132, Loss: 987.18823, Residuals: -1.19823, Convergence: 0.002898\n",
      "Epoch: 133, Loss: 984.47336, Residuals: -1.19150, Convergence: 0.002758\n",
      "Epoch: 134, Loss: 981.89249, Residuals: -1.18496, Convergence: 0.002628\n",
      "Epoch: 135, Loss: 979.43541, Residuals: -1.17861, Convergence: 0.002509\n",
      "Epoch: 136, Loss: 977.09274, Residuals: -1.17247, Convergence: 0.002398\n",
      "Epoch: 137, Loss: 974.85679, Residuals: -1.16654, Convergence: 0.002294\n",
      "Epoch: 138, Loss: 972.72043, Residuals: -1.16081, Convergence: 0.002196\n",
      "Epoch: 139, Loss: 970.67748, Residuals: -1.15531, Convergence: 0.002105\n",
      "Epoch: 140, Loss: 968.72242, Residuals: -1.15003, Convergence: 0.002018\n",
      "Epoch: 141, Loss: 966.85051, Residuals: -1.14496, Convergence: 0.001936\n",
      "Epoch: 142, Loss: 965.05670, Residuals: -1.14010, Convergence: 0.001859\n",
      "Epoch: 143, Loss: 963.33646, Residuals: -1.13546, Convergence: 0.001786\n",
      "Epoch: 144, Loss: 961.68597, Residuals: -1.13102, Convergence: 0.001716\n",
      "Epoch: 145, Loss: 960.10099, Residuals: -1.12678, Convergence: 0.001651\n",
      "Epoch: 146, Loss: 958.57784, Residuals: -1.12273, Convergence: 0.001589\n",
      "Epoch: 147, Loss: 957.11239, Residuals: -1.11887, Convergence: 0.001531\n",
      "Epoch: 148, Loss: 955.70162, Residuals: -1.11518, Convergence: 0.001476\n",
      "Epoch: 149, Loss: 954.34207, Residuals: -1.11166, Convergence: 0.001425\n",
      "Epoch: 150, Loss: 953.03055, Residuals: -1.10830, Convergence: 0.001376\n",
      "Epoch: 151, Loss: 951.76407, Residuals: -1.10510, Convergence: 0.001331\n",
      "Epoch: 152, Loss: 950.53988, Residuals: -1.10203, Convergence: 0.001288\n",
      "Epoch: 153, Loss: 949.35524, Residuals: -1.09910, Convergence: 0.001248\n",
      "Epoch: 154, Loss: 948.20797, Residuals: -1.09629, Convergence: 0.001210\n",
      "Epoch: 155, Loss: 947.09588, Residuals: -1.09361, Convergence: 0.001174\n",
      "Epoch: 156, Loss: 946.01620, Residuals: -1.09103, Convergence: 0.001141\n",
      "Epoch: 157, Loss: 944.96722, Residuals: -1.08855, Convergence: 0.001110\n",
      "Epoch: 158, Loss: 943.94674, Residuals: -1.08617, Convergence: 0.001081\n",
      "Epoch: 159, Loss: 942.95224, Residuals: -1.08388, Convergence: 0.001055\n",
      "Epoch: 160, Loss: 941.98186, Residuals: -1.08167, Convergence: 0.001030\n",
      "Epoch: 161, Loss: 941.03270, Residuals: -1.07953, Convergence: 0.001009\n",
      "Epoch: 162, Loss: 940.10255, Residuals: -1.07745, Convergence: 0.000989\n",
      "Evidence 11086.439\n",
      "\n",
      "Epoch: 162, Evidence: 11086.43945, Convergence: 1.016341\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.76e-01\n",
      "Epoch: 162, Loss: 2338.56760, Residuals: -1.07745, Convergence:   inf\n",
      "Epoch: 163, Loss: 2296.36938, Residuals: -1.08757, Convergence: 0.018376\n",
      "Epoch: 164, Loss: 2268.18805, Residuals: -1.08732, Convergence: 0.012425\n",
      "Epoch: 165, Loss: 2244.66953, Residuals: -1.08581, Convergence: 0.010477\n",
      "Epoch: 166, Loss: 2224.68660, Residuals: -1.08376, Convergence: 0.008982\n",
      "Epoch: 167, Loss: 2207.53891, Residuals: -1.08132, Convergence: 0.007768\n",
      "Epoch: 168, Loss: 2192.69828, Residuals: -1.07860, Convergence: 0.006768\n",
      "Epoch: 169, Loss: 2179.74681, Residuals: -1.07567, Convergence: 0.005942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 170, Loss: 2168.34549, Residuals: -1.07259, Convergence: 0.005258\n",
      "Epoch: 171, Loss: 2158.21153, Residuals: -1.06940, Convergence: 0.004696\n",
      "Epoch: 172, Loss: 2149.10750, Residuals: -1.06612, Convergence: 0.004236\n",
      "Epoch: 173, Loss: 2140.83627, Residuals: -1.06275, Convergence: 0.003864\n",
      "Epoch: 174, Loss: 2133.24325, Residuals: -1.05930, Convergence: 0.003559\n",
      "Epoch: 175, Loss: 2126.22108, Residuals: -1.05577, Convergence: 0.003303\n",
      "Epoch: 176, Loss: 2119.70923, Residuals: -1.05217, Convergence: 0.003072\n",
      "Epoch: 177, Loss: 2113.67479, Residuals: -1.04856, Convergence: 0.002855\n",
      "Epoch: 178, Loss: 2108.09747, Residuals: -1.04498, Convergence: 0.002646\n",
      "Epoch: 179, Loss: 2102.95271, Residuals: -1.04147, Convergence: 0.002446\n",
      "Epoch: 180, Loss: 2098.21215, Residuals: -1.03805, Convergence: 0.002259\n",
      "Epoch: 181, Loss: 2093.84290, Residuals: -1.03475, Convergence: 0.002087\n",
      "Epoch: 182, Loss: 2089.80982, Residuals: -1.03159, Convergence: 0.001930\n",
      "Epoch: 183, Loss: 2086.07906, Residuals: -1.02856, Convergence: 0.001788\n",
      "Epoch: 184, Loss: 2082.61856, Residuals: -1.02568, Convergence: 0.001662\n",
      "Epoch: 185, Loss: 2079.39962, Residuals: -1.02295, Convergence: 0.001548\n",
      "Epoch: 186, Loss: 2076.39549, Residuals: -1.02035, Convergence: 0.001447\n",
      "Epoch: 187, Loss: 2073.58306, Residuals: -1.01789, Convergence: 0.001356\n",
      "Epoch: 188, Loss: 2070.94153, Residuals: -1.01556, Convergence: 0.001276\n",
      "Epoch: 189, Loss: 2068.45323, Residuals: -1.01336, Convergence: 0.001203\n",
      "Epoch: 190, Loss: 2066.10199, Residuals: -1.01128, Convergence: 0.001138\n",
      "Epoch: 191, Loss: 2063.87573, Residuals: -1.00932, Convergence: 0.001079\n",
      "Epoch: 192, Loss: 2061.76204, Residuals: -1.00747, Convergence: 0.001025\n",
      "Epoch: 193, Loss: 2059.75268, Residuals: -1.00572, Convergence: 0.000976\n",
      "Evidence 14266.559\n",
      "\n",
      "Epoch: 193, Evidence: 14266.55859, Convergence: 0.222907\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.39e-01\n",
      "Epoch: 193, Loss: 2463.78328, Residuals: -1.00572, Convergence:   inf\n",
      "Epoch: 194, Loss: 2448.79749, Residuals: -1.00353, Convergence: 0.006120\n",
      "Epoch: 195, Loss: 2436.70218, Residuals: -1.00043, Convergence: 0.004964\n",
      "Epoch: 196, Loss: 2426.28700, Residuals: -0.99724, Convergence: 0.004293\n",
      "Epoch: 197, Loss: 2417.23750, Residuals: -0.99412, Convergence: 0.003744\n",
      "Epoch: 198, Loss: 2409.32634, Residuals: -0.99115, Convergence: 0.003284\n",
      "Epoch: 199, Loss: 2402.37758, Residuals: -0.98837, Convergence: 0.002892\n",
      "Epoch: 200, Loss: 2396.24457, Residuals: -0.98577, Convergence: 0.002559\n",
      "Epoch: 201, Loss: 2390.80601, Residuals: -0.98336, Convergence: 0.002275\n",
      "Epoch: 202, Loss: 2385.95941, Residuals: -0.98113, Convergence: 0.002031\n",
      "Epoch: 203, Loss: 2381.61772, Residuals: -0.97907, Convergence: 0.001823\n",
      "Epoch: 204, Loss: 2377.70851, Residuals: -0.97716, Convergence: 0.001644\n",
      "Epoch: 205, Loss: 2374.17064, Residuals: -0.97539, Convergence: 0.001490\n",
      "Epoch: 206, Loss: 2370.95102, Residuals: -0.97375, Convergence: 0.001358\n",
      "Epoch: 207, Loss: 2368.00839, Residuals: -0.97223, Convergence: 0.001243\n",
      "Epoch: 208, Loss: 2365.30537, Residuals: -0.97081, Convergence: 0.001143\n",
      "Epoch: 209, Loss: 2362.81152, Residuals: -0.96949, Convergence: 0.001055\n",
      "Epoch: 210, Loss: 2360.50212, Residuals: -0.96826, Convergence: 0.000978\n",
      "Evidence 14692.018\n",
      "\n",
      "Epoch: 210, Evidence: 14692.01758, Convergence: 0.028959\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.37e-01\n",
      "Epoch: 210, Loss: 2468.34112, Residuals: -0.96826, Convergence:   inf\n",
      "Epoch: 211, Loss: 2461.47242, Residuals: -0.96557, Convergence: 0.002790\n",
      "Epoch: 212, Loss: 2455.77091, Residuals: -0.96292, Convergence: 0.002322\n",
      "Epoch: 213, Loss: 2450.90528, Residuals: -0.96052, Convergence: 0.001985\n",
      "Epoch: 214, Loss: 2446.69810, Residuals: -0.95837, Convergence: 0.001720\n",
      "Epoch: 215, Loss: 2443.01961, Residuals: -0.95645, Convergence: 0.001506\n",
      "Epoch: 216, Loss: 2439.76691, Residuals: -0.95475, Convergence: 0.001333\n",
      "Epoch: 217, Loss: 2436.86181, Residuals: -0.95324, Convergence: 0.001192\n",
      "Epoch: 218, Loss: 2434.24173, Residuals: -0.95189, Convergence: 0.001076\n",
      "Epoch: 219, Loss: 2431.85838, Residuals: -0.95069, Convergence: 0.000980\n",
      "Evidence 14784.411\n",
      "\n",
      "Epoch: 219, Evidence: 14784.41113, Convergence: 0.006249\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.65e-01\n",
      "Epoch: 219, Loss: 2469.86036, Residuals: -0.95069, Convergence:   inf\n",
      "Epoch: 220, Loss: 2465.85706, Residuals: -0.94841, Convergence: 0.001623\n",
      "Epoch: 221, Loss: 2462.49813, Residuals: -0.94640, Convergence: 0.001364\n",
      "Epoch: 222, Loss: 2459.60291, Residuals: -0.94468, Convergence: 0.001177\n",
      "Epoch: 223, Loss: 2457.06283, Residuals: -0.94320, Convergence: 0.001034\n",
      "Epoch: 224, Loss: 2454.80206, Residuals: -0.94193, Convergence: 0.000921\n",
      "Evidence 14816.614\n",
      "\n",
      "Epoch: 224, Evidence: 14816.61426, Convergence: 0.002173\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.14e-01\n",
      "Epoch: 224, Loss: 2470.85504, Residuals: -0.94193, Convergence:   inf\n",
      "Epoch: 225, Loss: 2468.00306, Residuals: -0.94006, Convergence: 0.001156\n",
      "Epoch: 226, Loss: 2465.58696, Residuals: -0.93848, Convergence: 0.000980\n",
      "Evidence 14829.382\n",
      "\n",
      "Epoch: 226, Evidence: 14829.38184, Convergence: 0.000861\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.78e-01\n",
      "Epoch: 226, Loss: 2471.64454, Residuals: -0.93848, Convergence:   inf\n",
      "Epoch: 227, Loss: 2467.14636, Residuals: -0.93580, Convergence: 0.001823\n",
      "Epoch: 228, Loss: 2463.65003, Residuals: -0.93368, Convergence: 0.001419\n",
      "Epoch: 229, Loss: 2460.78619, Residuals: -0.93214, Convergence: 0.001164\n",
      "Epoch: 230, Loss: 2458.34511, Residuals: -0.93108, Convergence: 0.000993\n",
      "Evidence 14847.504\n",
      "\n",
      "Epoch: 230, Evidence: 14847.50391, Convergence: 0.002080\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.48e-01\n",
      "Epoch: 230, Loss: 2471.93807, Residuals: -0.93108, Convergence:   inf\n",
      "Epoch: 231, Loss: 2468.95224, Residuals: -0.92881, Convergence: 0.001209\n",
      "Epoch: 232, Loss: 2466.54637, Residuals: -0.92750, Convergence: 0.000975\n",
      "Evidence 14858.424\n",
      "\n",
      "Epoch: 232, Evidence: 14858.42383, Convergence: 0.000735\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.26e-01\n",
      "Epoch: 232, Loss: 2472.22328, Residuals: -0.92750, Convergence:   inf\n",
      "Epoch: 233, Loss: 2467.83218, Residuals: -0.92453, Convergence: 0.001779\n",
      "Epoch: 234, Loss: 2464.57916, Residuals: -0.92523, Convergence: 0.001320\n",
      "Epoch: 235, Loss: 2461.88901, Residuals: -0.92539, Convergence: 0.001093\n",
      "Epoch: 236, Loss: 2459.55401, Residuals: -0.92734, Convergence: 0.000949\n",
      "Evidence 14874.655\n",
      "\n",
      "Epoch: 236, Evidence: 14874.65527, Convergence: 0.001825\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.12e-01\n",
      "Epoch: 236, Loss: 2471.75737, Residuals: -0.92734, Convergence:   inf\n",
      "Epoch: 237, Loss: 2470.09880, Residuals: -0.92677, Convergence: 0.000671\n",
      "Evidence 14881.069\n",
      "\n",
      "Epoch: 237, Evidence: 14881.06934, Convergence: 0.000431\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 9.18e-02\n",
      "Epoch: 237, Loss: 2472.72660, Residuals: -0.92677, Convergence:   inf\n",
      "Epoch: 238, Loss: 2524.58021, Residuals: -0.96668, Convergence: -0.020539\n",
      "Epoch: 238, Loss: 2470.20005, Residuals: -0.92483, Convergence: 0.001023\n",
      "Epoch: 239, Loss: 2469.67743, Residuals: -0.92650, Convergence: 0.000212\n",
      "Evidence 14886.500\n",
      "\n",
      "Epoch: 239, Evidence: 14886.50000, Convergence: 0.000796\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.67e-02\n",
      "Epoch: 239, Loss: 2472.67755, Residuals: -0.92650, Convergence:   inf\n",
      "Epoch: 240, Loss: 2535.47590, Residuals: -0.97292, Convergence: -0.024768\n",
      "Epoch: 240, Loss: 2470.24717, Residuals: -0.92416, Convergence: 0.000984\n",
      "Evidence 14891.083\n",
      "\n",
      "Epoch: 240, Evidence: 14891.08301, Convergence: 0.001103\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.05e-02\n",
      "Epoch: 240, Loss: 2472.42025, Residuals: -0.92416, Convergence:   inf\n",
      "Epoch: 241, Loss: 2471.64169, Residuals: -0.92339, Convergence: 0.000315\n",
      "Evidence 14893.237\n",
      "\n",
      "Epoch: 241, Evidence: 14893.23730, Convergence: 0.000145\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 6.05e-02\n",
      "Epoch: 241, Loss: 2472.80776, Residuals: -0.92339, Convergence:   inf\n",
      "Epoch: 242, Loss: 2526.05588, Residuals: -0.97050, Convergence: -0.021080\n",
      "Epoch: 242, Loss: 2470.85646, Residuals: -0.92274, Convergence: 0.000790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence 14896.483\n",
      "\n",
      "Epoch: 242, Evidence: 14896.48340, Convergence: 0.000363\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.63e-02\n",
      "Epoch: 242, Loss: 2472.56358, Residuals: -0.92274, Convergence:   inf\n",
      "Epoch: 243, Loss: 2477.48691, Residuals: -0.93365, Convergence: -0.001987\n",
      "Epoch: 243, Loss: 2472.33834, Residuals: -0.92271, Convergence: 0.000091\n",
      "Evidence 14897.773\n",
      "\n",
      "Epoch: 243, Evidence: 14897.77344, Convergence: 0.000449\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 382.01848, Residuals: -4.53507, Convergence:   inf\n",
      "Epoch: 1, Loss: 356.26847, Residuals: -4.41521, Convergence: 0.072277\n",
      "Epoch: 2, Loss: 335.23824, Residuals: -4.25197, Convergence: 0.062732\n",
      "Epoch: 3, Loss: 319.17420, Residuals: -4.08829, Convergence: 0.050330\n",
      "Epoch: 4, Loss: 306.93422, Residuals: -3.94409, Convergence: 0.039878\n",
      "Epoch: 5, Loss: 297.23497, Residuals: -3.81656, Convergence: 0.032632\n",
      "Epoch: 6, Loss: 289.36533, Residuals: -3.70538, Convergence: 0.027196\n",
      "Epoch: 7, Loss: 282.83582, Residuals: -3.61006, Convergence: 0.023086\n",
      "Epoch: 8, Loss: 277.28385, Residuals: -3.52859, Convergence: 0.020023\n",
      "Epoch: 9, Loss: 272.45312, Residuals: -3.45865, Convergence: 0.017731\n",
      "Epoch: 10, Loss: 268.16141, Residuals: -3.39819, Convergence: 0.016004\n",
      "Epoch: 11, Loss: 264.27681, Residuals: -3.34542, Convergence: 0.014699\n",
      "Epoch: 12, Loss: 260.70306, Residuals: -3.29885, Convergence: 0.013708\n",
      "Epoch: 13, Loss: 257.37032, Residuals: -3.25719, Convergence: 0.012949\n",
      "Epoch: 14, Loss: 254.22915, Residuals: -3.21931, Convergence: 0.012356\n",
      "Epoch: 15, Loss: 251.24737, Residuals: -3.18428, Convergence: 0.011868\n",
      "Epoch: 16, Loss: 248.40742, Residuals: -3.15144, Convergence: 0.011433\n",
      "Epoch: 17, Loss: 245.69721, Residuals: -3.12037, Convergence: 0.011031\n",
      "Epoch: 18, Loss: 243.09662, Residuals: -3.09065, Convergence: 0.010698\n",
      "Epoch: 19, Loss: 240.57433, Residuals: -3.06174, Convergence: 0.010484\n",
      "Epoch: 20, Loss: 238.09523, Residuals: -3.03304, Convergence: 0.010412\n",
      "Epoch: 21, Loss: 235.63000, Residuals: -3.00405, Convergence: 0.010462\n",
      "Epoch: 22, Loss: 233.15863, Residuals: -2.97443, Convergence: 0.010600\n",
      "Epoch: 23, Loss: 230.65970, Residuals: -2.94393, Convergence: 0.010834\n",
      "Epoch: 24, Loss: 228.09309, Residuals: -2.91212, Convergence: 0.011253\n",
      "Epoch: 25, Loss: 225.40249, Residuals: -2.87830, Convergence: 0.011937\n",
      "Epoch: 26, Loss: 222.57361, Residuals: -2.84224, Convergence: 0.012710\n",
      "Epoch: 27, Loss: 219.71090, Residuals: -2.80497, Convergence: 0.013029\n",
      "Epoch: 28, Loss: 216.93115, Residuals: -2.76787, Convergence: 0.012814\n",
      "Epoch: 29, Loss: 214.26012, Residuals: -2.73138, Convergence: 0.012466\n",
      "Epoch: 30, Loss: 211.68301, Residuals: -2.69546, Convergence: 0.012174\n",
      "Epoch: 31, Loss: 209.18134, Residuals: -2.66000, Convergence: 0.011959\n",
      "Epoch: 32, Loss: 206.74088, Residuals: -2.62489, Convergence: 0.011804\n",
      "Epoch: 33, Loss: 204.35215, Residuals: -2.59005, Convergence: 0.011689\n",
      "Epoch: 34, Loss: 202.00956, Residuals: -2.55542, Convergence: 0.011596\n",
      "Epoch: 35, Loss: 199.71046, Residuals: -2.52097, Convergence: 0.011512\n",
      "Epoch: 36, Loss: 197.45439, Residuals: -2.48667, Convergence: 0.011426\n",
      "Epoch: 37, Loss: 195.24231, Residuals: -2.45251, Convergence: 0.011330\n",
      "Epoch: 38, Loss: 193.07604, Residuals: -2.41849, Convergence: 0.011220\n",
      "Epoch: 39, Loss: 190.95776, Residuals: -2.38461, Convergence: 0.011093\n",
      "Epoch: 40, Loss: 188.88966, Residuals: -2.35089, Convergence: 0.010949\n",
      "Epoch: 41, Loss: 186.87370, Residuals: -2.31735, Convergence: 0.010788\n",
      "Epoch: 42, Loss: 184.91153, Residuals: -2.28400, Convergence: 0.010611\n",
      "Epoch: 43, Loss: 183.00445, Residuals: -2.25087, Convergence: 0.010421\n",
      "Epoch: 44, Loss: 181.15355, Residuals: -2.21798, Convergence: 0.010217\n",
      "Epoch: 45, Loss: 179.35983, Residuals: -2.18538, Convergence: 0.010001\n",
      "Epoch: 46, Loss: 177.62437, Residuals: -2.15309, Convergence: 0.009770\n",
      "Epoch: 47, Loss: 175.94837, Residuals: -2.12116, Convergence: 0.009526\n",
      "Epoch: 48, Loss: 174.33308, Residuals: -2.08965, Convergence: 0.009266\n",
      "Epoch: 49, Loss: 172.77957, Residuals: -2.05859, Convergence: 0.008991\n",
      "Epoch: 50, Loss: 171.28841, Residuals: -2.02806, Convergence: 0.008706\n",
      "Epoch: 51, Loss: 169.85958, Residuals: -1.99808, Convergence: 0.008412\n",
      "Epoch: 52, Loss: 168.49229, Residuals: -1.96870, Convergence: 0.008115\n",
      "Epoch: 53, Loss: 167.18512, Residuals: -1.93994, Convergence: 0.007819\n",
      "Epoch: 54, Loss: 165.93616, Residuals: -1.91183, Convergence: 0.007527\n",
      "Epoch: 55, Loss: 164.74320, Residuals: -1.88438, Convergence: 0.007241\n",
      "Epoch: 56, Loss: 163.60389, Residuals: -1.85760, Convergence: 0.006964\n",
      "Epoch: 57, Loss: 162.51586, Residuals: -1.83150, Convergence: 0.006695\n",
      "Epoch: 58, Loss: 161.47675, Residuals: -1.80609, Convergence: 0.006435\n",
      "Epoch: 59, Loss: 160.48426, Residuals: -1.78137, Convergence: 0.006184\n",
      "Epoch: 60, Loss: 159.53610, Residuals: -1.75735, Convergence: 0.005943\n",
      "Epoch: 61, Loss: 158.62999, Residuals: -1.73403, Convergence: 0.005712\n",
      "Epoch: 62, Loss: 157.76360, Residuals: -1.71141, Convergence: 0.005492\n",
      "Epoch: 63, Loss: 156.93462, Residuals: -1.68947, Convergence: 0.005282\n",
      "Epoch: 64, Loss: 156.14077, Residuals: -1.66820, Convergence: 0.005084\n",
      "Epoch: 65, Loss: 155.37987, Residuals: -1.64759, Convergence: 0.004897\n",
      "Epoch: 66, Loss: 154.64992, Residuals: -1.62761, Convergence: 0.004720\n",
      "Epoch: 67, Loss: 153.94915, Residuals: -1.60824, Convergence: 0.004552\n",
      "Epoch: 68, Loss: 153.27610, Residuals: -1.58947, Convergence: 0.004391\n",
      "Epoch: 69, Loss: 152.62957, Residuals: -1.57127, Convergence: 0.004236\n",
      "Epoch: 70, Loss: 152.00861, Residuals: -1.55364, Convergence: 0.004085\n",
      "Epoch: 71, Loss: 151.41244, Residuals: -1.53657, Convergence: 0.003937\n",
      "Epoch: 72, Loss: 150.84044, Residuals: -1.52004, Convergence: 0.003792\n",
      "Epoch: 73, Loss: 150.29208, Residuals: -1.50405, Convergence: 0.003649\n",
      "Epoch: 74, Loss: 149.76683, Residuals: -1.48860, Convergence: 0.003507\n",
      "Epoch: 75, Loss: 149.26421, Residuals: -1.47368, Convergence: 0.003367\n",
      "Epoch: 76, Loss: 148.78372, Residuals: -1.45927, Convergence: 0.003229\n",
      "Epoch: 77, Loss: 148.32482, Residuals: -1.44538, Convergence: 0.003094\n",
      "Epoch: 78, Loss: 147.88693, Residuals: -1.43200, Convergence: 0.002961\n",
      "Epoch: 79, Loss: 147.46945, Residuals: -1.41912, Convergence: 0.002831\n",
      "Epoch: 80, Loss: 147.07175, Residuals: -1.40672, Convergence: 0.002704\n",
      "Epoch: 81, Loss: 146.69317, Residuals: -1.39480, Convergence: 0.002581\n",
      "Epoch: 82, Loss: 146.33302, Residuals: -1.38336, Convergence: 0.002461\n",
      "Epoch: 83, Loss: 145.99059, Residuals: -1.37237, Convergence: 0.002346\n",
      "Epoch: 84, Loss: 145.66521, Residuals: -1.36182, Convergence: 0.002234\n",
      "Epoch: 85, Loss: 145.35616, Residuals: -1.35171, Convergence: 0.002126\n",
      "Epoch: 86, Loss: 145.06278, Residuals: -1.34203, Convergence: 0.002022\n",
      "Epoch: 87, Loss: 144.78442, Residuals: -1.33275, Convergence: 0.001923\n",
      "Epoch: 88, Loss: 144.52043, Residuals: -1.32387, Convergence: 0.001827\n",
      "Epoch: 89, Loss: 144.27022, Residuals: -1.31537, Convergence: 0.001734\n",
      "Epoch: 90, Loss: 144.03321, Residuals: -1.30725, Convergence: 0.001645\n",
      "Epoch: 91, Loss: 143.80889, Residuals: -1.29949, Convergence: 0.001560\n",
      "Epoch: 92, Loss: 143.59672, Residuals: -1.29209, Convergence: 0.001477\n",
      "Epoch: 93, Loss: 143.39625, Residuals: -1.28503, Convergence: 0.001398\n",
      "Epoch: 94, Loss: 143.20702, Residuals: -1.27830, Convergence: 0.001321\n",
      "Epoch: 95, Loss: 143.02858, Residuals: -1.27191, Convergence: 0.001248\n",
      "Epoch: 96, Loss: 142.86050, Residuals: -1.26584, Convergence: 0.001177\n",
      "Epoch: 97, Loss: 142.70233, Residuals: -1.26008, Convergence: 0.001108\n",
      "Epoch: 98, Loss: 142.55358, Residuals: -1.25463, Convergence: 0.001043\n",
      "Epoch: 99, Loss: 142.41371, Residuals: -1.24949, Convergence: 0.000982\n",
      "Evidence -183.868\n",
      "\n",
      "Epoch: 99, Evidence: -183.86806, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.24e-01\n",
      "Epoch: 99, Loss: 1371.02483, Residuals: -1.24949, Convergence:   inf\n",
      "Epoch: 100, Loss: 1308.16797, Residuals: -1.27816, Convergence: 0.048050\n",
      "Epoch: 101, Loss: 1260.45177, Residuals: -1.30042, Convergence: 0.037856\n",
      "Epoch: 102, Loss: 1224.42023, Residuals: -1.31584, Convergence: 0.029427\n",
      "Epoch: 103, Loss: 1196.28138, Residuals: -1.32609, Convergence: 0.023522\n",
      "Epoch: 104, Loss: 1173.40394, Residuals: -1.33324, Convergence: 0.019497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 105, Loss: 1154.32136, Residuals: -1.33838, Convergence: 0.016531\n",
      "Epoch: 106, Loss: 1138.15167, Residuals: -1.34199, Convergence: 0.014207\n",
      "Epoch: 107, Loss: 1124.29047, Residuals: -1.34434, Convergence: 0.012329\n",
      "Epoch: 108, Loss: 1112.28691, Residuals: -1.34560, Convergence: 0.010792\n",
      "Epoch: 109, Loss: 1101.78836, Residuals: -1.34590, Convergence: 0.009529\n",
      "Epoch: 110, Loss: 1092.50897, Residuals: -1.34533, Convergence: 0.008494\n",
      "Epoch: 111, Loss: 1084.21308, Residuals: -1.34398, Convergence: 0.007652\n",
      "Epoch: 112, Loss: 1076.70065, Residuals: -1.34190, Convergence: 0.006977\n",
      "Epoch: 113, Loss: 1069.79934, Residuals: -1.33912, Convergence: 0.006451\n",
      "Epoch: 114, Loss: 1063.35572, Residuals: -1.33567, Convergence: 0.006060\n",
      "Epoch: 115, Loss: 1057.23321, Residuals: -1.33155, Convergence: 0.005791\n",
      "Epoch: 116, Loss: 1051.31061, Residuals: -1.32676, Convergence: 0.005634\n",
      "Epoch: 117, Loss: 1045.48762, Residuals: -1.32130, Convergence: 0.005570\n",
      "Epoch: 118, Loss: 1039.69394, Residuals: -1.31520, Convergence: 0.005572\n",
      "Epoch: 119, Loss: 1033.90590, Residuals: -1.30851, Convergence: 0.005598\n",
      "Epoch: 120, Loss: 1028.15256, Residuals: -1.30133, Convergence: 0.005596\n",
      "Epoch: 121, Loss: 1022.50755, Residuals: -1.29375, Convergence: 0.005521\n",
      "Epoch: 122, Loss: 1017.06011, Residuals: -1.28587, Convergence: 0.005356\n",
      "Epoch: 123, Loss: 1011.88472, Residuals: -1.27779, Convergence: 0.005115\n",
      "Epoch: 124, Loss: 1007.02285, Residuals: -1.26959, Convergence: 0.004828\n",
      "Epoch: 125, Loss: 1002.48389, Residuals: -1.26134, Convergence: 0.004528\n",
      "Epoch: 126, Loss: 998.25560, Residuals: -1.25310, Convergence: 0.004236\n",
      "Epoch: 127, Loss: 994.31455, Residuals: -1.24493, Convergence: 0.003964\n",
      "Epoch: 128, Loss: 990.63405, Residuals: -1.23687, Convergence: 0.003715\n",
      "Epoch: 129, Loss: 987.18834, Residuals: -1.22896, Convergence: 0.003490\n",
      "Epoch: 130, Loss: 983.95351, Residuals: -1.22123, Convergence: 0.003288\n",
      "Epoch: 131, Loss: 980.90968, Residuals: -1.21370, Convergence: 0.003103\n",
      "Epoch: 132, Loss: 978.03923, Residuals: -1.20638, Convergence: 0.002935\n",
      "Epoch: 133, Loss: 975.32801, Residuals: -1.19931, Convergence: 0.002780\n",
      "Epoch: 134, Loss: 972.76263, Residuals: -1.19249, Convergence: 0.002637\n",
      "Epoch: 135, Loss: 970.33231, Residuals: -1.18592, Convergence: 0.002505\n",
      "Epoch: 136, Loss: 968.02761, Residuals: -1.17961, Convergence: 0.002381\n",
      "Epoch: 137, Loss: 965.83941, Residuals: -1.17356, Convergence: 0.002266\n",
      "Epoch: 138, Loss: 963.75999, Residuals: -1.16777, Convergence: 0.002158\n",
      "Epoch: 139, Loss: 961.78137, Residuals: -1.16224, Convergence: 0.002057\n",
      "Epoch: 140, Loss: 959.89703, Residuals: -1.15697, Convergence: 0.001963\n",
      "Epoch: 141, Loss: 958.10002, Residuals: -1.15193, Convergence: 0.001876\n",
      "Epoch: 142, Loss: 956.38412, Residuals: -1.14714, Convergence: 0.001794\n",
      "Epoch: 143, Loss: 954.74372, Residuals: -1.14257, Convergence: 0.001718\n",
      "Epoch: 144, Loss: 953.17298, Residuals: -1.13821, Convergence: 0.001648\n",
      "Epoch: 145, Loss: 951.66679, Residuals: -1.13406, Convergence: 0.001583\n",
      "Epoch: 146, Loss: 950.21992, Residuals: -1.13011, Convergence: 0.001523\n",
      "Epoch: 147, Loss: 948.82779, Residuals: -1.12634, Convergence: 0.001467\n",
      "Epoch: 148, Loss: 947.48644, Residuals: -1.12275, Convergence: 0.001416\n",
      "Epoch: 149, Loss: 946.19102, Residuals: -1.11932, Convergence: 0.001369\n",
      "Epoch: 150, Loss: 944.93819, Residuals: -1.11605, Convergence: 0.001326\n",
      "Epoch: 151, Loss: 943.72412, Residuals: -1.11292, Convergence: 0.001286\n",
      "Epoch: 152, Loss: 942.54519, Residuals: -1.10992, Convergence: 0.001251\n",
      "Epoch: 153, Loss: 941.39793, Residuals: -1.10704, Convergence: 0.001219\n",
      "Epoch: 154, Loss: 940.27967, Residuals: -1.10428, Convergence: 0.001189\n",
      "Epoch: 155, Loss: 939.18697, Residuals: -1.10163, Convergence: 0.001163\n",
      "Epoch: 156, Loss: 938.11699, Residuals: -1.09908, Convergence: 0.001141\n",
      "Epoch: 157, Loss: 937.06715, Residuals: -1.09661, Convergence: 0.001120\n",
      "Epoch: 158, Loss: 936.03419, Residuals: -1.09423, Convergence: 0.001104\n",
      "Epoch: 159, Loss: 935.01534, Residuals: -1.09192, Convergence: 0.001090\n",
      "Epoch: 160, Loss: 934.00791, Residuals: -1.08968, Convergence: 0.001079\n",
      "Epoch: 161, Loss: 933.00940, Residuals: -1.08749, Convergence: 0.001070\n",
      "Epoch: 162, Loss: 932.01703, Residuals: -1.08536, Convergence: 0.001065\n",
      "Epoch: 163, Loss: 931.02852, Residuals: -1.08327, Convergence: 0.001062\n",
      "Epoch: 164, Loss: 930.04132, Residuals: -1.08122, Convergence: 0.001061\n",
      "Epoch: 165, Loss: 929.05365, Residuals: -1.07921, Convergence: 0.001063\n",
      "Epoch: 166, Loss: 928.06431, Residuals: -1.07722, Convergence: 0.001066\n",
      "Epoch: 167, Loss: 927.07237, Residuals: -1.07526, Convergence: 0.001070\n",
      "Epoch: 168, Loss: 926.07749, Residuals: -1.07332, Convergence: 0.001074\n",
      "Epoch: 169, Loss: 925.08078, Residuals: -1.07140, Convergence: 0.001077\n",
      "Epoch: 170, Loss: 924.08368, Residuals: -1.06949, Convergence: 0.001079\n",
      "Epoch: 171, Loss: 923.08882, Residuals: -1.06760, Convergence: 0.001078\n",
      "Epoch: 172, Loss: 922.09950, Residuals: -1.06574, Convergence: 0.001073\n",
      "Epoch: 173, Loss: 921.11989, Residuals: -1.06389, Convergence: 0.001063\n",
      "Epoch: 174, Loss: 920.15535, Residuals: -1.06208, Convergence: 0.001048\n",
      "Epoch: 175, Loss: 919.21007, Residuals: -1.06029, Convergence: 0.001028\n",
      "Epoch: 176, Loss: 918.28989, Residuals: -1.05855, Convergence: 0.001002\n",
      "Epoch: 177, Loss: 917.39879, Residuals: -1.05685, Convergence: 0.000971\n",
      "Evidence 11118.583\n",
      "\n",
      "Epoch: 177, Evidence: 11118.58301, Convergence: 1.016537\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.74e-01\n",
      "Epoch: 177, Loss: 2335.56250, Residuals: -1.05685, Convergence:   inf\n",
      "Epoch: 178, Loss: 2298.41176, Residuals: -1.06532, Convergence: 0.016164\n",
      "Epoch: 179, Loss: 2272.93293, Residuals: -1.06496, Convergence: 0.011210\n",
      "Epoch: 180, Loss: 2251.75919, Residuals: -1.06380, Convergence: 0.009403\n",
      "Epoch: 181, Loss: 2233.95671, Residuals: -1.06240, Convergence: 0.007969\n",
      "Epoch: 182, Loss: 2218.86163, Residuals: -1.06086, Convergence: 0.006803\n",
      "Epoch: 183, Loss: 2205.95396, Residuals: -1.05923, Convergence: 0.005851\n",
      "Epoch: 184, Loss: 2194.80643, Residuals: -1.05749, Convergence: 0.005079\n",
      "Epoch: 185, Loss: 2185.06539, Residuals: -1.05565, Convergence: 0.004458\n",
      "Epoch: 186, Loss: 2176.43620, Residuals: -1.05368, Convergence: 0.003965\n",
      "Epoch: 187, Loss: 2168.68377, Residuals: -1.05157, Convergence: 0.003575\n",
      "Epoch: 188, Loss: 2161.62925, Residuals: -1.04929, Convergence: 0.003264\n",
      "Epoch: 189, Loss: 2155.14648, Residuals: -1.04685, Convergence: 0.003008\n",
      "Epoch: 190, Loss: 2149.15871, Residuals: -1.04427, Convergence: 0.002786\n",
      "Epoch: 191, Loss: 2143.62107, Residuals: -1.04160, Convergence: 0.002583\n",
      "Epoch: 192, Loss: 2138.50277, Residuals: -1.03889, Convergence: 0.002393\n",
      "Epoch: 193, Loss: 2133.78145, Residuals: -1.03618, Convergence: 0.002213\n",
      "Epoch: 194, Loss: 2129.43460, Residuals: -1.03350, Convergence: 0.002041\n",
      "Epoch: 195, Loss: 2125.43887, Residuals: -1.03089, Convergence: 0.001880\n",
      "Epoch: 196, Loss: 2121.76990, Residuals: -1.02837, Convergence: 0.001729\n",
      "Epoch: 197, Loss: 2118.40337, Residuals: -1.02594, Convergence: 0.001589\n",
      "Epoch: 198, Loss: 2115.31236, Residuals: -1.02363, Convergence: 0.001461\n",
      "Epoch: 199, Loss: 2112.47246, Residuals: -1.02142, Convergence: 0.001344\n",
      "Epoch: 200, Loss: 2109.85859, Residuals: -1.01931, Convergence: 0.001239\n",
      "Epoch: 201, Loss: 2107.44816, Residuals: -1.01732, Convergence: 0.001144\n",
      "Epoch: 202, Loss: 2105.21971, Residuals: -1.01542, Convergence: 0.001059\n",
      "Epoch: 203, Loss: 2103.15383, Residuals: -1.01362, Convergence: 0.000982\n",
      "Evidence 14258.883\n",
      "\n",
      "Epoch: 203, Evidence: 14258.88281, Convergence: 0.220235\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.37e-01\n",
      "Epoch: 203, Loss: 2467.05491, Residuals: -1.01362, Convergence:   inf\n",
      "Epoch: 204, Loss: 2454.05830, Residuals: -1.01129, Convergence: 0.005296\n",
      "Epoch: 205, Loss: 2443.48496, Residuals: -1.00831, Convergence: 0.004327\n",
      "Epoch: 206, Loss: 2434.38029, Residuals: -1.00534, Convergence: 0.003740\n",
      "Epoch: 207, Loss: 2426.47319, Residuals: -1.00248, Convergence: 0.003259\n",
      "Epoch: 208, Loss: 2419.56069, Residuals: -0.99978, Convergence: 0.002857\n",
      "Epoch: 209, Loss: 2413.47932, Residuals: -0.99725, Convergence: 0.002520\n",
      "Epoch: 210, Loss: 2408.09970, Residuals: -0.99489, Convergence: 0.002234\n",
      "Epoch: 211, Loss: 2403.31319, Residuals: -0.99269, Convergence: 0.001992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 212, Loss: 2399.03299, Residuals: -0.99064, Convergence: 0.001784\n",
      "Epoch: 213, Loss: 2395.18389, Residuals: -0.98872, Convergence: 0.001607\n",
      "Epoch: 214, Loss: 2391.70542, Residuals: -0.98693, Convergence: 0.001454\n",
      "Epoch: 215, Loss: 2388.54593, Residuals: -0.98526, Convergence: 0.001323\n",
      "Epoch: 216, Loss: 2385.66190, Residuals: -0.98370, Convergence: 0.001209\n",
      "Epoch: 217, Loss: 2383.01789, Residuals: -0.98225, Convergence: 0.001110\n",
      "Epoch: 218, Loss: 2380.58304, Residuals: -0.98089, Convergence: 0.001023\n",
      "Epoch: 219, Loss: 2378.33070, Residuals: -0.97963, Convergence: 0.000947\n",
      "Evidence 14618.979\n",
      "\n",
      "Epoch: 219, Evidence: 14618.97949, Convergence: 0.024632\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.35e-01\n",
      "Epoch: 219, Loss: 2472.10001, Residuals: -0.97963, Convergence:   inf\n",
      "Epoch: 220, Loss: 2465.80971, Residuals: -0.97638, Convergence: 0.002551\n",
      "Epoch: 221, Loss: 2460.56983, Residuals: -0.97344, Convergence: 0.002130\n",
      "Epoch: 222, Loss: 2456.09536, Residuals: -0.97087, Convergence: 0.001822\n",
      "Epoch: 223, Loss: 2452.22251, Residuals: -0.96862, Convergence: 0.001579\n",
      "Epoch: 224, Loss: 2448.83022, Residuals: -0.96665, Convergence: 0.001385\n",
      "Epoch: 225, Loss: 2445.82526, Residuals: -0.96493, Convergence: 0.001229\n",
      "Epoch: 226, Loss: 2443.13520, Residuals: -0.96342, Convergence: 0.001101\n",
      "Epoch: 227, Loss: 2440.70415, Residuals: -0.96211, Convergence: 0.000996\n",
      "Evidence 14699.005\n",
      "\n",
      "Epoch: 227, Evidence: 14699.00488, Convergence: 0.005444\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.64e-01\n",
      "Epoch: 227, Loss: 2473.73185, Residuals: -0.96211, Convergence:   inf\n",
      "Epoch: 228, Loss: 2469.82208, Residuals: -0.95956, Convergence: 0.001583\n",
      "Epoch: 229, Loss: 2466.53419, Residuals: -0.95745, Convergence: 0.001333\n",
      "Epoch: 230, Loss: 2463.69819, Residuals: -0.95570, Convergence: 0.001151\n",
      "Epoch: 231, Loss: 2461.20834, Residuals: -0.95423, Convergence: 0.001012\n",
      "Epoch: 232, Loss: 2458.99025, Residuals: -0.95302, Convergence: 0.000902\n",
      "Evidence 14729.312\n",
      "\n",
      "Epoch: 232, Evidence: 14729.31250, Convergence: 0.002058\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.13e-01\n",
      "Epoch: 232, Loss: 2474.61674, Residuals: -0.95302, Convergence:   inf\n",
      "Epoch: 233, Loss: 2471.78789, Residuals: -0.95105, Convergence: 0.001144\n",
      "Epoch: 234, Loss: 2469.38384, Residuals: -0.94948, Convergence: 0.000974\n",
      "Evidence 14742.013\n",
      "\n",
      "Epoch: 234, Evidence: 14742.01270, Convergence: 0.000861\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.77e-01\n",
      "Epoch: 234, Loss: 2475.33689, Residuals: -0.94948, Convergence:   inf\n",
      "Epoch: 235, Loss: 2470.76914, Residuals: -0.94666, Convergence: 0.001849\n",
      "Epoch: 236, Loss: 2467.25490, Residuals: -0.94476, Convergence: 0.001424\n",
      "Epoch: 237, Loss: 2464.39749, Residuals: -0.94356, Convergence: 0.001159\n",
      "Epoch: 238, Loss: 2461.97187, Residuals: -0.94294, Convergence: 0.000985\n",
      "Evidence 14760.052\n",
      "\n",
      "Epoch: 238, Evidence: 14760.05176, Convergence: 0.002083\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.47e-01\n",
      "Epoch: 238, Loss: 2475.45276, Residuals: -0.94294, Convergence:   inf\n",
      "Epoch: 239, Loss: 2472.34701, Residuals: -0.94064, Convergence: 0.001256\n",
      "Epoch: 240, Loss: 2469.88831, Residuals: -0.93957, Convergence: 0.000995\n",
      "Evidence 14771.266\n",
      "\n",
      "Epoch: 240, Evidence: 14771.26562, Convergence: 0.000759\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.25e-01\n",
      "Epoch: 240, Loss: 2475.65557, Residuals: -0.93957, Convergence:   inf\n",
      "Epoch: 241, Loss: 2471.05809, Residuals: -0.93646, Convergence: 0.001861\n",
      "Epoch: 242, Loss: 2467.83064, Residuals: -0.93788, Convergence: 0.001308\n",
      "Epoch: 243, Loss: 2465.17637, Residuals: -0.93887, Convergence: 0.001077\n",
      "Epoch: 244, Loss: 2462.85702, Residuals: -0.94160, Convergence: 0.000942\n",
      "Evidence 14787.418\n",
      "\n",
      "Epoch: 244, Evidence: 14787.41797, Convergence: 0.001851\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.11e-01\n",
      "Epoch: 244, Loss: 2474.90731, Residuals: -0.94160, Convergence:   inf\n",
      "Epoch: 245, Loss: 2473.09102, Residuals: -0.93998, Convergence: 0.000734\n",
      "Evidence 14794.335\n",
      "\n",
      "Epoch: 245, Evidence: 14794.33496, Convergence: 0.000468\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 9.07e-02\n",
      "Epoch: 245, Loss: 2475.81532, Residuals: -0.93998, Convergence:   inf\n",
      "Epoch: 246, Loss: 2521.59031, Residuals: -0.98121, Convergence: -0.018153\n",
      "Epoch: 246, Loss: 2473.35490, Residuals: -0.93753, Convergence: 0.000995\n",
      "Evidence 14798.838\n",
      "\n",
      "Epoch: 246, Evidence: 14798.83789, Convergence: 0.000772\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.34e-02\n",
      "Epoch: 246, Loss: 2475.30485, Residuals: -0.93753, Convergence:   inf\n",
      "Epoch: 247, Loss: 2478.90661, Residuals: -0.94079, Convergence: -0.001453\n",
      "Epoch: 247, Loss: 2474.95265, Residuals: -0.93691, Convergence: 0.000142\n",
      "Evidence 14800.770\n",
      "\n",
      "Epoch: 247, Evidence: 14800.76953, Convergence: 0.000902\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 382.31156, Residuals: -4.52519, Convergence:   inf\n",
      "Epoch: 1, Loss: 356.56064, Residuals: -4.40453, Convergence: 0.072220\n",
      "Epoch: 2, Loss: 335.37600, Residuals: -4.23830, Convergence: 0.063167\n",
      "Epoch: 3, Loss: 319.23595, Residuals: -4.07202, Convergence: 0.050558\n",
      "Epoch: 4, Loss: 306.92566, Residuals: -3.92553, Convergence: 0.040108\n",
      "Epoch: 5, Loss: 297.15585, Residuals: -3.79601, Convergence: 0.032878\n",
      "Epoch: 6, Loss: 289.21996, Residuals: -3.68329, Convergence: 0.027439\n",
      "Epoch: 7, Loss: 282.62864, Residuals: -3.58693, Convergence: 0.023321\n",
      "Epoch: 8, Loss: 277.01979, Residuals: -3.50483, Convergence: 0.020247\n",
      "Epoch: 9, Loss: 272.13830, Residuals: -3.43457, Convergence: 0.017938\n",
      "Epoch: 10, Loss: 267.80310, Residuals: -3.37400, Convergence: 0.016188\n",
      "Epoch: 11, Loss: 263.88342, Residuals: -3.32128, Convergence: 0.014854\n",
      "Epoch: 12, Loss: 260.28414, Residuals: -3.27488, Convergence: 0.013828\n",
      "Epoch: 13, Loss: 256.93635, Residuals: -3.23348, Convergence: 0.013030\n",
      "Epoch: 14, Loss: 253.79027, Residuals: -3.19592, Convergence: 0.012396\n",
      "Epoch: 15, Loss: 250.81050, Residuals: -3.16123, Convergence: 0.011881\n",
      "Epoch: 16, Loss: 247.97367, Residuals: -3.12864, Convergence: 0.011440\n",
      "Epoch: 17, Loss: 245.26409, Residuals: -3.09768, Convergence: 0.011048\n",
      "Epoch: 18, Loss: 242.66379, Residuals: -3.06796, Convergence: 0.010716\n",
      "Epoch: 19, Loss: 240.14579, Residuals: -3.03904, Convergence: 0.010485\n",
      "Epoch: 20, Loss: 237.67713, Residuals: -3.01040, Convergence: 0.010387\n",
      "Epoch: 21, Loss: 235.22691, Residuals: -2.98154, Convergence: 0.010416\n",
      "Epoch: 22, Loss: 232.77305, Residuals: -2.95210, Convergence: 0.010542\n",
      "Epoch: 23, Loss: 230.29988, Residuals: -2.92188, Convergence: 0.010739\n",
      "Epoch: 24, Loss: 227.78290, Residuals: -2.89061, Convergence: 0.011050\n",
      "Epoch: 25, Loss: 225.17699, Residuals: -2.85779, Convergence: 0.011573\n",
      "Epoch: 26, Loss: 222.43303, Residuals: -2.82281, Convergence: 0.012336\n",
      "Epoch: 27, Loss: 219.57257, Residuals: -2.78582, Convergence: 0.013027\n",
      "Epoch: 28, Loss: 216.72174, Residuals: -2.74818, Convergence: 0.013154\n",
      "Epoch: 29, Loss: 213.96999, Residuals: -2.71103, Convergence: 0.012860\n",
      "Epoch: 30, Loss: 211.32669, Residuals: -2.67463, Convergence: 0.012508\n",
      "Epoch: 31, Loss: 208.77638, Residuals: -2.63895, Convergence: 0.012216\n",
      "Epoch: 32, Loss: 206.30304, Residuals: -2.60387, Convergence: 0.011989\n",
      "Epoch: 33, Loss: 203.89451, Residuals: -2.56933, Convergence: 0.011813\n",
      "Epoch: 34, Loss: 201.54236, Residuals: -2.53524, Convergence: 0.011671\n",
      "Epoch: 35, Loss: 199.24104, Residuals: -2.50155, Convergence: 0.011550\n",
      "Epoch: 36, Loss: 196.98706, Residuals: -2.46819, Convergence: 0.011442\n",
      "Epoch: 37, Loss: 194.77837, Residuals: -2.43512, Convergence: 0.011340\n",
      "Epoch: 38, Loss: 192.61390, Residuals: -2.40229, Convergence: 0.011237\n",
      "Epoch: 39, Loss: 190.49331, Residuals: -2.36966, Convergence: 0.011132\n",
      "Epoch: 40, Loss: 188.41687, Residuals: -2.33717, Convergence: 0.011020\n",
      "Epoch: 41, Loss: 186.38531, Residuals: -2.30480, Convergence: 0.010900\n",
      "Epoch: 42, Loss: 184.39976, Residuals: -2.27252, Convergence: 0.010768\n",
      "Epoch: 43, Loss: 182.46164, Residuals: -2.24033, Convergence: 0.010622\n",
      "Epoch: 44, Loss: 180.57259, Residuals: -2.20824, Convergence: 0.010461\n",
      "Epoch: 45, Loss: 178.73441, Residuals: -2.17626, Convergence: 0.010284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46, Loss: 176.94910, Residuals: -2.14443, Convergence: 0.010089\n",
      "Epoch: 47, Loss: 175.21883, Residuals: -2.11281, Convergence: 0.009875\n",
      "Epoch: 48, Loss: 173.54592, Residuals: -2.08144, Convergence: 0.009640\n",
      "Epoch: 49, Loss: 171.93265, Residuals: -2.05039, Convergence: 0.009383\n",
      "Epoch: 50, Loss: 170.38098, Residuals: -2.01972, Convergence: 0.009107\n",
      "Epoch: 51, Loss: 168.89233, Residuals: -1.98949, Convergence: 0.008814\n",
      "Epoch: 52, Loss: 167.46736, Residuals: -1.95978, Convergence: 0.008509\n",
      "Epoch: 53, Loss: 166.10592, Residuals: -1.93062, Convergence: 0.008196\n",
      "Epoch: 54, Loss: 164.80706, Residuals: -1.90206, Convergence: 0.007881\n",
      "Epoch: 55, Loss: 163.56921, Residuals: -1.87413, Convergence: 0.007568\n",
      "Epoch: 56, Loss: 162.39025, Residuals: -1.84686, Convergence: 0.007260\n",
      "Epoch: 57, Loss: 161.26760, Residuals: -1.82027, Convergence: 0.006961\n",
      "Epoch: 58, Loss: 160.19845, Residuals: -1.79436, Convergence: 0.006674\n",
      "Epoch: 59, Loss: 159.17984, Residuals: -1.76913, Convergence: 0.006399\n",
      "Epoch: 60, Loss: 158.20876, Residuals: -1.74458, Convergence: 0.006138\n",
      "Epoch: 61, Loss: 157.28230, Residuals: -1.72069, Convergence: 0.005890\n",
      "Epoch: 62, Loss: 156.39775, Residuals: -1.69747, Convergence: 0.005656\n",
      "Epoch: 63, Loss: 155.55266, Residuals: -1.67489, Convergence: 0.005433\n",
      "Epoch: 64, Loss: 154.74484, Residuals: -1.65294, Convergence: 0.005220\n",
      "Epoch: 65, Loss: 153.97241, Residuals: -1.63162, Convergence: 0.005017\n",
      "Epoch: 66, Loss: 153.23378, Residuals: -1.61092, Convergence: 0.004820\n",
      "Epoch: 67, Loss: 152.52753, Residuals: -1.59084, Convergence: 0.004630\n",
      "Epoch: 68, Loss: 151.85247, Residuals: -1.57137, Convergence: 0.004446\n",
      "Epoch: 69, Loss: 151.20749, Residuals: -1.55252, Convergence: 0.004266\n",
      "Epoch: 70, Loss: 150.59160, Residuals: -1.53427, Convergence: 0.004090\n",
      "Epoch: 71, Loss: 150.00385, Residuals: -1.51663, Convergence: 0.003918\n",
      "Epoch: 72, Loss: 149.44332, Residuals: -1.49959, Convergence: 0.003751\n",
      "Epoch: 73, Loss: 148.90910, Residuals: -1.48315, Convergence: 0.003588\n",
      "Epoch: 74, Loss: 148.40028, Residuals: -1.46730, Convergence: 0.003429\n",
      "Epoch: 75, Loss: 147.91595, Residuals: -1.45203, Convergence: 0.003274\n",
      "Epoch: 76, Loss: 147.45518, Residuals: -1.43735, Convergence: 0.003125\n",
      "Epoch: 77, Loss: 147.01704, Residuals: -1.42322, Convergence: 0.002980\n",
      "Epoch: 78, Loss: 146.60061, Residuals: -1.40966, Convergence: 0.002841\n",
      "Epoch: 79, Loss: 146.20495, Residuals: -1.39663, Convergence: 0.002706\n",
      "Epoch: 80, Loss: 145.82915, Residuals: -1.38412, Convergence: 0.002577\n",
      "Epoch: 81, Loss: 145.47232, Residuals: -1.37213, Convergence: 0.002453\n",
      "Epoch: 82, Loss: 145.13355, Residuals: -1.36064, Convergence: 0.002334\n",
      "Epoch: 83, Loss: 144.81201, Residuals: -1.34962, Convergence: 0.002220\n",
      "Epoch: 84, Loss: 144.50685, Residuals: -1.33907, Convergence: 0.002112\n",
      "Epoch: 85, Loss: 144.21730, Residuals: -1.32896, Convergence: 0.002008\n",
      "Epoch: 86, Loss: 143.94257, Residuals: -1.31928, Convergence: 0.001909\n",
      "Epoch: 87, Loss: 143.68194, Residuals: -1.31002, Convergence: 0.001814\n",
      "Epoch: 88, Loss: 143.43473, Residuals: -1.30115, Convergence: 0.001724\n",
      "Epoch: 89, Loss: 143.20029, Residuals: -1.29266, Convergence: 0.001637\n",
      "Epoch: 90, Loss: 142.97799, Residuals: -1.28454, Convergence: 0.001555\n",
      "Epoch: 91, Loss: 142.76728, Residuals: -1.27676, Convergence: 0.001476\n",
      "Epoch: 92, Loss: 142.56761, Residuals: -1.26932, Convergence: 0.001401\n",
      "Epoch: 93, Loss: 142.37849, Residuals: -1.26220, Convergence: 0.001328\n",
      "Epoch: 94, Loss: 142.19945, Residuals: -1.25538, Convergence: 0.001259\n",
      "Epoch: 95, Loss: 142.03008, Residuals: -1.24886, Convergence: 0.001193\n",
      "Epoch: 96, Loss: 141.86999, Residuals: -1.24262, Convergence: 0.001128\n",
      "Epoch: 97, Loss: 141.71881, Residuals: -1.23665, Convergence: 0.001067\n",
      "Epoch: 98, Loss: 141.57623, Residuals: -1.23094, Convergence: 0.001007\n",
      "Epoch: 99, Loss: 141.44194, Residuals: -1.22548, Convergence: 0.000949\n",
      "Evidence -183.165\n",
      "\n",
      "Epoch: 99, Evidence: -183.16550, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.25e-01\n",
      "Epoch: 99, Loss: 1363.60959, Residuals: -1.22548, Convergence:   inf\n",
      "Epoch: 100, Loss: 1303.09794, Residuals: -1.25493, Convergence: 0.046437\n",
      "Epoch: 101, Loss: 1256.40872, Residuals: -1.27790, Convergence: 0.037161\n",
      "Epoch: 102, Loss: 1220.63544, Residuals: -1.29459, Convergence: 0.029307\n",
      "Epoch: 103, Loss: 1192.68889, Residuals: -1.30644, Convergence: 0.023432\n",
      "Epoch: 104, Loss: 1170.14287, Residuals: -1.31516, Convergence: 0.019268\n",
      "Epoch: 105, Loss: 1151.47697, Residuals: -1.32178, Convergence: 0.016210\n",
      "Epoch: 106, Loss: 1135.74927, Residuals: -1.32678, Convergence: 0.013848\n",
      "Epoch: 107, Loss: 1122.32171, Residuals: -1.33038, Convergence: 0.011964\n",
      "Epoch: 108, Loss: 1110.72647, Residuals: -1.33276, Convergence: 0.010439\n",
      "Epoch: 109, Loss: 1100.60112, Residuals: -1.33402, Convergence: 0.009200\n",
      "Epoch: 110, Loss: 1091.65616, Residuals: -1.33427, Convergence: 0.008194\n",
      "Epoch: 111, Loss: 1083.65415, Residuals: -1.33358, Convergence: 0.007384\n",
      "Epoch: 112, Loss: 1076.39638, Residuals: -1.33203, Convergence: 0.006743\n",
      "Epoch: 113, Loss: 1069.71252, Residuals: -1.32967, Convergence: 0.006248\n",
      "Epoch: 114, Loss: 1063.45551, Residuals: -1.32654, Convergence: 0.005884\n",
      "Epoch: 115, Loss: 1057.49641, Residuals: -1.32267, Convergence: 0.005635\n",
      "Epoch: 116, Loss: 1051.72323, Residuals: -1.31807, Convergence: 0.005489\n",
      "Epoch: 117, Loss: 1046.04288, Residuals: -1.31278, Convergence: 0.005430\n",
      "Epoch: 118, Loss: 1040.38704, Residuals: -1.30682, Convergence: 0.005436\n",
      "Epoch: 119, Loss: 1034.72622, Residuals: -1.30026, Convergence: 0.005471\n",
      "Epoch: 120, Loss: 1029.08018, Residuals: -1.29318, Convergence: 0.005486\n",
      "Epoch: 121, Loss: 1023.51627, Residuals: -1.28567, Convergence: 0.005436\n",
      "Epoch: 122, Loss: 1018.12617, Residuals: -1.27785, Convergence: 0.005294\n",
      "Epoch: 123, Loss: 1012.98977, Residuals: -1.26980, Convergence: 0.005071\n",
      "Epoch: 124, Loss: 1008.15421, Residuals: -1.26162, Convergence: 0.004796\n",
      "Epoch: 125, Loss: 1003.63106, Residuals: -1.25338, Convergence: 0.004507\n",
      "Epoch: 126, Loss: 999.40784, Residuals: -1.24515, Convergence: 0.004226\n",
      "Epoch: 127, Loss: 995.46037, Residuals: -1.23698, Convergence: 0.003965\n",
      "Epoch: 128, Loss: 991.76068, Residuals: -1.22892, Convergence: 0.003730\n",
      "Epoch: 129, Loss: 988.28240, Residuals: -1.22101, Convergence: 0.003520\n",
      "Epoch: 130, Loss: 985.00186, Residuals: -1.21326, Convergence: 0.003330\n",
      "Epoch: 131, Loss: 981.89862, Residuals: -1.20571, Convergence: 0.003160\n",
      "Epoch: 132, Loss: 978.95628, Residuals: -1.19836, Convergence: 0.003006\n",
      "Epoch: 133, Loss: 976.16099, Residuals: -1.19123, Convergence: 0.002864\n",
      "Epoch: 134, Loss: 973.50051, Residuals: -1.18434, Convergence: 0.002733\n",
      "Epoch: 135, Loss: 970.96521, Residuals: -1.17769, Convergence: 0.002611\n",
      "Epoch: 136, Loss: 968.54670, Residuals: -1.17128, Convergence: 0.002497\n",
      "Epoch: 137, Loss: 966.23797, Residuals: -1.16511, Convergence: 0.002389\n",
      "Epoch: 138, Loss: 964.03175, Residuals: -1.15919, Convergence: 0.002289\n",
      "Epoch: 139, Loss: 961.92208, Residuals: -1.15351, Convergence: 0.002193\n",
      "Epoch: 140, Loss: 959.90332, Residuals: -1.14806, Convergence: 0.002103\n",
      "Epoch: 141, Loss: 957.96991, Residuals: -1.14285, Convergence: 0.002018\n",
      "Epoch: 142, Loss: 956.11660, Residuals: -1.13786, Convergence: 0.001938\n",
      "Epoch: 143, Loss: 954.33846, Residuals: -1.13308, Convergence: 0.001863\n",
      "Epoch: 144, Loss: 952.63070, Residuals: -1.12851, Convergence: 0.001793\n",
      "Epoch: 145, Loss: 950.98855, Residuals: -1.12415, Convergence: 0.001727\n",
      "Epoch: 146, Loss: 949.40792, Residuals: -1.11997, Convergence: 0.001665\n",
      "Epoch: 147, Loss: 947.88466, Residuals: -1.11597, Convergence: 0.001607\n",
      "Epoch: 148, Loss: 946.41478, Residuals: -1.11213, Convergence: 0.001553\n",
      "Epoch: 149, Loss: 944.99382, Residuals: -1.10846, Convergence: 0.001504\n",
      "Epoch: 150, Loss: 943.61801, Residuals: -1.10494, Convergence: 0.001458\n",
      "Epoch: 151, Loss: 942.28323, Residuals: -1.10156, Convergence: 0.001417\n",
      "Epoch: 152, Loss: 940.98537, Residuals: -1.09830, Convergence: 0.001379\n",
      "Epoch: 153, Loss: 939.72030, Residuals: -1.09516, Convergence: 0.001346\n",
      "Epoch: 154, Loss: 938.48283, Residuals: -1.09213, Convergence: 0.001319\n",
      "Epoch: 155, Loss: 937.26859, Residuals: -1.08919, Convergence: 0.001296\n",
      "Epoch: 156, Loss: 936.07273, Residuals: -1.08634, Convergence: 0.001278\n",
      "Epoch: 157, Loss: 934.89021, Residuals: -1.08355, Convergence: 0.001265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 158, Loss: 933.71675, Residuals: -1.08083, Convergence: 0.001257\n",
      "Epoch: 159, Loss: 932.54919, Residuals: -1.07815, Convergence: 0.001252\n",
      "Epoch: 160, Loss: 931.38504, Residuals: -1.07551, Convergence: 0.001250\n",
      "Epoch: 161, Loss: 930.22422, Residuals: -1.07291, Convergence: 0.001248\n",
      "Epoch: 162, Loss: 929.06857, Residuals: -1.07035, Convergence: 0.001244\n",
      "Epoch: 163, Loss: 927.92173, Residuals: -1.06782, Convergence: 0.001236\n",
      "Epoch: 164, Loss: 926.78893, Residuals: -1.06534, Convergence: 0.001222\n",
      "Epoch: 165, Loss: 925.67614, Residuals: -1.06291, Convergence: 0.001202\n",
      "Epoch: 166, Loss: 924.58896, Residuals: -1.06054, Convergence: 0.001176\n",
      "Epoch: 167, Loss: 923.53259, Residuals: -1.05824, Convergence: 0.001144\n",
      "Epoch: 168, Loss: 922.51058, Residuals: -1.05601, Convergence: 0.001108\n",
      "Epoch: 169, Loss: 921.52603, Residuals: -1.05386, Convergence: 0.001068\n",
      "Epoch: 170, Loss: 920.58068, Residuals: -1.05179, Convergence: 0.001027\n",
      "Epoch: 171, Loss: 919.67513, Residuals: -1.04980, Convergence: 0.000985\n",
      "Evidence 11133.272\n",
      "\n",
      "Epoch: 171, Evidence: 11133.27246, Convergence: 1.016452\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.76e-01\n",
      "Epoch: 171, Loss: 2343.80533, Residuals: -1.04980, Convergence:   inf\n",
      "Epoch: 172, Loss: 2304.25489, Residuals: -1.05777, Convergence: 0.017164\n",
      "Epoch: 173, Loss: 2277.05058, Residuals: -1.05632, Convergence: 0.011947\n",
      "Epoch: 174, Loss: 2254.49267, Residuals: -1.05396, Convergence: 0.010006\n",
      "Epoch: 175, Loss: 2235.53608, Residuals: -1.05140, Convergence: 0.008480\n",
      "Epoch: 176, Loss: 2219.46899, Residuals: -1.04874, Convergence: 0.007239\n",
      "Epoch: 177, Loss: 2205.73870, Residuals: -1.04603, Convergence: 0.006225\n",
      "Epoch: 178, Loss: 2193.89288, Residuals: -1.04328, Convergence: 0.005399\n",
      "Epoch: 179, Loss: 2183.55512, Residuals: -1.04047, Convergence: 0.004734\n",
      "Epoch: 180, Loss: 2174.41434, Residuals: -1.03761, Convergence: 0.004204\n",
      "Epoch: 181, Loss: 2166.22060, Residuals: -1.03466, Convergence: 0.003783\n",
      "Epoch: 182, Loss: 2158.78665, Residuals: -1.03162, Convergence: 0.003444\n",
      "Epoch: 183, Loss: 2151.98436, Residuals: -1.02850, Convergence: 0.003161\n",
      "Epoch: 184, Loss: 2145.73414, Residuals: -1.02531, Convergence: 0.002913\n",
      "Epoch: 185, Loss: 2139.98697, Residuals: -1.02211, Convergence: 0.002686\n",
      "Epoch: 186, Loss: 2134.70349, Residuals: -1.01893, Convergence: 0.002475\n",
      "Epoch: 187, Loss: 2129.84506, Residuals: -1.01581, Convergence: 0.002281\n",
      "Epoch: 188, Loss: 2125.37255, Residuals: -1.01279, Convergence: 0.002104\n",
      "Epoch: 189, Loss: 2121.24468, Residuals: -1.00987, Convergence: 0.001946\n",
      "Epoch: 190, Loss: 2117.42222, Residuals: -1.00706, Convergence: 0.001805\n",
      "Epoch: 191, Loss: 2113.86803, Residuals: -1.00438, Convergence: 0.001681\n",
      "Epoch: 192, Loss: 2110.54979, Residuals: -1.00181, Convergence: 0.001572\n",
      "Epoch: 193, Loss: 2107.43862, Residuals: -0.99936, Convergence: 0.001476\n",
      "Epoch: 194, Loss: 2104.51234, Residuals: -0.99703, Convergence: 0.001390\n",
      "Epoch: 195, Loss: 2101.75097, Residuals: -0.99481, Convergence: 0.001314\n",
      "Epoch: 196, Loss: 2099.14038, Residuals: -0.99270, Convergence: 0.001244\n",
      "Epoch: 197, Loss: 2096.66748, Residuals: -0.99071, Convergence: 0.001179\n",
      "Epoch: 198, Loss: 2094.32347, Residuals: -0.98882, Convergence: 0.001119\n",
      "Epoch: 199, Loss: 2092.10008, Residuals: -0.98703, Convergence: 0.001063\n",
      "Epoch: 200, Loss: 2089.99177, Residuals: -0.98535, Convergence: 0.001009\n",
      "Epoch: 201, Loss: 2087.99259, Residuals: -0.98375, Convergence: 0.000957\n",
      "Evidence 14319.766\n",
      "\n",
      "Epoch: 201, Evidence: 14319.76562, Convergence: 0.222524\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.36e-01\n",
      "Epoch: 201, Loss: 2470.46724, Residuals: -0.98375, Convergence:   inf\n",
      "Epoch: 202, Loss: 2457.03958, Residuals: -0.98053, Convergence: 0.005465\n",
      "Epoch: 203, Loss: 2445.99712, Residuals: -0.97675, Convergence: 0.004515\n",
      "Epoch: 204, Loss: 2436.42339, Residuals: -0.97315, Convergence: 0.003929\n",
      "Epoch: 205, Loss: 2428.08627, Residuals: -0.96979, Convergence: 0.003434\n",
      "Epoch: 206, Loss: 2420.80108, Residuals: -0.96671, Convergence: 0.003009\n",
      "Epoch: 207, Loss: 2414.40947, Residuals: -0.96391, Convergence: 0.002647\n",
      "Epoch: 208, Loss: 2408.78041, Residuals: -0.96139, Convergence: 0.002337\n",
      "Epoch: 209, Loss: 2403.80128, Residuals: -0.95911, Convergence: 0.002071\n",
      "Epoch: 210, Loss: 2399.37540, Residuals: -0.95707, Convergence: 0.001845\n",
      "Epoch: 211, Loss: 2395.42292, Residuals: -0.95524, Convergence: 0.001650\n",
      "Epoch: 212, Loss: 2391.87438, Residuals: -0.95360, Convergence: 0.001484\n",
      "Epoch: 213, Loss: 2388.67301, Residuals: -0.95213, Convergence: 0.001340\n",
      "Epoch: 214, Loss: 2385.76963, Residuals: -0.95081, Convergence: 0.001217\n",
      "Epoch: 215, Loss: 2383.12403, Residuals: -0.94962, Convergence: 0.001110\n",
      "Epoch: 216, Loss: 2380.70154, Residuals: -0.94856, Convergence: 0.001018\n",
      "Epoch: 217, Loss: 2378.47357, Residuals: -0.94761, Convergence: 0.000937\n",
      "Evidence 14700.024\n",
      "\n",
      "Epoch: 217, Evidence: 14700.02441, Convergence: 0.025868\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.33e-01\n",
      "Epoch: 217, Loss: 2475.77123, Residuals: -0.94761, Convergence:   inf\n",
      "Epoch: 218, Loss: 2469.30224, Residuals: -0.94417, Convergence: 0.002620\n",
      "Epoch: 219, Loss: 2463.93156, Residuals: -0.94121, Convergence: 0.002180\n",
      "Epoch: 220, Loss: 2459.37311, Residuals: -0.93876, Convergence: 0.001854\n",
      "Epoch: 221, Loss: 2455.45469, Residuals: -0.93673, Convergence: 0.001596\n",
      "Epoch: 222, Loss: 2452.04395, Residuals: -0.93505, Convergence: 0.001391\n",
      "Epoch: 223, Loss: 2449.04005, Residuals: -0.93366, Convergence: 0.001227\n",
      "Epoch: 224, Loss: 2446.36421, Residuals: -0.93251, Convergence: 0.001094\n",
      "Epoch: 225, Loss: 2443.95620, Residuals: -0.93157, Convergence: 0.000985\n",
      "Evidence 14782.253\n",
      "\n",
      "Epoch: 225, Evidence: 14782.25293, Convergence: 0.005563\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.60e-01\n",
      "Epoch: 225, Loss: 2477.55165, Residuals: -0.93157, Convergence:   inf\n",
      "Epoch: 226, Loss: 2473.64430, Residuals: -0.92899, Convergence: 0.001580\n",
      "Epoch: 227, Loss: 2470.38817, Residuals: -0.92698, Convergence: 0.001318\n",
      "Epoch: 228, Loss: 2467.60062, Residuals: -0.92541, Convergence: 0.001130\n",
      "Epoch: 229, Loss: 2465.16838, Residuals: -0.92418, Convergence: 0.000987\n",
      "Evidence 14810.061\n",
      "\n",
      "Epoch: 229, Evidence: 14810.06055, Convergence: 0.001878\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.10e-01\n",
      "Epoch: 229, Loss: 2478.58757, Residuals: -0.92418, Convergence:   inf\n",
      "Epoch: 230, Loss: 2475.68689, Residuals: -0.92218, Convergence: 0.001172\n",
      "Epoch: 231, Loss: 2473.25630, Residuals: -0.92067, Convergence: 0.000983\n",
      "Evidence 14822.109\n",
      "\n",
      "Epoch: 231, Evidence: 14822.10938, Convergence: 0.000813\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.74e-01\n",
      "Epoch: 231, Loss: 2479.31562, Residuals: -0.92067, Convergence:   inf\n",
      "Epoch: 232, Loss: 2474.80708, Residuals: -0.91798, Convergence: 0.001822\n",
      "Epoch: 233, Loss: 2471.34754, Residuals: -0.91622, Convergence: 0.001400\n",
      "Epoch: 234, Loss: 2468.54209, Residuals: -0.91514, Convergence: 0.001136\n",
      "Epoch: 235, Loss: 2466.15719, Residuals: -0.91462, Convergence: 0.000967\n",
      "Evidence 14839.790\n",
      "\n",
      "Epoch: 235, Evidence: 14839.79004, Convergence: 0.002003\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.44e-01\n",
      "Epoch: 235, Loss: 2479.47126, Residuals: -0.91462, Convergence:   inf\n",
      "Epoch: 236, Loss: 2476.49740, Residuals: -0.91214, Convergence: 0.001201\n",
      "Epoch: 237, Loss: 2474.14519, Residuals: -0.91097, Convergence: 0.000951\n",
      "Evidence 14850.683\n",
      "\n",
      "Epoch: 237, Evidence: 14850.68262, Convergence: 0.000733\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.22e-01\n",
      "Epoch: 237, Loss: 2479.63991, Residuals: -0.91097, Convergence:   inf\n",
      "Epoch: 238, Loss: 2475.35154, Residuals: -0.90740, Convergence: 0.001732\n",
      "Epoch: 239, Loss: 2472.26252, Residuals: -0.90869, Convergence: 0.001249\n",
      "Epoch: 240, Loss: 2469.61865, Residuals: -0.90988, Convergence: 0.001071\n",
      "Epoch: 241, Loss: 2467.25369, Residuals: -0.91259, Convergence: 0.000959\n",
      "Evidence 14866.303\n",
      "\n",
      "Epoch: 241, Evidence: 14866.30273, Convergence: 0.001783\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.08e-01\n",
      "Epoch: 241, Loss: 2478.92815, Residuals: -0.91259, Convergence:   inf\n",
      "Epoch: 242, Loss: 2477.11565, Residuals: -0.90935, Convergence: 0.000732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence 14873.139\n",
      "\n",
      "Epoch: 242, Evidence: 14873.13867, Convergence: 0.000460\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.88e-02\n",
      "Epoch: 242, Loss: 2479.80095, Residuals: -0.90935, Convergence:   inf\n",
      "Epoch: 243, Loss: 2515.82694, Residuals: -0.94941, Convergence: -0.014320\n",
      "Epoch: 243, Loss: 2477.82470, Residuals: -0.90775, Convergence: 0.000798\n",
      "Evidence 14877.051\n",
      "\n",
      "Epoch: 243, Evidence: 14877.05078, Convergence: 0.000722\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.20e-02\n",
      "Epoch: 243, Loss: 2479.24671, Residuals: -0.90775, Convergence:   inf\n",
      "Epoch: 244, Loss: 2484.68452, Residuals: -0.91254, Convergence: -0.002189\n",
      "Epoch: 244, Loss: 2479.32549, Residuals: -0.90741, Convergence: -0.000032\n",
      "Evidence 14878.665\n",
      "\n",
      "Epoch: 244, Evidence: 14878.66504, Convergence: 0.000831\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 381.13164, Residuals: -4.51200, Convergence:   inf\n",
      "Epoch: 1, Loss: 355.42137, Residuals: -4.39245, Convergence: 0.072337\n",
      "Epoch: 2, Loss: 334.36458, Residuals: -4.22912, Convergence: 0.062976\n",
      "Epoch: 3, Loss: 318.28369, Residuals: -4.06528, Convergence: 0.050524\n",
      "Epoch: 4, Loss: 306.02089, Residuals: -3.92089, Convergence: 0.040072\n",
      "Epoch: 5, Loss: 296.29480, Residuals: -3.79316, Convergence: 0.032826\n",
      "Epoch: 6, Loss: 288.39798, Residuals: -3.68183, Convergence: 0.027382\n",
      "Epoch: 7, Loss: 281.84222, Residuals: -3.58644, Convergence: 0.023260\n",
      "Epoch: 8, Loss: 276.26621, Residuals: -3.50501, Convergence: 0.020183\n",
      "Epoch: 9, Loss: 271.41482, Residuals: -3.43521, Convergence: 0.017874\n",
      "Epoch: 10, Loss: 267.10687, Residuals: -3.37495, Convergence: 0.016128\n",
      "Epoch: 11, Loss: 263.21140, Residuals: -3.32243, Convergence: 0.014800\n",
      "Epoch: 12, Loss: 259.63303, Residuals: -3.27613, Convergence: 0.013782\n",
      "Epoch: 13, Loss: 256.30249, Residuals: -3.23472, Convergence: 0.012995\n",
      "Epoch: 14, Loss: 253.16978, Residuals: -3.19704, Convergence: 0.012374\n",
      "Epoch: 15, Loss: 250.20006, Residuals: -3.16210, Convergence: 0.011869\n",
      "Epoch: 16, Loss: 247.37181, Residuals: -3.12919, Convergence: 0.011433\n",
      "Epoch: 17, Loss: 244.67137, Residuals: -3.09788, Convergence: 0.011037\n",
      "Epoch: 18, Loss: 242.08104, Residuals: -3.06785, Convergence: 0.010700\n",
      "Epoch: 19, Loss: 239.57236, Residuals: -3.03866, Convergence: 0.010471\n",
      "Epoch: 20, Loss: 237.11041, Residuals: -3.00979, Convergence: 0.010383\n",
      "Epoch: 21, Loss: 234.66240, Residuals: -2.98070, Convergence: 0.010432\n",
      "Epoch: 22, Loss: 232.20373, Residuals: -2.95102, Convergence: 0.010588\n",
      "Epoch: 23, Loss: 229.71324, Residuals: -2.92046, Convergence: 0.010842\n",
      "Epoch: 24, Loss: 227.15637, Residuals: -2.88865, Convergence: 0.011256\n",
      "Epoch: 25, Loss: 224.47867, Residuals: -2.85495, Convergence: 0.011929\n",
      "Epoch: 26, Loss: 221.65106, Residuals: -2.81896, Convergence: 0.012757\n",
      "Epoch: 27, Loss: 218.76165, Residuals: -2.78156, Convergence: 0.013208\n",
      "Epoch: 28, Loss: 215.94419, Residuals: -2.74428, Convergence: 0.013047\n",
      "Epoch: 29, Loss: 213.24108, Residuals: -2.70776, Convergence: 0.012676\n",
      "Epoch: 30, Loss: 210.64146, Residuals: -2.67200, Convergence: 0.012341\n",
      "Epoch: 31, Loss: 208.12690, Residuals: -2.63689, Convergence: 0.012082\n",
      "Epoch: 32, Loss: 205.68257, Residuals: -2.60229, Convergence: 0.011884\n",
      "Epoch: 33, Loss: 203.29817, Residuals: -2.56808, Convergence: 0.011729\n",
      "Epoch: 34, Loss: 200.96710, Residuals: -2.53419, Convergence: 0.011599\n",
      "Epoch: 35, Loss: 198.68547, Residuals: -2.50053, Convergence: 0.011484\n",
      "Epoch: 36, Loss: 196.45140, Residuals: -2.46707, Convergence: 0.011372\n",
      "Epoch: 37, Loss: 194.26439, Residuals: -2.43376, Convergence: 0.011258\n",
      "Epoch: 38, Loss: 192.12489, Residuals: -2.40059, Convergence: 0.011136\n",
      "Epoch: 39, Loss: 190.03389, Residuals: -2.36756, Convergence: 0.011003\n",
      "Epoch: 40, Loss: 187.99266, Residuals: -2.33466, Convergence: 0.010858\n",
      "Epoch: 41, Loss: 186.00251, Residuals: -2.30192, Convergence: 0.010700\n",
      "Epoch: 42, Loss: 184.06462, Residuals: -2.26934, Convergence: 0.010528\n",
      "Epoch: 43, Loss: 182.18009, Residuals: -2.23696, Convergence: 0.010344\n",
      "Epoch: 44, Loss: 180.34987, Residuals: -2.20480, Convergence: 0.010148\n",
      "Epoch: 45, Loss: 178.57495, Residuals: -2.17288, Convergence: 0.009939\n",
      "Epoch: 46, Loss: 176.85642, Residuals: -2.14124, Convergence: 0.009717\n",
      "Epoch: 47, Loss: 175.19543, Residuals: -2.10992, Convergence: 0.009481\n",
      "Epoch: 48, Loss: 173.59315, Residuals: -2.07895, Convergence: 0.009230\n",
      "Epoch: 49, Loss: 172.05050, Residuals: -2.04837, Convergence: 0.008966\n",
      "Epoch: 50, Loss: 170.56795, Residuals: -2.01824, Convergence: 0.008692\n",
      "Epoch: 51, Loss: 169.14536, Residuals: -1.98859, Convergence: 0.008410\n",
      "Epoch: 52, Loss: 167.78196, Residuals: -1.95947, Convergence: 0.008126\n",
      "Epoch: 53, Loss: 166.47628, Residuals: -1.93091, Convergence: 0.007843\n",
      "Epoch: 54, Loss: 165.22637, Residuals: -1.90293, Convergence: 0.007565\n",
      "Epoch: 55, Loss: 164.02978, Residuals: -1.87555, Convergence: 0.007295\n",
      "Epoch: 56, Loss: 162.88378, Residuals: -1.84879, Convergence: 0.007036\n",
      "Epoch: 57, Loss: 161.78552, Residuals: -1.82264, Convergence: 0.006788\n",
      "Epoch: 58, Loss: 160.73222, Residuals: -1.79711, Convergence: 0.006553\n",
      "Epoch: 59, Loss: 159.72131, Residuals: -1.77218, Convergence: 0.006329\n",
      "Epoch: 60, Loss: 158.75051, Residuals: -1.74786, Convergence: 0.006115\n",
      "Epoch: 61, Loss: 157.81787, Residuals: -1.72413, Convergence: 0.005910\n",
      "Epoch: 62, Loss: 156.92179, Residuals: -1.70100, Convergence: 0.005710\n",
      "Epoch: 63, Loss: 156.06096, Residuals: -1.67846, Convergence: 0.005516\n",
      "Epoch: 64, Loss: 155.23428, Residuals: -1.65652, Convergence: 0.005325\n",
      "Epoch: 65, Loss: 154.44082, Residuals: -1.63518, Convergence: 0.005138\n",
      "Epoch: 66, Loss: 153.67973, Residuals: -1.61445, Convergence: 0.004952\n",
      "Epoch: 67, Loss: 152.95024, Residuals: -1.59432, Convergence: 0.004769\n",
      "Epoch: 68, Loss: 152.25164, Residuals: -1.57481, Convergence: 0.004588\n",
      "Epoch: 69, Loss: 151.58319, Residuals: -1.55591, Convergence: 0.004410\n",
      "Epoch: 70, Loss: 150.94415, Residuals: -1.53763, Convergence: 0.004234\n",
      "Epoch: 71, Loss: 150.33380, Residuals: -1.51996, Convergence: 0.004060\n",
      "Epoch: 72, Loss: 149.75136, Residuals: -1.50289, Convergence: 0.003889\n",
      "Epoch: 73, Loss: 149.19600, Residuals: -1.48644, Convergence: 0.003722\n",
      "Epoch: 74, Loss: 148.66693, Residuals: -1.47058, Convergence: 0.003559\n",
      "Epoch: 75, Loss: 148.16326, Residuals: -1.45531, Convergence: 0.003399\n",
      "Epoch: 76, Loss: 147.68411, Residuals: -1.44062, Convergence: 0.003244\n",
      "Epoch: 77, Loss: 147.22857, Residuals: -1.42651, Convergence: 0.003094\n",
      "Epoch: 78, Loss: 146.79571, Residuals: -1.41295, Convergence: 0.002949\n",
      "Epoch: 79, Loss: 146.38460, Residuals: -1.39993, Convergence: 0.002808\n",
      "Epoch: 80, Loss: 145.99430, Residuals: -1.38745, Convergence: 0.002673\n",
      "Epoch: 81, Loss: 145.62389, Residuals: -1.37548, Convergence: 0.002544\n",
      "Epoch: 82, Loss: 145.27245, Residuals: -1.36400, Convergence: 0.002419\n",
      "Epoch: 83, Loss: 144.93907, Residuals: -1.35301, Convergence: 0.002300\n",
      "Epoch: 84, Loss: 144.62289, Residuals: -1.34248, Convergence: 0.002186\n",
      "Epoch: 85, Loss: 144.32306, Residuals: -1.33240, Convergence: 0.002077\n",
      "Epoch: 86, Loss: 144.03877, Residuals: -1.32275, Convergence: 0.001974\n",
      "Epoch: 87, Loss: 143.76922, Residuals: -1.31352, Convergence: 0.001875\n",
      "Epoch: 88, Loss: 143.51369, Residuals: -1.30467, Convergence: 0.001781\n",
      "Epoch: 89, Loss: 143.27147, Residuals: -1.29621, Convergence: 0.001691\n",
      "Epoch: 90, Loss: 143.04188, Residuals: -1.28811, Convergence: 0.001605\n",
      "Epoch: 91, Loss: 142.82430, Residuals: -1.28036, Convergence: 0.001523\n",
      "Epoch: 92, Loss: 142.61816, Residuals: -1.27294, Convergence: 0.001445\n",
      "Epoch: 93, Loss: 142.42289, Residuals: -1.26584, Convergence: 0.001371\n",
      "Epoch: 94, Loss: 142.23800, Residuals: -1.25905, Convergence: 0.001300\n",
      "Epoch: 95, Loss: 142.06300, Residuals: -1.25254, Convergence: 0.001232\n",
      "Epoch: 96, Loss: 141.89747, Residuals: -1.24631, Convergence: 0.001167\n",
      "Epoch: 97, Loss: 141.74102, Residuals: -1.24035, Convergence: 0.001104\n",
      "Epoch: 98, Loss: 141.59327, Residuals: -1.23464, Convergence: 0.001043\n",
      "Epoch: 99, Loss: 141.45391, Residuals: -1.22918, Convergence: 0.000985\n",
      "Evidence -183.505\n",
      "\n",
      "Epoch: 99, Evidence: -183.50490, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.25e-01\n",
      "Epoch: 99, Loss: 1360.03991, Residuals: -1.22918, Convergence:   inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Loss: 1296.10410, Residuals: -1.26111, Convergence: 0.049329\n",
      "Epoch: 101, Loss: 1247.86252, Residuals: -1.28598, Convergence: 0.038659\n",
      "Epoch: 102, Loss: 1211.76217, Residuals: -1.30372, Convergence: 0.029792\n",
      "Epoch: 103, Loss: 1183.87632, Residuals: -1.31625, Convergence: 0.023555\n",
      "Epoch: 104, Loss: 1161.42527, Residuals: -1.32565, Convergence: 0.019331\n",
      "Epoch: 105, Loss: 1142.84695, Residuals: -1.33299, Convergence: 0.016256\n",
      "Epoch: 106, Loss: 1127.21320, Residuals: -1.33871, Convergence: 0.013869\n",
      "Epoch: 107, Loss: 1113.89518, Residuals: -1.34302, Convergence: 0.011956\n",
      "Epoch: 108, Loss: 1102.42715, Residuals: -1.34607, Convergence: 0.010403\n",
      "Epoch: 109, Loss: 1092.44511, Residuals: -1.34797, Convergence: 0.009137\n",
      "Epoch: 110, Loss: 1083.65778, Residuals: -1.34882, Convergence: 0.008109\n",
      "Epoch: 111, Loss: 1075.82495, Residuals: -1.34872, Convergence: 0.007281\n",
      "Epoch: 112, Loss: 1068.74683, Residuals: -1.34773, Convergence: 0.006623\n",
      "Epoch: 113, Loss: 1062.25340, Residuals: -1.34592, Convergence: 0.006113\n",
      "Epoch: 114, Loss: 1056.19903, Residuals: -1.34333, Convergence: 0.005732\n",
      "Epoch: 115, Loss: 1050.45764, Residuals: -1.33998, Convergence: 0.005466\n",
      "Epoch: 116, Loss: 1044.92033, Residuals: -1.33591, Convergence: 0.005299\n",
      "Epoch: 117, Loss: 1039.49619, Residuals: -1.33112, Convergence: 0.005218\n",
      "Epoch: 118, Loss: 1034.11508, Residuals: -1.32566, Convergence: 0.005204\n",
      "Epoch: 119, Loss: 1028.73520, Residuals: -1.31955, Convergence: 0.005230\n",
      "Epoch: 120, Loss: 1023.35090, Residuals: -1.31287, Convergence: 0.005261\n",
      "Epoch: 121, Loss: 1017.99796, Residuals: -1.30571, Convergence: 0.005258\n",
      "Epoch: 122, Loss: 1012.74612, Residuals: -1.29816, Convergence: 0.005186\n",
      "Epoch: 123, Loss: 1007.67569, Residuals: -1.29035, Convergence: 0.005032\n",
      "Epoch: 124, Loss: 1002.85170, Residuals: -1.28235, Convergence: 0.004810\n",
      "Epoch: 125, Loss: 998.31095, Residuals: -1.27426, Convergence: 0.004548\n",
      "Epoch: 126, Loss: 994.06010, Residuals: -1.26615, Convergence: 0.004276\n",
      "Epoch: 127, Loss: 990.08720, Residuals: -1.25807, Convergence: 0.004013\n",
      "Epoch: 128, Loss: 986.37028, Residuals: -1.25006, Convergence: 0.003768\n",
      "Epoch: 129, Loss: 982.88430, Residuals: -1.24218, Convergence: 0.003547\n",
      "Epoch: 130, Loss: 979.60531, Residuals: -1.23444, Convergence: 0.003347\n",
      "Epoch: 131, Loss: 976.51221, Residuals: -1.22687, Convergence: 0.003167\n",
      "Epoch: 132, Loss: 973.58678, Residuals: -1.21950, Convergence: 0.003005\n",
      "Epoch: 133, Loss: 970.81436, Residuals: -1.21234, Convergence: 0.002856\n",
      "Epoch: 134, Loss: 968.18246, Residuals: -1.20540, Convergence: 0.002718\n",
      "Epoch: 135, Loss: 965.68098, Residuals: -1.19870, Convergence: 0.002590\n",
      "Epoch: 136, Loss: 963.30121, Residuals: -1.19224, Convergence: 0.002470\n",
      "Epoch: 137, Loss: 961.03553, Residuals: -1.18603, Convergence: 0.002358\n",
      "Epoch: 138, Loss: 958.87757, Residuals: -1.18006, Convergence: 0.002251\n",
      "Epoch: 139, Loss: 956.82094, Residuals: -1.17434, Convergence: 0.002149\n",
      "Epoch: 140, Loss: 954.85993, Residuals: -1.16888, Convergence: 0.002054\n",
      "Epoch: 141, Loss: 952.98895, Residuals: -1.16365, Convergence: 0.001963\n",
      "Epoch: 142, Loss: 951.20272, Residuals: -1.15866, Convergence: 0.001878\n",
      "Epoch: 143, Loss: 949.49582, Residuals: -1.15390, Convergence: 0.001798\n",
      "Epoch: 144, Loss: 947.86316, Residuals: -1.14937, Convergence: 0.001722\n",
      "Epoch: 145, Loss: 946.30073, Residuals: -1.14504, Convergence: 0.001651\n",
      "Epoch: 146, Loss: 944.80360, Residuals: -1.14092, Convergence: 0.001585\n",
      "Epoch: 147, Loss: 943.36822, Residuals: -1.13699, Convergence: 0.001522\n",
      "Epoch: 148, Loss: 941.98990, Residuals: -1.13325, Convergence: 0.001463\n",
      "Epoch: 149, Loss: 940.66624, Residuals: -1.12968, Convergence: 0.001407\n",
      "Epoch: 150, Loss: 939.39369, Residuals: -1.12628, Convergence: 0.001355\n",
      "Epoch: 151, Loss: 938.16884, Residuals: -1.12303, Convergence: 0.001306\n",
      "Epoch: 152, Loss: 936.98927, Residuals: -1.11993, Convergence: 0.001259\n",
      "Epoch: 153, Loss: 935.85222, Residuals: -1.11696, Convergence: 0.001215\n",
      "Epoch: 154, Loss: 934.75489, Residuals: -1.11413, Convergence: 0.001174\n",
      "Epoch: 155, Loss: 933.69479, Residuals: -1.11143, Convergence: 0.001135\n",
      "Epoch: 156, Loss: 932.66908, Residuals: -1.10883, Convergence: 0.001100\n",
      "Epoch: 157, Loss: 931.67480, Residuals: -1.10635, Convergence: 0.001067\n",
      "Epoch: 158, Loss: 930.70928, Residuals: -1.10396, Convergence: 0.001037\n",
      "Epoch: 159, Loss: 929.76931, Residuals: -1.10166, Convergence: 0.001011\n",
      "Epoch: 160, Loss: 928.85154, Residuals: -1.09945, Convergence: 0.000988\n",
      "Evidence 11102.545\n",
      "\n",
      "Epoch: 160, Evidence: 11102.54492, Convergence: 1.016528\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.79e-01\n",
      "Epoch: 160, Loss: 2342.30150, Residuals: -1.09945, Convergence:   inf\n",
      "Epoch: 161, Loss: 2303.11434, Residuals: -1.10811, Convergence: 0.017015\n",
      "Epoch: 162, Loss: 2275.45515, Residuals: -1.10786, Convergence: 0.012155\n",
      "Epoch: 163, Loss: 2252.35048, Residuals: -1.10627, Convergence: 0.010258\n",
      "Epoch: 164, Loss: 2232.69457, Residuals: -1.10418, Convergence: 0.008804\n",
      "Epoch: 165, Loss: 2215.80303, Residuals: -1.10176, Convergence: 0.007623\n",
      "Epoch: 166, Loss: 2201.16234, Residuals: -1.09906, Convergence: 0.006651\n",
      "Epoch: 167, Loss: 2188.36297, Residuals: -1.09614, Convergence: 0.005849\n",
      "Epoch: 168, Loss: 2177.07085, Residuals: -1.09303, Convergence: 0.005187\n",
      "Epoch: 169, Loss: 2167.01043, Residuals: -1.08977, Convergence: 0.004643\n",
      "Epoch: 170, Loss: 2157.95260, Residuals: -1.08638, Convergence: 0.004197\n",
      "Epoch: 171, Loss: 2149.70902, Residuals: -1.08286, Convergence: 0.003835\n",
      "Epoch: 172, Loss: 2142.13458, Residuals: -1.07923, Convergence: 0.003536\n",
      "Epoch: 173, Loss: 2135.12811, Residuals: -1.07549, Convergence: 0.003282\n",
      "Epoch: 174, Loss: 2128.62627, Residuals: -1.07169, Convergence: 0.003054\n",
      "Epoch: 175, Loss: 2122.59421, Residuals: -1.06786, Convergence: 0.002842\n",
      "Epoch: 176, Loss: 2117.01256, Residuals: -1.06406, Convergence: 0.002637\n",
      "Epoch: 177, Loss: 2111.86229, Residuals: -1.06033, Convergence: 0.002439\n",
      "Epoch: 178, Loss: 2107.12073, Residuals: -1.05670, Convergence: 0.002250\n",
      "Epoch: 179, Loss: 2102.76115, Residuals: -1.05321, Convergence: 0.002073\n",
      "Epoch: 180, Loss: 2098.75531, Residuals: -1.04988, Convergence: 0.001909\n",
      "Epoch: 181, Loss: 2095.07134, Residuals: -1.04670, Convergence: 0.001758\n",
      "Epoch: 182, Loss: 2091.67861, Residuals: -1.04368, Convergence: 0.001622\n",
      "Epoch: 183, Loss: 2088.54775, Residuals: -1.04082, Convergence: 0.001499\n",
      "Epoch: 184, Loss: 2085.65079, Residuals: -1.03812, Convergence: 0.001389\n",
      "Epoch: 185, Loss: 2082.96241, Residuals: -1.03557, Convergence: 0.001291\n",
      "Epoch: 186, Loss: 2080.46038, Residuals: -1.03316, Convergence: 0.001203\n",
      "Epoch: 187, Loss: 2078.12269, Residuals: -1.03089, Convergence: 0.001125\n",
      "Epoch: 188, Loss: 2075.93212, Residuals: -1.02874, Convergence: 0.001055\n",
      "Epoch: 189, Loss: 2073.87332, Residuals: -1.02672, Convergence: 0.000993\n",
      "Evidence 14287.503\n",
      "\n",
      "Epoch: 189, Evidence: 14287.50293, Convergence: 0.222919\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.42e-01\n",
      "Epoch: 189, Loss: 2468.68508, Residuals: -1.02672, Convergence:   inf\n",
      "Epoch: 190, Loss: 2454.87417, Residuals: -1.02312, Convergence: 0.005626\n",
      "Epoch: 191, Loss: 2443.64294, Residuals: -1.01905, Convergence: 0.004596\n",
      "Epoch: 192, Loss: 2433.97810, Residuals: -1.01510, Convergence: 0.003971\n",
      "Epoch: 193, Loss: 2425.59492, Residuals: -1.01137, Convergence: 0.003456\n",
      "Epoch: 194, Loss: 2418.27899, Residuals: -1.00788, Convergence: 0.003025\n",
      "Epoch: 195, Loss: 2411.85433, Residuals: -1.00464, Convergence: 0.002664\n",
      "Epoch: 196, Loss: 2406.17794, Residuals: -1.00163, Convergence: 0.002359\n",
      "Epoch: 197, Loss: 2401.13074, Residuals: -0.99884, Convergence: 0.002102\n",
      "Epoch: 198, Loss: 2396.61621, Residuals: -0.99627, Convergence: 0.001884\n",
      "Epoch: 199, Loss: 2392.55258, Residuals: -0.99389, Convergence: 0.001698\n",
      "Epoch: 200, Loss: 2388.87467, Residuals: -0.99169, Convergence: 0.001540\n",
      "Epoch: 201, Loss: 2385.52695, Residuals: -0.98967, Convergence: 0.001403\n",
      "Epoch: 202, Loss: 2382.46505, Residuals: -0.98780, Convergence: 0.001285\n",
      "Epoch: 203, Loss: 2379.65004, Residuals: -0.98608, Convergence: 0.001183\n",
      "Epoch: 204, Loss: 2377.05307, Residuals: -0.98449, Convergence: 0.001093\n",
      "Epoch: 205, Loss: 2374.64672, Residuals: -0.98304, Convergence: 0.001013\n",
      "Epoch: 206, Loss: 2372.41036, Residuals: -0.98170, Convergence: 0.000943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence 14694.977\n",
      "\n",
      "Epoch: 206, Evidence: 14694.97656, Convergence: 0.027729\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.39e-01\n",
      "Epoch: 206, Loss: 2473.95084, Residuals: -0.98170, Convergence:   inf\n",
      "Epoch: 207, Loss: 2467.55574, Residuals: -0.97810, Convergence: 0.002592\n",
      "Epoch: 208, Loss: 2462.23264, Residuals: -0.97492, Convergence: 0.002162\n",
      "Epoch: 209, Loss: 2457.68570, Residuals: -0.97214, Convergence: 0.001850\n",
      "Epoch: 210, Loss: 2453.74624, Residuals: -0.96973, Convergence: 0.001605\n",
      "Epoch: 211, Loss: 2450.28795, Residuals: -0.96764, Convergence: 0.001411\n",
      "Epoch: 212, Loss: 2447.21662, Residuals: -0.96582, Convergence: 0.001255\n",
      "Epoch: 213, Loss: 2444.46061, Residuals: -0.96425, Convergence: 0.001127\n",
      "Epoch: 214, Loss: 2441.96352, Residuals: -0.96288, Convergence: 0.001023\n",
      "Epoch: 215, Loss: 2439.68348, Residuals: -0.96171, Convergence: 0.000935\n",
      "Evidence 14780.258\n",
      "\n",
      "Epoch: 215, Evidence: 14780.25781, Convergence: 0.005770\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.66e-01\n",
      "Epoch: 215, Loss: 2475.82149, Residuals: -0.96171, Convergence:   inf\n",
      "Epoch: 216, Loss: 2472.00284, Residuals: -0.95914, Convergence: 0.001545\n",
      "Epoch: 217, Loss: 2468.79310, Residuals: -0.95703, Convergence: 0.001300\n",
      "Epoch: 218, Loss: 2466.01385, Residuals: -0.95529, Convergence: 0.001127\n",
      "Epoch: 219, Loss: 2463.56143, Residuals: -0.95384, Convergence: 0.000995\n",
      "Evidence 14808.581\n",
      "\n",
      "Epoch: 219, Evidence: 14808.58105, Convergence: 0.001913\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.15e-01\n",
      "Epoch: 219, Loss: 2477.05886, Residuals: -0.95384, Convergence:   inf\n",
      "Epoch: 220, Loss: 2474.15855, Residuals: -0.95184, Convergence: 0.001172\n",
      "Epoch: 221, Loss: 2471.69698, Residuals: -0.95023, Convergence: 0.000996\n",
      "Evidence 14820.551\n",
      "\n",
      "Epoch: 221, Evidence: 14820.55078, Convergence: 0.000808\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.79e-01\n",
      "Epoch: 221, Loss: 2477.93095, Residuals: -0.95023, Convergence:   inf\n",
      "Epoch: 222, Loss: 2473.26143, Residuals: -0.94753, Convergence: 0.001888\n",
      "Epoch: 223, Loss: 2469.67579, Residuals: -0.94557, Convergence: 0.001452\n",
      "Epoch: 224, Loss: 2466.73921, Residuals: -0.94427, Convergence: 0.001190\n",
      "Epoch: 225, Loss: 2464.23408, Residuals: -0.94355, Convergence: 0.001017\n",
      "Epoch: 226, Loss: 2462.03218, Residuals: -0.94320, Convergence: 0.000894\n",
      "Evidence 14841.181\n",
      "\n",
      "Epoch: 226, Evidence: 14841.18066, Convergence: 0.002197\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.48e-01\n",
      "Epoch: 226, Loss: 2478.16206, Residuals: -0.94320, Convergence:   inf\n",
      "Epoch: 227, Loss: 2475.19452, Residuals: -0.94108, Convergence: 0.001199\n",
      "Epoch: 228, Loss: 2472.80822, Residuals: -0.94008, Convergence: 0.000965\n",
      "Evidence 14852.645\n",
      "\n",
      "Epoch: 228, Evidence: 14852.64453, Convergence: 0.000772\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.26e-01\n",
      "Epoch: 228, Loss: 2478.46062, Residuals: -0.94008, Convergence:   inf\n",
      "Epoch: 229, Loss: 2474.03766, Residuals: -0.93743, Convergence: 0.001788\n",
      "Epoch: 230, Loss: 2470.77501, Residuals: -0.93886, Convergence: 0.001320\n",
      "Epoch: 231, Loss: 2468.11583, Residuals: -0.93958, Convergence: 0.001077\n",
      "Epoch: 232, Loss: 2465.81103, Residuals: -0.94240, Convergence: 0.000935\n",
      "Evidence 14868.471\n",
      "\n",
      "Epoch: 232, Evidence: 14868.47070, Convergence: 0.001835\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.12e-01\n",
      "Epoch: 232, Loss: 2477.91812, Residuals: -0.94240, Convergence:   inf\n",
      "Epoch: 233, Loss: 2476.00609, Residuals: -0.94159, Convergence: 0.000772\n",
      "Evidence 14875.037\n",
      "\n",
      "Epoch: 233, Evidence: 14875.03711, Convergence: 0.000441\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 9.12e-02\n",
      "Epoch: 233, Loss: 2478.87332, Residuals: -0.94159, Convergence:   inf\n",
      "Epoch: 234, Loss: 2522.12358, Residuals: -0.98052, Convergence: -0.017148\n",
      "Epoch: 234, Loss: 2476.35877, Residuals: -0.94001, Convergence: 0.001015\n",
      "Epoch: 235, Loss: 2476.08207, Residuals: -0.94220, Convergence: 0.000112\n",
      "Evidence 14880.064\n",
      "\n",
      "Epoch: 235, Evidence: 14880.06445, Convergence: 0.000779\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.60e-02\n",
      "Epoch: 235, Loss: 2478.81039, Residuals: -0.94220, Convergence:   inf\n",
      "Epoch: 236, Loss: 2534.00696, Residuals: -0.99492, Convergence: -0.021782\n",
      "Epoch: 236, Loss: 2476.32383, Residuals: -0.94064, Convergence: 0.001004\n",
      "Epoch: 237, Loss: 2476.32895, Residuals: -0.94435, Convergence: -0.000002\n",
      "Evidence 14884.679\n",
      "\n",
      "Epoch: 237, Evidence: 14884.67871, Convergence: 0.001089\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 6.30e-02\n",
      "Epoch: 237, Loss: 2478.69685, Residuals: -0.94435, Convergence:   inf\n",
      "Epoch: 238, Loss: 2476.39738, Residuals: -0.94192, Convergence: 0.000929\n",
      "Evidence 14889.288\n",
      "\n",
      "Epoch: 238, Evidence: 14889.28809, Convergence: 0.000310\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.92e-02\n",
      "Epoch: 238, Loss: 2478.38815, Residuals: -0.94192, Convergence:   inf\n",
      "Epoch: 239, Loss: 2482.40541, Residuals: -0.95187, Convergence: -0.001618\n",
      "Epoch: 239, Loss: 2478.20848, Residuals: -0.94178, Convergence: 0.000072\n",
      "Evidence 14890.617\n",
      "\n",
      "Epoch: 239, Evidence: 14890.61719, Convergence: 0.000399\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.21e-02\n",
      "Epoch: 239, Loss: 2478.83438, Residuals: -0.94178, Convergence:   inf\n",
      "Epoch: 240, Loss: 2536.97537, Residuals: -0.99760, Convergence: -0.022917\n",
      "Epoch: 240, Loss: 2477.14063, Residuals: -0.94145, Convergence: 0.000684\n",
      "Evidence 14893.504\n",
      "\n",
      "Epoch: 240, Evidence: 14893.50391, Convergence: 0.000593\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 379.91739, Residuals: -4.50849, Convergence:   inf\n",
      "Epoch: 1, Loss: 354.26706, Residuals: -4.38785, Convergence: 0.072404\n",
      "Epoch: 2, Loss: 333.31055, Residuals: -4.22399, Convergence: 0.062874\n",
      "Epoch: 3, Loss: 317.26191, Residuals: -4.05917, Convergence: 0.050585\n",
      "Epoch: 4, Loss: 305.02504, Residuals: -3.91379, Convergence: 0.040118\n",
      "Epoch: 5, Loss: 295.32414, Residuals: -3.78525, Convergence: 0.032848\n",
      "Epoch: 6, Loss: 287.45179, Residuals: -3.67324, Convergence: 0.027387\n",
      "Epoch: 7, Loss: 280.91904, Residuals: -3.57728, Convergence: 0.023255\n",
      "Epoch: 8, Loss: 275.36364, Residuals: -3.49531, Convergence: 0.020175\n",
      "Epoch: 9, Loss: 270.52986, Residuals: -3.42499, Convergence: 0.017868\n",
      "Epoch: 10, Loss: 266.23590, Residuals: -3.36422, Convergence: 0.016128\n",
      "Epoch: 11, Loss: 262.35008, Residuals: -3.31121, Convergence: 0.014812\n",
      "Epoch: 12, Loss: 258.77633, Residuals: -3.26442, Convergence: 0.013810\n",
      "Epoch: 13, Loss: 255.44521, Residuals: -3.22255, Convergence: 0.013040\n",
      "Epoch: 14, Loss: 252.30850, Residuals: -3.18446, Convergence: 0.012432\n",
      "Epoch: 15, Loss: 249.33684, Residuals: -3.14925, Convergence: 0.011918\n",
      "Epoch: 16, Loss: 246.51686, Residuals: -3.11636, Convergence: 0.011439\n",
      "Epoch: 17, Loss: 243.83951, Residuals: -3.08543, Convergence: 0.010980\n",
      "Epoch: 18, Loss: 241.28567, Residuals: -3.05612, Convergence: 0.010584\n",
      "Epoch: 19, Loss: 238.82419, Residuals: -3.02793, Convergence: 0.010307\n",
      "Epoch: 20, Loss: 236.41875, Residuals: -3.00027, Convergence: 0.010174\n",
      "Epoch: 21, Loss: 234.03433, Residuals: -2.97258, Convergence: 0.010188\n",
      "Epoch: 22, Loss: 231.64049, Residuals: -2.94441, Convergence: 0.010334\n",
      "Epoch: 23, Loss: 229.20910, Residuals: -2.91534, Convergence: 0.010608\n",
      "Epoch: 24, Loss: 226.70426, Residuals: -2.88495, Convergence: 0.011049\n",
      "Epoch: 25, Loss: 224.07430, Residuals: -2.85261, Convergence: 0.011737\n",
      "Epoch: 26, Loss: 221.28134, Residuals: -2.81784, Convergence: 0.012622\n",
      "Epoch: 27, Loss: 218.39106, Residuals: -2.78124, Convergence: 0.013234\n",
      "Epoch: 28, Loss: 215.54301, Residuals: -2.74431, Convergence: 0.013213\n",
      "Epoch: 29, Loss: 212.79680, Residuals: -2.70786, Convergence: 0.012905\n",
      "Epoch: 30, Loss: 210.14722, Residuals: -2.67193, Convergence: 0.012608\n",
      "Epoch: 31, Loss: 207.57727, Residuals: -2.63644, Convergence: 0.012381\n",
      "Epoch: 32, Loss: 205.07309, Residuals: -2.60127, Convergence: 0.012211\n",
      "Epoch: 33, Loss: 202.62572, Residuals: -2.56635, Convergence: 0.012078\n",
      "Epoch: 34, Loss: 200.23018, Residuals: -2.53162, Convergence: 0.011964\n",
      "Epoch: 35, Loss: 197.88432, Residuals: -2.49706, Convergence: 0.011855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36, Loss: 195.58775, Residuals: -2.46265, Convergence: 0.011742\n",
      "Epoch: 37, Loss: 193.34120, Residuals: -2.42839, Convergence: 0.011620\n",
      "Epoch: 38, Loss: 191.14590, Residuals: -2.39426, Convergence: 0.011485\n",
      "Epoch: 39, Loss: 189.00325, Residuals: -2.36028, Convergence: 0.011337\n",
      "Epoch: 40, Loss: 186.91467, Residuals: -2.32646, Convergence: 0.011174\n",
      "Epoch: 41, Loss: 184.88152, Residuals: -2.29280, Convergence: 0.010997\n",
      "Epoch: 42, Loss: 182.90526, Residuals: -2.25933, Convergence: 0.010805\n",
      "Epoch: 43, Loss: 180.98747, Residuals: -2.22607, Convergence: 0.010596\n",
      "Epoch: 44, Loss: 179.12999, Residuals: -2.19308, Convergence: 0.010369\n",
      "Epoch: 45, Loss: 177.33486, Residuals: -2.16040, Convergence: 0.010123\n",
      "Epoch: 46, Loss: 175.60415, Residuals: -2.12810, Convergence: 0.009856\n",
      "Epoch: 47, Loss: 173.93969, Residuals: -2.09623, Convergence: 0.009569\n",
      "Epoch: 48, Loss: 172.34275, Residuals: -2.06486, Convergence: 0.009266\n",
      "Epoch: 49, Loss: 170.81378, Residuals: -2.03407, Convergence: 0.008951\n",
      "Epoch: 50, Loss: 169.35233, Residuals: -2.00389, Convergence: 0.008630\n",
      "Epoch: 51, Loss: 167.95698, Residuals: -1.97437, Convergence: 0.008308\n",
      "Epoch: 52, Loss: 166.62544, Residuals: -1.94555, Convergence: 0.007991\n",
      "Epoch: 53, Loss: 165.35467, Residuals: -1.91743, Convergence: 0.007685\n",
      "Epoch: 54, Loss: 164.14117, Residuals: -1.89002, Convergence: 0.007393\n",
      "Epoch: 55, Loss: 162.98119, Residuals: -1.86330, Convergence: 0.007117\n",
      "Epoch: 56, Loss: 161.87099, Residuals: -1.83726, Convergence: 0.006859\n",
      "Epoch: 57, Loss: 160.80709, Residuals: -1.81186, Convergence: 0.006616\n",
      "Epoch: 58, Loss: 159.78639, Residuals: -1.78709, Convergence: 0.006388\n",
      "Epoch: 59, Loss: 158.80626, Residuals: -1.76294, Convergence: 0.006172\n",
      "Epoch: 60, Loss: 157.86450, Residuals: -1.73937, Convergence: 0.005966\n",
      "Epoch: 61, Loss: 156.95932, Residuals: -1.71640, Convergence: 0.005767\n",
      "Epoch: 62, Loss: 156.08925, Residuals: -1.69401, Convergence: 0.005574\n",
      "Epoch: 63, Loss: 155.25304, Residuals: -1.67220, Convergence: 0.005386\n",
      "Epoch: 64, Loss: 154.44965, Residuals: -1.65097, Convergence: 0.005202\n",
      "Epoch: 65, Loss: 153.67813, Residuals: -1.63033, Convergence: 0.005020\n",
      "Epoch: 66, Loss: 152.93761, Residuals: -1.61027, Convergence: 0.004842\n",
      "Epoch: 67, Loss: 152.22726, Residuals: -1.59079, Convergence: 0.004666\n",
      "Epoch: 68, Loss: 151.54629, Residuals: -1.57189, Convergence: 0.004493\n",
      "Epoch: 69, Loss: 150.89390, Residuals: -1.55358, Convergence: 0.004323\n",
      "Epoch: 70, Loss: 150.26931, Residuals: -1.53584, Convergence: 0.004156\n",
      "Epoch: 71, Loss: 149.67173, Residuals: -1.51867, Convergence: 0.003993\n",
      "Epoch: 72, Loss: 149.10035, Residuals: -1.50206, Convergence: 0.003832\n",
      "Epoch: 73, Loss: 148.55439, Residuals: -1.48602, Convergence: 0.003675\n",
      "Epoch: 74, Loss: 148.03305, Residuals: -1.47052, Convergence: 0.003522\n",
      "Epoch: 75, Loss: 147.53551, Residuals: -1.45557, Convergence: 0.003372\n",
      "Epoch: 76, Loss: 147.06095, Residuals: -1.44114, Convergence: 0.003227\n",
      "Epoch: 77, Loss: 146.60855, Residuals: -1.42724, Convergence: 0.003086\n",
      "Epoch: 78, Loss: 146.17751, Residuals: -1.41384, Convergence: 0.002949\n",
      "Epoch: 79, Loss: 145.76698, Residuals: -1.40095, Convergence: 0.002816\n",
      "Epoch: 80, Loss: 145.37615, Residuals: -1.38853, Convergence: 0.002688\n",
      "Epoch: 81, Loss: 145.00419, Residuals: -1.37659, Convergence: 0.002565\n",
      "Epoch: 82, Loss: 144.65029, Residuals: -1.36511, Convergence: 0.002447\n",
      "Epoch: 83, Loss: 144.31366, Residuals: -1.35407, Convergence: 0.002333\n",
      "Epoch: 84, Loss: 143.99350, Residuals: -1.34347, Convergence: 0.002223\n",
      "Epoch: 85, Loss: 143.68904, Residuals: -1.33329, Convergence: 0.002119\n",
      "Epoch: 86, Loss: 143.39954, Residuals: -1.32351, Convergence: 0.002019\n",
      "Epoch: 87, Loss: 143.12429, Residuals: -1.31412, Convergence: 0.001923\n",
      "Epoch: 88, Loss: 142.86258, Residuals: -1.30512, Convergence: 0.001832\n",
      "Epoch: 89, Loss: 142.61379, Residuals: -1.29648, Convergence: 0.001745\n",
      "Epoch: 90, Loss: 142.37727, Residuals: -1.28819, Convergence: 0.001661\n",
      "Epoch: 91, Loss: 142.15247, Residuals: -1.28024, Convergence: 0.001581\n",
      "Epoch: 92, Loss: 141.93883, Residuals: -1.27262, Convergence: 0.001505\n",
      "Epoch: 93, Loss: 141.73587, Residuals: -1.26532, Convergence: 0.001432\n",
      "Epoch: 94, Loss: 141.54313, Residuals: -1.25831, Convergence: 0.001362\n",
      "Epoch: 95, Loss: 141.36019, Residuals: -1.25161, Convergence: 0.001294\n",
      "Epoch: 96, Loss: 141.18669, Residuals: -1.24518, Convergence: 0.001229\n",
      "Epoch: 97, Loss: 141.02226, Residuals: -1.23903, Convergence: 0.001166\n",
      "Epoch: 98, Loss: 140.86663, Residuals: -1.23315, Convergence: 0.001105\n",
      "Epoch: 99, Loss: 140.71949, Residuals: -1.22753, Convergence: 0.001046\n",
      "Epoch: 100, Loss: 140.58060, Residuals: -1.22215, Convergence: 0.000988\n",
      "Evidence -182.068\n",
      "\n",
      "Epoch: 100, Evidence: -182.06792, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.25e-01\n",
      "Epoch: 100, Loss: 1369.44965, Residuals: -1.22215, Convergence:   inf\n",
      "Epoch: 101, Loss: 1306.73440, Residuals: -1.25104, Convergence: 0.047994\n",
      "Epoch: 102, Loss: 1258.92225, Residuals: -1.27379, Convergence: 0.037979\n",
      "Epoch: 103, Loss: 1222.76241, Residuals: -1.29001, Convergence: 0.029572\n",
      "Epoch: 104, Loss: 1194.61652, Residuals: -1.30121, Convergence: 0.023561\n",
      "Epoch: 105, Loss: 1171.85305, Residuals: -1.30930, Convergence: 0.019425\n",
      "Epoch: 106, Loss: 1152.95252, Residuals: -1.31536, Convergence: 0.016393\n",
      "Epoch: 107, Loss: 1136.99520, Residuals: -1.31985, Convergence: 0.014035\n",
      "Epoch: 108, Loss: 1123.35366, Residuals: -1.32302, Convergence: 0.012144\n",
      "Epoch: 109, Loss: 1111.56175, Residuals: -1.32502, Convergence: 0.010608\n",
      "Epoch: 110, Loss: 1101.25634, Residuals: -1.32598, Convergence: 0.009358\n",
      "Epoch: 111, Loss: 1092.14483, Residuals: -1.32600, Convergence: 0.008343\n",
      "Epoch: 112, Loss: 1083.98642, Residuals: -1.32517, Convergence: 0.007526\n",
      "Epoch: 113, Loss: 1076.57946, Residuals: -1.32354, Convergence: 0.006880\n",
      "Epoch: 114, Loss: 1069.75257, Residuals: -1.32116, Convergence: 0.006382\n",
      "Epoch: 115, Loss: 1063.35743, Residuals: -1.31807, Convergence: 0.006014\n",
      "Epoch: 116, Loss: 1057.26567, Residuals: -1.31428, Convergence: 0.005762\n",
      "Epoch: 117, Loss: 1051.36638, Residuals: -1.30980, Convergence: 0.005611\n",
      "Epoch: 118, Loss: 1045.56828, Residuals: -1.30466, Convergence: 0.005545\n",
      "Epoch: 119, Loss: 1039.80607, Residuals: -1.29889, Convergence: 0.005542\n",
      "Epoch: 120, Loss: 1034.05419, Residuals: -1.29254, Convergence: 0.005562\n",
      "Epoch: 121, Loss: 1028.33583, Residuals: -1.28571, Convergence: 0.005561\n",
      "Epoch: 122, Loss: 1022.72073, Residuals: -1.27850, Convergence: 0.005490\n",
      "Epoch: 123, Loss: 1017.29951, Residuals: -1.27100, Convergence: 0.005329\n",
      "Epoch: 124, Loss: 1012.15033, Residuals: -1.26331, Convergence: 0.005087\n",
      "Epoch: 125, Loss: 1007.31730, Residuals: -1.25550, Convergence: 0.004798\n",
      "Epoch: 126, Loss: 1002.80859, Residuals: -1.24765, Convergence: 0.004496\n",
      "Epoch: 127, Loss: 998.60921, Residuals: -1.23982, Convergence: 0.004205\n",
      "Epoch: 128, Loss: 994.69248, Residuals: -1.23204, Convergence: 0.003938\n",
      "Epoch: 129, Loss: 991.02823, Residuals: -1.22436, Convergence: 0.003697\n",
      "Epoch: 130, Loss: 987.58813, Residuals: -1.21682, Convergence: 0.003483\n",
      "Epoch: 131, Loss: 984.34691, Residuals: -1.20942, Convergence: 0.003293\n",
      "Epoch: 132, Loss: 981.28293, Residuals: -1.20221, Convergence: 0.003122\n",
      "Epoch: 133, Loss: 978.37885, Residuals: -1.19518, Convergence: 0.002968\n",
      "Epoch: 134, Loss: 975.61953, Residuals: -1.18836, Convergence: 0.002828\n",
      "Epoch: 135, Loss: 972.99345, Residuals: -1.18176, Convergence: 0.002699\n",
      "Epoch: 136, Loss: 970.49063, Residuals: -1.17538, Convergence: 0.002579\n",
      "Epoch: 137, Loss: 968.10282, Residuals: -1.16922, Convergence: 0.002466\n",
      "Epoch: 138, Loss: 965.82282, Residuals: -1.16330, Convergence: 0.002361\n",
      "Epoch: 139, Loss: 963.64423, Residuals: -1.15761, Convergence: 0.002261\n",
      "Epoch: 140, Loss: 961.56156, Residuals: -1.15215, Convergence: 0.002166\n",
      "Epoch: 141, Loss: 959.56929, Residuals: -1.14691, Convergence: 0.002076\n",
      "Epoch: 142, Loss: 957.66213, Residuals: -1.14190, Convergence: 0.001991\n",
      "Epoch: 143, Loss: 955.83578, Residuals: -1.13711, Convergence: 0.001911\n",
      "Epoch: 144, Loss: 954.08534, Residuals: -1.13252, Convergence: 0.001835\n",
      "Epoch: 145, Loss: 952.40604, Residuals: -1.12814, Convergence: 0.001763\n",
      "Epoch: 146, Loss: 950.79394, Residuals: -1.12394, Convergence: 0.001696\n",
      "Epoch: 147, Loss: 949.24503, Residuals: -1.11994, Convergence: 0.001632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 148, Loss: 947.75542, Residuals: -1.11610, Convergence: 0.001572\n",
      "Epoch: 149, Loss: 946.32126, Residuals: -1.11243, Convergence: 0.001516\n",
      "Epoch: 150, Loss: 944.93908, Residuals: -1.10892, Convergence: 0.001463\n",
      "Epoch: 151, Loss: 943.60557, Residuals: -1.10555, Convergence: 0.001413\n",
      "Epoch: 152, Loss: 942.31692, Residuals: -1.10232, Convergence: 0.001368\n",
      "Epoch: 153, Loss: 941.06931, Residuals: -1.09922, Convergence: 0.001326\n",
      "Epoch: 154, Loss: 939.85986, Residuals: -1.09624, Convergence: 0.001287\n",
      "Epoch: 155, Loss: 938.68434, Residuals: -1.09337, Convergence: 0.001252\n",
      "Epoch: 156, Loss: 937.53857, Residuals: -1.09060, Convergence: 0.001222\n",
      "Epoch: 157, Loss: 936.41899, Residuals: -1.08791, Convergence: 0.001196\n",
      "Epoch: 158, Loss: 935.32100, Residuals: -1.08530, Convergence: 0.001174\n",
      "Epoch: 159, Loss: 934.24059, Residuals: -1.08277, Convergence: 0.001156\n",
      "Epoch: 160, Loss: 933.17363, Residuals: -1.08029, Convergence: 0.001143\n",
      "Epoch: 161, Loss: 932.11617, Residuals: -1.07785, Convergence: 0.001134\n",
      "Epoch: 162, Loss: 931.06564, Residuals: -1.07545, Convergence: 0.001128\n",
      "Epoch: 163, Loss: 930.01979, Residuals: -1.07309, Convergence: 0.001125\n",
      "Epoch: 164, Loss: 928.97810, Residuals: -1.07075, Convergence: 0.001121\n",
      "Epoch: 165, Loss: 927.94198, Residuals: -1.06844, Convergence: 0.001117\n",
      "Epoch: 166, Loss: 926.91391, Residuals: -1.06616, Convergence: 0.001109\n",
      "Epoch: 167, Loss: 925.89799, Residuals: -1.06391, Convergence: 0.001097\n",
      "Epoch: 168, Loss: 924.89861, Residuals: -1.06170, Convergence: 0.001081\n",
      "Epoch: 169, Loss: 923.92075, Residuals: -1.05954, Convergence: 0.001058\n",
      "Epoch: 170, Loss: 922.96853, Residuals: -1.05743, Convergence: 0.001032\n",
      "Epoch: 171, Loss: 922.04547, Residuals: -1.05539, Convergence: 0.001001\n",
      "Epoch: 172, Loss: 921.15435, Residuals: -1.05342, Convergence: 0.000967\n",
      "Evidence 11206.182\n",
      "\n",
      "Epoch: 172, Evidence: 11206.18164, Convergence: 1.016247\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.75e-01\n",
      "Epoch: 172, Loss: 2348.33179, Residuals: -1.05342, Convergence:   inf\n",
      "Epoch: 173, Loss: 2310.18967, Residuals: -1.06268, Convergence: 0.016510\n",
      "Epoch: 174, Loss: 2283.45982, Residuals: -1.06132, Convergence: 0.011706\n",
      "Epoch: 175, Loss: 2261.23462, Residuals: -1.05918, Convergence: 0.009829\n",
      "Epoch: 176, Loss: 2242.55121, Residuals: -1.05684, Convergence: 0.008331\n",
      "Epoch: 177, Loss: 2226.71202, Residuals: -1.05442, Convergence: 0.007113\n",
      "Epoch: 178, Loss: 2213.17198, Residuals: -1.05194, Convergence: 0.006118\n",
      "Epoch: 179, Loss: 2201.48412, Residuals: -1.04942, Convergence: 0.005309\n",
      "Epoch: 180, Loss: 2191.27953, Residuals: -1.04686, Convergence: 0.004657\n",
      "Epoch: 181, Loss: 2182.24878, Residuals: -1.04424, Convergence: 0.004138\n",
      "Epoch: 182, Loss: 2174.14346, Residuals: -1.04153, Convergence: 0.003728\n",
      "Epoch: 183, Loss: 2166.77074, Residuals: -1.03872, Convergence: 0.003403\n",
      "Epoch: 184, Loss: 2159.99751, Residuals: -1.03580, Convergence: 0.003136\n",
      "Epoch: 185, Loss: 2153.74136, Residuals: -1.03279, Convergence: 0.002905\n",
      "Epoch: 186, Loss: 2147.95820, Residuals: -1.02974, Convergence: 0.002692\n",
      "Epoch: 187, Loss: 2142.61811, Residuals: -1.02669, Convergence: 0.002492\n",
      "Epoch: 188, Loss: 2137.69421, Residuals: -1.02369, Convergence: 0.002303\n",
      "Epoch: 189, Loss: 2133.15686, Residuals: -1.02077, Convergence: 0.002127\n",
      "Epoch: 190, Loss: 2128.97367, Residuals: -1.01795, Convergence: 0.001965\n",
      "Epoch: 191, Loss: 2125.11096, Residuals: -1.01524, Convergence: 0.001818\n",
      "Epoch: 192, Loss: 2121.53620, Residuals: -1.01264, Convergence: 0.001685\n",
      "Epoch: 193, Loss: 2118.22088, Residuals: -1.01016, Convergence: 0.001565\n",
      "Epoch: 194, Loss: 2115.13828, Residuals: -1.00778, Convergence: 0.001457\n",
      "Epoch: 195, Loss: 2112.26614, Residuals: -1.00552, Convergence: 0.001360\n",
      "Epoch: 196, Loss: 2109.58503, Residuals: -1.00336, Convergence: 0.001271\n",
      "Epoch: 197, Loss: 2107.07992, Residuals: -1.00131, Convergence: 0.001189\n",
      "Epoch: 198, Loss: 2104.73555, Residuals: -0.99935, Convergence: 0.001114\n",
      "Epoch: 199, Loss: 2102.54068, Residuals: -0.99748, Convergence: 0.001044\n",
      "Epoch: 200, Loss: 2100.48335, Residuals: -0.99570, Convergence: 0.000979\n",
      "Evidence 14386.114\n",
      "\n",
      "Epoch: 200, Evidence: 14386.11426, Convergence: 0.221042\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.37e-01\n",
      "Epoch: 200, Loss: 2476.64159, Residuals: -0.99570, Convergence:   inf\n",
      "Epoch: 201, Loss: 2463.47298, Residuals: -0.99331, Convergence: 0.005346\n",
      "Epoch: 202, Loss: 2452.57021, Residuals: -0.99033, Convergence: 0.004445\n",
      "Epoch: 203, Loss: 2443.11066, Residuals: -0.98740, Convergence: 0.003872\n",
      "Epoch: 204, Loss: 2434.85868, Residuals: -0.98462, Convergence: 0.003389\n",
      "Epoch: 205, Loss: 2427.62730, Residuals: -0.98201, Convergence: 0.002979\n",
      "Epoch: 206, Loss: 2421.26588, Residuals: -0.97960, Convergence: 0.002627\n",
      "Epoch: 207, Loss: 2415.64535, Residuals: -0.97738, Convergence: 0.002327\n",
      "Epoch: 208, Loss: 2410.65884, Residuals: -0.97533, Convergence: 0.002069\n",
      "Epoch: 209, Loss: 2406.21424, Residuals: -0.97344, Convergence: 0.001847\n",
      "Epoch: 210, Loss: 2402.23417, Residuals: -0.97170, Convergence: 0.001657\n",
      "Epoch: 211, Loss: 2398.65247, Residuals: -0.97010, Convergence: 0.001493\n",
      "Epoch: 212, Loss: 2395.41323, Residuals: -0.96861, Convergence: 0.001352\n",
      "Epoch: 213, Loss: 2392.46942, Residuals: -0.96724, Convergence: 0.001230\n",
      "Epoch: 214, Loss: 2389.78032, Residuals: -0.96597, Convergence: 0.001125\n",
      "Epoch: 215, Loss: 2387.31369, Residuals: -0.96479, Convergence: 0.001033\n",
      "Epoch: 216, Loss: 2385.04011, Residuals: -0.96370, Convergence: 0.000953\n",
      "Evidence 14765.407\n",
      "\n",
      "Epoch: 216, Evidence: 14765.40723, Convergence: 0.025688\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.36e-01\n",
      "Epoch: 216, Loss: 2481.72945, Residuals: -0.96370, Convergence:   inf\n",
      "Epoch: 217, Loss: 2475.02272, Residuals: -0.96071, Convergence: 0.002710\n",
      "Epoch: 218, Loss: 2469.45346, Residuals: -0.95812, Convergence: 0.002255\n",
      "Epoch: 219, Loss: 2464.72765, Residuals: -0.95592, Convergence: 0.001917\n",
      "Epoch: 220, Loss: 2460.66621, Residuals: -0.95406, Convergence: 0.001651\n",
      "Epoch: 221, Loss: 2457.13163, Residuals: -0.95248, Convergence: 0.001439\n",
      "Epoch: 222, Loss: 2454.01857, Residuals: -0.95115, Convergence: 0.001269\n",
      "Epoch: 223, Loss: 2451.24553, Residuals: -0.95001, Convergence: 0.001131\n",
      "Epoch: 224, Loss: 2448.74965, Residuals: -0.94905, Convergence: 0.001019\n",
      "Epoch: 225, Loss: 2446.48120, Residuals: -0.94823, Convergence: 0.000927\n",
      "Evidence 14851.907\n",
      "\n",
      "Epoch: 225, Evidence: 14851.90723, Convergence: 0.005824\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.63e-01\n",
      "Epoch: 225, Loss: 2483.15107, Residuals: -0.94823, Convergence:   inf\n",
      "Epoch: 226, Loss: 2479.10363, Residuals: -0.94612, Convergence: 0.001633\n",
      "Epoch: 227, Loss: 2475.73859, Residuals: -0.94450, Convergence: 0.001359\n",
      "Epoch: 228, Loss: 2472.86352, Residuals: -0.94324, Convergence: 0.001163\n",
      "Epoch: 229, Loss: 2470.35923, Residuals: -0.94225, Convergence: 0.001014\n",
      "Epoch: 230, Loss: 2468.13924, Residuals: -0.94148, Convergence: 0.000899\n",
      "Evidence 14884.434\n",
      "\n",
      "Epoch: 230, Evidence: 14884.43359, Convergence: 0.002185\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.11e-01\n",
      "Epoch: 230, Loss: 2484.01914, Residuals: -0.94148, Convergence:   inf\n",
      "Epoch: 231, Loss: 2481.10991, Residuals: -0.94005, Convergence: 0.001173\n",
      "Epoch: 232, Loss: 2478.67408, Residuals: -0.93901, Convergence: 0.000983\n",
      "Evidence 14897.570\n",
      "\n",
      "Epoch: 232, Evidence: 14897.57031, Convergence: 0.000882\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.75e-01\n",
      "Epoch: 232, Loss: 2484.70559, Residuals: -0.93901, Convergence:   inf\n",
      "Epoch: 233, Loss: 2480.11263, Residuals: -0.93754, Convergence: 0.001852\n",
      "Epoch: 234, Loss: 2476.62573, Residuals: -0.93655, Convergence: 0.001408\n",
      "Epoch: 235, Loss: 2473.80106, Residuals: -0.93593, Convergence: 0.001142\n",
      "Epoch: 236, Loss: 2471.40003, Residuals: -0.93563, Convergence: 0.000972\n",
      "Evidence 14915.570\n",
      "\n",
      "Epoch: 236, Evidence: 14915.57031, Convergence: 0.002088\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.45e-01\n",
      "Epoch: 236, Loss: 2484.84277, Residuals: -0.93563, Convergence:   inf\n",
      "Epoch: 237, Loss: 2481.75541, Residuals: -0.93427, Convergence: 0.001244\n",
      "Epoch: 238, Loss: 2479.31576, Residuals: -0.93379, Convergence: 0.000984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence 14926.843\n",
      "\n",
      "Epoch: 238, Evidence: 14926.84277, Convergence: 0.000755\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.23e-01\n",
      "Epoch: 238, Loss: 2485.04028, Residuals: -0.93379, Convergence:   inf\n",
      "Epoch: 239, Loss: 2480.47209, Residuals: -0.93270, Convergence: 0.001842\n",
      "Epoch: 240, Loss: 2477.18631, Residuals: -0.93433, Convergence: 0.001326\n",
      "Epoch: 241, Loss: 2474.48309, Residuals: -0.93451, Convergence: 0.001092\n",
      "Epoch: 242, Loss: 2472.09492, Residuals: -0.93680, Convergence: 0.000966\n",
      "Evidence 14943.048\n",
      "\n",
      "Epoch: 242, Evidence: 14943.04785, Convergence: 0.001839\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.09e-01\n",
      "Epoch: 242, Loss: 2484.29129, Residuals: -0.93680, Convergence:   inf\n",
      "Epoch: 243, Loss: 2482.08687, Residuals: -0.93595, Convergence: 0.000888\n",
      "Evidence 14950.175\n",
      "\n",
      "Epoch: 243, Evidence: 14950.17480, Convergence: 0.000477\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.93e-02\n",
      "Epoch: 243, Loss: 2485.26849, Residuals: -0.93595, Convergence:   inf\n",
      "Epoch: 244, Loss: 2524.57934, Residuals: -0.97628, Convergence: -0.015571\n",
      "Epoch: 244, Loss: 2482.71301, Residuals: -0.93676, Convergence: 0.001029\n",
      "Epoch: 245, Loss: 2482.37323, Residuals: -0.93761, Convergence: 0.000137\n",
      "Evidence 14955.398\n",
      "\n",
      "Epoch: 245, Evidence: 14955.39844, Convergence: 0.000826\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.35e-02\n",
      "Epoch: 245, Loss: 2485.17931, Residuals: -0.93761, Convergence:   inf\n",
      "Epoch: 246, Loss: 2538.67387, Residuals: -0.98438, Convergence: -0.021072\n",
      "Epoch: 246, Loss: 2482.50630, Residuals: -0.93749, Convergence: 0.001077\n",
      "Epoch: 247, Loss: 2482.24162, Residuals: -0.93780, Convergence: 0.000107\n",
      "Evidence 14960.691\n",
      "\n",
      "Epoch: 247, Evidence: 14960.69141, Convergence: 0.001179\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 6.29e-02\n",
      "Epoch: 247, Loss: 2484.98059, Residuals: -0.93780, Convergence:   inf\n",
      "Epoch: 248, Loss: 2482.71996, Residuals: -0.93951, Convergence: 0.000911\n",
      "Evidence 14964.687\n",
      "\n",
      "Epoch: 248, Evidence: 14964.68652, Convergence: 0.000267\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.98e-02\n",
      "Epoch: 248, Loss: 2484.59900, Residuals: -0.93951, Convergence:   inf\n",
      "Epoch: 249, Loss: 2492.05488, Residuals: -0.95755, Convergence: -0.002992\n",
      "Epoch: 249, Loss: 2484.51135, Residuals: -0.94164, Convergence: 0.000035\n",
      "Evidence 14966.059\n",
      "\n",
      "Epoch: 249, Evidence: 14966.05859, Convergence: 0.000359\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.04e-02\n",
      "Epoch: 249, Loss: 2485.17335, Residuals: -0.94164, Convergence:   inf\n",
      "Epoch: 250, Loss: 2546.90387, Residuals: -1.00088, Convergence: -0.024237\n",
      "Epoch: 250, Loss: 2483.24862, Residuals: -0.94133, Convergence: 0.000775\n",
      "Evidence 14969.340\n",
      "\n",
      "Epoch: 250, Evidence: 14969.33984, Convergence: 0.000578\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 382.41916, Residuals: -4.54985, Convergence:   inf\n",
      "Epoch: 1, Loss: 356.63587, Residuals: -4.42889, Convergence: 0.072296\n",
      "Epoch: 2, Loss: 335.51006, Residuals: -4.26364, Convergence: 0.062966\n",
      "Epoch: 3, Loss: 319.35088, Residuals: -4.09775, Convergence: 0.050600\n",
      "Epoch: 4, Loss: 307.03350, Residuals: -3.95202, Convergence: 0.040117\n",
      "Epoch: 5, Loss: 297.26487, Residuals: -3.82350, Convergence: 0.032862\n",
      "Epoch: 6, Loss: 289.33011, Residuals: -3.71165, Convergence: 0.027425\n",
      "Epoch: 7, Loss: 282.73879, Residuals: -3.61585, Convergence: 0.023312\n",
      "Epoch: 8, Loss: 277.13012, Residuals: -3.53405, Convergence: 0.020238\n",
      "Epoch: 9, Loss: 272.24907, Residuals: -3.46391, Convergence: 0.017929\n",
      "Epoch: 10, Loss: 267.91422, Residuals: -3.40331, Convergence: 0.016180\n",
      "Epoch: 11, Loss: 263.99406, Residuals: -3.35042, Convergence: 0.014849\n",
      "Epoch: 12, Loss: 260.39248, Residuals: -3.30373, Convergence: 0.013831\n",
      "Epoch: 13, Loss: 257.03933, Residuals: -3.26188, Convergence: 0.013045\n",
      "Epoch: 14, Loss: 253.88395, Residuals: -3.22373, Convergence: 0.012428\n",
      "Epoch: 15, Loss: 250.89131, Residuals: -3.18830, Convergence: 0.011928\n",
      "Epoch: 16, Loss: 248.03988, Residuals: -3.15489, Convergence: 0.011496\n",
      "Epoch: 17, Loss: 245.31531, Residuals: -3.12306, Convergence: 0.011106\n",
      "Epoch: 18, Loss: 242.69858, Residuals: -3.09245, Convergence: 0.010782\n",
      "Epoch: 19, Loss: 240.16058, Residuals: -3.06260, Convergence: 0.010568\n",
      "Epoch: 20, Loss: 237.66773, Residuals: -3.03297, Convergence: 0.010489\n",
      "Epoch: 21, Loss: 235.19132, Residuals: -3.00310, Convergence: 0.010529\n",
      "Epoch: 22, Loss: 232.71151, Residuals: -2.97273, Convergence: 0.010656\n",
      "Epoch: 23, Loss: 230.20671, Residuals: -2.94161, Convergence: 0.010881\n",
      "Epoch: 24, Loss: 227.63554, Residuals: -2.90931, Convergence: 0.011295\n",
      "Epoch: 25, Loss: 224.93825, Residuals: -2.87515, Convergence: 0.011991\n",
      "Epoch: 26, Loss: 222.09506, Residuals: -2.83881, Convergence: 0.012802\n",
      "Epoch: 27, Loss: 219.21138, Residuals: -2.80135, Convergence: 0.013155\n",
      "Epoch: 28, Loss: 216.41051, Residuals: -2.76420, Convergence: 0.012942\n",
      "Epoch: 29, Loss: 213.71913, Residuals: -2.72780, Convergence: 0.012593\n",
      "Epoch: 30, Loss: 211.12095, Residuals: -2.69207, Convergence: 0.012307\n",
      "Epoch: 31, Loss: 208.59658, Residuals: -2.65685, Convergence: 0.012102\n",
      "Epoch: 32, Loss: 206.13183, Residuals: -2.62198, Convergence: 0.011957\n",
      "Epoch: 33, Loss: 203.71791, Residuals: -2.58737, Convergence: 0.011849\n",
      "Epoch: 34, Loss: 201.35026, Residuals: -2.55292, Convergence: 0.011759\n",
      "Epoch: 35, Loss: 199.02728, Residuals: -2.51861, Convergence: 0.011672\n",
      "Epoch: 36, Loss: 196.74933, Residuals: -2.48441, Convergence: 0.011578\n",
      "Epoch: 37, Loss: 194.51799, Residuals: -2.45030, Convergence: 0.011471\n",
      "Epoch: 38, Loss: 192.33540, Residuals: -2.41630, Convergence: 0.011348\n",
      "Epoch: 39, Loss: 190.20397, Residuals: -2.38242, Convergence: 0.011206\n",
      "Epoch: 40, Loss: 188.12608, Residuals: -2.34869, Convergence: 0.011045\n",
      "Epoch: 41, Loss: 186.10386, Residuals: -2.31514, Convergence: 0.010866\n",
      "Epoch: 42, Loss: 184.13917, Residuals: -2.28181, Convergence: 0.010670\n",
      "Epoch: 43, Loss: 182.23361, Residuals: -2.24873, Convergence: 0.010457\n",
      "Epoch: 44, Loss: 180.38844, Residuals: -2.21594, Convergence: 0.010229\n",
      "Epoch: 45, Loss: 178.60479, Residuals: -2.18349, Convergence: 0.009987\n",
      "Epoch: 46, Loss: 176.88363, Residuals: -2.15142, Convergence: 0.009730\n",
      "Epoch: 47, Loss: 175.22579, Residuals: -2.11977, Convergence: 0.009461\n",
      "Epoch: 48, Loss: 173.63185, Residuals: -2.08860, Convergence: 0.009180\n",
      "Epoch: 49, Loss: 172.10201, Residuals: -2.05794, Convergence: 0.008889\n",
      "Epoch: 50, Loss: 170.63596, Residuals: -2.02784, Convergence: 0.008592\n",
      "Epoch: 51, Loss: 169.23275, Residuals: -1.99833, Convergence: 0.008292\n",
      "Epoch: 52, Loss: 167.89085, Residuals: -1.96943, Convergence: 0.007993\n",
      "Epoch: 53, Loss: 166.60819, Residuals: -1.94118, Convergence: 0.007699\n",
      "Epoch: 54, Loss: 165.38224, Residuals: -1.91359, Convergence: 0.007413\n",
      "Epoch: 55, Loss: 164.21018, Residuals: -1.88666, Convergence: 0.007138\n",
      "Epoch: 56, Loss: 163.08900, Residuals: -1.86039, Convergence: 0.006875\n",
      "Epoch: 57, Loss: 162.01571, Residuals: -1.83478, Convergence: 0.006625\n",
      "Epoch: 58, Loss: 160.98744, Residuals: -1.80982, Convergence: 0.006387\n",
      "Epoch: 59, Loss: 160.00161, Residuals: -1.78550, Convergence: 0.006161\n",
      "Epoch: 60, Loss: 159.05592, Residuals: -1.76180, Convergence: 0.005946\n",
      "Epoch: 61, Loss: 158.14844, Residuals: -1.73872, Convergence: 0.005738\n",
      "Epoch: 62, Loss: 157.27747, Residuals: -1.71625, Convergence: 0.005538\n",
      "Epoch: 63, Loss: 156.44161, Residuals: -1.69439, Convergence: 0.005343\n",
      "Epoch: 64, Loss: 155.63959, Residuals: -1.67314, Convergence: 0.005153\n",
      "Epoch: 65, Loss: 154.87029, Residuals: -1.65249, Convergence: 0.004967\n",
      "Epoch: 66, Loss: 154.13264, Residuals: -1.63246, Convergence: 0.004786\n",
      "Epoch: 67, Loss: 153.42565, Residuals: -1.61302, Convergence: 0.004608\n",
      "Epoch: 68, Loss: 152.74833, Residuals: -1.59419, Convergence: 0.004434\n",
      "Epoch: 69, Loss: 152.09973, Residuals: -1.57596, Convergence: 0.004264\n",
      "Epoch: 70, Loss: 151.47890, Residuals: -1.55831, Convergence: 0.004098\n",
      "Epoch: 71, Loss: 150.88491, Residuals: -1.54125, Convergence: 0.003937\n",
      "Epoch: 72, Loss: 150.31684, Residuals: -1.52477, Convergence: 0.003779\n",
      "Epoch: 73, Loss: 149.77378, Residuals: -1.50885, Convergence: 0.003626\n",
      "Epoch: 74, Loss: 149.25485, Residuals: -1.49348, Convergence: 0.003477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75, Loss: 148.75915, Residuals: -1.47866, Convergence: 0.003332\n",
      "Epoch: 76, Loss: 148.28582, Residuals: -1.46437, Convergence: 0.003192\n",
      "Epoch: 77, Loss: 147.83403, Residuals: -1.45059, Convergence: 0.003056\n",
      "Epoch: 78, Loss: 147.40293, Residuals: -1.43733, Convergence: 0.002925\n",
      "Epoch: 79, Loss: 146.99174, Residuals: -1.42455, Convergence: 0.002797\n",
      "Epoch: 80, Loss: 146.59964, Residuals: -1.41225, Convergence: 0.002675\n",
      "Epoch: 81, Loss: 146.22588, Residuals: -1.40042, Convergence: 0.002556\n",
      "Epoch: 82, Loss: 145.86969, Residuals: -1.38905, Convergence: 0.002442\n",
      "Epoch: 83, Loss: 145.53034, Residuals: -1.37811, Convergence: 0.002332\n",
      "Epoch: 84, Loss: 145.20713, Residuals: -1.36759, Convergence: 0.002226\n",
      "Epoch: 85, Loss: 144.89935, Residuals: -1.35749, Convergence: 0.002124\n",
      "Epoch: 86, Loss: 144.60636, Residuals: -1.34778, Convergence: 0.002026\n",
      "Epoch: 87, Loss: 144.32749, Residuals: -1.33846, Convergence: 0.001932\n",
      "Epoch: 88, Loss: 144.06213, Residuals: -1.32951, Convergence: 0.001842\n",
      "Epoch: 89, Loss: 143.80972, Residuals: -1.32093, Convergence: 0.001755\n",
      "Epoch: 90, Loss: 143.56968, Residuals: -1.31269, Convergence: 0.001672\n",
      "Epoch: 91, Loss: 143.34150, Residuals: -1.30478, Convergence: 0.001592\n",
      "Epoch: 92, Loss: 143.12467, Residuals: -1.29720, Convergence: 0.001515\n",
      "Epoch: 93, Loss: 142.91876, Residuals: -1.28994, Convergence: 0.001441\n",
      "Epoch: 94, Loss: 142.72334, Residuals: -1.28297, Convergence: 0.001369\n",
      "Epoch: 95, Loss: 142.53802, Residuals: -1.27630, Convergence: 0.001300\n",
      "Epoch: 96, Loss: 142.36243, Residuals: -1.26992, Convergence: 0.001233\n",
      "Epoch: 97, Loss: 142.19625, Residuals: -1.26381, Convergence: 0.001169\n",
      "Epoch: 98, Loss: 142.03917, Residuals: -1.25798, Convergence: 0.001106\n",
      "Epoch: 99, Loss: 141.89091, Residuals: -1.25240, Convergence: 0.001045\n",
      "Epoch: 100, Loss: 141.75118, Residuals: -1.24708, Convergence: 0.000986\n",
      "Evidence -183.255\n",
      "\n",
      "Epoch: 100, Evidence: -183.25476, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.25e-01\n",
      "Epoch: 100, Loss: 1368.08339, Residuals: -1.24708, Convergence:   inf\n",
      "Epoch: 101, Loss: 1305.32540, Residuals: -1.27710, Convergence: 0.048078\n",
      "Epoch: 102, Loss: 1257.69602, Residuals: -1.30050, Convergence: 0.037870\n",
      "Epoch: 103, Loss: 1221.84191, Residuals: -1.31711, Convergence: 0.029344\n",
      "Epoch: 104, Loss: 1194.05950, Residuals: -1.32857, Convergence: 0.023267\n",
      "Epoch: 105, Loss: 1171.66501, Residuals: -1.33690, Convergence: 0.019113\n",
      "Epoch: 106, Loss: 1153.11905, Residuals: -1.34315, Convergence: 0.016083\n",
      "Epoch: 107, Loss: 1137.50071, Residuals: -1.34781, Convergence: 0.013730\n",
      "Epoch: 108, Loss: 1124.18629, Residuals: -1.35112, Convergence: 0.011844\n",
      "Epoch: 109, Loss: 1112.71541, Residuals: -1.35323, Convergence: 0.010309\n",
      "Epoch: 110, Loss: 1102.72901, Residuals: -1.35427, Convergence: 0.009056\n",
      "Epoch: 111, Loss: 1093.93938, Residuals: -1.35435, Convergence: 0.008035\n",
      "Epoch: 112, Loss: 1086.11229, Residuals: -1.35357, Convergence: 0.007207\n",
      "Epoch: 113, Loss: 1079.05262, Residuals: -1.35199, Convergence: 0.006542\n",
      "Epoch: 114, Loss: 1072.59832, Residuals: -1.34968, Convergence: 0.006017\n",
      "Epoch: 115, Loss: 1066.61243, Residuals: -1.34670, Convergence: 0.005612\n",
      "Epoch: 116, Loss: 1060.98045, Residuals: -1.34309, Convergence: 0.005308\n",
      "Epoch: 117, Loss: 1055.60581, Residuals: -1.33888, Convergence: 0.005092\n",
      "Epoch: 118, Loss: 1050.40654, Residuals: -1.33410, Convergence: 0.004950\n",
      "Epoch: 119, Loss: 1045.31261, Residuals: -1.32875, Convergence: 0.004873\n",
      "Epoch: 120, Loss: 1040.26185, Residuals: -1.32287, Convergence: 0.004855\n",
      "Epoch: 121, Loss: 1035.20105, Residuals: -1.31648, Convergence: 0.004889\n",
      "Epoch: 122, Loss: 1030.09534, Residuals: -1.30961, Convergence: 0.004957\n",
      "Epoch: 123, Loss: 1024.94111, Residuals: -1.30235, Convergence: 0.005029\n",
      "Epoch: 124, Loss: 1019.78085, Residuals: -1.29477, Convergence: 0.005060\n",
      "Epoch: 125, Loss: 1014.69579, Residuals: -1.28697, Convergence: 0.005011\n",
      "Epoch: 126, Loss: 1009.78082, Residuals: -1.27903, Convergence: 0.004867\n",
      "Epoch: 127, Loss: 1005.11091, Residuals: -1.27104, Convergence: 0.004646\n",
      "Epoch: 128, Loss: 1000.72358, Residuals: -1.26304, Convergence: 0.004384\n",
      "Epoch: 129, Loss: 996.62383, Residuals: -1.25510, Convergence: 0.004114\n",
      "Epoch: 130, Loss: 992.79568, Residuals: -1.24725, Convergence: 0.003856\n",
      "Epoch: 131, Loss: 989.21449, Residuals: -1.23954, Convergence: 0.003620\n",
      "Epoch: 132, Loss: 985.85424, Residuals: -1.23200, Convergence: 0.003408\n",
      "Epoch: 133, Loss: 982.69156, Residuals: -1.22466, Convergence: 0.003218\n",
      "Epoch: 134, Loss: 979.70594, Residuals: -1.21752, Convergence: 0.003047\n",
      "Epoch: 135, Loss: 976.88123, Residuals: -1.21062, Convergence: 0.002892\n",
      "Epoch: 136, Loss: 974.20379, Residuals: -1.20396, Convergence: 0.002748\n",
      "Epoch: 137, Loss: 971.66220, Residuals: -1.19755, Convergence: 0.002616\n",
      "Epoch: 138, Loss: 969.24717, Residuals: -1.19139, Convergence: 0.002492\n",
      "Epoch: 139, Loss: 966.95027, Residuals: -1.18549, Convergence: 0.002375\n",
      "Epoch: 140, Loss: 964.76421, Residuals: -1.17985, Convergence: 0.002266\n",
      "Epoch: 141, Loss: 962.68248, Residuals: -1.17446, Convergence: 0.002162\n",
      "Epoch: 142, Loss: 960.69872, Residuals: -1.16932, Convergence: 0.002065\n",
      "Epoch: 143, Loss: 958.80669, Residuals: -1.16442, Convergence: 0.001973\n",
      "Epoch: 144, Loss: 957.00118, Residuals: -1.15976, Convergence: 0.001887\n",
      "Epoch: 145, Loss: 955.27660, Residuals: -1.15531, Convergence: 0.001805\n",
      "Epoch: 146, Loss: 953.62833, Residuals: -1.15108, Convergence: 0.001728\n",
      "Epoch: 147, Loss: 952.05163, Residuals: -1.14706, Convergence: 0.001656\n",
      "Epoch: 148, Loss: 950.54210, Residuals: -1.14322, Convergence: 0.001588\n",
      "Epoch: 149, Loss: 949.09600, Residuals: -1.13957, Convergence: 0.001524\n",
      "Epoch: 150, Loss: 947.70930, Residuals: -1.13609, Convergence: 0.001463\n",
      "Epoch: 151, Loss: 946.37891, Residuals: -1.13277, Convergence: 0.001406\n",
      "Epoch: 152, Loss: 945.10146, Residuals: -1.12960, Convergence: 0.001352\n",
      "Epoch: 153, Loss: 943.87369, Residuals: -1.12658, Convergence: 0.001301\n",
      "Epoch: 154, Loss: 942.69273, Residuals: -1.12369, Convergence: 0.001253\n",
      "Epoch: 155, Loss: 941.55554, Residuals: -1.12092, Convergence: 0.001208\n",
      "Epoch: 156, Loss: 940.45904, Residuals: -1.11827, Convergence: 0.001166\n",
      "Epoch: 157, Loss: 939.40049, Residuals: -1.11573, Convergence: 0.001127\n",
      "Epoch: 158, Loss: 938.37667, Residuals: -1.11329, Convergence: 0.001091\n",
      "Epoch: 159, Loss: 937.38427, Residuals: -1.11094, Convergence: 0.001059\n",
      "Epoch: 160, Loss: 936.42070, Residuals: -1.10868, Convergence: 0.001029\n",
      "Epoch: 161, Loss: 935.48191, Residuals: -1.10649, Convergence: 0.001004\n",
      "Epoch: 162, Loss: 934.56472, Residuals: -1.10436, Convergence: 0.000981\n",
      "Evidence 11171.133\n",
      "\n",
      "Epoch: 162, Evidence: 11171.13281, Convergence: 1.016404\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.76e-01\n",
      "Epoch: 162, Loss: 2347.26526, Residuals: -1.10436, Convergence:   inf\n",
      "Epoch: 163, Loss: 2307.91431, Residuals: -1.11234, Convergence: 0.017050\n",
      "Epoch: 164, Loss: 2280.23115, Residuals: -1.11083, Convergence: 0.012141\n",
      "Epoch: 165, Loss: 2257.05417, Residuals: -1.10827, Convergence: 0.010269\n",
      "Epoch: 166, Loss: 2237.34915, Residuals: -1.10530, Convergence: 0.008807\n",
      "Epoch: 167, Loss: 2220.42652, Residuals: -1.10208, Convergence: 0.007621\n",
      "Epoch: 168, Loss: 2205.77113, Residuals: -1.09867, Convergence: 0.006644\n",
      "Epoch: 169, Loss: 2192.97277, Residuals: -1.09513, Convergence: 0.005836\n",
      "Epoch: 170, Loss: 2181.69341, Residuals: -1.09150, Convergence: 0.005170\n",
      "Epoch: 171, Loss: 2171.65170, Residuals: -1.08778, Convergence: 0.004624\n",
      "Epoch: 172, Loss: 2162.60908, Residuals: -1.08399, Convergence: 0.004181\n",
      "Epoch: 173, Loss: 2154.37159, Residuals: -1.08012, Convergence: 0.003824\n",
      "Epoch: 174, Loss: 2146.78883, Residuals: -1.07618, Convergence: 0.003532\n",
      "Epoch: 175, Loss: 2139.76036, Residuals: -1.07216, Convergence: 0.003285\n",
      "Epoch: 176, Loss: 2133.22857, Residuals: -1.06811, Convergence: 0.003062\n",
      "Epoch: 177, Loss: 2127.16367, Residuals: -1.06407, Convergence: 0.002851\n",
      "Epoch: 178, Loss: 2121.54598, Residuals: -1.06008, Convergence: 0.002648\n",
      "Epoch: 179, Loss: 2116.35685, Residuals: -1.05618, Convergence: 0.002452\n",
      "Epoch: 180, Loss: 2111.57042, Residuals: -1.05241, Convergence: 0.002267\n",
      "Epoch: 181, Loss: 2107.15918, Residuals: -1.04880, Convergence: 0.002093\n",
      "Epoch: 182, Loss: 2103.08990, Residuals: -1.04534, Convergence: 0.001935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 183, Loss: 2099.33193, Residuals: -1.04206, Convergence: 0.001790\n",
      "Epoch: 184, Loss: 2095.85286, Residuals: -1.03894, Convergence: 0.001660\n",
      "Epoch: 185, Loss: 2092.62292, Residuals: -1.03599, Convergence: 0.001543\n",
      "Epoch: 186, Loss: 2089.61540, Residuals: -1.03320, Convergence: 0.001439\n",
      "Epoch: 187, Loss: 2086.80549, Residuals: -1.03057, Convergence: 0.001347\n",
      "Epoch: 188, Loss: 2084.17145, Residuals: -1.02808, Convergence: 0.001264\n",
      "Epoch: 189, Loss: 2081.69398, Residuals: -1.02574, Convergence: 0.001190\n",
      "Epoch: 190, Loss: 2079.35756, Residuals: -1.02354, Convergence: 0.001124\n",
      "Epoch: 191, Loss: 2077.14829, Residuals: -1.02146, Convergence: 0.001064\n",
      "Epoch: 192, Loss: 2075.05502, Residuals: -1.01951, Convergence: 0.001009\n",
      "Epoch: 193, Loss: 2073.06808, Residuals: -1.01768, Convergence: 0.000958\n",
      "Evidence 14334.121\n",
      "\n",
      "Epoch: 193, Evidence: 14334.12109, Convergence: 0.220661\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.37e-01\n",
      "Epoch: 193, Loss: 2471.35017, Residuals: -1.01768, Convergence:   inf\n",
      "Epoch: 194, Loss: 2457.15055, Residuals: -1.01370, Convergence: 0.005779\n",
      "Epoch: 195, Loss: 2445.63535, Residuals: -1.00954, Convergence: 0.004708\n",
      "Epoch: 196, Loss: 2435.78328, Residuals: -1.00566, Convergence: 0.004045\n",
      "Epoch: 197, Loss: 2427.28987, Residuals: -1.00211, Convergence: 0.003499\n",
      "Epoch: 198, Loss: 2419.91819, Residuals: -0.99889, Convergence: 0.003046\n",
      "Epoch: 199, Loss: 2413.47868, Residuals: -0.99598, Convergence: 0.002668\n",
      "Epoch: 200, Loss: 2407.81629, Residuals: -0.99336, Convergence: 0.002352\n",
      "Epoch: 201, Loss: 2402.80360, Residuals: -0.99099, Convergence: 0.002086\n",
      "Epoch: 202, Loss: 2398.33660, Residuals: -0.98886, Convergence: 0.001863\n",
      "Epoch: 203, Loss: 2394.32920, Residuals: -0.98693, Convergence: 0.001674\n",
      "Epoch: 204, Loss: 2390.71101, Residuals: -0.98519, Convergence: 0.001513\n",
      "Epoch: 205, Loss: 2387.42407, Residuals: -0.98360, Convergence: 0.001377\n",
      "Epoch: 206, Loss: 2384.42183, Residuals: -0.98217, Convergence: 0.001259\n",
      "Epoch: 207, Loss: 2381.66555, Residuals: -0.98086, Convergence: 0.001157\n",
      "Epoch: 208, Loss: 2379.12257, Residuals: -0.97967, Convergence: 0.001069\n",
      "Epoch: 209, Loss: 2376.76880, Residuals: -0.97857, Convergence: 0.000990\n",
      "Evidence 14733.905\n",
      "\n",
      "Epoch: 209, Evidence: 14733.90527, Convergence: 0.027134\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.33e-01\n",
      "Epoch: 209, Loss: 2476.63628, Residuals: -0.97857, Convergence:   inf\n",
      "Epoch: 210, Loss: 2470.16406, Residuals: -0.97514, Convergence: 0.002620\n",
      "Epoch: 211, Loss: 2464.79706, Residuals: -0.97222, Convergence: 0.002177\n",
      "Epoch: 212, Loss: 2460.21894, Residuals: -0.96976, Convergence: 0.001861\n",
      "Epoch: 213, Loss: 2456.25383, Residuals: -0.96769, Convergence: 0.001614\n",
      "Epoch: 214, Loss: 2452.77519, Residuals: -0.96594, Convergence: 0.001418\n",
      "Epoch: 215, Loss: 2449.68737, Residuals: -0.96446, Convergence: 0.001260\n",
      "Epoch: 216, Loss: 2446.91615, Residuals: -0.96320, Convergence: 0.001133\n",
      "Epoch: 217, Loss: 2444.40506, Residuals: -0.96212, Convergence: 0.001027\n",
      "Epoch: 218, Loss: 2442.11142, Residuals: -0.96121, Convergence: 0.000939\n",
      "Evidence 14817.895\n",
      "\n",
      "Epoch: 218, Evidence: 14817.89453, Convergence: 0.005668\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.60e-01\n",
      "Epoch: 218, Loss: 2478.21748, Residuals: -0.96121, Convergence:   inf\n",
      "Epoch: 219, Loss: 2474.45569, Residuals: -0.95864, Convergence: 0.001520\n",
      "Epoch: 220, Loss: 2471.29087, Residuals: -0.95664, Convergence: 0.001281\n",
      "Epoch: 221, Loss: 2468.54561, Residuals: -0.95506, Convergence: 0.001112\n",
      "Epoch: 222, Loss: 2466.11839, Residuals: -0.95380, Convergence: 0.000984\n",
      "Evidence 14845.976\n",
      "\n",
      "Epoch: 222, Evidence: 14845.97559, Convergence: 0.001891\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.09e-01\n",
      "Epoch: 222, Loss: 2479.34474, Residuals: -0.95380, Convergence:   inf\n",
      "Epoch: 223, Loss: 2476.50491, Residuals: -0.95176, Convergence: 0.001147\n",
      "Epoch: 224, Loss: 2474.08996, Residuals: -0.95023, Convergence: 0.000976\n",
      "Evidence 14857.752\n",
      "\n",
      "Epoch: 224, Evidence: 14857.75195, Convergence: 0.000793\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.74e-01\n",
      "Epoch: 224, Loss: 2480.15796, Residuals: -0.95023, Convergence:   inf\n",
      "Epoch: 225, Loss: 2475.57553, Residuals: -0.94746, Convergence: 0.001851\n",
      "Epoch: 226, Loss: 2472.05750, Residuals: -0.94588, Convergence: 0.001423\n",
      "Epoch: 227, Loss: 2469.16330, Residuals: -0.94489, Convergence: 0.001172\n",
      "Epoch: 228, Loss: 2466.68635, Residuals: -0.94436, Convergence: 0.001004\n",
      "Epoch: 229, Loss: 2464.50403, Residuals: -0.94412, Convergence: 0.000886\n",
      "Evidence 14878.010\n",
      "\n",
      "Epoch: 229, Evidence: 14878.00977, Convergence: 0.002153\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.44e-01\n",
      "Epoch: 229, Loss: 2480.31492, Residuals: -0.94412, Convergence:   inf\n",
      "Epoch: 230, Loss: 2477.45754, Residuals: -0.94196, Convergence: 0.001153\n",
      "Epoch: 231, Loss: 2475.14839, Residuals: -0.94105, Convergence: 0.000933\n",
      "Evidence 14889.080\n",
      "\n",
      "Epoch: 231, Evidence: 14889.08008, Convergence: 0.000744\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.22e-01\n",
      "Epoch: 231, Loss: 2480.60713, Residuals: -0.94105, Convergence:   inf\n",
      "Epoch: 232, Loss: 2476.35301, Residuals: -0.93850, Convergence: 0.001718\n",
      "Epoch: 233, Loss: 2473.20373, Residuals: -0.93994, Convergence: 0.001273\n",
      "Epoch: 234, Loss: 2470.57663, Residuals: -0.94051, Convergence: 0.001063\n",
      "Epoch: 235, Loss: 2468.31880, Residuals: -0.94366, Convergence: 0.000915\n",
      "Evidence 14904.332\n",
      "\n",
      "Epoch: 235, Evidence: 14904.33203, Convergence: 0.001766\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.08e-01\n",
      "Epoch: 235, Loss: 2480.04618, Residuals: -0.94366, Convergence:   inf\n",
      "Epoch: 236, Loss: 2478.48381, Residuals: -0.94287, Convergence: 0.000630\n",
      "Evidence 14910.371\n",
      "\n",
      "Epoch: 236, Evidence: 14910.37109, Convergence: 0.000405\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.77e-02\n",
      "Epoch: 236, Loss: 2481.07010, Residuals: -0.94287, Convergence:   inf\n",
      "Epoch: 237, Loss: 2530.28868, Residuals: -0.98633, Convergence: -0.019452\n",
      "Epoch: 237, Loss: 2478.68069, Residuals: -0.94130, Convergence: 0.000964\n",
      "Evidence 14914.862\n",
      "\n",
      "Epoch: 237, Evidence: 14914.86230, Convergence: 0.000706\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.03e-02\n",
      "Epoch: 237, Loss: 2480.54900, Residuals: -0.94130, Convergence:   inf\n",
      "Epoch: 238, Loss: 2484.32509, Residuals: -0.94223, Convergence: -0.001520\n",
      "Epoch: 238, Loss: 2480.32570, Residuals: -0.93973, Convergence: 0.000090\n",
      "Evidence 14916.611\n",
      "\n",
      "Epoch: 238, Evidence: 14916.61133, Convergence: 0.000823\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 383.28887, Residuals: -4.52866, Convergence:   inf\n",
      "Epoch: 1, Loss: 357.54916, Residuals: -4.40870, Convergence: 0.071989\n",
      "Epoch: 2, Loss: 336.51742, Residuals: -4.24565, Convergence: 0.062498\n",
      "Epoch: 3, Loss: 320.43280, Residuals: -4.08181, Convergence: 0.050197\n",
      "Epoch: 4, Loss: 308.15688, Residuals: -3.93729, Convergence: 0.039837\n",
      "Epoch: 5, Loss: 298.41516, Residuals: -3.80948, Convergence: 0.032645\n",
      "Epoch: 6, Loss: 290.50309, Residuals: -3.69823, Convergence: 0.027236\n",
      "Epoch: 7, Loss: 283.93232, Residuals: -3.60304, Convergence: 0.023142\n",
      "Epoch: 8, Loss: 278.34108, Residuals: -3.52182, Convergence: 0.020088\n",
      "Epoch: 9, Loss: 273.47374, Residuals: -3.45222, Convergence: 0.017798\n",
      "Epoch: 10, Loss: 269.14814, Residuals: -3.39210, Convergence: 0.016071\n",
      "Epoch: 11, Loss: 265.23180, Residuals: -3.33964, Convergence: 0.014766\n",
      "Epoch: 12, Loss: 261.62760, Residuals: -3.29330, Convergence: 0.013776\n",
      "Epoch: 13, Loss: 258.26475, Residuals: -3.25173, Convergence: 0.013021\n",
      "Epoch: 14, Loss: 255.09312, Residuals: -3.21378, Convergence: 0.012433\n",
      "Epoch: 15, Loss: 252.08072, Residuals: -3.17853, Convergence: 0.011950\n",
      "Epoch: 16, Loss: 249.21141, Residuals: -3.14538, Convergence: 0.011514\n",
      "Epoch: 17, Loss: 246.47462, Residuals: -3.11397, Convergence: 0.011104\n",
      "Epoch: 18, Loss: 243.85072, Residuals: -3.08396, Convergence: 0.010760\n",
      "Epoch: 19, Loss: 241.30781, Residuals: -3.05484, Convergence: 0.010538\n",
      "Epoch: 20, Loss: 238.80958, Residuals: -3.02603, Convergence: 0.010461\n",
      "Epoch: 21, Loss: 236.32467, Residuals: -2.99701, Convergence: 0.010515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Loss: 233.83099, Residuals: -2.96743, Convergence: 0.010664\n",
      "Epoch: 23, Loss: 231.30687, Residuals: -2.93703, Convergence: 0.010912\n",
      "Epoch: 24, Loss: 228.71381, Residuals: -2.90536, Convergence: 0.011338\n",
      "Epoch: 25, Loss: 225.99514, Residuals: -2.87175, Convergence: 0.012030\n",
      "Epoch: 26, Loss: 223.12537, Residuals: -2.83579, Convergence: 0.012862\n",
      "Epoch: 27, Loss: 220.19582, Residuals: -2.79837, Convergence: 0.013304\n",
      "Epoch: 28, Loss: 217.33322, Residuals: -2.76092, Convergence: 0.013171\n",
      "Epoch: 29, Loss: 214.57488, Residuals: -2.72398, Convergence: 0.012855\n",
      "Epoch: 30, Loss: 211.91019, Residuals: -2.68755, Convergence: 0.012575\n",
      "Epoch: 31, Loss: 209.32280, Residuals: -2.65151, Convergence: 0.012361\n",
      "Epoch: 32, Loss: 206.80044, Residuals: -2.61579, Convergence: 0.012197\n",
      "Epoch: 33, Loss: 204.33537, Residuals: -2.58031, Convergence: 0.012064\n",
      "Epoch: 34, Loss: 201.92336, Residuals: -2.54504, Convergence: 0.011945\n",
      "Epoch: 35, Loss: 199.56268, Residuals: -2.50997, Convergence: 0.011829\n",
      "Epoch: 36, Loss: 197.25322, Residuals: -2.47509, Convergence: 0.011708\n",
      "Epoch: 37, Loss: 194.99592, Residuals: -2.44040, Convergence: 0.011576\n",
      "Epoch: 38, Loss: 192.79231, Residuals: -2.40590, Convergence: 0.011430\n",
      "Epoch: 39, Loss: 190.64411, Residuals: -2.37162, Convergence: 0.011268\n",
      "Epoch: 40, Loss: 188.55305, Residuals: -2.33757, Convergence: 0.011090\n",
      "Epoch: 41, Loss: 186.52069, Residuals: -2.30378, Convergence: 0.010896\n",
      "Epoch: 42, Loss: 184.54829, Residuals: -2.27028, Convergence: 0.010688\n",
      "Epoch: 43, Loss: 182.63685, Residuals: -2.23709, Convergence: 0.010466\n",
      "Epoch: 44, Loss: 180.78712, Residuals: -2.20426, Convergence: 0.010232\n",
      "Epoch: 45, Loss: 178.99970, Residuals: -2.17184, Convergence: 0.009986\n",
      "Epoch: 46, Loss: 177.27513, Residuals: -2.13985, Convergence: 0.009728\n",
      "Epoch: 47, Loss: 175.61392, Residuals: -2.10835, Convergence: 0.009459\n",
      "Epoch: 48, Loss: 174.01651, Residuals: -2.07737, Convergence: 0.009180\n",
      "Epoch: 49, Loss: 172.48310, Residuals: -2.04697, Convergence: 0.008890\n",
      "Epoch: 50, Loss: 171.01353, Residuals: -2.01716, Convergence: 0.008593\n",
      "Epoch: 51, Loss: 169.60710, Residuals: -1.98797, Convergence: 0.008292\n",
      "Epoch: 52, Loss: 168.26259, Residuals: -1.95942, Convergence: 0.007991\n",
      "Epoch: 53, Loss: 166.97822, Residuals: -1.93152, Convergence: 0.007692\n",
      "Epoch: 54, Loss: 165.75177, Residuals: -1.90427, Convergence: 0.007399\n",
      "Epoch: 55, Loss: 164.58062, Residuals: -1.87768, Convergence: 0.007116\n",
      "Epoch: 56, Loss: 163.46196, Residuals: -1.85174, Convergence: 0.006844\n",
      "Epoch: 57, Loss: 162.39290, Residuals: -1.82644, Convergence: 0.006583\n",
      "Epoch: 58, Loss: 161.37056, Residuals: -1.80178, Convergence: 0.006335\n",
      "Epoch: 59, Loss: 160.39224, Residuals: -1.77775, Convergence: 0.006100\n",
      "Epoch: 60, Loss: 159.45538, Residuals: -1.75434, Convergence: 0.005875\n",
      "Epoch: 61, Loss: 158.55772, Residuals: -1.73154, Convergence: 0.005661\n",
      "Epoch: 62, Loss: 157.69714, Residuals: -1.70934, Convergence: 0.005457\n",
      "Epoch: 63, Loss: 156.87180, Residuals: -1.68775, Convergence: 0.005261\n",
      "Epoch: 64, Loss: 156.08002, Residuals: -1.66675, Convergence: 0.005073\n",
      "Epoch: 65, Loss: 155.32028, Residuals: -1.64634, Convergence: 0.004891\n",
      "Epoch: 66, Loss: 154.59122, Residuals: -1.62652, Convergence: 0.004716\n",
      "Epoch: 67, Loss: 153.89161, Residuals: -1.60728, Convergence: 0.004546\n",
      "Epoch: 68, Loss: 153.22035, Residuals: -1.58861, Convergence: 0.004381\n",
      "Epoch: 69, Loss: 152.57639, Residuals: -1.57052, Convergence: 0.004221\n",
      "Epoch: 70, Loss: 151.95881, Residuals: -1.55299, Convergence: 0.004064\n",
      "Epoch: 71, Loss: 151.36675, Residuals: -1.53603, Convergence: 0.003911\n",
      "Epoch: 72, Loss: 150.79940, Residuals: -1.51962, Convergence: 0.003762\n",
      "Epoch: 73, Loss: 150.25599, Residuals: -1.50376, Convergence: 0.003617\n",
      "Epoch: 74, Loss: 149.73582, Residuals: -1.48844, Convergence: 0.003474\n",
      "Epoch: 75, Loss: 149.23815, Residuals: -1.47366, Convergence: 0.003335\n",
      "Epoch: 76, Loss: 148.76233, Residuals: -1.45940, Convergence: 0.003199\n",
      "Epoch: 77, Loss: 148.30763, Residuals: -1.44566, Convergence: 0.003066\n",
      "Epoch: 78, Loss: 147.87337, Residuals: -1.43244, Convergence: 0.002937\n",
      "Epoch: 79, Loss: 147.45885, Residuals: -1.41971, Convergence: 0.002811\n",
      "Epoch: 80, Loss: 147.06336, Residuals: -1.40747, Convergence: 0.002689\n",
      "Epoch: 81, Loss: 146.68616, Residuals: -1.39571, Convergence: 0.002571\n",
      "Epoch: 82, Loss: 146.32652, Residuals: -1.38441, Convergence: 0.002458\n",
      "Epoch: 83, Loss: 145.98370, Residuals: -1.37356, Convergence: 0.002348\n",
      "Epoch: 84, Loss: 145.65695, Residuals: -1.36316, Convergence: 0.002243\n",
      "Epoch: 85, Loss: 145.34552, Residuals: -1.35318, Convergence: 0.002143\n",
      "Epoch: 86, Loss: 145.04868, Residuals: -1.34361, Convergence: 0.002047\n",
      "Epoch: 87, Loss: 144.76570, Residuals: -1.33444, Convergence: 0.001955\n",
      "Epoch: 88, Loss: 144.49588, Residuals: -1.32564, Convergence: 0.001867\n",
      "Epoch: 89, Loss: 144.23854, Residuals: -1.31722, Convergence: 0.001784\n",
      "Epoch: 90, Loss: 143.99303, Residuals: -1.30915, Convergence: 0.001705\n",
      "Epoch: 91, Loss: 143.75873, Residuals: -1.30142, Convergence: 0.001630\n",
      "Epoch: 92, Loss: 143.53505, Residuals: -1.29401, Convergence: 0.001558\n",
      "Epoch: 93, Loss: 143.32145, Residuals: -1.28691, Convergence: 0.001490\n",
      "Epoch: 94, Loss: 143.11741, Residuals: -1.28011, Convergence: 0.001426\n",
      "Epoch: 95, Loss: 142.92248, Residuals: -1.27360, Convergence: 0.001364\n",
      "Epoch: 96, Loss: 142.73621, Residuals: -1.26735, Convergence: 0.001305\n",
      "Epoch: 97, Loss: 142.55822, Residuals: -1.26136, Convergence: 0.001249\n",
      "Epoch: 98, Loss: 142.38815, Residuals: -1.25562, Convergence: 0.001194\n",
      "Epoch: 99, Loss: 142.22568, Residuals: -1.25012, Convergence: 0.001142\n",
      "Epoch: 100, Loss: 142.07054, Residuals: -1.24485, Convergence: 0.001092\n",
      "Epoch: 101, Loss: 141.92247, Residuals: -1.23979, Convergence: 0.001043\n",
      "Epoch: 102, Loss: 141.78126, Residuals: -1.23493, Convergence: 0.000996\n",
      "Evidence -183.570\n",
      "\n",
      "Epoch: 102, Evidence: -183.57019, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.25e-01\n",
      "Epoch: 102, Loss: 1372.81592, Residuals: -1.23493, Convergence:   inf\n",
      "Epoch: 103, Loss: 1310.63176, Residuals: -1.26586, Convergence: 0.047446\n",
      "Epoch: 104, Loss: 1262.87557, Residuals: -1.29018, Convergence: 0.037815\n",
      "Epoch: 105, Loss: 1226.53658, Residuals: -1.30789, Convergence: 0.029627\n",
      "Epoch: 106, Loss: 1198.23744, Residuals: -1.32055, Convergence: 0.023617\n",
      "Epoch: 107, Loss: 1175.39548, Residuals: -1.33004, Convergence: 0.019433\n",
      "Epoch: 108, Loss: 1156.45767, Residuals: -1.33741, Convergence: 0.016376\n",
      "Epoch: 109, Loss: 1140.47572, Residuals: -1.34312, Convergence: 0.014013\n",
      "Epoch: 110, Loss: 1126.80446, Residuals: -1.34737, Convergence: 0.012133\n",
      "Epoch: 111, Loss: 1114.96411, Residuals: -1.35033, Convergence: 0.010619\n",
      "Epoch: 112, Loss: 1104.57911, Residuals: -1.35209, Convergence: 0.009402\n",
      "Epoch: 113, Loss: 1095.34634, Residuals: -1.35275, Convergence: 0.008429\n",
      "Epoch: 114, Loss: 1087.01529, Residuals: -1.35239, Convergence: 0.007664\n",
      "Epoch: 115, Loss: 1079.37731, Residuals: -1.35107, Convergence: 0.007076\n",
      "Epoch: 116, Loss: 1072.25910, Residuals: -1.34883, Convergence: 0.006639\n",
      "Epoch: 117, Loss: 1065.51718, Residuals: -1.34572, Convergence: 0.006327\n",
      "Epoch: 118, Loss: 1059.03493, Residuals: -1.34177, Convergence: 0.006121\n",
      "Epoch: 119, Loss: 1052.72126, Residuals: -1.33703, Convergence: 0.005997\n",
      "Epoch: 120, Loss: 1046.51182, Residuals: -1.33156, Convergence: 0.005933\n",
      "Epoch: 121, Loss: 1040.37520, Residuals: -1.32542, Convergence: 0.005898\n",
      "Epoch: 122, Loss: 1034.32112, Residuals: -1.31872, Convergence: 0.005853\n",
      "Epoch: 123, Loss: 1028.39928, Residuals: -1.31157, Convergence: 0.005758\n",
      "Epoch: 124, Loss: 1022.68181, Residuals: -1.30408, Convergence: 0.005591\n",
      "Epoch: 125, Loss: 1017.23679, Residuals: -1.29634, Convergence: 0.005353\n",
      "Epoch: 126, Loss: 1012.10727, Residuals: -1.28843, Convergence: 0.005068\n",
      "Epoch: 127, Loss: 1007.30444, Residuals: -1.28044, Convergence: 0.004768\n",
      "Epoch: 128, Loss: 1002.81702, Residuals: -1.27242, Convergence: 0.004475\n",
      "Epoch: 129, Loss: 998.62038, Residuals: -1.26442, Convergence: 0.004202\n",
      "Epoch: 130, Loss: 994.68554, Residuals: -1.25648, Convergence: 0.003956\n",
      "Epoch: 131, Loss: 990.98421, Residuals: -1.24865, Convergence: 0.003735\n",
      "Epoch: 132, Loss: 987.49107, Residuals: -1.24095, Convergence: 0.003537\n",
      "Epoch: 133, Loss: 984.18481, Residuals: -1.23341, Convergence: 0.003359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 134, Loss: 981.04757, Residuals: -1.22604, Convergence: 0.003198\n",
      "Epoch: 135, Loss: 978.06489, Residuals: -1.21887, Convergence: 0.003050\n",
      "Epoch: 136, Loss: 975.22488, Residuals: -1.21191, Convergence: 0.002912\n",
      "Epoch: 137, Loss: 972.51758, Residuals: -1.20517, Convergence: 0.002784\n",
      "Epoch: 138, Loss: 969.93455, Residuals: -1.19865, Convergence: 0.002663\n",
      "Epoch: 139, Loss: 967.46871, Residuals: -1.19238, Convergence: 0.002549\n",
      "Epoch: 140, Loss: 965.11337, Residuals: -1.18633, Convergence: 0.002440\n",
      "Epoch: 141, Loss: 962.86244, Residuals: -1.18052, Convergence: 0.002338\n",
      "Epoch: 142, Loss: 960.71009, Residuals: -1.17495, Convergence: 0.002240\n",
      "Epoch: 143, Loss: 958.65104, Residuals: -1.16961, Convergence: 0.002148\n",
      "Epoch: 144, Loss: 956.67969, Residuals: -1.16449, Convergence: 0.002061\n",
      "Epoch: 145, Loss: 954.79152, Residuals: -1.15958, Convergence: 0.001978\n",
      "Epoch: 146, Loss: 952.98141, Residuals: -1.15488, Convergence: 0.001899\n",
      "Epoch: 147, Loss: 951.24434, Residuals: -1.15039, Convergence: 0.001826\n",
      "Epoch: 148, Loss: 949.57556, Residuals: -1.14608, Convergence: 0.001757\n",
      "Epoch: 149, Loss: 947.97048, Residuals: -1.14195, Convergence: 0.001693\n",
      "Epoch: 150, Loss: 946.42372, Residuals: -1.13799, Convergence: 0.001634\n",
      "Epoch: 151, Loss: 944.93077, Residuals: -1.13418, Convergence: 0.001580\n",
      "Epoch: 152, Loss: 943.48625, Residuals: -1.13053, Convergence: 0.001531\n",
      "Epoch: 153, Loss: 942.08448, Residuals: -1.12700, Convergence: 0.001488\n",
      "Epoch: 154, Loss: 940.72032, Residuals: -1.12360, Convergence: 0.001450\n",
      "Epoch: 155, Loss: 939.38826, Residuals: -1.12031, Convergence: 0.001418\n",
      "Epoch: 156, Loss: 938.08213, Residuals: -1.11711, Convergence: 0.001392\n",
      "Epoch: 157, Loss: 936.79733, Residuals: -1.11400, Convergence: 0.001371\n",
      "Epoch: 158, Loss: 935.52935, Residuals: -1.11096, Convergence: 0.001355\n",
      "Epoch: 159, Loss: 934.27478, Residuals: -1.10798, Convergence: 0.001343\n",
      "Epoch: 160, Loss: 933.03183, Residuals: -1.10505, Convergence: 0.001332\n",
      "Epoch: 161, Loss: 931.80047, Residuals: -1.10217, Convergence: 0.001321\n",
      "Epoch: 162, Loss: 930.58343, Residuals: -1.09935, Convergence: 0.001308\n",
      "Epoch: 163, Loss: 929.38398, Residuals: -1.09659, Convergence: 0.001291\n",
      "Epoch: 164, Loss: 928.20708, Residuals: -1.09389, Convergence: 0.001268\n",
      "Epoch: 165, Loss: 927.05755, Residuals: -1.09125, Convergence: 0.001240\n",
      "Epoch: 166, Loss: 925.93973, Residuals: -1.08870, Convergence: 0.001207\n",
      "Epoch: 167, Loss: 924.85709, Residuals: -1.08623, Convergence: 0.001171\n",
      "Epoch: 168, Loss: 923.81264, Residuals: -1.08385, Convergence: 0.001131\n",
      "Epoch: 169, Loss: 922.80754, Residuals: -1.08155, Convergence: 0.001089\n",
      "Epoch: 170, Loss: 921.84245, Residuals: -1.07934, Convergence: 0.001047\n",
      "Epoch: 171, Loss: 920.91756, Residuals: -1.07722, Convergence: 0.001004\n",
      "Epoch: 172, Loss: 920.03172, Residuals: -1.07519, Convergence: 0.000963\n",
      "Evidence 11187.447\n",
      "\n",
      "Epoch: 172, Evidence: 11187.44727, Convergence: 1.016409\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.75e-01\n",
      "Epoch: 172, Loss: 2346.00070, Residuals: -1.07519, Convergence:   inf\n",
      "Epoch: 173, Loss: 2306.17365, Residuals: -1.08361, Convergence: 0.017270\n",
      "Epoch: 174, Loss: 2279.54490, Residuals: -1.08311, Convergence: 0.011682\n",
      "Epoch: 175, Loss: 2257.49104, Residuals: -1.08168, Convergence: 0.009769\n",
      "Epoch: 176, Loss: 2238.90312, Residuals: -1.07995, Convergence: 0.008302\n",
      "Epoch: 177, Loss: 2223.08815, Residuals: -1.07802, Convergence: 0.007114\n",
      "Epoch: 178, Loss: 2209.51157, Residuals: -1.07593, Convergence: 0.006145\n",
      "Epoch: 179, Loss: 2197.73395, Residuals: -1.07370, Convergence: 0.005359\n",
      "Epoch: 180, Loss: 2187.39333, Residuals: -1.07133, Convergence: 0.004727\n",
      "Epoch: 181, Loss: 2178.19199, Residuals: -1.06883, Convergence: 0.004224\n",
      "Epoch: 182, Loss: 2169.89803, Residuals: -1.06617, Convergence: 0.003822\n",
      "Epoch: 183, Loss: 2162.34195, Residuals: -1.06337, Convergence: 0.003494\n",
      "Epoch: 184, Loss: 2155.41251, Residuals: -1.06044, Convergence: 0.003215\n",
      "Epoch: 185, Loss: 2149.03944, Residuals: -1.05744, Convergence: 0.002966\n",
      "Epoch: 186, Loss: 2143.17286, Residuals: -1.05440, Convergence: 0.002737\n",
      "Epoch: 187, Loss: 2137.77104, Residuals: -1.05137, Convergence: 0.002527\n",
      "Epoch: 188, Loss: 2132.79146, Residuals: -1.04839, Convergence: 0.002335\n",
      "Epoch: 189, Loss: 2128.19385, Residuals: -1.04548, Convergence: 0.002160\n",
      "Epoch: 190, Loss: 2123.93758, Residuals: -1.04267, Convergence: 0.002004\n",
      "Epoch: 191, Loss: 2119.98670, Residuals: -1.03995, Convergence: 0.001864\n",
      "Epoch: 192, Loss: 2116.30873, Residuals: -1.03735, Convergence: 0.001738\n",
      "Epoch: 193, Loss: 2112.87620, Residuals: -1.03486, Convergence: 0.001625\n",
      "Epoch: 194, Loss: 2109.66624, Residuals: -1.03248, Convergence: 0.001522\n",
      "Epoch: 195, Loss: 2106.65910, Residuals: -1.03021, Convergence: 0.001427\n",
      "Epoch: 196, Loss: 2103.83834, Residuals: -1.02806, Convergence: 0.001341\n",
      "Epoch: 197, Loss: 2101.19148, Residuals: -1.02600, Convergence: 0.001260\n",
      "Epoch: 198, Loss: 2098.70494, Residuals: -1.02405, Convergence: 0.001185\n",
      "Epoch: 199, Loss: 2096.36984, Residuals: -1.02219, Convergence: 0.001114\n",
      "Epoch: 200, Loss: 2094.17467, Residuals: -1.02042, Convergence: 0.001048\n",
      "Epoch: 201, Loss: 2092.11238, Residuals: -1.01873, Convergence: 0.000986\n",
      "Evidence 14316.136\n",
      "\n",
      "Epoch: 201, Evidence: 14316.13574, Convergence: 0.218543\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.37e-01\n",
      "Epoch: 201, Loss: 2467.80993, Residuals: -1.01873, Convergence:   inf\n",
      "Epoch: 202, Loss: 2454.33103, Residuals: -1.01612, Convergence: 0.005492\n",
      "Epoch: 203, Loss: 2443.25122, Residuals: -1.01302, Convergence: 0.004535\n",
      "Epoch: 204, Loss: 2433.66799, Residuals: -1.01000, Convergence: 0.003938\n",
      "Epoch: 205, Loss: 2425.34515, Residuals: -1.00715, Convergence: 0.003432\n",
      "Epoch: 206, Loss: 2418.09515, Residuals: -1.00450, Convergence: 0.002998\n",
      "Epoch: 207, Loss: 2411.75661, Residuals: -1.00206, Convergence: 0.002628\n",
      "Epoch: 208, Loss: 2406.19282, Residuals: -0.99984, Convergence: 0.002312\n",
      "Epoch: 209, Loss: 2401.28387, Residuals: -0.99781, Convergence: 0.002044\n",
      "Epoch: 210, Loss: 2396.93146, Residuals: -0.99596, Convergence: 0.001816\n",
      "Epoch: 211, Loss: 2393.05064, Residuals: -0.99427, Convergence: 0.001622\n",
      "Epoch: 212, Loss: 2389.56991, Residuals: -0.99272, Convergence: 0.001457\n",
      "Epoch: 213, Loss: 2386.43183, Residuals: -0.99129, Convergence: 0.001315\n",
      "Epoch: 214, Loss: 2383.58615, Residuals: -0.98997, Convergence: 0.001194\n",
      "Epoch: 215, Loss: 2380.99220, Residuals: -0.98875, Convergence: 0.001089\n",
      "Epoch: 216, Loss: 2378.61569, Residuals: -0.98761, Convergence: 0.000999\n",
      "Evidence 14685.777\n",
      "\n",
      "Epoch: 216, Evidence: 14685.77734, Convergence: 0.025170\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.35e-01\n",
      "Epoch: 216, Loss: 2473.20803, Residuals: -0.98761, Convergence:   inf\n",
      "Epoch: 217, Loss: 2466.54724, Residuals: -0.98458, Convergence: 0.002700\n",
      "Epoch: 218, Loss: 2461.04507, Residuals: -0.98192, Convergence: 0.002236\n",
      "Epoch: 219, Loss: 2456.39397, Residuals: -0.97964, Convergence: 0.001893\n",
      "Epoch: 220, Loss: 2452.40956, Residuals: -0.97768, Convergence: 0.001625\n",
      "Epoch: 221, Loss: 2448.95032, Residuals: -0.97600, Convergence: 0.001413\n",
      "Epoch: 222, Loss: 2445.90864, Residuals: -0.97456, Convergence: 0.001244\n",
      "Epoch: 223, Loss: 2443.20352, Residuals: -0.97331, Convergence: 0.001107\n",
      "Epoch: 224, Loss: 2440.77125, Residuals: -0.97223, Convergence: 0.000997\n",
      "Evidence 14767.516\n",
      "\n",
      "Epoch: 224, Evidence: 14767.51562, Convergence: 0.005535\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.63e-01\n",
      "Epoch: 224, Loss: 2474.86145, Residuals: -0.97223, Convergence:   inf\n",
      "Epoch: 225, Loss: 2470.83308, Residuals: -0.96987, Convergence: 0.001630\n",
      "Epoch: 226, Loss: 2467.50251, Residuals: -0.96795, Convergence: 0.001350\n",
      "Epoch: 227, Loss: 2464.66489, Residuals: -0.96638, Convergence: 0.001151\n",
      "Epoch: 228, Loss: 2462.19619, Residuals: -0.96508, Convergence: 0.001003\n",
      "Epoch: 229, Loss: 2460.01086, Residuals: -0.96401, Convergence: 0.000888\n",
      "Evidence 14798.479\n",
      "\n",
      "Epoch: 229, Evidence: 14798.47949, Convergence: 0.002092\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.11e-01\n",
      "Epoch: 229, Loss: 2475.74724, Residuals: -0.96401, Convergence:   inf\n",
      "Epoch: 230, Loss: 2472.92472, Residuals: -0.96218, Convergence: 0.001141\n",
      "Epoch: 231, Loss: 2470.56123, Residuals: -0.96071, Convergence: 0.000957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence 14811.274\n",
      "\n",
      "Epoch: 231, Evidence: 14811.27441, Convergence: 0.000864\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.75e-01\n",
      "Epoch: 231, Loss: 2476.41804, Residuals: -0.96071, Convergence:   inf\n",
      "Epoch: 232, Loss: 2471.98489, Residuals: -0.95821, Convergence: 0.001793\n",
      "Epoch: 233, Loss: 2468.57660, Residuals: -0.95632, Convergence: 0.001381\n",
      "Epoch: 234, Loss: 2465.80044, Residuals: -0.95508, Convergence: 0.001126\n",
      "Epoch: 235, Loss: 2463.43662, Residuals: -0.95438, Convergence: 0.000960\n",
      "Evidence 14828.909\n",
      "\n",
      "Epoch: 235, Evidence: 14828.90918, Convergence: 0.002052\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.45e-01\n",
      "Epoch: 235, Loss: 2476.61719, Residuals: -0.95438, Convergence:   inf\n",
      "Epoch: 236, Loss: 2473.62971, Residuals: -0.95200, Convergence: 0.001208\n",
      "Epoch: 237, Loss: 2471.25937, Residuals: -0.95077, Convergence: 0.000959\n",
      "Evidence 14839.746\n",
      "\n",
      "Epoch: 237, Evidence: 14839.74609, Convergence: 0.000730\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.23e-01\n",
      "Epoch: 237, Loss: 2476.82298, Residuals: -0.95077, Convergence:   inf\n",
      "Epoch: 238, Loss: 2472.43175, Residuals: -0.94778, Convergence: 0.001776\n",
      "Epoch: 239, Loss: 2469.27115, Residuals: -0.94854, Convergence: 0.001280\n",
      "Epoch: 240, Loss: 2466.60183, Residuals: -0.94868, Convergence: 0.001082\n",
      "Epoch: 241, Loss: 2464.26141, Residuals: -0.95099, Convergence: 0.000950\n",
      "Evidence 14855.590\n",
      "\n",
      "Epoch: 241, Evidence: 14855.58984, Convergence: 0.001796\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.08e-01\n",
      "Epoch: 241, Loss: 2476.14955, Residuals: -0.95099, Convergence:   inf\n",
      "Epoch: 242, Loss: 2473.97061, Residuals: -0.94740, Convergence: 0.000881\n",
      "Evidence 14862.662\n",
      "\n",
      "Epoch: 242, Evidence: 14862.66211, Convergence: 0.000476\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.93e-02\n",
      "Epoch: 242, Loss: 2477.04786, Residuals: -0.94740, Convergence:   inf\n",
      "Epoch: 243, Loss: 2509.64426, Residuals: -0.97670, Convergence: -0.012988\n",
      "Epoch: 243, Loss: 2474.96333, Residuals: -0.94633, Convergence: 0.000842\n",
      "Evidence 14866.707\n",
      "\n",
      "Epoch: 243, Evidence: 14866.70703, Convergence: 0.000748\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.22e-02\n",
      "Epoch: 243, Loss: 2476.49672, Residuals: -0.94633, Convergence:   inf\n",
      "Epoch: 244, Loss: 2481.27596, Residuals: -0.94471, Convergence: -0.001926\n",
      "Epoch: 244, Loss: 2476.31659, Residuals: -0.94356, Convergence: 0.000073\n",
      "Evidence 14868.566\n",
      "\n",
      "Epoch: 244, Evidence: 14868.56641, Convergence: 0.000873\n",
      "Total samples: 183, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 384.48630, Residuals: -4.52645, Convergence:   inf\n",
      "Epoch: 1, Loss: 358.64287, Residuals: -4.40536, Convergence: 0.072059\n",
      "Epoch: 2, Loss: 337.59693, Residuals: -4.24089, Convergence: 0.062340\n",
      "Epoch: 3, Loss: 321.51850, Residuals: -4.07654, Convergence: 0.050008\n",
      "Epoch: 4, Loss: 309.26394, Residuals: -3.93238, Convergence: 0.039625\n",
      "Epoch: 5, Loss: 299.54940, Residuals: -3.80541, Convergence: 0.032430\n",
      "Epoch: 6, Loss: 291.66677, Residuals: -3.69504, Convergence: 0.027026\n",
      "Epoch: 7, Loss: 285.12573, Residuals: -3.60058, Convergence: 0.022941\n",
      "Epoch: 8, Loss: 279.56188, Residuals: -3.51975, Convergence: 0.019902\n",
      "Epoch: 9, Loss: 274.71693, Residuals: -3.45013, Convergence: 0.017636\n",
      "Epoch: 10, Loss: 270.40638, Residuals: -3.38964, Convergence: 0.015941\n",
      "Epoch: 11, Loss: 266.49592, Residuals: -3.33651, Convergence: 0.014674\n",
      "Epoch: 12, Loss: 262.88706, Residuals: -3.28929, Convergence: 0.013728\n",
      "Epoch: 13, Loss: 259.50845, Residuals: -3.24674, Convergence: 0.013019\n",
      "Epoch: 14, Loss: 256.31043, Residuals: -3.20778, Convergence: 0.012477\n",
      "Epoch: 15, Loss: 253.26254, Residuals: -3.17157, Convergence: 0.012035\n",
      "Epoch: 16, Loss: 250.35064, Residuals: -3.13751, Convergence: 0.011631\n",
      "Epoch: 17, Loss: 247.56603, Residuals: -3.10528, Convergence: 0.011248\n",
      "Epoch: 18, Loss: 244.89062, Residuals: -3.07450, Convergence: 0.010925\n",
      "Epoch: 19, Loss: 242.29378, Residuals: -3.04469, Convergence: 0.010718\n",
      "Epoch: 20, Loss: 239.74007, Residuals: -3.01525, Convergence: 0.010652\n",
      "Epoch: 21, Loss: 237.19818, Residuals: -2.98566, Convergence: 0.010716\n",
      "Epoch: 22, Loss: 234.64518, Residuals: -2.95552, Convergence: 0.010880\n",
      "Epoch: 23, Loss: 232.05858, Residuals: -2.92454, Convergence: 0.011146\n",
      "Epoch: 24, Loss: 229.40040, Residuals: -2.89225, Convergence: 0.011588\n",
      "Epoch: 25, Loss: 226.61679, Residuals: -2.85798, Convergence: 0.012283\n",
      "Epoch: 26, Loss: 223.69020, Residuals: -2.82142, Convergence: 0.013083\n",
      "Epoch: 27, Loss: 220.71897, Residuals: -2.78354, Convergence: 0.013462\n",
      "Epoch: 28, Loss: 217.82417, Residuals: -2.74574, Convergence: 0.013290\n",
      "Epoch: 29, Loss: 215.03693, Residuals: -2.70851, Convergence: 0.012962\n",
      "Epoch: 30, Loss: 212.34443, Residuals: -2.67184, Convergence: 0.012680\n",
      "Epoch: 31, Loss: 209.72931, Residuals: -2.63564, Convergence: 0.012469\n",
      "Epoch: 32, Loss: 207.17863, Residuals: -2.59978, Convergence: 0.012312\n",
      "Epoch: 33, Loss: 204.68441, Residuals: -2.56422, Convergence: 0.012186\n",
      "Epoch: 34, Loss: 202.24264, Residuals: -2.52888, Convergence: 0.012073\n",
      "Epoch: 35, Loss: 199.85207, Residuals: -2.49374, Convergence: 0.011962\n",
      "Epoch: 36, Loss: 197.51325, Residuals: -2.45878, Convergence: 0.011841\n",
      "Epoch: 37, Loss: 195.22758, Residuals: -2.42398, Convergence: 0.011708\n",
      "Epoch: 38, Loss: 192.99681, Residuals: -2.38935, Convergence: 0.011559\n",
      "Epoch: 39, Loss: 190.82254, Residuals: -2.35489, Convergence: 0.011394\n",
      "Epoch: 40, Loss: 188.70601, Residuals: -2.32061, Convergence: 0.011216\n",
      "Epoch: 41, Loss: 186.64805, Residuals: -2.28652, Convergence: 0.011026\n",
      "Epoch: 42, Loss: 184.64904, Residuals: -2.25263, Convergence: 0.010826\n",
      "Epoch: 43, Loss: 182.70913, Residuals: -2.21896, Convergence: 0.010618\n",
      "Epoch: 44, Loss: 180.82830, Residuals: -2.18553, Convergence: 0.010401\n",
      "Epoch: 45, Loss: 179.00672, Residuals: -2.15236, Convergence: 0.010176\n",
      "Epoch: 46, Loss: 177.24485, Residuals: -2.11948, Convergence: 0.009940\n",
      "Epoch: 47, Loss: 175.54362, Residuals: -2.08693, Convergence: 0.009691\n",
      "Epoch: 48, Loss: 173.90430, Residuals: -2.05476, Convergence: 0.009427\n",
      "Epoch: 49, Loss: 172.32827, Residuals: -2.02303, Convergence: 0.009146\n",
      "Epoch: 50, Loss: 170.81677, Residuals: -1.99178, Convergence: 0.008849\n",
      "Epoch: 51, Loss: 169.37057, Residuals: -1.96107, Convergence: 0.008539\n",
      "Epoch: 52, Loss: 167.98985, Residuals: -1.93096, Convergence: 0.008219\n",
      "Epoch: 53, Loss: 166.67416, Residuals: -1.90149, Convergence: 0.007894\n",
      "Epoch: 54, Loss: 165.42246, Residuals: -1.87270, Convergence: 0.007567\n",
      "Epoch: 55, Loss: 164.23323, Residuals: -1.84463, Convergence: 0.007241\n",
      "Epoch: 56, Loss: 163.10453, Residuals: -1.81730, Convergence: 0.006920\n",
      "Epoch: 57, Loss: 162.03412, Residuals: -1.79073, Convergence: 0.006606\n",
      "Epoch: 58, Loss: 161.01946, Residuals: -1.76496, Convergence: 0.006301\n",
      "Epoch: 59, Loss: 160.05777, Residuals: -1.73997, Convergence: 0.006008\n",
      "Epoch: 60, Loss: 159.14609, Residuals: -1.71578, Convergence: 0.005729\n",
      "Epoch: 61, Loss: 158.28130, Residuals: -1.69239, Convergence: 0.005464\n",
      "Epoch: 62, Loss: 157.46023, Residuals: -1.66978, Convergence: 0.005214\n",
      "Epoch: 63, Loss: 156.67973, Residuals: -1.64794, Convergence: 0.004982\n",
      "Epoch: 64, Loss: 155.93674, Residuals: -1.62685, Convergence: 0.004765\n",
      "Epoch: 65, Loss: 155.22837, Residuals: -1.60648, Convergence: 0.004563\n",
      "Epoch: 66, Loss: 154.55194, Residuals: -1.58681, Convergence: 0.004377\n",
      "Epoch: 67, Loss: 153.90497, Residuals: -1.56781, Convergence: 0.004204\n",
      "Epoch: 68, Loss: 153.28526, Residuals: -1.54946, Convergence: 0.004043\n",
      "Epoch: 69, Loss: 152.69083, Residuals: -1.53172, Convergence: 0.003893\n",
      "Epoch: 70, Loss: 152.11996, Residuals: -1.51458, Convergence: 0.003753\n",
      "Epoch: 71, Loss: 151.57121, Residuals: -1.49800, Convergence: 0.003620\n",
      "Epoch: 72, Loss: 151.04344, Residuals: -1.48196, Convergence: 0.003494\n",
      "Epoch: 73, Loss: 150.53570, Residuals: -1.46646, Convergence: 0.003373\n",
      "Epoch: 74, Loss: 150.04730, Residuals: -1.45146, Convergence: 0.003255\n",
      "Epoch: 75, Loss: 149.57770, Residuals: -1.43695, Convergence: 0.003140\n",
      "Epoch: 76, Loss: 149.12650, Residuals: -1.42294, Convergence: 0.003026\n",
      "Epoch: 77, Loss: 148.69336, Residuals: -1.40940, Convergence: 0.002913\n",
      "Epoch: 78, Loss: 148.27801, Residuals: -1.39632, Convergence: 0.002801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 79, Loss: 147.88019, Residuals: -1.38371, Convergence: 0.002690\n",
      "Epoch: 80, Loss: 147.49963, Residuals: -1.37156, Convergence: 0.002580\n",
      "Epoch: 81, Loss: 147.13608, Residuals: -1.35985, Convergence: 0.002471\n",
      "Epoch: 82, Loss: 146.78922, Residuals: -1.34858, Convergence: 0.002363\n",
      "Epoch: 83, Loss: 146.45875, Residuals: -1.33775, Convergence: 0.002256\n",
      "Epoch: 84, Loss: 146.14430, Residuals: -1.32735, Convergence: 0.002152\n",
      "Epoch: 85, Loss: 145.84551, Residuals: -1.31737, Convergence: 0.002049\n",
      "Epoch: 86, Loss: 145.56195, Residuals: -1.30781, Convergence: 0.001948\n",
      "Epoch: 87, Loss: 145.29321, Residuals: -1.29866, Convergence: 0.001850\n",
      "Epoch: 88, Loss: 145.03879, Residuals: -1.28992, Convergence: 0.001754\n",
      "Epoch: 89, Loss: 144.79819, Residuals: -1.28158, Convergence: 0.001662\n",
      "Epoch: 90, Loss: 144.57083, Residuals: -1.27363, Convergence: 0.001573\n",
      "Epoch: 91, Loss: 144.35606, Residuals: -1.26607, Convergence: 0.001488\n",
      "Epoch: 92, Loss: 144.15315, Residuals: -1.25890, Convergence: 0.001408\n",
      "Epoch: 93, Loss: 143.96126, Residuals: -1.25210, Convergence: 0.001333\n",
      "Epoch: 94, Loss: 143.77943, Residuals: -1.24567, Convergence: 0.001265\n",
      "Epoch: 95, Loss: 143.60661, Residuals: -1.23959, Convergence: 0.001203\n",
      "Epoch: 96, Loss: 143.44163, Residuals: -1.23384, Convergence: 0.001150\n",
      "Epoch: 97, Loss: 143.28328, Residuals: -1.22840, Convergence: 0.001105\n",
      "Epoch: 98, Loss: 143.13035, Residuals: -1.22324, Convergence: 0.001068\n",
      "Epoch: 99, Loss: 142.98173, Residuals: -1.21833, Convergence: 0.001039\n",
      "Epoch: 100, Loss: 142.83642, Residuals: -1.21365, Convergence: 0.001017\n",
      "Epoch: 101, Loss: 142.69360, Residuals: -1.20917, Convergence: 0.001001\n",
      "Epoch: 102, Loss: 142.55269, Residuals: -1.20486, Convergence: 0.000988\n",
      "Evidence -184.179\n",
      "\n",
      "Epoch: 102, Evidence: -184.17928, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.24e-01\n",
      "Epoch: 102, Loss: 1372.80168, Residuals: -1.20486, Convergence:   inf\n",
      "Epoch: 103, Loss: 1311.25974, Residuals: -1.23351, Convergence: 0.046933\n",
      "Epoch: 104, Loss: 1263.90525, Residuals: -1.25773, Convergence: 0.037467\n",
      "Epoch: 105, Loss: 1227.96397, Residuals: -1.27623, Convergence: 0.029269\n",
      "Epoch: 106, Loss: 1200.01310, Residuals: -1.28981, Convergence: 0.023292\n",
      "Epoch: 107, Loss: 1177.49304, Residuals: -1.30005, Convergence: 0.019125\n",
      "Epoch: 108, Loss: 1158.89044, Residuals: -1.30793, Convergence: 0.016052\n",
      "Epoch: 109, Loss: 1143.27004, Residuals: -1.31399, Convergence: 0.013663\n",
      "Epoch: 110, Loss: 1129.98804, Residuals: -1.31854, Convergence: 0.011754\n",
      "Epoch: 111, Loss: 1118.56581, Residuals: -1.32178, Convergence: 0.010211\n",
      "Epoch: 112, Loss: 1108.63308, Residuals: -1.32387, Convergence: 0.008959\n",
      "Epoch: 113, Loss: 1099.89235, Residuals: -1.32492, Convergence: 0.007947\n",
      "Epoch: 114, Loss: 1092.10230, Residuals: -1.32504, Convergence: 0.007133\n",
      "Epoch: 115, Loss: 1085.05963, Residuals: -1.32431, Convergence: 0.006491\n",
      "Epoch: 116, Loss: 1078.59163, Residuals: -1.32278, Convergence: 0.005997\n",
      "Epoch: 117, Loss: 1072.54598, Residuals: -1.32050, Convergence: 0.005637\n",
      "Epoch: 118, Loss: 1066.78707, Residuals: -1.31747, Convergence: 0.005398\n",
      "Epoch: 119, Loss: 1061.19246, Residuals: -1.31372, Convergence: 0.005272\n",
      "Epoch: 120, Loss: 1055.65627, Residuals: -1.30925, Convergence: 0.005244\n",
      "Epoch: 121, Loss: 1050.09700, Residuals: -1.30407, Convergence: 0.005294\n",
      "Epoch: 122, Loss: 1044.47264, Residuals: -1.29823, Convergence: 0.005385\n",
      "Epoch: 123, Loss: 1038.79531, Residuals: -1.29181, Convergence: 0.005465\n",
      "Epoch: 124, Loss: 1033.13342, Residuals: -1.28490, Convergence: 0.005480\n",
      "Epoch: 125, Loss: 1027.59025, Residuals: -1.27763, Convergence: 0.005394\n",
      "Epoch: 126, Loss: 1022.26831, Residuals: -1.27008, Convergence: 0.005206\n",
      "Epoch: 127, Loss: 1017.23703, Residuals: -1.26236, Convergence: 0.004946\n",
      "Epoch: 128, Loss: 1012.52742, Residuals: -1.25455, Convergence: 0.004651\n",
      "Epoch: 129, Loss: 1008.13845, Residuals: -1.24673, Convergence: 0.004354\n",
      "Epoch: 130, Loss: 1004.05181, Residuals: -1.23895, Convergence: 0.004070\n",
      "Epoch: 131, Loss: 1000.24084, Residuals: -1.23125, Convergence: 0.003810\n",
      "Epoch: 132, Loss: 996.67713, Residuals: -1.22369, Convergence: 0.003576\n",
      "Epoch: 133, Loss: 993.33405, Residuals: -1.21628, Convergence: 0.003366\n",
      "Epoch: 134, Loss: 990.18750, Residuals: -1.20904, Convergence: 0.003178\n",
      "Epoch: 135, Loss: 987.21632, Residuals: -1.20200, Convergence: 0.003010\n",
      "Epoch: 136, Loss: 984.40229, Residuals: -1.19517, Convergence: 0.002859\n",
      "Epoch: 137, Loss: 981.73079, Residuals: -1.18855, Convergence: 0.002721\n",
      "Epoch: 138, Loss: 979.18834, Residuals: -1.18216, Convergence: 0.002596\n",
      "Epoch: 139, Loss: 976.76451, Residuals: -1.17600, Convergence: 0.002481\n",
      "Epoch: 140, Loss: 974.45004, Residuals: -1.17007, Convergence: 0.002375\n",
      "Epoch: 141, Loss: 972.23713, Residuals: -1.16437, Convergence: 0.002276\n",
      "Epoch: 142, Loss: 970.11925, Residuals: -1.15890, Convergence: 0.002183\n",
      "Epoch: 143, Loss: 968.09082, Residuals: -1.15365, Convergence: 0.002095\n",
      "Epoch: 144, Loss: 966.14668, Residuals: -1.14863, Convergence: 0.002012\n",
      "Epoch: 145, Loss: 964.28250, Residuals: -1.14383, Convergence: 0.001933\n",
      "Epoch: 146, Loss: 962.49437, Residuals: -1.13924, Convergence: 0.001858\n",
      "Epoch: 147, Loss: 960.77840, Residuals: -1.13485, Convergence: 0.001786\n",
      "Epoch: 148, Loss: 959.13191, Residuals: -1.13065, Convergence: 0.001717\n",
      "Epoch: 149, Loss: 957.55140, Residuals: -1.12665, Convergence: 0.001651\n",
      "Epoch: 150, Loss: 956.03403, Residuals: -1.12282, Convergence: 0.001587\n",
      "Epoch: 151, Loss: 954.57681, Residuals: -1.11916, Convergence: 0.001527\n",
      "Epoch: 152, Loss: 953.17698, Residuals: -1.11566, Convergence: 0.001469\n",
      "Epoch: 153, Loss: 951.83173, Residuals: -1.11232, Convergence: 0.001413\n",
      "Epoch: 154, Loss: 950.53804, Residuals: -1.10912, Convergence: 0.001361\n",
      "Epoch: 155, Loss: 949.29262, Residuals: -1.10605, Convergence: 0.001312\n",
      "Epoch: 156, Loss: 948.09221, Residuals: -1.10311, Convergence: 0.001266\n",
      "Epoch: 157, Loss: 946.93373, Residuals: -1.10028, Convergence: 0.001223\n",
      "Epoch: 158, Loss: 945.81325, Residuals: -1.09757, Convergence: 0.001185\n",
      "Epoch: 159, Loss: 944.72664, Residuals: -1.09494, Convergence: 0.001150\n",
      "Epoch: 160, Loss: 943.67019, Residuals: -1.09241, Convergence: 0.001120\n",
      "Epoch: 161, Loss: 942.63947, Residuals: -1.08996, Convergence: 0.001093\n",
      "Epoch: 162, Loss: 941.63045, Residuals: -1.08757, Convergence: 0.001072\n",
      "Epoch: 163, Loss: 940.63811, Residuals: -1.08524, Convergence: 0.001055\n",
      "Epoch: 164, Loss: 939.65859, Residuals: -1.08296, Convergence: 0.001042\n",
      "Epoch: 165, Loss: 938.68773, Residuals: -1.08071, Convergence: 0.001034\n",
      "Epoch: 166, Loss: 937.72149, Residuals: -1.07849, Convergence: 0.001030\n",
      "Epoch: 167, Loss: 936.75840, Residuals: -1.07628, Convergence: 0.001028\n",
      "Epoch: 168, Loss: 935.79618, Residuals: -1.07409, Convergence: 0.001028\n",
      "Epoch: 169, Loss: 934.83575, Residuals: -1.07192, Convergence: 0.001027\n",
      "Epoch: 170, Loss: 933.87883, Residuals: -1.06975, Convergence: 0.001025\n",
      "Epoch: 171, Loss: 932.92928, Residuals: -1.06761, Convergence: 0.001018\n",
      "Epoch: 172, Loss: 931.99176, Residuals: -1.06549, Convergence: 0.001006\n",
      "Epoch: 173, Loss: 931.07176, Residuals: -1.06340, Convergence: 0.000988\n",
      "Evidence 11234.779\n",
      "\n",
      "Epoch: 173, Evidence: 11234.77930, Convergence: 1.016394\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 5.73e-01\n",
      "Epoch: 173, Loss: 2357.06410, Residuals: -1.06340, Convergence:   inf\n",
      "Epoch: 174, Loss: 2318.21686, Residuals: -1.06994, Convergence: 0.016757\n",
      "Epoch: 175, Loss: 2292.12602, Residuals: -1.06775, Convergence: 0.011383\n",
      "Epoch: 176, Loss: 2270.32880, Residuals: -1.06500, Convergence: 0.009601\n",
      "Epoch: 177, Loss: 2251.94850, Residuals: -1.06213, Convergence: 0.008162\n",
      "Epoch: 178, Loss: 2236.32523, Residuals: -1.05920, Convergence: 0.006986\n",
      "Epoch: 179, Loss: 2222.92507, Residuals: -1.05625, Convergence: 0.006028\n",
      "Epoch: 180, Loss: 2211.30631, Residuals: -1.05327, Convergence: 0.005254\n",
      "Epoch: 181, Loss: 2201.10524, Residuals: -1.05025, Convergence: 0.004635\n",
      "Epoch: 182, Loss: 2192.03355, Residuals: -1.04715, Convergence: 0.004138\n",
      "Epoch: 183, Loss: 2183.87784, Residuals: -1.04396, Convergence: 0.003735\n",
      "Epoch: 184, Loss: 2176.49309, Residuals: -1.04071, Convergence: 0.003393\n",
      "Epoch: 185, Loss: 2169.78823, Residuals: -1.03743, Convergence: 0.003090\n",
      "Epoch: 186, Loss: 2163.70098, Residuals: -1.03417, Convergence: 0.002813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 187, Loss: 2158.17599, Residuals: -1.03098, Convergence: 0.002560\n",
      "Epoch: 188, Loss: 2153.16236, Residuals: -1.02790, Convergence: 0.002328\n",
      "Epoch: 189, Loss: 2148.60658, Residuals: -1.02496, Convergence: 0.002120\n",
      "Epoch: 190, Loss: 2144.45628, Residuals: -1.02216, Convergence: 0.001935\n",
      "Epoch: 191, Loss: 2140.66248, Residuals: -1.01952, Convergence: 0.001772\n",
      "Epoch: 192, Loss: 2137.18030, Residuals: -1.01704, Convergence: 0.001629\n",
      "Epoch: 193, Loss: 2133.96856, Residuals: -1.01472, Convergence: 0.001505\n",
      "Epoch: 194, Loss: 2130.99093, Residuals: -1.01253, Convergence: 0.001397\n",
      "Epoch: 195, Loss: 2128.21661, Residuals: -1.01048, Convergence: 0.001304\n",
      "Epoch: 196, Loss: 2125.61831, Residuals: -1.00855, Convergence: 0.001222\n",
      "Epoch: 197, Loss: 2123.17366, Residuals: -1.00674, Convergence: 0.001151\n",
      "Epoch: 198, Loss: 2120.86234, Residuals: -1.00502, Convergence: 0.001090\n",
      "Epoch: 199, Loss: 2118.66772, Residuals: -1.00339, Convergence: 0.001036\n",
      "Epoch: 200, Loss: 2116.57765, Residuals: -1.00184, Convergence: 0.000987\n",
      "Evidence 14341.244\n",
      "\n",
      "Epoch: 200, Evidence: 14341.24414, Convergence: 0.216611\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 4.34e-01\n",
      "Epoch: 200, Loss: 2479.46494, Residuals: -1.00184, Convergence:   inf\n",
      "Epoch: 201, Loss: 2466.79146, Residuals: -0.99867, Convergence: 0.005138\n",
      "Epoch: 202, Loss: 2456.19159, Residuals: -0.99514, Convergence: 0.004316\n",
      "Epoch: 203, Loss: 2446.78613, Residuals: -0.99176, Convergence: 0.003844\n",
      "Epoch: 204, Loss: 2438.38567, Residuals: -0.98858, Convergence: 0.003445\n",
      "Epoch: 205, Loss: 2430.85550, Residuals: -0.98561, Convergence: 0.003098\n",
      "Epoch: 206, Loss: 2424.09180, Residuals: -0.98287, Convergence: 0.002790\n",
      "Epoch: 207, Loss: 2418.01042, Residuals: -0.98035, Convergence: 0.002515\n",
      "Epoch: 208, Loss: 2412.53803, Residuals: -0.97803, Convergence: 0.002268\n",
      "Epoch: 209, Loss: 2407.60964, Residuals: -0.97591, Convergence: 0.002047\n",
      "Epoch: 210, Loss: 2403.16548, Residuals: -0.97396, Convergence: 0.001849\n",
      "Epoch: 211, Loss: 2399.15052, Residuals: -0.97218, Convergence: 0.001673\n",
      "Epoch: 212, Loss: 2395.51563, Residuals: -0.97055, Convergence: 0.001517\n",
      "Epoch: 213, Loss: 2392.21630, Residuals: -0.96905, Convergence: 0.001379\n",
      "Epoch: 214, Loss: 2389.21344, Residuals: -0.96767, Convergence: 0.001257\n",
      "Epoch: 215, Loss: 2386.47123, Residuals: -0.96639, Convergence: 0.001149\n",
      "Epoch: 216, Loss: 2383.95905, Residuals: -0.96522, Convergence: 0.001054\n",
      "Epoch: 217, Loss: 2381.64977, Residuals: -0.96412, Convergence: 0.000970\n",
      "Evidence 14705.569\n",
      "\n",
      "Epoch: 217, Evidence: 14705.56934, Convergence: 0.024775\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 3.31e-01\n",
      "Epoch: 217, Loss: 2483.70892, Residuals: -0.96412, Convergence:   inf\n",
      "Epoch: 218, Loss: 2476.48132, Residuals: -0.96066, Convergence: 0.002918\n",
      "Epoch: 219, Loss: 2470.47883, Residuals: -0.95775, Convergence: 0.002430\n",
      "Epoch: 220, Loss: 2465.38138, Residuals: -0.95535, Convergence: 0.002068\n",
      "Epoch: 221, Loss: 2461.00037, Residuals: -0.95334, Convergence: 0.001780\n",
      "Epoch: 222, Loss: 2457.19225, Residuals: -0.95166, Convergence: 0.001550\n",
      "Epoch: 223, Loss: 2453.84753, Residuals: -0.95023, Convergence: 0.001363\n",
      "Epoch: 224, Loss: 2450.88031, Residuals: -0.94901, Convergence: 0.001211\n",
      "Epoch: 225, Loss: 2448.22338, Residuals: -0.94796, Convergence: 0.001085\n",
      "Epoch: 226, Loss: 2445.82385, Residuals: -0.94705, Convergence: 0.000981\n",
      "Evidence 14798.174\n",
      "\n",
      "Epoch: 226, Evidence: 14798.17383, Convergence: 0.006258\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.58e-01\n",
      "Epoch: 226, Loss: 2484.95780, Residuals: -0.94705, Convergence:   inf\n",
      "Epoch: 227, Loss: 2480.56074, Residuals: -0.94453, Convergence: 0.001773\n",
      "Epoch: 228, Loss: 2476.92075, Residuals: -0.94259, Convergence: 0.001470\n",
      "Epoch: 229, Loss: 2473.82643, Residuals: -0.94106, Convergence: 0.001251\n",
      "Epoch: 230, Loss: 2471.14836, Residuals: -0.93983, Convergence: 0.001084\n",
      "Epoch: 231, Loss: 2468.79486, Residuals: -0.93882, Convergence: 0.000953\n",
      "Evidence 14832.836\n",
      "\n",
      "Epoch: 231, Evidence: 14832.83594, Convergence: 0.002337\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.06e-01\n",
      "Epoch: 231, Loss: 2485.67765, Residuals: -0.93882, Convergence:   inf\n",
      "Epoch: 232, Loss: 2482.56295, Residuals: -0.93706, Convergence: 0.001255\n",
      "Epoch: 233, Loss: 2479.98458, Residuals: -0.93573, Convergence: 0.001040\n",
      "Epoch: 234, Loss: 2477.77901, Residuals: -0.93467, Convergence: 0.000890\n",
      "Evidence 14848.988\n",
      "\n",
      "Epoch: 234, Evidence: 14848.98828, Convergence: 0.001088\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.69e-01\n",
      "Epoch: 234, Loss: 2486.19806, Residuals: -0.93467, Convergence:   inf\n",
      "Epoch: 235, Loss: 2483.76896, Residuals: -0.93324, Convergence: 0.000978\n",
      "Evidence 14855.797\n",
      "\n",
      "Epoch: 235, Evidence: 14855.79688, Convergence: 0.000458\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.43e-01\n",
      "Epoch: 235, Loss: 2486.65820, Residuals: -0.93324, Convergence:   inf\n",
      "Epoch: 236, Loss: 2482.65882, Residuals: -0.93141, Convergence: 0.001611\n",
      "Epoch: 237, Loss: 2479.55264, Residuals: -0.92986, Convergence: 0.001253\n",
      "Epoch: 238, Loss: 2477.02861, Residuals: -0.92866, Convergence: 0.001019\n",
      "Epoch: 239, Loss: 2474.88042, Residuals: -0.92791, Convergence: 0.000868\n",
      "Evidence 14870.803\n",
      "\n",
      "Epoch: 239, Evidence: 14870.80273, Convergence: 0.001467\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.20e-01\n",
      "Epoch: 239, Loss: 2486.67513, Residuals: -0.92791, Convergence:   inf\n",
      "Epoch: 240, Loss: 2484.01150, Residuals: -0.92571, Convergence: 0.001072\n",
      "Epoch: 241, Loss: 2481.87537, Residuals: -0.92443, Convergence: 0.000861\n",
      "Evidence 14880.332\n",
      "\n",
      "Epoch: 241, Evidence: 14880.33203, Convergence: 0.000640\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.03e-01\n",
      "Epoch: 241, Loss: 2486.72940, Residuals: -0.92443, Convergence:   inf\n",
      "Epoch: 242, Loss: 2482.79233, Residuals: -0.92096, Convergence: 0.001586\n",
      "Epoch: 243, Loss: 2479.92730, Residuals: -0.92133, Convergence: 0.001155\n",
      "Epoch: 244, Loss: 2477.49427, Residuals: -0.92164, Convergence: 0.000982\n",
      "Evidence 14892.488\n",
      "\n",
      "Epoch: 244, Evidence: 14892.48828, Convergence: 0.001456\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 8.54e-02\n",
      "Epoch: 244, Loss: 2486.70106, Residuals: -0.92164, Convergence:   inf\n",
      "Epoch: 245, Loss: 2484.17090, Residuals: -0.91954, Convergence: 0.001019\n",
      "Epoch: 246, Loss: 2483.28358, Residuals: -0.92098, Convergence: 0.000357\n",
      "Evidence 14899.719\n",
      "\n",
      "Epoch: 246, Evidence: 14899.71875, Convergence: 0.000485\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.03e-02\n",
      "Epoch: 246, Loss: 2486.75935, Residuals: -0.92098, Convergence:   inf\n",
      "Epoch: 247, Loss: 2534.65629, Residuals: -0.96180, Convergence: -0.018897\n",
      "Epoch: 247, Loss: 2484.45925, Residuals: -0.91758, Convergence: 0.000926\n",
      "Evidence 14904.521\n",
      "\n",
      "Epoch: 247, Evidence: 14904.52148, Convergence: 0.000807\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 6.48e-02\n",
      "Epoch: 247, Loss: 2486.28146, Residuals: -0.91758, Convergence:   inf\n",
      "Epoch: 248, Loss: 2488.46405, Residuals: -0.91492, Convergence: -0.000877\n",
      "Evidence 14903.878\n",
      "\n",
      "Epoch: 248, Evidence: 14903.87793, Convergence: 0.000764\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 382.32314, Residuals: -4.53724, Convergence:   inf\n",
      "Epoch: 1, Loss: 356.60124, Residuals: -4.41739, Convergence: 0.072131\n",
      "Epoch: 2, Loss: 335.56757, Residuals: -4.25362, Convergence: 0.062681\n",
      "Epoch: 3, Loss: 319.49982, Residuals: -4.08948, Convergence: 0.050290\n",
      "Epoch: 4, Loss: 307.24847, Residuals: -3.94481, Convergence: 0.039874\n",
      "Epoch: 5, Loss: 297.53330, Residuals: -3.81689, Convergence: 0.032652\n",
      "Epoch: 6, Loss: 289.64718, Residuals: -3.70544, Convergence: 0.027227\n",
      "Epoch: 7, Loss: 283.10188, Residuals: -3.60999, Convergence: 0.023120\n",
      "Epoch: 8, Loss: 277.53614, Residuals: -3.52848, Convergence: 0.020054\n",
      "Epoch: 9, Loss: 272.69506, Residuals: -3.45858, Convergence: 0.017753\n",
      "Epoch: 10, Loss: 268.39753, Residuals: -3.39818, Convergence: 0.016012\n",
      "Epoch: 11, Loss: 264.51254, Residuals: -3.34550, Convergence: 0.014687\n",
      "Epoch: 12, Loss: 260.94459, Residuals: -3.29900, Convergence: 0.013673\n",
      "Epoch: 13, Loss: 257.62436, Residuals: -3.25738, Convergence: 0.012888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Loss: 254.50234, Residuals: -3.21948, Convergence: 0.012267\n",
      "Epoch: 15, Loss: 251.54553, Residuals: -3.18433, Convergence: 0.011755\n",
      "Epoch: 16, Loss: 248.73551, Residuals: -3.15129, Convergence: 0.011297\n",
      "Epoch: 17, Loss: 246.06016, Residuals: -3.11995, Convergence: 0.010873\n",
      "Epoch: 18, Loss: 243.49996, Residuals: -3.08996, Convergence: 0.010514\n",
      "Epoch: 19, Loss: 241.02351, Residuals: -3.06084, Convergence: 0.010275\n",
      "Epoch: 20, Loss: 238.59354, Residuals: -3.03201, Convergence: 0.010185\n",
      "Epoch: 21, Loss: 236.17532, Residuals: -3.00295, Convergence: 0.010239\n",
      "Epoch: 22, Loss: 233.74159, Residuals: -2.97326, Convergence: 0.010412\n",
      "Epoch: 23, Loss: 231.26643, Residuals: -2.94262, Convergence: 0.010703\n",
      "Epoch: 24, Loss: 228.70835, Residuals: -2.91061, Convergence: 0.011185\n",
      "Epoch: 25, Loss: 226.00809, Residuals: -2.87651, Convergence: 0.011948\n",
      "Epoch: 26, Loss: 223.14727, Residuals: -2.84004, Convergence: 0.012820\n",
      "Epoch: 27, Loss: 220.23480, Residuals: -2.80230, Convergence: 0.013224\n",
      "Epoch: 28, Loss: 217.39603, Residuals: -2.76475, Convergence: 0.013058\n",
      "Epoch: 29, Loss: 214.65924, Residuals: -2.72785, Convergence: 0.012749\n",
      "Epoch: 30, Loss: 212.01032, Residuals: -2.69157, Convergence: 0.012494\n",
      "Epoch: 31, Loss: 209.43219, Residuals: -2.65577, Convergence: 0.012310\n",
      "Epoch: 32, Loss: 206.91288, Residuals: -2.62031, Convergence: 0.012176\n",
      "Epoch: 33, Loss: 204.44550, Residuals: -2.58512, Convergence: 0.012069\n",
      "Epoch: 34, Loss: 202.02692, Residuals: -2.55013, Convergence: 0.011972\n",
      "Epoch: 35, Loss: 199.65637, Residuals: -2.51533, Convergence: 0.011873\n",
      "Epoch: 36, Loss: 197.33444, Residuals: -2.48070, Convergence: 0.011766\n",
      "Epoch: 37, Loss: 195.06238, Residuals: -2.44624, Convergence: 0.011648\n",
      "Epoch: 38, Loss: 192.84150, Residuals: -2.41195, Convergence: 0.011517\n",
      "Epoch: 39, Loss: 190.67297, Residuals: -2.37786, Convergence: 0.011373\n",
      "Epoch: 40, Loss: 188.55761, Residuals: -2.34397, Convergence: 0.011219\n",
      "Epoch: 41, Loss: 186.49586, Residuals: -2.31032, Convergence: 0.011055\n",
      "Epoch: 42, Loss: 184.48788, Residuals: -2.27692, Convergence: 0.010884\n",
      "Epoch: 43, Loss: 182.53367, Residuals: -2.24381, Convergence: 0.010706\n",
      "Epoch: 44, Loss: 180.63346, Residuals: -2.21099, Convergence: 0.010520\n",
      "Epoch: 45, Loss: 178.78793, Residuals: -2.17848, Convergence: 0.010322\n",
      "Epoch: 46, Loss: 176.99847, Residuals: -2.14632, Convergence: 0.010110\n",
      "Epoch: 47, Loss: 175.26720, Residuals: -2.11452, Convergence: 0.009878\n",
      "Epoch: 48, Loss: 173.59669, Residuals: -2.08311, Convergence: 0.009623\n",
      "Epoch: 49, Loss: 171.98953, Residuals: -2.05213, Convergence: 0.009345\n",
      "Epoch: 50, Loss: 170.44793, Residuals: -2.02163, Convergence: 0.009044\n",
      "Epoch: 51, Loss: 168.97339, Residuals: -1.99167, Convergence: 0.008726\n",
      "Epoch: 52, Loss: 167.56662, Residuals: -1.96228, Convergence: 0.008395\n",
      "Epoch: 53, Loss: 166.22749, Residuals: -1.93351, Convergence: 0.008056\n",
      "Epoch: 54, Loss: 164.95515, Residuals: -1.90540, Convergence: 0.007713\n",
      "Epoch: 55, Loss: 163.74811, Residuals: -1.87799, Convergence: 0.007371\n",
      "Epoch: 56, Loss: 162.60443, Residuals: -1.85130, Convergence: 0.007033\n",
      "Epoch: 57, Loss: 161.52178, Residuals: -1.82536, Convergence: 0.006703\n",
      "Epoch: 58, Loss: 160.49752, Residuals: -1.80018, Convergence: 0.006382\n",
      "Epoch: 59, Loss: 159.52881, Residuals: -1.77577, Convergence: 0.006072\n",
      "Epoch: 60, Loss: 158.61259, Residuals: -1.75212, Convergence: 0.005776\n",
      "Epoch: 61, Loss: 157.74569, Residuals: -1.72925, Convergence: 0.005496\n",
      "Epoch: 62, Loss: 156.92484, Residuals: -1.70713, Convergence: 0.005231\n",
      "Epoch: 63, Loss: 156.14670, Residuals: -1.68576, Convergence: 0.004983\n",
      "Epoch: 64, Loss: 155.40793, Residuals: -1.66512, Convergence: 0.004754\n",
      "Epoch: 65, Loss: 154.70529, Residuals: -1.64517, Convergence: 0.004542\n",
      "Epoch: 66, Loss: 154.03570, Residuals: -1.62589, Convergence: 0.004347\n",
      "Epoch: 67, Loss: 153.39633, Residuals: -1.60724, Convergence: 0.004168\n",
      "Epoch: 68, Loss: 152.78465, Residuals: -1.58921, Convergence: 0.004004\n",
      "Epoch: 69, Loss: 152.19851, Residuals: -1.57174, Convergence: 0.003851\n",
      "Epoch: 70, Loss: 151.63609, Residuals: -1.55484, Convergence: 0.003709\n",
      "Epoch: 71, Loss: 151.09589, Residuals: -1.53846, Convergence: 0.003575\n",
      "Epoch: 72, Loss: 150.57673, Residuals: -1.52259, Convergence: 0.003448\n",
      "Epoch: 73, Loss: 150.07765, Residuals: -1.50721, Convergence: 0.003326\n",
      "Epoch: 74, Loss: 149.59786, Residuals: -1.49232, Convergence: 0.003207\n",
      "Epoch: 75, Loss: 149.13675, Residuals: -1.47791, Convergence: 0.003092\n",
      "Epoch: 76, Loss: 148.69377, Residuals: -1.46395, Convergence: 0.002979\n",
      "Epoch: 77, Loss: 148.26846, Residuals: -1.45045, Convergence: 0.002869\n",
      "Epoch: 78, Loss: 147.86039, Residuals: -1.43739, Convergence: 0.002760\n",
      "Epoch: 79, Loss: 147.46913, Residuals: -1.42478, Convergence: 0.002653\n",
      "Epoch: 80, Loss: 147.09429, Residuals: -1.41259, Convergence: 0.002548\n",
      "Epoch: 81, Loss: 146.73543, Residuals: -1.40083, Convergence: 0.002446\n",
      "Epoch: 82, Loss: 146.39215, Residuals: -1.38948, Convergence: 0.002345\n",
      "Epoch: 83, Loss: 146.06397, Residuals: -1.37854, Convergence: 0.002247\n",
      "Epoch: 84, Loss: 145.75045, Residuals: -1.36799, Convergence: 0.002151\n",
      "Epoch: 85, Loss: 145.45109, Residuals: -1.35783, Convergence: 0.002058\n",
      "Epoch: 86, Loss: 145.16539, Residuals: -1.34805, Convergence: 0.001968\n",
      "Epoch: 87, Loss: 144.89285, Residuals: -1.33863, Convergence: 0.001881\n",
      "Epoch: 88, Loss: 144.63294, Residuals: -1.32957, Convergence: 0.001797\n",
      "Epoch: 89, Loss: 144.38515, Residuals: -1.32086, Convergence: 0.001716\n",
      "Epoch: 90, Loss: 144.14896, Residuals: -1.31248, Convergence: 0.001638\n",
      "Epoch: 91, Loss: 143.92387, Residuals: -1.30443, Convergence: 0.001564\n",
      "Epoch: 92, Loss: 143.70938, Residuals: -1.29669, Convergence: 0.001493\n",
      "Epoch: 93, Loss: 143.50499, Residuals: -1.28925, Convergence: 0.001424\n",
      "Epoch: 94, Loss: 143.31027, Residuals: -1.28210, Convergence: 0.001359\n",
      "Epoch: 95, Loss: 143.12475, Residuals: -1.27523, Convergence: 0.001296\n",
      "Epoch: 96, Loss: 142.94804, Residuals: -1.26863, Convergence: 0.001236\n",
      "Epoch: 97, Loss: 142.77974, Residuals: -1.26229, Convergence: 0.001179\n",
      "Epoch: 98, Loss: 142.61949, Residuals: -1.25619, Convergence: 0.001124\n",
      "Epoch: 99, Loss: 142.46697, Residuals: -1.25034, Convergence: 0.001071\n",
      "Epoch: 100, Loss: 142.32187, Residuals: -1.24471, Convergence: 0.001020\n",
      "Epoch: 101, Loss: 142.18390, Residuals: -1.23930, Convergence: 0.000970\n",
      "Evidence -184.050\n",
      "\n",
      "Epoch: 101, Evidence: -184.05034, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.26e-01\n",
      "Epoch: 101, Loss: 1375.76474, Residuals: -1.23930, Convergence:   inf\n",
      "Epoch: 102, Loss: 1314.27147, Residuals: -1.26748, Convergence: 0.046789\n",
      "Epoch: 103, Loss: 1266.30272, Residuals: -1.28974, Convergence: 0.037881\n",
      "Epoch: 104, Loss: 1229.22415, Residuals: -1.30612, Convergence: 0.030164\n",
      "Epoch: 105, Loss: 1200.12742, Residuals: -1.31786, Convergence: 0.024245\n",
      "Epoch: 106, Loss: 1176.62974, Residuals: -1.32660, Convergence: 0.019970\n",
      "Epoch: 107, Loss: 1157.18414, Residuals: -1.33334, Convergence: 0.016804\n",
      "Epoch: 108, Loss: 1140.81302, Residuals: -1.33853, Convergence: 0.014350\n",
      "Epoch: 109, Loss: 1126.84866, Residuals: -1.34241, Convergence: 0.012392\n",
      "Epoch: 110, Loss: 1114.79910, Residuals: -1.34509, Convergence: 0.010809\n",
      "Epoch: 111, Loss: 1104.28412, Residuals: -1.34670, Convergence: 0.009522\n",
      "Epoch: 112, Loss: 1095.00054, Residuals: -1.34730, Convergence: 0.008478\n",
      "Epoch: 113, Loss: 1086.70121, Residuals: -1.34698, Convergence: 0.007637\n",
      "Epoch: 114, Loss: 1079.18149, Residuals: -1.34580, Convergence: 0.006968\n",
      "Epoch: 115, Loss: 1072.27000, Residuals: -1.34381, Convergence: 0.006446\n",
      "Epoch: 116, Loss: 1065.82333, Residuals: -1.34106, Convergence: 0.006049\n",
      "Epoch: 117, Loss: 1059.71911, Residuals: -1.33757, Convergence: 0.005760\n",
      "Epoch: 118, Loss: 1053.85308, Residuals: -1.33339, Convergence: 0.005566\n",
      "Epoch: 119, Loss: 1048.13562, Residuals: -1.32852, Convergence: 0.005455\n",
      "Epoch: 120, Loss: 1042.49307, Residuals: -1.32302, Convergence: 0.005413\n",
      "Epoch: 121, Loss: 1036.87491, Residuals: -1.31692, Convergence: 0.005418\n",
      "Epoch: 122, Loss: 1031.26672, Residuals: -1.31031, Convergence: 0.005438\n",
      "Epoch: 123, Loss: 1025.70023, Residuals: -1.30328, Convergence: 0.005427\n",
      "Epoch: 124, Loss: 1020.24641, Residuals: -1.29593, Convergence: 0.005346\n",
      "Epoch: 125, Loss: 1014.98950, Residuals: -1.28834, Convergence: 0.005179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 126, Loss: 1009.99595, Residuals: -1.28059, Convergence: 0.004944\n",
      "Epoch: 127, Loss: 1005.29859, Residuals: -1.27275, Convergence: 0.004673\n",
      "Epoch: 128, Loss: 1000.90025, Residuals: -1.26486, Convergence: 0.004394\n",
      "Epoch: 129, Loss: 996.78315, Residuals: -1.25698, Convergence: 0.004130\n",
      "Epoch: 130, Loss: 992.92160, Residuals: -1.24915, Convergence: 0.003889\n",
      "Epoch: 131, Loss: 989.28825, Residuals: -1.24140, Convergence: 0.003673\n",
      "Epoch: 132, Loss: 985.85765, Residuals: -1.23376, Convergence: 0.003480\n",
      "Epoch: 133, Loss: 982.60763, Residuals: -1.22626, Convergence: 0.003308\n",
      "Epoch: 134, Loss: 979.52000, Residuals: -1.21892, Convergence: 0.003152\n",
      "Epoch: 135, Loss: 976.57990, Residuals: -1.21177, Convergence: 0.003011\n",
      "Epoch: 136, Loss: 973.77528, Residuals: -1.20481, Convergence: 0.002880\n",
      "Epoch: 137, Loss: 971.09630, Residuals: -1.19805, Convergence: 0.002759\n",
      "Epoch: 138, Loss: 968.53501, Residuals: -1.19151, Convergence: 0.002644\n",
      "Epoch: 139, Loss: 966.08442, Residuals: -1.18520, Convergence: 0.002537\n",
      "Epoch: 140, Loss: 963.73898, Residuals: -1.17912, Convergence: 0.002434\n",
      "Epoch: 141, Loss: 961.49321, Residuals: -1.17327, Convergence: 0.002336\n",
      "Epoch: 142, Loss: 959.34238, Residuals: -1.16764, Convergence: 0.002242\n",
      "Epoch: 143, Loss: 957.28171, Residuals: -1.16225, Convergence: 0.002153\n",
      "Epoch: 144, Loss: 955.30657, Residuals: -1.15707, Convergence: 0.002068\n",
      "Epoch: 145, Loss: 953.41259, Residuals: -1.15212, Convergence: 0.001987\n",
      "Epoch: 146, Loss: 951.59518, Residuals: -1.14737, Convergence: 0.001910\n",
      "Epoch: 147, Loss: 949.84933, Residuals: -1.14282, Convergence: 0.001838\n",
      "Epoch: 148, Loss: 948.17105, Residuals: -1.13846, Convergence: 0.001770\n",
      "Epoch: 149, Loss: 946.55484, Residuals: -1.13429, Convergence: 0.001707\n",
      "Epoch: 150, Loss: 944.99560, Residuals: -1.13028, Convergence: 0.001650\n",
      "Epoch: 151, Loss: 943.48795, Residuals: -1.12644, Convergence: 0.001598\n",
      "Epoch: 152, Loss: 942.02671, Residuals: -1.12274, Convergence: 0.001551\n",
      "Epoch: 153, Loss: 940.60583, Residuals: -1.11917, Convergence: 0.001511\n",
      "Epoch: 154, Loss: 939.21965, Residuals: -1.11573, Convergence: 0.001476\n",
      "Epoch: 155, Loss: 937.86268, Residuals: -1.11239, Convergence: 0.001447\n",
      "Epoch: 156, Loss: 936.52949, Residuals: -1.10914, Convergence: 0.001424\n",
      "Epoch: 157, Loss: 935.21563, Residuals: -1.10597, Convergence: 0.001405\n",
      "Epoch: 158, Loss: 933.91784, Residuals: -1.10288, Convergence: 0.001390\n",
      "Epoch: 159, Loss: 932.63403, Residuals: -1.09985, Convergence: 0.001377\n",
      "Epoch: 160, Loss: 931.36411, Residuals: -1.09688, Convergence: 0.001364\n",
      "Epoch: 161, Loss: 930.10950, Residuals: -1.09397, Convergence: 0.001349\n",
      "Epoch: 162, Loss: 928.87356, Residuals: -1.09113, Convergence: 0.001331\n",
      "Epoch: 163, Loss: 927.65991, Residuals: -1.08835, Convergence: 0.001308\n",
      "Epoch: 164, Loss: 926.47260, Residuals: -1.08564, Convergence: 0.001282\n",
      "Epoch: 165, Loss: 925.31575, Residuals: -1.08301, Convergence: 0.001250\n",
      "Epoch: 166, Loss: 924.19259, Residuals: -1.08047, Convergence: 0.001215\n",
      "Epoch: 167, Loss: 923.10596, Residuals: -1.07800, Convergence: 0.001177\n",
      "Epoch: 168, Loss: 922.05758, Residuals: -1.07563, Convergence: 0.001137\n",
      "Epoch: 169, Loss: 921.04874, Residuals: -1.07334, Convergence: 0.001095\n",
      "Epoch: 170, Loss: 920.08006, Residuals: -1.07114, Convergence: 0.001053\n",
      "Epoch: 171, Loss: 919.15210, Residuals: -1.06903, Convergence: 0.001010\n",
      "Epoch: 172, Loss: 918.26371, Residuals: -1.06700, Convergence: 0.000967\n",
      "Evidence 11119.862\n",
      "\n",
      "Epoch: 172, Evidence: 11119.86230, Convergence: 1.016551\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.76e-01\n",
      "Epoch: 172, Loss: 2340.68452, Residuals: -1.06700, Convergence:   inf\n",
      "Epoch: 173, Loss: 2299.09467, Residuals: -1.07472, Convergence: 0.018090\n",
      "Epoch: 174, Loss: 2271.93970, Residuals: -1.07330, Convergence: 0.011952\n",
      "Epoch: 175, Loss: 2249.54962, Residuals: -1.07100, Convergence: 0.009953\n",
      "Epoch: 176, Loss: 2230.72888, Residuals: -1.06846, Convergence: 0.008437\n",
      "Epoch: 177, Loss: 2214.74642, Residuals: -1.06580, Convergence: 0.007216\n",
      "Epoch: 178, Loss: 2201.04129, Residuals: -1.06303, Convergence: 0.006227\n",
      "Epoch: 179, Loss: 2189.16065, Residuals: -1.06018, Convergence: 0.005427\n",
      "Epoch: 180, Loss: 2178.73520, Residuals: -1.05724, Convergence: 0.004785\n",
      "Epoch: 181, Loss: 2169.46881, Residuals: -1.05421, Convergence: 0.004271\n",
      "Epoch: 182, Loss: 2161.13672, Residuals: -1.05109, Convergence: 0.003855\n",
      "Epoch: 183, Loss: 2153.57958, Residuals: -1.04789, Convergence: 0.003509\n",
      "Epoch: 184, Loss: 2146.69141, Residuals: -1.04464, Convergence: 0.003209\n",
      "Epoch: 185, Loss: 2140.40083, Residuals: -1.04138, Convergence: 0.002939\n",
      "Epoch: 186, Loss: 2134.65508, Residuals: -1.03815, Convergence: 0.002692\n",
      "Epoch: 187, Loss: 2129.40531, Residuals: -1.03499, Convergence: 0.002465\n",
      "Epoch: 188, Loss: 2124.60662, Residuals: -1.03193, Convergence: 0.002259\n",
      "Epoch: 189, Loss: 2120.21190, Residuals: -1.02898, Convergence: 0.002073\n",
      "Epoch: 190, Loss: 2116.17911, Residuals: -1.02615, Convergence: 0.001906\n",
      "Epoch: 191, Loss: 2112.46779, Residuals: -1.02344, Convergence: 0.001757\n",
      "Epoch: 192, Loss: 2109.04296, Residuals: -1.02085, Convergence: 0.001624\n",
      "Epoch: 193, Loss: 2105.87344, Residuals: -1.01837, Convergence: 0.001505\n",
      "Epoch: 194, Loss: 2102.93218, Residuals: -1.01601, Convergence: 0.001399\n",
      "Epoch: 195, Loss: 2100.19701, Residuals: -1.01375, Convergence: 0.001302\n",
      "Epoch: 196, Loss: 2097.64864, Residuals: -1.01159, Convergence: 0.001215\n",
      "Epoch: 197, Loss: 2095.27037, Residuals: -1.00954, Convergence: 0.001135\n",
      "Epoch: 198, Loss: 2093.04777, Residuals: -1.00757, Convergence: 0.001062\n",
      "Epoch: 199, Loss: 2090.96813, Residuals: -1.00570, Convergence: 0.000995\n",
      "Evidence 14277.345\n",
      "\n",
      "Epoch: 199, Evidence: 14277.34473, Convergence: 0.221153\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.39e-01\n",
      "Epoch: 199, Loss: 2462.11823, Residuals: -1.00570, Convergence:   inf\n",
      "Epoch: 200, Loss: 2448.86651, Residuals: -1.00332, Convergence: 0.005411\n",
      "Epoch: 201, Loss: 2438.03749, Residuals: -1.00025, Convergence: 0.004442\n",
      "Epoch: 202, Loss: 2428.61585, Residuals: -0.99720, Convergence: 0.003879\n",
      "Epoch: 203, Loss: 2420.38006, Residuals: -0.99427, Convergence: 0.003403\n",
      "Epoch: 204, Loss: 2413.15960, Residuals: -0.99152, Convergence: 0.002992\n",
      "Epoch: 205, Loss: 2406.81131, Residuals: -0.98896, Convergence: 0.002638\n",
      "Epoch: 206, Loss: 2401.21053, Residuals: -0.98657, Convergence: 0.002332\n",
      "Epoch: 207, Loss: 2396.25126, Residuals: -0.98436, Convergence: 0.002070\n",
      "Epoch: 208, Loss: 2391.84028, Residuals: -0.98230, Convergence: 0.001844\n",
      "Epoch: 209, Loss: 2387.89915, Residuals: -0.98038, Convergence: 0.001650\n",
      "Epoch: 210, Loss: 2384.36062, Residuals: -0.97860, Convergence: 0.001484\n",
      "Epoch: 211, Loss: 2381.16805, Residuals: -0.97693, Convergence: 0.001341\n",
      "Epoch: 212, Loss: 2378.27270, Residuals: -0.97537, Convergence: 0.001217\n",
      "Epoch: 213, Loss: 2375.63439, Residuals: -0.97391, Convergence: 0.001111\n",
      "Epoch: 214, Loss: 2373.21888, Residuals: -0.97254, Convergence: 0.001018\n",
      "Epoch: 215, Loss: 2370.99691, Residuals: -0.97125, Convergence: 0.000937\n",
      "Evidence 14653.546\n",
      "\n",
      "Epoch: 215, Evidence: 14653.54590, Convergence: 0.025673\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.36e-01\n",
      "Epoch: 215, Loss: 2466.77238, Residuals: -0.97125, Convergence:   inf\n",
      "Epoch: 216, Loss: 2460.09633, Residuals: -0.96827, Convergence: 0.002714\n",
      "Epoch: 217, Loss: 2454.58151, Residuals: -0.96563, Convergence: 0.002247\n",
      "Epoch: 218, Loss: 2449.90541, Residuals: -0.96336, Convergence: 0.001909\n",
      "Epoch: 219, Loss: 2445.88802, Residuals: -0.96139, Convergence: 0.001643\n",
      "Epoch: 220, Loss: 2442.39655, Residuals: -0.95968, Convergence: 0.001430\n",
      "Epoch: 221, Loss: 2439.32708, Residuals: -0.95818, Convergence: 0.001258\n",
      "Epoch: 222, Loss: 2436.59983, Residuals: -0.95686, Convergence: 0.001119\n",
      "Epoch: 223, Loss: 2434.15211, Residuals: -0.95569, Convergence: 0.001006\n",
      "Epoch: 224, Loss: 2431.93686, Residuals: -0.95465, Convergence: 0.000911\n",
      "Evidence 14740.403\n",
      "\n",
      "Epoch: 224, Evidence: 14740.40332, Convergence: 0.005892\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.63e-01\n",
      "Epoch: 224, Loss: 2468.15122, Residuals: -0.95465, Convergence:   inf\n",
      "Epoch: 225, Loss: 2464.18176, Residuals: -0.95240, Convergence: 0.001611\n",
      "Epoch: 226, Loss: 2460.89912, Residuals: -0.95062, Convergence: 0.001334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 227, Loss: 2458.09992, Residuals: -0.94917, Convergence: 0.001139\n",
      "Epoch: 228, Loss: 2455.66685, Residuals: -0.94798, Convergence: 0.000991\n",
      "Evidence 14770.282\n",
      "\n",
      "Epoch: 228, Evidence: 14770.28223, Convergence: 0.002023\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.12e-01\n",
      "Epoch: 228, Loss: 2469.15840, Residuals: -0.94798, Convergence:   inf\n",
      "Epoch: 229, Loss: 2466.17939, Residuals: -0.94630, Convergence: 0.001208\n",
      "Epoch: 230, Loss: 2463.70537, Residuals: -0.94498, Convergence: 0.001004\n",
      "Epoch: 231, Loss: 2461.57768, Residuals: -0.94393, Convergence: 0.000864\n",
      "Evidence 14785.102\n",
      "\n",
      "Epoch: 231, Evidence: 14785.10156, Convergence: 0.001002\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.75e-01\n",
      "Epoch: 231, Loss: 2469.80946, Residuals: -0.94393, Convergence:   inf\n",
      "Epoch: 232, Loss: 2467.45637, Residuals: -0.94255, Convergence: 0.000954\n",
      "Evidence 14791.745\n",
      "\n",
      "Epoch: 232, Evidence: 14791.74512, Convergence: 0.000449\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.49e-01\n",
      "Epoch: 232, Loss: 2470.33513, Residuals: -0.94255, Convergence:   inf\n",
      "Epoch: 233, Loss: 2466.36043, Residuals: -0.94073, Convergence: 0.001612\n",
      "Epoch: 234, Loss: 2463.29859, Residuals: -0.93928, Convergence: 0.001243\n",
      "Epoch: 235, Loss: 2460.82138, Residuals: -0.93816, Convergence: 0.001007\n",
      "Epoch: 236, Loss: 2458.72348, Residuals: -0.93746, Convergence: 0.000853\n",
      "Evidence 14806.669\n",
      "\n",
      "Epoch: 236, Evidence: 14806.66895, Convergence: 0.001457\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.25e-01\n",
      "Epoch: 236, Loss: 2470.34484, Residuals: -0.93746, Convergence:   inf\n",
      "Epoch: 237, Loss: 2467.67026, Residuals: -0.93539, Convergence: 0.001084\n",
      "Epoch: 238, Loss: 2465.54248, Residuals: -0.93417, Convergence: 0.000863\n",
      "Evidence 14816.242\n",
      "\n",
      "Epoch: 238, Evidence: 14816.24219, Convergence: 0.000646\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.07e-01\n",
      "Epoch: 238, Loss: 2470.39150, Residuals: -0.93417, Convergence:   inf\n",
      "Epoch: 239, Loss: 2466.40606, Residuals: -0.93076, Convergence: 0.001616\n",
      "Epoch: 240, Loss: 2463.59781, Residuals: -0.93089, Convergence: 0.001140\n",
      "Epoch: 241, Loss: 2461.23998, Residuals: -0.93132, Convergence: 0.000958\n",
      "Evidence 14828.451\n",
      "\n",
      "Epoch: 241, Evidence: 14828.45117, Convergence: 0.001469\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.90e-02\n",
      "Epoch: 241, Loss: 2470.31211, Residuals: -0.93132, Convergence:   inf\n",
      "Epoch: 242, Loss: 2467.78314, Residuals: -0.92889, Convergence: 0.001025\n",
      "Epoch: 243, Loss: 2466.64992, Residuals: -0.93024, Convergence: 0.000459\n",
      "Evidence 14836.166\n",
      "\n",
      "Epoch: 243, Evidence: 14836.16602, Convergence: 0.000520\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.38e-02\n",
      "Epoch: 243, Loss: 2470.24274, Residuals: -0.93024, Convergence:   inf\n",
      "Epoch: 244, Loss: 2514.45295, Residuals: -0.96585, Convergence: -0.017582\n",
      "Epoch: 244, Loss: 2468.22465, Residuals: -0.92654, Convergence: 0.000818\n",
      "Evidence 14840.688\n",
      "\n",
      "Epoch: 244, Evidence: 14840.68750, Convergence: 0.000825\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 6.83e-02\n",
      "Epoch: 244, Loss: 2469.76450, Residuals: -0.92654, Convergence:   inf\n",
      "Epoch: 245, Loss: 2473.69003, Residuals: -0.92276, Convergence: -0.001587\n",
      "Epoch: 245, Loss: 2469.64336, Residuals: -0.92241, Convergence: 0.000049\n",
      "Evidence 14842.239\n",
      "\n",
      "Epoch: 245, Evidence: 14842.23926, Convergence: 0.000929\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 381.46488, Residuals: -4.50269, Convergence:   inf\n",
      "Epoch: 1, Loss: 355.91852, Residuals: -4.38428, Convergence: 0.071776\n",
      "Epoch: 2, Loss: 334.95748, Residuals: -4.22190, Convergence: 0.062578\n",
      "Epoch: 3, Loss: 318.86970, Residuals: -4.05734, Convergence: 0.050453\n",
      "Epoch: 4, Loss: 306.64064, Residuals: -3.91308, Convergence: 0.039881\n",
      "Epoch: 5, Loss: 296.96208, Residuals: -3.78586, Convergence: 0.032592\n",
      "Epoch: 6, Loss: 289.11975, Residuals: -3.67501, Convergence: 0.027125\n",
      "Epoch: 7, Loss: 282.62213, Residuals: -3.57995, Convergence: 0.022990\n",
      "Epoch: 8, Loss: 277.10500, Residuals: -3.49870, Convergence: 0.019910\n",
      "Epoch: 9, Loss: 272.30985, Residuals: -3.42894, Convergence: 0.017609\n",
      "Epoch: 10, Loss: 268.05227, Residuals: -3.36859, Convergence: 0.015883\n",
      "Epoch: 11, Loss: 264.19864, Residuals: -3.31586, Convergence: 0.014586\n",
      "Epoch: 12, Loss: 260.65160, Residuals: -3.26922, Convergence: 0.013608\n",
      "Epoch: 13, Loss: 257.34114, Residuals: -3.22736, Convergence: 0.012864\n",
      "Epoch: 14, Loss: 254.21861, Residuals: -3.18916, Convergence: 0.012283\n",
      "Epoch: 15, Loss: 251.25346, Residuals: -3.15371, Convergence: 0.011801\n",
      "Epoch: 16, Loss: 248.43071, Residuals: -3.12041, Convergence: 0.011362\n",
      "Epoch: 17, Loss: 245.74175, Residuals: -3.08891, Convergence: 0.010942\n",
      "Epoch: 18, Loss: 243.17001, Residuals: -3.05890, Convergence: 0.010576\n",
      "Epoch: 19, Loss: 240.68659, Residuals: -3.02992, Convergence: 0.010318\n",
      "Epoch: 20, Loss: 238.25743, Residuals: -3.00145, Convergence: 0.010196\n",
      "Epoch: 21, Loss: 235.85300, Residuals: -2.97301, Convergence: 0.010195\n",
      "Epoch: 22, Loss: 233.45326, Residuals: -2.94425, Convergence: 0.010279\n",
      "Epoch: 23, Loss: 231.03970, Residuals: -2.91496, Convergence: 0.010447\n",
      "Epoch: 24, Loss: 228.57875, Residuals: -2.88472, Convergence: 0.010766\n",
      "Epoch: 25, Loss: 226.01825, Residuals: -2.85289, Convergence: 0.011329\n",
      "Epoch: 26, Loss: 223.32473, Residuals: -2.81901, Convergence: 0.012061\n",
      "Epoch: 27, Loss: 220.56524, Residuals: -2.78369, Convergence: 0.012511\n",
      "Epoch: 28, Loss: 217.86402, Residuals: -2.74834, Convergence: 0.012399\n",
      "Epoch: 29, Loss: 215.26808, Residuals: -2.71362, Convergence: 0.012059\n",
      "Epoch: 30, Loss: 212.76765, Residuals: -2.67956, Convergence: 0.011752\n",
      "Epoch: 31, Loss: 210.34248, Residuals: -2.64602, Convergence: 0.011530\n",
      "Epoch: 32, Loss: 207.97533, Residuals: -2.61284, Convergence: 0.011382\n",
      "Epoch: 33, Loss: 205.65388, Residuals: -2.57990, Convergence: 0.011288\n",
      "Epoch: 34, Loss: 203.37029, Residuals: -2.54710, Convergence: 0.011229\n",
      "Epoch: 35, Loss: 201.12026, Residuals: -2.51435, Convergence: 0.011188\n",
      "Epoch: 36, Loss: 198.90217, Residuals: -2.48161, Convergence: 0.011152\n",
      "Epoch: 37, Loss: 196.71618, Residuals: -2.44885, Convergence: 0.011112\n",
      "Epoch: 38, Loss: 194.56357, Residuals: -2.41603, Convergence: 0.011064\n",
      "Epoch: 39, Loss: 192.44612, Residuals: -2.38317, Convergence: 0.011003\n",
      "Epoch: 40, Loss: 190.36585, Residuals: -2.35025, Convergence: 0.010928\n",
      "Epoch: 41, Loss: 188.32481, Residuals: -2.31729, Convergence: 0.010838\n",
      "Epoch: 42, Loss: 186.32503, Residuals: -2.28431, Convergence: 0.010733\n",
      "Epoch: 43, Loss: 184.36860, Residuals: -2.25134, Convergence: 0.010612\n",
      "Epoch: 44, Loss: 182.45769, Residuals: -2.21841, Convergence: 0.010473\n",
      "Epoch: 45, Loss: 180.59461, Residuals: -2.18556, Convergence: 0.010316\n",
      "Epoch: 46, Loss: 178.78174, Residuals: -2.15285, Convergence: 0.010140\n",
      "Epoch: 47, Loss: 177.02150, Residuals: -2.12033, Convergence: 0.009944\n",
      "Epoch: 48, Loss: 175.31613, Residuals: -2.08806, Convergence: 0.009727\n",
      "Epoch: 49, Loss: 173.66758, Residuals: -2.05612, Convergence: 0.009493\n",
      "Epoch: 50, Loss: 172.07736, Residuals: -2.02456, Convergence: 0.009241\n",
      "Epoch: 51, Loss: 170.54652, Residuals: -1.99344, Convergence: 0.008976\n",
      "Epoch: 52, Loss: 169.07556, Residuals: -1.96283, Convergence: 0.008700\n",
      "Epoch: 53, Loss: 167.66453, Residuals: -1.93276, Convergence: 0.008416\n",
      "Epoch: 54, Loss: 166.31305, Residuals: -1.90329, Convergence: 0.008126\n",
      "Epoch: 55, Loss: 165.02039, Residuals: -1.87446, Convergence: 0.007833\n",
      "Epoch: 56, Loss: 163.78547, Residuals: -1.84631, Convergence: 0.007540\n",
      "Epoch: 57, Loss: 162.60699, Residuals: -1.81886, Convergence: 0.007247\n",
      "Epoch: 58, Loss: 161.48337, Residuals: -1.79215, Convergence: 0.006958\n",
      "Epoch: 59, Loss: 160.41279, Residuals: -1.76619, Convergence: 0.006674\n",
      "Epoch: 60, Loss: 159.39323, Residuals: -1.74100, Convergence: 0.006397\n",
      "Epoch: 61, Loss: 158.42245, Residuals: -1.71658, Convergence: 0.006128\n",
      "Epoch: 62, Loss: 157.49805, Residuals: -1.69293, Convergence: 0.005869\n",
      "Epoch: 63, Loss: 156.61755, Residuals: -1.67004, Convergence: 0.005622\n",
      "Epoch: 64, Loss: 155.77842, Residuals: -1.64789, Convergence: 0.005387\n",
      "Epoch: 65, Loss: 154.97824, Residuals: -1.62647, Convergence: 0.005163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66, Loss: 154.21479, Residuals: -1.60576, Convergence: 0.004951\n",
      "Epoch: 67, Loss: 153.48608, Residuals: -1.58572, Convergence: 0.004748\n",
      "Epoch: 68, Loss: 152.79039, Residuals: -1.56634, Convergence: 0.004553\n",
      "Epoch: 69, Loss: 152.12625, Residuals: -1.54760, Convergence: 0.004366\n",
      "Epoch: 70, Loss: 151.49243, Residuals: -1.52950, Convergence: 0.004184\n",
      "Epoch: 71, Loss: 150.88784, Residuals: -1.51202, Convergence: 0.004007\n",
      "Epoch: 72, Loss: 150.31151, Residuals: -1.49514, Convergence: 0.003834\n",
      "Epoch: 73, Loss: 149.76253, Residuals: -1.47887, Convergence: 0.003666\n",
      "Epoch: 74, Loss: 149.24003, Residuals: -1.46319, Convergence: 0.003501\n",
      "Epoch: 75, Loss: 148.74314, Residuals: -1.44809, Convergence: 0.003341\n",
      "Epoch: 76, Loss: 148.27099, Residuals: -1.43356, Convergence: 0.003184\n",
      "Epoch: 77, Loss: 147.82269, Residuals: -1.41960, Convergence: 0.003033\n",
      "Epoch: 78, Loss: 147.39736, Residuals: -1.40619, Convergence: 0.002886\n",
      "Epoch: 79, Loss: 146.99408, Residuals: -1.39332, Convergence: 0.002744\n",
      "Epoch: 80, Loss: 146.61195, Residuals: -1.38098, Convergence: 0.002606\n",
      "Epoch: 81, Loss: 146.25009, Residuals: -1.36916, Convergence: 0.002474\n",
      "Epoch: 82, Loss: 145.90760, Residuals: -1.35784, Convergence: 0.002347\n",
      "Epoch: 83, Loss: 145.58363, Residuals: -1.34702, Convergence: 0.002225\n",
      "Epoch: 84, Loss: 145.27732, Residuals: -1.33667, Convergence: 0.002108\n",
      "Epoch: 85, Loss: 144.98788, Residuals: -1.32679, Convergence: 0.001996\n",
      "Epoch: 86, Loss: 144.71449, Residuals: -1.31737, Convergence: 0.001889\n",
      "Epoch: 87, Loss: 144.45638, Residuals: -1.30840, Convergence: 0.001787\n",
      "Epoch: 88, Loss: 144.21276, Residuals: -1.29986, Convergence: 0.001689\n",
      "Epoch: 89, Loss: 143.98283, Residuals: -1.29174, Convergence: 0.001597\n",
      "Epoch: 90, Loss: 143.76576, Residuals: -1.28405, Convergence: 0.001510\n",
      "Epoch: 91, Loss: 143.56066, Residuals: -1.27676, Convergence: 0.001429\n",
      "Epoch: 92, Loss: 143.36654, Residuals: -1.26987, Convergence: 0.001354\n",
      "Epoch: 93, Loss: 143.18235, Residuals: -1.26336, Convergence: 0.001286\n",
      "Epoch: 94, Loss: 143.00694, Residuals: -1.25721, Convergence: 0.001227\n",
      "Epoch: 95, Loss: 142.83907, Residuals: -1.25140, Convergence: 0.001175\n",
      "Epoch: 96, Loss: 142.67748, Residuals: -1.24591, Convergence: 0.001133\n",
      "Epoch: 97, Loss: 142.52093, Residuals: -1.24071, Convergence: 0.001098\n",
      "Epoch: 98, Loss: 142.36829, Residuals: -1.23577, Convergence: 0.001072\n",
      "Epoch: 99, Loss: 142.21857, Residuals: -1.23105, Convergence: 0.001053\n",
      "Epoch: 100, Loss: 142.07101, Residuals: -1.22652, Convergence: 0.001039\n",
      "Epoch: 101, Loss: 141.92506, Residuals: -1.22217, Convergence: 0.001028\n",
      "Epoch: 102, Loss: 141.78039, Residuals: -1.21796, Convergence: 0.001020\n",
      "Epoch: 103, Loss: 141.63685, Residuals: -1.21388, Convergence: 0.001013\n",
      "Epoch: 104, Loss: 141.49446, Residuals: -1.20992, Convergence: 0.001006\n",
      "Epoch: 105, Loss: 141.35333, Residuals: -1.20606, Convergence: 0.000998\n",
      "Evidence -182.106\n",
      "\n",
      "Epoch: 105, Evidence: -182.10635, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.23e-01\n",
      "Epoch: 105, Loss: 1383.74712, Residuals: -1.20606, Convergence:   inf\n",
      "Epoch: 106, Loss: 1324.80618, Residuals: -1.23209, Convergence: 0.044490\n",
      "Epoch: 107, Loss: 1279.45964, Residuals: -1.25446, Convergence: 0.035442\n",
      "Epoch: 108, Loss: 1244.79643, Residuals: -1.27211, Convergence: 0.027846\n",
      "Epoch: 109, Loss: 1217.52019, Residuals: -1.28553, Convergence: 0.022403\n",
      "Epoch: 110, Loss: 1195.33533, Residuals: -1.29581, Convergence: 0.018560\n",
      "Epoch: 111, Loss: 1176.90630, Residuals: -1.30363, Convergence: 0.015659\n",
      "Epoch: 112, Loss: 1161.37280, Residuals: -1.30945, Convergence: 0.013375\n",
      "Epoch: 113, Loss: 1148.12353, Residuals: -1.31363, Convergence: 0.011540\n",
      "Epoch: 114, Loss: 1136.69969, Residuals: -1.31643, Convergence: 0.010050\n",
      "Epoch: 115, Loss: 1126.74267, Residuals: -1.31804, Convergence: 0.008837\n",
      "Epoch: 116, Loss: 1117.96692, Residuals: -1.31861, Convergence: 0.007850\n",
      "Epoch: 117, Loss: 1110.13694, Residuals: -1.31827, Convergence: 0.007053\n",
      "Epoch: 118, Loss: 1103.05715, Residuals: -1.31710, Convergence: 0.006418\n",
      "Epoch: 119, Loss: 1096.55823, Residuals: -1.31516, Convergence: 0.005927\n",
      "Epoch: 120, Loss: 1090.49160, Residuals: -1.31251, Convergence: 0.005563\n",
      "Epoch: 121, Loss: 1084.72300, Residuals: -1.30917, Convergence: 0.005318\n",
      "Epoch: 122, Loss: 1079.13073, Residuals: -1.30515, Convergence: 0.005182\n",
      "Epoch: 123, Loss: 1073.60656, Residuals: -1.30046, Convergence: 0.005145\n",
      "Epoch: 124, Loss: 1068.06343, Residuals: -1.29511, Convergence: 0.005190\n",
      "Epoch: 125, Loss: 1062.45186, Residuals: -1.28915, Convergence: 0.005282\n",
      "Epoch: 126, Loss: 1056.77707, Residuals: -1.28265, Convergence: 0.005370\n",
      "Epoch: 127, Loss: 1051.10606, Residuals: -1.27570, Convergence: 0.005395\n",
      "Epoch: 128, Loss: 1045.54744, Residuals: -1.26841, Convergence: 0.005316\n",
      "Epoch: 129, Loss: 1040.21028, Residuals: -1.26087, Convergence: 0.005131\n",
      "Epoch: 130, Loss: 1035.16922, Residuals: -1.25316, Convergence: 0.004870\n",
      "Epoch: 131, Loss: 1030.45436, Residuals: -1.24536, Convergence: 0.004576\n",
      "Epoch: 132, Loss: 1026.06180, Residuals: -1.23753, Convergence: 0.004281\n",
      "Epoch: 133, Loss: 1021.96837, Residuals: -1.22973, Convergence: 0.004005\n",
      "Epoch: 134, Loss: 1018.14370, Residuals: -1.22199, Convergence: 0.003757\n",
      "Epoch: 135, Loss: 1014.55722, Residuals: -1.21437, Convergence: 0.003535\n",
      "Epoch: 136, Loss: 1011.18141, Residuals: -1.20690, Convergence: 0.003338\n",
      "Epoch: 137, Loss: 1007.99260, Residuals: -1.19959, Convergence: 0.003164\n",
      "Epoch: 138, Loss: 1004.97212, Residuals: -1.19248, Convergence: 0.003006\n",
      "Epoch: 139, Loss: 1002.10412, Residuals: -1.18557, Convergence: 0.002862\n",
      "Epoch: 140, Loss: 999.37607, Residuals: -1.17888, Convergence: 0.002730\n",
      "Epoch: 141, Loss: 996.77819, Residuals: -1.17244, Convergence: 0.002606\n",
      "Epoch: 142, Loss: 994.30253, Residuals: -1.16623, Convergence: 0.002490\n",
      "Epoch: 143, Loss: 991.94117, Residuals: -1.16027, Convergence: 0.002381\n",
      "Epoch: 144, Loss: 989.68861, Residuals: -1.15456, Convergence: 0.002276\n",
      "Epoch: 145, Loss: 987.53871, Residuals: -1.14909, Convergence: 0.002177\n",
      "Epoch: 146, Loss: 985.48565, Residuals: -1.14387, Convergence: 0.002083\n",
      "Epoch: 147, Loss: 983.52451, Residuals: -1.13889, Convergence: 0.001994\n",
      "Epoch: 148, Loss: 981.64948, Residuals: -1.13415, Convergence: 0.001910\n",
      "Epoch: 149, Loss: 979.85643, Residuals: -1.12962, Convergence: 0.001830\n",
      "Epoch: 150, Loss: 978.13982, Residuals: -1.12531, Convergence: 0.001755\n",
      "Epoch: 151, Loss: 976.49483, Residuals: -1.12121, Convergence: 0.001685\n",
      "Epoch: 152, Loss: 974.91724, Residuals: -1.11730, Convergence: 0.001618\n",
      "Epoch: 153, Loss: 973.40225, Residuals: -1.11357, Convergence: 0.001556\n",
      "Epoch: 154, Loss: 971.94484, Residuals: -1.11001, Convergence: 0.001499\n",
      "Epoch: 155, Loss: 970.54138, Residuals: -1.10661, Convergence: 0.001446\n",
      "Epoch: 156, Loss: 969.18635, Residuals: -1.10336, Convergence: 0.001398\n",
      "Epoch: 157, Loss: 967.87538, Residuals: -1.10025, Convergence: 0.001354\n",
      "Epoch: 158, Loss: 966.60301, Residuals: -1.09726, Convergence: 0.001316\n",
      "Epoch: 159, Loss: 965.36390, Residuals: -1.09438, Convergence: 0.001284\n",
      "Epoch: 160, Loss: 964.15257, Residuals: -1.09161, Convergence: 0.001256\n",
      "Epoch: 161, Loss: 962.96274, Residuals: -1.08892, Convergence: 0.001236\n",
      "Epoch: 162, Loss: 961.78922, Residuals: -1.08630, Convergence: 0.001220\n",
      "Epoch: 163, Loss: 960.62525, Residuals: -1.08374, Convergence: 0.001212\n",
      "Epoch: 164, Loss: 959.46597, Residuals: -1.08122, Convergence: 0.001208\n",
      "Epoch: 165, Loss: 958.30667, Residuals: -1.07874, Convergence: 0.001210\n",
      "Epoch: 166, Loss: 957.14510, Residuals: -1.07628, Convergence: 0.001214\n",
      "Epoch: 167, Loss: 955.98067, Residuals: -1.07383, Convergence: 0.001218\n",
      "Epoch: 168, Loss: 954.81564, Residuals: -1.07141, Convergence: 0.001220\n",
      "Epoch: 169, Loss: 953.65547, Residuals: -1.06901, Convergence: 0.001217\n",
      "Epoch: 170, Loss: 952.50601, Residuals: -1.06664, Convergence: 0.001207\n",
      "Epoch: 171, Loss: 951.37490, Residuals: -1.06431, Convergence: 0.001189\n",
      "Epoch: 172, Loss: 950.27001, Residuals: -1.06203, Convergence: 0.001163\n",
      "Epoch: 173, Loss: 949.19712, Residuals: -1.05982, Convergence: 0.001130\n",
      "Epoch: 174, Loss: 948.16169, Residuals: -1.05768, Convergence: 0.001092\n",
      "Epoch: 175, Loss: 947.16693, Residuals: -1.05562, Convergence: 0.001050\n",
      "Epoch: 176, Loss: 946.21505, Residuals: -1.05364, Convergence: 0.001006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 177, Loss: 945.30708, Residuals: -1.05173, Convergence: 0.000961\n",
      "Evidence 11336.177\n",
      "\n",
      "Epoch: 177, Evidence: 11336.17676, Convergence: 1.016064\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.73e-01\n",
      "Epoch: 177, Loss: 2356.06276, Residuals: -1.05173, Convergence:   inf\n",
      "Epoch: 178, Loss: 2319.59470, Residuals: -1.05909, Convergence: 0.015722\n",
      "Epoch: 179, Loss: 2294.52609, Residuals: -1.05746, Convergence: 0.010925\n",
      "Epoch: 180, Loss: 2273.65840, Residuals: -1.05533, Convergence: 0.009178\n",
      "Epoch: 181, Loss: 2256.14169, Residuals: -1.05302, Convergence: 0.007764\n",
      "Epoch: 182, Loss: 2241.30849, Residuals: -1.05063, Convergence: 0.006618\n",
      "Epoch: 183, Loss: 2228.62377, Residuals: -1.04818, Convergence: 0.005692\n",
      "Epoch: 184, Loss: 2217.64938, Residuals: -1.04569, Convergence: 0.004949\n",
      "Epoch: 185, Loss: 2208.02582, Residuals: -1.04315, Convergence: 0.004358\n",
      "Epoch: 186, Loss: 2199.45851, Residuals: -1.04054, Convergence: 0.003895\n",
      "Epoch: 187, Loss: 2191.71391, Residuals: -1.03783, Convergence: 0.003534\n",
      "Epoch: 188, Loss: 2184.61695, Residuals: -1.03502, Convergence: 0.003249\n",
      "Epoch: 189, Loss: 2178.04940, Residuals: -1.03210, Convergence: 0.003015\n",
      "Epoch: 190, Loss: 2171.93840, Residuals: -1.02909, Convergence: 0.002814\n",
      "Epoch: 191, Loss: 2166.24146, Residuals: -1.02603, Convergence: 0.002630\n",
      "Epoch: 192, Loss: 2160.93168, Residuals: -1.02297, Convergence: 0.002457\n",
      "Epoch: 193, Loss: 2155.98583, Residuals: -1.01994, Convergence: 0.002294\n",
      "Epoch: 194, Loss: 2151.38468, Residuals: -1.01698, Convergence: 0.002139\n",
      "Epoch: 195, Loss: 2147.10710, Residuals: -1.01412, Convergence: 0.001992\n",
      "Epoch: 196, Loss: 2143.13396, Residuals: -1.01138, Convergence: 0.001854\n",
      "Epoch: 197, Loss: 2139.44588, Residuals: -1.00877, Convergence: 0.001724\n",
      "Epoch: 198, Loss: 2136.02405, Residuals: -1.00629, Convergence: 0.001602\n",
      "Epoch: 199, Loss: 2132.84983, Residuals: -1.00396, Convergence: 0.001488\n",
      "Epoch: 200, Loss: 2129.90558, Residuals: -1.00176, Convergence: 0.001382\n",
      "Epoch: 201, Loss: 2127.17298, Residuals: -0.99970, Convergence: 0.001285\n",
      "Epoch: 202, Loss: 2124.63525, Residuals: -0.99776, Convergence: 0.001194\n",
      "Epoch: 203, Loss: 2122.27530, Residuals: -0.99595, Convergence: 0.001112\n",
      "Epoch: 204, Loss: 2120.07784, Residuals: -0.99424, Convergence: 0.001036\n",
      "Epoch: 205, Loss: 2118.02915, Residuals: -0.99264, Convergence: 0.000967\n",
      "Evidence 14378.717\n",
      "\n",
      "Epoch: 205, Evidence: 14378.71680, Convergence: 0.211600\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.33e-01\n",
      "Epoch: 205, Loss: 2475.31641, Residuals: -0.99264, Convergence:   inf\n",
      "Epoch: 206, Loss: 2462.45849, Residuals: -0.98930, Convergence: 0.005222\n",
      "Epoch: 207, Loss: 2451.95684, Residuals: -0.98585, Convergence: 0.004283\n",
      "Epoch: 208, Loss: 2442.96103, Residuals: -0.98265, Convergence: 0.003682\n",
      "Epoch: 209, Loss: 2435.20882, Residuals: -0.97974, Convergence: 0.003183\n",
      "Epoch: 210, Loss: 2428.48780, Residuals: -0.97712, Convergence: 0.002768\n",
      "Epoch: 211, Loss: 2422.62489, Residuals: -0.97479, Convergence: 0.002420\n",
      "Epoch: 212, Loss: 2417.47785, Residuals: -0.97272, Convergence: 0.002129\n",
      "Epoch: 213, Loss: 2412.93078, Residuals: -0.97087, Convergence: 0.001884\n",
      "Epoch: 214, Loss: 2408.88928, Residuals: -0.96922, Convergence: 0.001678\n",
      "Epoch: 215, Loss: 2405.27383, Residuals: -0.96775, Convergence: 0.001503\n",
      "Epoch: 216, Loss: 2402.02046, Residuals: -0.96642, Convergence: 0.001354\n",
      "Epoch: 217, Loss: 2399.07547, Residuals: -0.96523, Convergence: 0.001228\n",
      "Epoch: 218, Loss: 2396.39528, Residuals: -0.96415, Convergence: 0.001118\n",
      "Epoch: 219, Loss: 2393.94314, Residuals: -0.96317, Convergence: 0.001024\n",
      "Epoch: 220, Loss: 2391.68931, Residuals: -0.96227, Convergence: 0.000942\n",
      "Evidence 14718.156\n",
      "\n",
      "Epoch: 220, Evidence: 14718.15625, Convergence: 0.023063\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.29e-01\n",
      "Epoch: 220, Loss: 2480.80705, Residuals: -0.96227, Convergence:   inf\n",
      "Epoch: 221, Loss: 2474.47766, Residuals: -0.95931, Convergence: 0.002558\n",
      "Epoch: 222, Loss: 2469.26807, Residuals: -0.95676, Convergence: 0.002110\n",
      "Epoch: 223, Loss: 2464.86714, Residuals: -0.95464, Convergence: 0.001785\n",
      "Epoch: 224, Loss: 2461.09144, Residuals: -0.95288, Convergence: 0.001534\n",
      "Epoch: 225, Loss: 2457.80590, Residuals: -0.95140, Convergence: 0.001337\n",
      "Epoch: 226, Loss: 2454.91082, Residuals: -0.95016, Convergence: 0.001179\n",
      "Epoch: 227, Loss: 2452.32907, Residuals: -0.94911, Convergence: 0.001053\n",
      "Epoch: 228, Loss: 2450.00302, Residuals: -0.94823, Convergence: 0.000949\n",
      "Evidence 14794.761\n",
      "\n",
      "Epoch: 228, Evidence: 14794.76074, Convergence: 0.005178\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.57e-01\n",
      "Epoch: 228, Loss: 2482.54365, Residuals: -0.94823, Convergence:   inf\n",
      "Epoch: 229, Loss: 2478.64880, Residuals: -0.94604, Convergence: 0.001571\n",
      "Epoch: 230, Loss: 2475.42327, Residuals: -0.94432, Convergence: 0.001303\n",
      "Epoch: 231, Loss: 2472.66571, Residuals: -0.94294, Convergence: 0.001115\n",
      "Epoch: 232, Loss: 2470.25966, Residuals: -0.94183, Convergence: 0.000974\n",
      "Evidence 14822.445\n",
      "\n",
      "Epoch: 232, Evidence: 14822.44531, Convergence: 0.001868\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.07e-01\n",
      "Epoch: 232, Loss: 2483.57547, Residuals: -0.94183, Convergence:   inf\n",
      "Epoch: 233, Loss: 2480.63861, Residuals: -0.94008, Convergence: 0.001184\n",
      "Epoch: 234, Loss: 2478.18078, Residuals: -0.93873, Convergence: 0.000992\n",
      "Evidence 14834.638\n",
      "\n",
      "Epoch: 234, Evidence: 14834.63770, Convergence: 0.000822\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.72e-01\n",
      "Epoch: 234, Loss: 2484.31403, Residuals: -0.93873, Convergence:   inf\n",
      "Epoch: 235, Loss: 2479.66734, Residuals: -0.93633, Convergence: 0.001874\n",
      "Epoch: 236, Loss: 2476.12558, Residuals: -0.93460, Convergence: 0.001430\n",
      "Epoch: 237, Loss: 2473.27874, Residuals: -0.93340, Convergence: 0.001151\n",
      "Epoch: 238, Loss: 2470.88634, Residuals: -0.93267, Convergence: 0.000968\n",
      "Evidence 14852.508\n",
      "\n",
      "Epoch: 238, Evidence: 14852.50781, Convergence: 0.002024\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.42e-01\n",
      "Epoch: 238, Loss: 2484.33536, Residuals: -0.93267, Convergence:   inf\n",
      "Epoch: 239, Loss: 2481.20625, Residuals: -0.93027, Convergence: 0.001261\n",
      "Epoch: 240, Loss: 2478.75222, Residuals: -0.92894, Convergence: 0.000990\n",
      "Evidence 14863.842\n",
      "\n",
      "Epoch: 240, Evidence: 14863.84180, Convergence: 0.000763\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.20e-01\n",
      "Epoch: 240, Loss: 2484.48495, Residuals: -0.92894, Convergence:   inf\n",
      "Epoch: 241, Loss: 2479.95336, Residuals: -0.92516, Convergence: 0.001827\n",
      "Epoch: 242, Loss: 2476.76962, Residuals: -0.92563, Convergence: 0.001285\n",
      "Epoch: 243, Loss: 2474.09996, Residuals: -0.92608, Convergence: 0.001079\n",
      "Epoch: 244, Loss: 2471.79047, Residuals: -0.92839, Convergence: 0.000934\n",
      "Evidence 14879.721\n",
      "\n",
      "Epoch: 244, Evidence: 14879.72070, Convergence: 0.001829\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.07e-01\n",
      "Epoch: 244, Loss: 2483.64404, Residuals: -0.92839, Convergence:   inf\n",
      "Epoch: 245, Loss: 2481.81411, Residuals: -0.92621, Convergence: 0.000737\n",
      "Evidence 14886.826\n",
      "\n",
      "Epoch: 245, Evidence: 14886.82617, Convergence: 0.000477\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.74e-02\n",
      "Epoch: 245, Loss: 2484.60478, Residuals: -0.92621, Convergence:   inf\n",
      "Epoch: 246, Loss: 2528.62269, Residuals: -0.96594, Convergence: -0.017408\n",
      "Epoch: 246, Loss: 2482.35009, Residuals: -0.92310, Convergence: 0.000908\n",
      "Evidence 14891.059\n",
      "\n",
      "Epoch: 246, Evidence: 14891.05859, Convergence: 0.000761\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.09e-02\n",
      "Epoch: 246, Loss: 2483.96600, Residuals: -0.92310, Convergence:   inf\n",
      "Epoch: 247, Loss: 2487.25539, Residuals: -0.92378, Convergence: -0.001322\n",
      "Epoch: 247, Loss: 2483.53214, Residuals: -0.92128, Convergence: 0.000175\n",
      "Evidence 14893.144\n",
      "\n",
      "Epoch: 247, Evidence: 14893.14355, Convergence: 0.000901\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 380.92693, Residuals: -4.50752, Convergence:   inf\n",
      "Epoch: 1, Loss: 355.37590, Residuals: -4.38766, Convergence: 0.071899\n",
      "Epoch: 2, Loss: 334.43979, Residuals: -4.22415, Convergence: 0.062601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss: 318.46684, Residuals: -4.05996, Convergence: 0.050156\n",
      "Epoch: 4, Loss: 306.29199, Residuals: -3.91524, Convergence: 0.039749\n",
      "Epoch: 5, Loss: 296.64477, Residuals: -3.78714, Convergence: 0.032521\n",
      "Epoch: 6, Loss: 288.82054, Residuals: -3.67532, Convergence: 0.027090\n",
      "Epoch: 7, Loss: 282.32860, Residuals: -3.57936, Convergence: 0.022994\n",
      "Epoch: 8, Loss: 276.80487, Residuals: -3.49728, Convergence: 0.019955\n",
      "Epoch: 9, Loss: 271.99261, Residuals: -3.42683, Convergence: 0.017693\n",
      "Epoch: 10, Loss: 267.70985, Residuals: -3.36595, Convergence: 0.015998\n",
      "Epoch: 11, Loss: 263.82548, Residuals: -3.31292, Convergence: 0.014723\n",
      "Epoch: 12, Loss: 260.24477, Residuals: -3.26623, Convergence: 0.013759\n",
      "Epoch: 13, Loss: 256.90008, Residuals: -3.22460, Convergence: 0.013019\n",
      "Epoch: 14, Loss: 253.74437, Residuals: -3.18686, Convergence: 0.012437\n",
      "Epoch: 15, Loss: 250.74718, Residuals: -3.15205, Convergence: 0.011953\n",
      "Epoch: 16, Loss: 247.89175, Residuals: -3.11944, Convergence: 0.011519\n",
      "Epoch: 17, Loss: 245.16732, Residuals: -3.08857, Convergence: 0.011113\n",
      "Epoch: 18, Loss: 242.55643, Residuals: -3.05905, Convergence: 0.010764\n",
      "Epoch: 19, Loss: 240.03057, Residuals: -3.03037, Convergence: 0.010523\n",
      "Epoch: 20, Loss: 237.55658, Residuals: -3.00198, Convergence: 0.010414\n",
      "Epoch: 21, Loss: 235.10607, Residuals: -2.97342, Convergence: 0.010423\n",
      "Epoch: 22, Loss: 232.66008, Residuals: -2.94439, Convergence: 0.010513\n",
      "Epoch: 23, Loss: 230.20081, Residuals: -2.91471, Convergence: 0.010683\n",
      "Epoch: 24, Loss: 227.69459, Residuals: -2.88403, Convergence: 0.011007\n",
      "Epoch: 25, Loss: 225.08771, Residuals: -2.85171, Convergence: 0.011582\n",
      "Epoch: 26, Loss: 222.34361, Residuals: -2.81729, Convergence: 0.012342\n",
      "Epoch: 27, Loss: 219.52862, Residuals: -2.78140, Convergence: 0.012823\n",
      "Epoch: 28, Loss: 216.77155, Residuals: -2.74547, Convergence: 0.012719\n",
      "Epoch: 29, Loss: 214.12256, Residuals: -2.71020, Convergence: 0.012371\n",
      "Epoch: 30, Loss: 211.57249, Residuals: -2.67561, Convergence: 0.012053\n",
      "Epoch: 31, Loss: 209.10128, Residuals: -2.64155, Convergence: 0.011818\n",
      "Epoch: 32, Loss: 206.69196, Residuals: -2.60788, Convergence: 0.011657\n",
      "Epoch: 33, Loss: 204.33258, Residuals: -2.57448, Convergence: 0.011547\n",
      "Epoch: 34, Loss: 202.01559, Residuals: -2.54126, Convergence: 0.011469\n",
      "Epoch: 35, Loss: 199.73698, Residuals: -2.50817, Convergence: 0.011408\n",
      "Epoch: 36, Loss: 197.49532, Residuals: -2.47517, Convergence: 0.011350\n",
      "Epoch: 37, Loss: 195.29095, Residuals: -2.44224, Convergence: 0.011288\n",
      "Epoch: 38, Loss: 193.12528, Residuals: -2.40937, Convergence: 0.011214\n",
      "Epoch: 39, Loss: 191.00024, Residuals: -2.37658, Convergence: 0.011126\n",
      "Epoch: 40, Loss: 188.91778, Residuals: -2.34387, Convergence: 0.011023\n",
      "Epoch: 41, Loss: 186.87958, Residuals: -2.31126, Convergence: 0.010907\n",
      "Epoch: 42, Loss: 184.88689, Residuals: -2.27877, Convergence: 0.010778\n",
      "Epoch: 43, Loss: 182.94048, Residuals: -2.24643, Convergence: 0.010640\n",
      "Epoch: 44, Loss: 181.04073, Residuals: -2.21424, Convergence: 0.010494\n",
      "Epoch: 45, Loss: 179.18778, Residuals: -2.18222, Convergence: 0.010341\n",
      "Epoch: 46, Loss: 177.38178, Residuals: -2.15038, Convergence: 0.010181\n",
      "Epoch: 47, Loss: 175.62305, Residuals: -2.11872, Convergence: 0.010014\n",
      "Epoch: 48, Loss: 173.91221, Residuals: -2.08727, Convergence: 0.009837\n",
      "Epoch: 49, Loss: 172.25020, Residuals: -2.05605, Convergence: 0.009649\n",
      "Epoch: 50, Loss: 170.63818, Residuals: -2.02506, Convergence: 0.009447\n",
      "Epoch: 51, Loss: 169.07743, Residuals: -1.99437, Convergence: 0.009231\n",
      "Epoch: 52, Loss: 167.56916, Residuals: -1.96399, Convergence: 0.009001\n",
      "Epoch: 53, Loss: 166.11446, Residuals: -1.93398, Convergence: 0.008757\n",
      "Epoch: 54, Loss: 164.71412, Residuals: -1.90439, Convergence: 0.008502\n",
      "Epoch: 55, Loss: 163.36868, Residuals: -1.87528, Convergence: 0.008236\n",
      "Epoch: 56, Loss: 162.07833, Residuals: -1.84667, Convergence: 0.007961\n",
      "Epoch: 57, Loss: 160.84296, Residuals: -1.81863, Convergence: 0.007681\n",
      "Epoch: 58, Loss: 159.66218, Residuals: -1.79120, Convergence: 0.007396\n",
      "Epoch: 59, Loss: 158.53532, Residuals: -1.76440, Convergence: 0.007108\n",
      "Epoch: 60, Loss: 157.46151, Residuals: -1.73828, Convergence: 0.006820\n",
      "Epoch: 61, Loss: 156.43967, Residuals: -1.71285, Convergence: 0.006532\n",
      "Epoch: 62, Loss: 155.46854, Residuals: -1.68815, Convergence: 0.006246\n",
      "Epoch: 63, Loss: 154.54676, Residuals: -1.66419, Convergence: 0.005964\n",
      "Epoch: 64, Loss: 153.67284, Residuals: -1.64098, Convergence: 0.005687\n",
      "Epoch: 65, Loss: 152.84521, Residuals: -1.61853, Convergence: 0.005415\n",
      "Epoch: 66, Loss: 152.06224, Residuals: -1.59685, Convergence: 0.005149\n",
      "Epoch: 67, Loss: 151.32225, Residuals: -1.57594, Convergence: 0.004890\n",
      "Epoch: 68, Loss: 150.62354, Residuals: -1.55581, Convergence: 0.004639\n",
      "Epoch: 69, Loss: 149.96439, Residuals: -1.53644, Convergence: 0.004395\n",
      "Epoch: 70, Loss: 149.34311, Residuals: -1.51783, Convergence: 0.004160\n",
      "Epoch: 71, Loss: 148.75799, Residuals: -1.49998, Convergence: 0.003933\n",
      "Epoch: 72, Loss: 148.20737, Residuals: -1.48286, Convergence: 0.003715\n",
      "Epoch: 73, Loss: 147.68963, Residuals: -1.46648, Convergence: 0.003506\n",
      "Epoch: 74, Loss: 147.20319, Residuals: -1.45081, Convergence: 0.003305\n",
      "Epoch: 75, Loss: 146.74652, Residuals: -1.43583, Convergence: 0.003112\n",
      "Epoch: 76, Loss: 146.31813, Residuals: -1.42154, Convergence: 0.002928\n",
      "Epoch: 77, Loss: 145.91660, Residuals: -1.40792, Convergence: 0.002752\n",
      "Epoch: 78, Loss: 145.54054, Residuals: -1.39495, Convergence: 0.002584\n",
      "Epoch: 79, Loss: 145.18859, Residuals: -1.38261, Convergence: 0.002424\n",
      "Epoch: 80, Loss: 144.85946, Residuals: -1.37089, Convergence: 0.002272\n",
      "Epoch: 81, Loss: 144.55184, Residuals: -1.35976, Convergence: 0.002128\n",
      "Epoch: 82, Loss: 144.26444, Residuals: -1.34923, Convergence: 0.001992\n",
      "Epoch: 83, Loss: 143.99592, Residuals: -1.33926, Convergence: 0.001865\n",
      "Epoch: 84, Loss: 143.74494, Residuals: -1.32984, Convergence: 0.001746\n",
      "Epoch: 85, Loss: 143.51006, Residuals: -1.32095, Convergence: 0.001637\n",
      "Epoch: 86, Loss: 143.28976, Residuals: -1.31257, Convergence: 0.001537\n",
      "Epoch: 87, Loss: 143.08245, Residuals: -1.30468, Convergence: 0.001449\n",
      "Epoch: 88, Loss: 142.88649, Residuals: -1.29723, Convergence: 0.001371\n",
      "Epoch: 89, Loss: 142.70021, Residuals: -1.29020, Convergence: 0.001305\n",
      "Epoch: 90, Loss: 142.52197, Residuals: -1.28354, Convergence: 0.001251\n",
      "Epoch: 91, Loss: 142.35026, Residuals: -1.27722, Convergence: 0.001206\n",
      "Epoch: 92, Loss: 142.18377, Residuals: -1.27120, Convergence: 0.001171\n",
      "Epoch: 93, Loss: 142.02135, Residuals: -1.26543, Convergence: 0.001144\n",
      "Epoch: 94, Loss: 141.86218, Residuals: -1.25988, Convergence: 0.001122\n",
      "Epoch: 95, Loss: 141.70564, Residuals: -1.25453, Convergence: 0.001105\n",
      "Epoch: 96, Loss: 141.55135, Residuals: -1.24935, Convergence: 0.001090\n",
      "Epoch: 97, Loss: 141.39911, Residuals: -1.24433, Convergence: 0.001077\n",
      "Epoch: 98, Loss: 141.24885, Residuals: -1.23944, Convergence: 0.001064\n",
      "Epoch: 99, Loss: 141.10060, Residuals: -1.23469, Convergence: 0.001051\n",
      "Epoch: 100, Loss: 140.95444, Residuals: -1.23007, Convergence: 0.001037\n",
      "Epoch: 101, Loss: 140.81050, Residuals: -1.22556, Convergence: 0.001022\n",
      "Epoch: 102, Loss: 140.66891, Residuals: -1.22118, Convergence: 0.001007\n",
      "Epoch: 103, Loss: 140.52977, Residuals: -1.21691, Convergence: 0.000990\n",
      "Evidence -181.422\n",
      "\n",
      "Epoch: 103, Evidence: -181.42151, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.23e-01\n",
      "Epoch: 103, Loss: 1366.29694, Residuals: -1.21691, Convergence:   inf\n",
      "Epoch: 104, Loss: 1307.20559, Residuals: -1.24256, Convergence: 0.045204\n",
      "Epoch: 105, Loss: 1261.24887, Residuals: -1.26375, Convergence: 0.036437\n",
      "Epoch: 106, Loss: 1225.91476, Residuals: -1.28018, Convergence: 0.028823\n",
      "Epoch: 107, Loss: 1198.15472, Residuals: -1.29269, Convergence: 0.023169\n",
      "Epoch: 108, Loss: 1175.62100, Residuals: -1.30253, Convergence: 0.019168\n",
      "Epoch: 109, Loss: 1156.91882, Residuals: -1.31036, Convergence: 0.016166\n",
      "Epoch: 110, Loss: 1141.17255, Residuals: -1.31649, Convergence: 0.013798\n",
      "Epoch: 111, Loss: 1127.76501, Residuals: -1.32112, Convergence: 0.011889\n",
      "Epoch: 112, Loss: 1116.22646, Residuals: -1.32440, Convergence: 0.010337\n",
      "Epoch: 113, Loss: 1106.18774, Residuals: -1.32648, Convergence: 0.009075\n",
      "Epoch: 114, Loss: 1097.35110, Residuals: -1.32748, Convergence: 0.008053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 115, Loss: 1089.47284, Residuals: -1.32752, Convergence: 0.007231\n",
      "Epoch: 116, Loss: 1082.35292, Residuals: -1.32667, Convergence: 0.006578\n",
      "Epoch: 117, Loss: 1075.82681, Residuals: -1.32501, Convergence: 0.006066\n",
      "Epoch: 118, Loss: 1069.76151, Residuals: -1.32262, Convergence: 0.005670\n",
      "Epoch: 119, Loss: 1064.05154, Residuals: -1.31953, Convergence: 0.005366\n",
      "Epoch: 120, Loss: 1058.61458, Residuals: -1.31581, Convergence: 0.005136\n",
      "Epoch: 121, Loss: 1053.38487, Residuals: -1.31149, Convergence: 0.004965\n",
      "Epoch: 122, Loss: 1048.30648, Residuals: -1.30659, Convergence: 0.004844\n",
      "Epoch: 123, Loss: 1043.32408, Residuals: -1.30114, Convergence: 0.004776\n",
      "Epoch: 124, Loss: 1038.38056, Residuals: -1.29513, Convergence: 0.004761\n",
      "Epoch: 125, Loss: 1033.41949, Residuals: -1.28859, Convergence: 0.004801\n",
      "Epoch: 126, Loss: 1028.39754, Residuals: -1.28155, Convergence: 0.004883\n",
      "Epoch: 127, Loss: 1023.30498, Residuals: -1.27405, Convergence: 0.004977\n",
      "Epoch: 128, Loss: 1018.17885, Residuals: -1.26619, Convergence: 0.005035\n",
      "Epoch: 129, Loss: 1013.09678, Residuals: -1.25808, Convergence: 0.005016\n",
      "Epoch: 130, Loss: 1008.15042, Residuals: -1.24980, Convergence: 0.004906\n",
      "Epoch: 131, Loss: 1003.41548, Residuals: -1.24143, Convergence: 0.004719\n",
      "Epoch: 132, Loss: 998.93524, Residuals: -1.23303, Convergence: 0.004485\n",
      "Epoch: 133, Loss: 994.72382, Residuals: -1.22467, Convergence: 0.004234\n",
      "Epoch: 134, Loss: 990.77413, Residuals: -1.21639, Convergence: 0.003986\n",
      "Epoch: 135, Loss: 987.07091, Residuals: -1.20824, Convergence: 0.003752\n",
      "Epoch: 136, Loss: 983.59449, Residuals: -1.20026, Convergence: 0.003534\n",
      "Epoch: 137, Loss: 980.32671, Residuals: -1.19247, Convergence: 0.003333\n",
      "Epoch: 138, Loss: 977.25072, Residuals: -1.18490, Convergence: 0.003148\n",
      "Epoch: 139, Loss: 974.35175, Residuals: -1.17759, Convergence: 0.002975\n",
      "Epoch: 140, Loss: 971.61759, Residuals: -1.17053, Convergence: 0.002814\n",
      "Epoch: 141, Loss: 969.03653, Residuals: -1.16376, Convergence: 0.002664\n",
      "Epoch: 142, Loss: 966.59856, Residuals: -1.15726, Convergence: 0.002522\n",
      "Epoch: 143, Loss: 964.29417, Residuals: -1.15106, Convergence: 0.002390\n",
      "Epoch: 144, Loss: 962.11353, Residuals: -1.14513, Convergence: 0.002267\n",
      "Epoch: 145, Loss: 960.04813, Residuals: -1.13950, Convergence: 0.002151\n",
      "Epoch: 146, Loss: 958.08965, Residuals: -1.13413, Convergence: 0.002044\n",
      "Epoch: 147, Loss: 956.22932, Residuals: -1.12903, Convergence: 0.001945\n",
      "Epoch: 148, Loss: 954.45914, Residuals: -1.12419, Convergence: 0.001855\n",
      "Epoch: 149, Loss: 952.77135, Residuals: -1.11959, Convergence: 0.001771\n",
      "Epoch: 150, Loss: 951.15861, Residuals: -1.11522, Convergence: 0.001696\n",
      "Epoch: 151, Loss: 949.61385, Residuals: -1.11107, Convergence: 0.001627\n",
      "Epoch: 152, Loss: 948.12988, Residuals: -1.10712, Convergence: 0.001565\n",
      "Epoch: 153, Loss: 946.70069, Residuals: -1.10336, Convergence: 0.001510\n",
      "Epoch: 154, Loss: 945.32005, Residuals: -1.09978, Convergence: 0.001461\n",
      "Epoch: 155, Loss: 943.98196, Residuals: -1.09637, Convergence: 0.001417\n",
      "Epoch: 156, Loss: 942.68114, Residuals: -1.09310, Convergence: 0.001380\n",
      "Epoch: 157, Loss: 941.41216, Residuals: -1.08998, Convergence: 0.001348\n",
      "Epoch: 158, Loss: 940.17035, Residuals: -1.08698, Convergence: 0.001321\n",
      "Epoch: 159, Loss: 938.95111, Residuals: -1.08409, Convergence: 0.001299\n",
      "Epoch: 160, Loss: 937.75083, Residuals: -1.08131, Convergence: 0.001280\n",
      "Epoch: 161, Loss: 936.56550, Residuals: -1.07862, Convergence: 0.001266\n",
      "Epoch: 162, Loss: 935.39206, Residuals: -1.07601, Convergence: 0.001254\n",
      "Epoch: 163, Loss: 934.22821, Residuals: -1.07348, Convergence: 0.001246\n",
      "Epoch: 164, Loss: 933.07195, Residuals: -1.07101, Convergence: 0.001239\n",
      "Epoch: 165, Loss: 931.92200, Residuals: -1.06859, Convergence: 0.001234\n",
      "Epoch: 166, Loss: 930.77784, Residuals: -1.06623, Convergence: 0.001229\n",
      "Epoch: 167, Loss: 929.64005, Residuals: -1.06391, Convergence: 0.001224\n",
      "Epoch: 168, Loss: 928.50964, Residuals: -1.06163, Convergence: 0.001217\n",
      "Epoch: 169, Loss: 927.38953, Residuals: -1.05940, Convergence: 0.001208\n",
      "Epoch: 170, Loss: 926.28330, Residuals: -1.05720, Convergence: 0.001194\n",
      "Epoch: 171, Loss: 925.19632, Residuals: -1.05506, Convergence: 0.001175\n",
      "Epoch: 172, Loss: 924.13400, Residuals: -1.05296, Convergence: 0.001150\n",
      "Epoch: 173, Loss: 923.10246, Residuals: -1.05093, Convergence: 0.001117\n",
      "Epoch: 174, Loss: 922.10675, Residuals: -1.04897, Convergence: 0.001080\n",
      "Epoch: 175, Loss: 921.15097, Residuals: -1.04709, Convergence: 0.001038\n",
      "Epoch: 176, Loss: 920.23827, Residuals: -1.04528, Convergence: 0.000992\n",
      "Evidence 11268.717\n",
      "\n",
      "Epoch: 176, Evidence: 11268.71680, Convergence: 1.016100\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.71e-01\n",
      "Epoch: 176, Loss: 2349.01984, Residuals: -1.04528, Convergence:   inf\n",
      "Epoch: 177, Loss: 2309.54220, Residuals: -1.05278, Convergence: 0.017093\n",
      "Epoch: 178, Loss: 2283.54683, Residuals: -1.05049, Convergence: 0.011384\n",
      "Epoch: 179, Loss: 2261.75961, Residuals: -1.04803, Convergence: 0.009633\n",
      "Epoch: 180, Loss: 2243.40833, Residuals: -1.04558, Convergence: 0.008180\n",
      "Epoch: 181, Loss: 2227.87087, Residuals: -1.04317, Convergence: 0.006974\n",
      "Epoch: 182, Loss: 2214.62168, Residuals: -1.04080, Convergence: 0.005983\n",
      "Epoch: 183, Loss: 2203.22173, Residuals: -1.03847, Convergence: 0.005174\n",
      "Epoch: 184, Loss: 2193.29922, Residuals: -1.03616, Convergence: 0.004524\n",
      "Epoch: 185, Loss: 2184.54805, Residuals: -1.03383, Convergence: 0.004006\n",
      "Epoch: 186, Loss: 2176.72179, Residuals: -1.03148, Convergence: 0.003595\n",
      "Epoch: 187, Loss: 2169.63562, Residuals: -1.02907, Convergence: 0.003266\n",
      "Epoch: 188, Loss: 2163.16426, Residuals: -1.02660, Convergence: 0.002992\n",
      "Epoch: 189, Loss: 2157.22751, Residuals: -1.02409, Convergence: 0.002752\n",
      "Epoch: 190, Loss: 2151.77637, Residuals: -1.02157, Convergence: 0.002533\n",
      "Epoch: 191, Loss: 2146.77052, Residuals: -1.01907, Convergence: 0.002332\n",
      "Epoch: 192, Loss: 2142.17286, Residuals: -1.01663, Convergence: 0.002146\n",
      "Epoch: 193, Loss: 2137.94398, Residuals: -1.01427, Convergence: 0.001978\n",
      "Epoch: 194, Loss: 2134.04376, Residuals: -1.01199, Convergence: 0.001828\n",
      "Epoch: 195, Loss: 2130.43313, Residuals: -1.00980, Convergence: 0.001695\n",
      "Epoch: 196, Loss: 2127.07601, Residuals: -1.00771, Convergence: 0.001578\n",
      "Epoch: 197, Loss: 2123.93983, Residuals: -1.00570, Convergence: 0.001477\n",
      "Epoch: 198, Loss: 2120.99674, Residuals: -1.00379, Convergence: 0.001388\n",
      "Epoch: 199, Loss: 2118.22343, Residuals: -1.00195, Convergence: 0.001309\n",
      "Epoch: 200, Loss: 2115.60120, Residuals: -1.00020, Convergence: 0.001239\n",
      "Epoch: 201, Loss: 2113.11532, Residuals: -0.99853, Convergence: 0.001176\n",
      "Epoch: 202, Loss: 2110.75402, Residuals: -0.99693, Convergence: 0.001119\n",
      "Epoch: 203, Loss: 2108.50959, Residuals: -0.99541, Convergence: 0.001064\n",
      "Epoch: 204, Loss: 2106.37543, Residuals: -0.99396, Convergence: 0.001013\n",
      "Epoch: 205, Loss: 2104.34565, Residuals: -0.99258, Convergence: 0.000965\n",
      "Evidence 14424.006\n",
      "\n",
      "Epoch: 205, Evidence: 14424.00586, Convergence: 0.218753\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.33e-01\n",
      "Epoch: 205, Loss: 2472.21593, Residuals: -0.99258, Convergence:   inf\n",
      "Epoch: 206, Loss: 2458.43558, Residuals: -0.98916, Convergence: 0.005605\n",
      "Epoch: 207, Loss: 2446.97994, Residuals: -0.98551, Convergence: 0.004682\n",
      "Epoch: 208, Loss: 2437.00973, Residuals: -0.98210, Convergence: 0.004091\n",
      "Epoch: 209, Loss: 2428.31246, Residuals: -0.97899, Convergence: 0.003582\n",
      "Epoch: 210, Loss: 2420.70564, Residuals: -0.97619, Convergence: 0.003142\n",
      "Epoch: 211, Loss: 2414.03150, Residuals: -0.97368, Convergence: 0.002765\n",
      "Epoch: 212, Loss: 2408.15519, Residuals: -0.97144, Convergence: 0.002440\n",
      "Epoch: 213, Loss: 2402.96206, Residuals: -0.96944, Convergence: 0.002161\n",
      "Epoch: 214, Loss: 2398.35368, Residuals: -0.96765, Convergence: 0.001921\n",
      "Epoch: 215, Loss: 2394.24605, Residuals: -0.96605, Convergence: 0.001716\n",
      "Epoch: 216, Loss: 2390.56953, Residuals: -0.96461, Convergence: 0.001538\n",
      "Epoch: 217, Loss: 2387.26301, Residuals: -0.96331, Convergence: 0.001385\n",
      "Epoch: 218, Loss: 2384.27545, Residuals: -0.96214, Convergence: 0.001253\n",
      "Epoch: 219, Loss: 2381.56429, Residuals: -0.96108, Convergence: 0.001138\n",
      "Epoch: 220, Loss: 2379.09195, Residuals: -0.96012, Convergence: 0.001039\n",
      "Epoch: 221, Loss: 2376.82889, Residuals: -0.95924, Convergence: 0.000952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence 14812.134\n",
      "\n",
      "Epoch: 221, Evidence: 14812.13379, Convergence: 0.026203\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.30e-01\n",
      "Epoch: 221, Loss: 2476.12212, Residuals: -0.95924, Convergence:   inf\n",
      "Epoch: 222, Loss: 2469.15316, Residuals: -0.95589, Convergence: 0.002822\n",
      "Epoch: 223, Loss: 2463.36360, Residuals: -0.95304, Convergence: 0.002350\n",
      "Epoch: 224, Loss: 2458.46145, Residuals: -0.95068, Convergence: 0.001994\n",
      "Epoch: 225, Loss: 2454.25912, Residuals: -0.94872, Convergence: 0.001712\n",
      "Epoch: 226, Loss: 2450.61557, Residuals: -0.94709, Convergence: 0.001487\n",
      "Epoch: 227, Loss: 2447.42264, Residuals: -0.94573, Convergence: 0.001305\n",
      "Epoch: 228, Loss: 2444.59593, Residuals: -0.94458, Convergence: 0.001156\n",
      "Epoch: 229, Loss: 2442.06871, Residuals: -0.94362, Convergence: 0.001035\n",
      "Epoch: 230, Loss: 2439.79006, Residuals: -0.94281, Convergence: 0.000934\n",
      "Evidence 14902.870\n",
      "\n",
      "Epoch: 230, Evidence: 14902.87012, Convergence: 0.006089\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.58e-01\n",
      "Epoch: 230, Loss: 2477.24188, Residuals: -0.94281, Convergence:   inf\n",
      "Epoch: 231, Loss: 2473.12541, Residuals: -0.94037, Convergence: 0.001664\n",
      "Epoch: 232, Loss: 2469.70970, Residuals: -0.93846, Convergence: 0.001383\n",
      "Epoch: 233, Loss: 2466.80512, Residuals: -0.93696, Convergence: 0.001177\n",
      "Epoch: 234, Loss: 2464.29116, Residuals: -0.93578, Convergence: 0.001020\n",
      "Epoch: 235, Loss: 2462.08283, Residuals: -0.93484, Convergence: 0.000897\n",
      "Evidence 14935.946\n",
      "\n",
      "Epoch: 235, Evidence: 14935.94629, Convergence: 0.002215\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.07e-01\n",
      "Epoch: 235, Loss: 2477.95410, Residuals: -0.93484, Convergence:   inf\n",
      "Epoch: 236, Loss: 2475.04896, Residuals: -0.93312, Convergence: 0.001174\n",
      "Epoch: 237, Loss: 2472.63481, Residuals: -0.93182, Convergence: 0.000976\n",
      "Evidence 14949.076\n",
      "\n",
      "Epoch: 237, Evidence: 14949.07617, Convergence: 0.000878\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.72e-01\n",
      "Epoch: 237, Loss: 2478.54464, Residuals: -0.93182, Convergence:   inf\n",
      "Epoch: 238, Loss: 2474.22992, Residuals: -0.93008, Convergence: 0.001744\n",
      "Epoch: 239, Loss: 2470.82692, Residuals: -0.92847, Convergence: 0.001377\n",
      "Epoch: 240, Loss: 2468.10065, Residuals: -0.92733, Convergence: 0.001105\n",
      "Epoch: 241, Loss: 2465.81724, Residuals: -0.92668, Convergence: 0.000926\n",
      "Evidence 14966.360\n",
      "\n",
      "Epoch: 241, Evidence: 14966.36035, Convergence: 0.002032\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.42e-01\n",
      "Epoch: 241, Loss: 2478.55097, Residuals: -0.92668, Convergence:   inf\n",
      "Epoch: 242, Loss: 2475.70120, Residuals: -0.92456, Convergence: 0.001151\n",
      "Epoch: 243, Loss: 2473.45276, Residuals: -0.92328, Convergence: 0.000909\n",
      "Evidence 14977.164\n",
      "\n",
      "Epoch: 243, Evidence: 14977.16406, Convergence: 0.000721\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.20e-01\n",
      "Epoch: 243, Loss: 2478.63849, Residuals: -0.92328, Convergence:   inf\n",
      "Epoch: 244, Loss: 2474.59784, Residuals: -0.91975, Convergence: 0.001633\n",
      "Epoch: 245, Loss: 2471.74043, Residuals: -0.92061, Convergence: 0.001156\n",
      "Epoch: 246, Loss: 2469.28597, Residuals: -0.92156, Convergence: 0.000994\n",
      "Evidence 14989.876\n",
      "\n",
      "Epoch: 246, Evidence: 14989.87598, Convergence: 0.001569\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 9.79e-02\n",
      "Epoch: 246, Loss: 2478.56872, Residuals: -0.92156, Convergence:   inf\n",
      "Epoch: 247, Loss: 2476.16315, Residuals: -0.91847, Convergence: 0.000971\n",
      "Evidence 14996.481\n",
      "\n",
      "Epoch: 247, Evidence: 14996.48145, Convergence: 0.000440\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.81e-02\n",
      "Epoch: 247, Loss: 2478.18186, Residuals: -0.91847, Convergence:   inf\n",
      "Epoch: 248, Loss: 2479.02971, Residuals: -0.91936, Convergence: -0.000342\n",
      "Evidence 14997.582\n",
      "\n",
      "Epoch: 248, Evidence: 14997.58203, Convergence: 0.000514\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 6.60e-02\n",
      "Epoch: 248, Loss: 2479.12530, Residuals: -0.91936, Convergence:   inf\n",
      "Epoch: 249, Loss: 22397.23081, Residuals: -0.85315, Convergence: -0.889311\n",
      "Epoch: 249, Loss: 2532.87791, Residuals: -0.96802, Convergence: -0.021222\n",
      "Epoch: 249, Loss: 2476.68196, Residuals: -0.91356, Convergence: 0.000987\n",
      "Evidence 15004.197\n",
      "\n",
      "Epoch: 249, Evidence: 15004.19727, Convergence: 0.000954\n",
      "Total samples: 182, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 382.71735, Residuals: -4.52586, Convergence:   inf\n",
      "Epoch: 1, Loss: 356.91947, Residuals: -4.40342, Convergence: 0.072279\n",
      "Epoch: 2, Loss: 335.84964, Residuals: -4.23808, Convergence: 0.062736\n",
      "Epoch: 3, Loss: 319.72792, Residuals: -4.07222, Convergence: 0.050423\n",
      "Epoch: 4, Loss: 307.43915, Residuals: -3.92646, Convergence: 0.039971\n",
      "Epoch: 5, Loss: 297.70001, Residuals: -3.79806, Convergence: 0.032715\n",
      "Epoch: 6, Loss: 289.79372, Residuals: -3.68637, Convergence: 0.027282\n",
      "Epoch: 7, Loss: 283.22901, Residuals: -3.59072, Convergence: 0.023178\n",
      "Epoch: 8, Loss: 277.64455, Residuals: -3.50906, Convergence: 0.020114\n",
      "Epoch: 9, Loss: 272.78434, Residuals: -3.43901, Convergence: 0.017817\n",
      "Epoch: 10, Loss: 268.46587, Residuals: -3.37845, Convergence: 0.016086\n",
      "Epoch: 11, Loss: 264.55658, Residuals: -3.32559, Convergence: 0.014777\n",
      "Epoch: 12, Loss: 260.95948, Residuals: -3.27889, Convergence: 0.013784\n",
      "Epoch: 13, Loss: 257.60405, Residuals: -3.23706, Convergence: 0.013026\n",
      "Epoch: 14, Loss: 254.43998, Residuals: -3.19895, Convergence: 0.012435\n",
      "Epoch: 15, Loss: 251.43368, Residuals: -3.16359, Convergence: 0.011957\n",
      "Epoch: 16, Loss: 248.56637, Residuals: -3.13029, Convergence: 0.011535\n",
      "Epoch: 17, Loss: 245.82695, Residuals: -3.09860, Convergence: 0.011144\n",
      "Epoch: 18, Loss: 243.19864, Residuals: -3.06814, Convergence: 0.010807\n",
      "Epoch: 19, Loss: 240.65308, Residuals: -3.03845, Convergence: 0.010578\n",
      "Epoch: 20, Loss: 238.15645, Residuals: -3.00900, Convergence: 0.010483\n",
      "Epoch: 21, Loss: 235.67901, Residuals: -2.97932, Convergence: 0.010512\n",
      "Epoch: 22, Loss: 233.19916, Residuals: -2.94913, Convergence: 0.010634\n",
      "Epoch: 23, Loss: 230.69332, Residuals: -2.91821, Convergence: 0.010862\n",
      "Epoch: 24, Loss: 228.11892, Residuals: -2.88610, Convergence: 0.011285\n",
      "Epoch: 25, Loss: 225.41671, Residuals: -2.85213, Convergence: 0.011988\n",
      "Epoch: 26, Loss: 222.56853, Residuals: -2.81600, Convergence: 0.012797\n",
      "Epoch: 27, Loss: 219.67645, Residuals: -2.77875, Convergence: 0.013165\n",
      "Epoch: 28, Loss: 216.85753, Residuals: -2.74174, Convergence: 0.012999\n",
      "Epoch: 29, Loss: 214.13782, Residuals: -2.70537, Convergence: 0.012701\n",
      "Epoch: 30, Loss: 211.50327, Residuals: -2.66957, Convergence: 0.012456\n",
      "Epoch: 31, Loss: 208.93707, Residuals: -2.63418, Convergence: 0.012282\n",
      "Epoch: 32, Loss: 206.42744, Residuals: -2.59907, Convergence: 0.012157\n",
      "Epoch: 33, Loss: 203.96753, Residuals: -2.56416, Convergence: 0.012060\n",
      "Epoch: 34, Loss: 201.55401, Residuals: -2.52938, Convergence: 0.011975\n",
      "Epoch: 35, Loss: 199.18574, Residuals: -2.49469, Convergence: 0.011890\n",
      "Epoch: 36, Loss: 196.86282, Residuals: -2.46009, Convergence: 0.011800\n",
      "Epoch: 37, Loss: 194.58591, Residuals: -2.42556, Convergence: 0.011701\n",
      "Epoch: 38, Loss: 192.35589, Residuals: -2.39110, Convergence: 0.011593\n",
      "Epoch: 39, Loss: 190.17363, Residuals: -2.35672, Convergence: 0.011475\n",
      "Epoch: 40, Loss: 188.04005, Residuals: -2.32241, Convergence: 0.011346\n",
      "Epoch: 41, Loss: 185.95602, Residuals: -2.28818, Convergence: 0.011207\n",
      "Epoch: 42, Loss: 183.92247, Residuals: -2.25406, Convergence: 0.011057\n",
      "Epoch: 43, Loss: 181.94040, Residuals: -2.22005, Convergence: 0.010894\n",
      "Epoch: 44, Loss: 180.01090, Residuals: -2.18619, Convergence: 0.010719\n",
      "Epoch: 45, Loss: 178.13520, Residuals: -2.15250, Convergence: 0.010530\n",
      "Epoch: 46, Loss: 176.31466, Residuals: -2.11903, Convergence: 0.010326\n",
      "Epoch: 47, Loss: 174.55076, Residuals: -2.08583, Convergence: 0.010105\n",
      "Epoch: 48, Loss: 172.84508, Residuals: -2.05293, Convergence: 0.009868\n",
      "Epoch: 49, Loss: 171.19921, Residuals: -2.02040, Convergence: 0.009614\n",
      "Epoch: 50, Loss: 169.61452, Residuals: -1.98829, Convergence: 0.009343\n",
      "Epoch: 51, Loss: 168.09200, Residuals: -1.95666, Convergence: 0.009058\n",
      "Epoch: 52, Loss: 166.63219, Residuals: -1.92557, Convergence: 0.008761\n",
      "Epoch: 53, Loss: 165.23498, Residuals: -1.89504, Convergence: 0.008456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54, Loss: 163.89978, Residuals: -1.86514, Convergence: 0.008146\n",
      "Epoch: 55, Loss: 162.62548, Residuals: -1.83589, Convergence: 0.007836\n",
      "Epoch: 56, Loss: 161.41061, Residuals: -1.80732, Convergence: 0.007527\n",
      "Epoch: 57, Loss: 160.25344, Residuals: -1.77946, Convergence: 0.007221\n",
      "Epoch: 58, Loss: 159.15206, Residuals: -1.75232, Convergence: 0.006920\n",
      "Epoch: 59, Loss: 158.10448, Residuals: -1.72592, Convergence: 0.006626\n",
      "Epoch: 60, Loss: 157.10866, Residuals: -1.70027, Convergence: 0.006338\n",
      "Epoch: 61, Loss: 156.16262, Residuals: -1.67539, Convergence: 0.006058\n",
      "Epoch: 62, Loss: 155.26439, Residuals: -1.65127, Convergence: 0.005785\n",
      "Epoch: 63, Loss: 154.41208, Residuals: -1.62791, Convergence: 0.005520\n",
      "Epoch: 64, Loss: 153.60387, Residuals: -1.60533, Convergence: 0.005262\n",
      "Epoch: 65, Loss: 152.83799, Residuals: -1.58353, Convergence: 0.005011\n",
      "Epoch: 66, Loss: 152.11273, Residuals: -1.56250, Convergence: 0.004768\n",
      "Epoch: 67, Loss: 151.42640, Residuals: -1.54224, Convergence: 0.004532\n",
      "Epoch: 68, Loss: 150.77725, Residuals: -1.52276, Convergence: 0.004305\n",
      "Epoch: 69, Loss: 150.16354, Residuals: -1.50404, Convergence: 0.004087\n",
      "Epoch: 70, Loss: 149.58338, Residuals: -1.48608, Convergence: 0.003879\n",
      "Epoch: 71, Loss: 149.03478, Residuals: -1.46886, Convergence: 0.003681\n",
      "Epoch: 72, Loss: 148.51566, Residuals: -1.45236, Convergence: 0.003495\n",
      "Epoch: 73, Loss: 148.02386, Residuals: -1.43657, Convergence: 0.003322\n",
      "Epoch: 74, Loss: 147.55717, Residuals: -1.42146, Convergence: 0.003163\n",
      "Epoch: 75, Loss: 147.11347, Residuals: -1.40699, Convergence: 0.003016\n",
      "Epoch: 76, Loss: 146.69080, Residuals: -1.39313, Convergence: 0.002881\n",
      "Epoch: 77, Loss: 146.28739, Residuals: -1.37985, Convergence: 0.002758\n",
      "Epoch: 78, Loss: 145.90171, Residuals: -1.36713, Convergence: 0.002643\n",
      "Epoch: 79, Loss: 145.53250, Residuals: -1.35493, Convergence: 0.002537\n",
      "Epoch: 80, Loss: 145.17870, Residuals: -1.34323, Convergence: 0.002437\n",
      "Epoch: 81, Loss: 144.83944, Residuals: -1.33201, Convergence: 0.002342\n",
      "Epoch: 82, Loss: 144.51398, Residuals: -1.32125, Convergence: 0.002252\n",
      "Epoch: 83, Loss: 144.20168, Residuals: -1.31094, Convergence: 0.002166\n",
      "Epoch: 84, Loss: 143.90198, Residuals: -1.30105, Convergence: 0.002083\n",
      "Epoch: 85, Loss: 143.61437, Residuals: -1.29156, Convergence: 0.002003\n",
      "Epoch: 86, Loss: 143.33838, Residuals: -1.28248, Convergence: 0.001925\n",
      "Epoch: 87, Loss: 143.07354, Residuals: -1.27377, Convergence: 0.001851\n",
      "Epoch: 88, Loss: 142.81944, Residuals: -1.26543, Convergence: 0.001779\n",
      "Epoch: 89, Loss: 142.57565, Residuals: -1.25744, Convergence: 0.001710\n",
      "Epoch: 90, Loss: 142.34180, Residuals: -1.24978, Convergence: 0.001643\n",
      "Epoch: 91, Loss: 142.11752, Residuals: -1.24245, Convergence: 0.001578\n",
      "Epoch: 92, Loss: 141.90246, Residuals: -1.23543, Convergence: 0.001516\n",
      "Epoch: 93, Loss: 141.69629, Residuals: -1.22871, Convergence: 0.001455\n",
      "Epoch: 94, Loss: 141.49871, Residuals: -1.22227, Convergence: 0.001396\n",
      "Epoch: 95, Loss: 141.30944, Residuals: -1.21610, Convergence: 0.001339\n",
      "Epoch: 96, Loss: 141.12822, Residuals: -1.21020, Convergence: 0.001284\n",
      "Epoch: 97, Loss: 140.95480, Residuals: -1.20454, Convergence: 0.001230\n",
      "Epoch: 98, Loss: 140.78898, Residuals: -1.19912, Convergence: 0.001178\n",
      "Epoch: 99, Loss: 140.63055, Residuals: -1.19393, Convergence: 0.001127\n",
      "Epoch: 100, Loss: 140.47933, Residuals: -1.18895, Convergence: 0.001076\n",
      "Epoch: 101, Loss: 140.33517, Residuals: -1.18419, Convergence: 0.001027\n",
      "Epoch: 102, Loss: 140.19792, Residuals: -1.17962, Convergence: 0.000979\n",
      "Evidence -181.791\n",
      "\n",
      "Epoch: 102, Evidence: -181.79100, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.25e-01\n",
      "Epoch: 102, Loss: 1364.98826, Residuals: -1.17962, Convergence:   inf\n",
      "Epoch: 103, Loss: 1302.34887, Residuals: -1.20925, Convergence: 0.048097\n",
      "Epoch: 104, Loss: 1255.25768, Residuals: -1.23266, Convergence: 0.037515\n",
      "Epoch: 105, Loss: 1220.07931, Residuals: -1.24927, Convergence: 0.028833\n",
      "Epoch: 106, Loss: 1192.87189, Residuals: -1.26085, Convergence: 0.022808\n",
      "Epoch: 107, Loss: 1170.94757, Residuals: -1.26945, Convergence: 0.018724\n",
      "Epoch: 108, Loss: 1152.82241, Residuals: -1.27610, Convergence: 0.015722\n",
      "Epoch: 109, Loss: 1137.61048, Residuals: -1.28125, Convergence: 0.013372\n",
      "Epoch: 110, Loss: 1124.70786, Residuals: -1.28510, Convergence: 0.011472\n",
      "Epoch: 111, Loss: 1113.66440, Residuals: -1.28783, Convergence: 0.009916\n",
      "Epoch: 112, Loss: 1104.12976, Residuals: -1.28955, Convergence: 0.008635\n",
      "Epoch: 113, Loss: 1095.82351, Residuals: -1.29039, Convergence: 0.007580\n",
      "Epoch: 114, Loss: 1088.51864, Residuals: -1.29044, Convergence: 0.006711\n",
      "Epoch: 115, Loss: 1082.02812, Residuals: -1.28979, Convergence: 0.005998\n",
      "Epoch: 116, Loss: 1076.19666, Residuals: -1.28852, Convergence: 0.005419\n",
      "Epoch: 117, Loss: 1070.89367, Residuals: -1.28670, Convergence: 0.004952\n",
      "Epoch: 118, Loss: 1066.00735, Residuals: -1.28439, Convergence: 0.004584\n",
      "Epoch: 119, Loss: 1061.44147, Residuals: -1.28162, Convergence: 0.004302\n",
      "Epoch: 120, Loss: 1057.11085, Residuals: -1.27842, Convergence: 0.004097\n",
      "Epoch: 121, Loss: 1052.94163, Residuals: -1.27482, Convergence: 0.003960\n",
      "Epoch: 122, Loss: 1048.87014, Residuals: -1.27083, Convergence: 0.003882\n",
      "Epoch: 123, Loss: 1044.84260, Residuals: -1.26645, Convergence: 0.003855\n",
      "Epoch: 124, Loss: 1040.81511, Residuals: -1.26170, Convergence: 0.003870\n",
      "Epoch: 125, Loss: 1036.75427, Residuals: -1.25657, Convergence: 0.003917\n",
      "Epoch: 126, Loss: 1032.63591, Residuals: -1.25108, Convergence: 0.003988\n",
      "Epoch: 127, Loss: 1028.44983, Residuals: -1.24525, Convergence: 0.004070\n",
      "Epoch: 128, Loss: 1024.20572, Residuals: -1.23912, Convergence: 0.004144\n",
      "Epoch: 129, Loss: 1019.93910, Residuals: -1.23276, Convergence: 0.004183\n",
      "Epoch: 130, Loss: 1015.71194, Residuals: -1.22624, Convergence: 0.004162\n",
      "Epoch: 131, Loss: 1011.59667, Residuals: -1.21963, Convergence: 0.004068\n",
      "Epoch: 132, Loss: 1007.65631, Residuals: -1.21298, Convergence: 0.003910\n",
      "Epoch: 133, Loss: 1003.92949, Residuals: -1.20634, Convergence: 0.003712\n",
      "Epoch: 134, Loss: 1000.42896, Residuals: -1.19974, Convergence: 0.003499\n",
      "Epoch: 135, Loss: 997.14891, Residuals: -1.19322, Convergence: 0.003289\n",
      "Epoch: 136, Loss: 994.07313, Residuals: -1.18679, Convergence: 0.003094\n",
      "Epoch: 137, Loss: 991.18173, Residuals: -1.18048, Convergence: 0.002917\n",
      "Epoch: 138, Loss: 988.45604, Residuals: -1.17432, Convergence: 0.002758\n",
      "Epoch: 139, Loss: 985.87853, Residuals: -1.16831, Convergence: 0.002614\n",
      "Epoch: 140, Loss: 983.43440, Residuals: -1.16246, Convergence: 0.002485\n",
      "Epoch: 141, Loss: 981.11170, Residuals: -1.15680, Convergence: 0.002367\n",
      "Epoch: 142, Loss: 978.89997, Residuals: -1.15131, Convergence: 0.002259\n",
      "Epoch: 143, Loss: 976.79065, Residuals: -1.14602, Convergence: 0.002159\n",
      "Epoch: 144, Loss: 974.77589, Residuals: -1.14092, Convergence: 0.002067\n",
      "Epoch: 145, Loss: 972.84939, Residuals: -1.13601, Convergence: 0.001980\n",
      "Epoch: 146, Loss: 971.00491, Residuals: -1.13128, Convergence: 0.001900\n",
      "Epoch: 147, Loss: 969.23739, Residuals: -1.12675, Convergence: 0.001824\n",
      "Epoch: 148, Loss: 967.54219, Residuals: -1.12239, Convergence: 0.001752\n",
      "Epoch: 149, Loss: 965.91455, Residuals: -1.11821, Convergence: 0.001685\n",
      "Epoch: 150, Loss: 964.35092, Residuals: -1.11420, Convergence: 0.001621\n",
      "Epoch: 151, Loss: 962.84740, Residuals: -1.11036, Convergence: 0.001562\n",
      "Epoch: 152, Loss: 961.40067, Residuals: -1.10667, Convergence: 0.001505\n",
      "Epoch: 153, Loss: 960.00758, Residuals: -1.10314, Convergence: 0.001451\n",
      "Epoch: 154, Loss: 958.66534, Residuals: -1.09975, Convergence: 0.001400\n",
      "Epoch: 155, Loss: 957.37067, Residuals: -1.09650, Convergence: 0.001352\n",
      "Epoch: 156, Loss: 956.12152, Residuals: -1.09338, Convergence: 0.001306\n",
      "Epoch: 157, Loss: 954.91497, Residuals: -1.09039, Convergence: 0.001264\n",
      "Epoch: 158, Loss: 953.74814, Residuals: -1.08752, Convergence: 0.001223\n",
      "Epoch: 159, Loss: 952.61840, Residuals: -1.08475, Convergence: 0.001186\n",
      "Epoch: 160, Loss: 951.52314, Residuals: -1.08209, Convergence: 0.001151\n",
      "Epoch: 161, Loss: 950.45892, Residuals: -1.07953, Convergence: 0.001120\n",
      "Epoch: 162, Loss: 949.42288, Residuals: -1.07706, Convergence: 0.001091\n",
      "Epoch: 163, Loss: 948.41139, Residuals: -1.07467, Convergence: 0.001067\n",
      "Epoch: 164, Loss: 947.42109, Residuals: -1.07235, Convergence: 0.001045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 165, Loss: 946.44832, Residuals: -1.07009, Convergence: 0.001028\n",
      "Epoch: 166, Loss: 945.48940, Residuals: -1.06789, Convergence: 0.001014\n",
      "Epoch: 167, Loss: 944.54111, Residuals: -1.06574, Convergence: 0.001004\n",
      "Epoch: 168, Loss: 943.60046, Residuals: -1.06362, Convergence: 0.000997\n",
      "Evidence 11247.094\n",
      "\n",
      "Epoch: 168, Evidence: 11247.09375, Convergence: 1.016163\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 5.74e-01\n",
      "Epoch: 168, Loss: 2340.21508, Residuals: -1.06362, Convergence:   inf\n",
      "Epoch: 169, Loss: 2300.46190, Residuals: -1.07245, Convergence: 0.017281\n",
      "Epoch: 170, Loss: 2272.76789, Residuals: -1.07098, Convergence: 0.012185\n",
      "Epoch: 171, Loss: 2249.52711, Residuals: -1.06853, Convergence: 0.010331\n",
      "Epoch: 172, Loss: 2229.72709, Residuals: -1.06570, Convergence: 0.008880\n",
      "Epoch: 173, Loss: 2212.72885, Residuals: -1.06265, Convergence: 0.007682\n",
      "Epoch: 174, Loss: 2198.05117, Residuals: -1.05948, Convergence: 0.006678\n",
      "Epoch: 175, Loss: 2185.30449, Residuals: -1.05623, Convergence: 0.005833\n",
      "Epoch: 176, Loss: 2174.16306, Residuals: -1.05296, Convergence: 0.005124\n",
      "Epoch: 177, Loss: 2164.34800, Residuals: -1.04968, Convergence: 0.004535\n",
      "Epoch: 178, Loss: 2155.61870, Residuals: -1.04640, Convergence: 0.004050\n",
      "Epoch: 179, Loss: 2147.76736, Residuals: -1.04312, Convergence: 0.003656\n",
      "Epoch: 180, Loss: 2140.61818, Residuals: -1.03981, Convergence: 0.003340\n",
      "Epoch: 181, Loss: 2134.03130, Residuals: -1.03647, Convergence: 0.003087\n",
      "Epoch: 182, Loss: 2127.90381, Residuals: -1.03309, Convergence: 0.002880\n",
      "Epoch: 183, Loss: 2122.17170, Residuals: -1.02966, Convergence: 0.002701\n",
      "Epoch: 184, Loss: 2116.80257, Residuals: -1.02622, Convergence: 0.002536\n",
      "Epoch: 185, Loss: 2111.78281, Residuals: -1.02280, Convergence: 0.002377\n",
      "Epoch: 186, Loss: 2107.10574, Residuals: -1.01944, Convergence: 0.002220\n",
      "Epoch: 187, Loss: 2102.76139, Residuals: -1.01617, Convergence: 0.002066\n",
      "Epoch: 188, Loss: 2098.73529, Residuals: -1.01302, Convergence: 0.001918\n",
      "Epoch: 189, Loss: 2095.00855, Residuals: -1.00999, Convergence: 0.001779\n",
      "Epoch: 190, Loss: 2091.55881, Residuals: -1.00710, Convergence: 0.001649\n",
      "Epoch: 191, Loss: 2088.36241, Residuals: -1.00436, Convergence: 0.001531\n",
      "Epoch: 192, Loss: 2085.39549, Residuals: -1.00175, Convergence: 0.001423\n",
      "Epoch: 193, Loss: 2082.63425, Residuals: -0.99928, Convergence: 0.001326\n",
      "Epoch: 194, Loss: 2080.05715, Residuals: -0.99694, Convergence: 0.001239\n",
      "Epoch: 195, Loss: 2077.64328, Residuals: -0.99473, Convergence: 0.001162\n",
      "Epoch: 196, Loss: 2075.37407, Residuals: -0.99262, Convergence: 0.001093\n",
      "Epoch: 197, Loss: 2073.23257, Residuals: -0.99063, Convergence: 0.001033\n",
      "Epoch: 198, Loss: 2071.20591, Residuals: -0.98873, Convergence: 0.000978\n",
      "Evidence 14413.497\n",
      "\n",
      "Epoch: 198, Evidence: 14413.49707, Convergence: 0.219683\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 4.36e-01\n",
      "Epoch: 198, Loss: 2453.64369, Residuals: -0.98873, Convergence:   inf\n",
      "Epoch: 199, Loss: 2439.50669, Residuals: -0.98495, Convergence: 0.005795\n",
      "Epoch: 200, Loss: 2427.99349, Residuals: -0.98083, Convergence: 0.004742\n",
      "Epoch: 201, Loss: 2418.11612, Residuals: -0.97694, Convergence: 0.004085\n",
      "Epoch: 202, Loss: 2409.59059, Residuals: -0.97334, Convergence: 0.003538\n",
      "Epoch: 203, Loss: 2402.19098, Residuals: -0.97004, Convergence: 0.003080\n",
      "Epoch: 204, Loss: 2395.72770, Residuals: -0.96703, Convergence: 0.002698\n",
      "Epoch: 205, Loss: 2390.04550, Residuals: -0.96429, Convergence: 0.002377\n",
      "Epoch: 206, Loss: 2385.01679, Residuals: -0.96179, Convergence: 0.002108\n",
      "Epoch: 207, Loss: 2380.53484, Residuals: -0.95952, Convergence: 0.001883\n",
      "Epoch: 208, Loss: 2376.51383, Residuals: -0.95745, Convergence: 0.001692\n",
      "Epoch: 209, Loss: 2372.88218, Residuals: -0.95555, Convergence: 0.001530\n",
      "Epoch: 210, Loss: 2369.58319, Residuals: -0.95382, Convergence: 0.001392\n",
      "Epoch: 211, Loss: 2366.57012, Residuals: -0.95223, Convergence: 0.001273\n",
      "Epoch: 212, Loss: 2363.80389, Residuals: -0.95077, Convergence: 0.001170\n",
      "Epoch: 213, Loss: 2361.25387, Residuals: -0.94943, Convergence: 0.001080\n",
      "Epoch: 214, Loss: 2358.89398, Residuals: -0.94820, Convergence: 0.001000\n",
      "Epoch: 215, Loss: 2356.70205, Residuals: -0.94707, Convergence: 0.000930\n",
      "Evidence 14822.558\n",
      "\n",
      "Epoch: 215, Evidence: 14822.55762, Convergence: 0.027597\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 3.31e-01\n",
      "Epoch: 215, Loss: 2458.13433, Residuals: -0.94707, Convergence:   inf\n",
      "Epoch: 216, Loss: 2451.68467, Residuals: -0.94372, Convergence: 0.002631\n",
      "Epoch: 217, Loss: 2446.36356, Residuals: -0.94093, Convergence: 0.002175\n",
      "Epoch: 218, Loss: 2441.85879, Residuals: -0.93863, Convergence: 0.001845\n",
      "Epoch: 219, Loss: 2437.98365, Residuals: -0.93673, Convergence: 0.001589\n",
      "Epoch: 220, Loss: 2434.60216, Residuals: -0.93515, Convergence: 0.001389\n",
      "Epoch: 221, Loss: 2431.61203, Residuals: -0.93385, Convergence: 0.001230\n",
      "Epoch: 222, Loss: 2428.93804, Residuals: -0.93277, Convergence: 0.001101\n",
      "Epoch: 223, Loss: 2426.52261, Residuals: -0.93187, Convergence: 0.000995\n",
      "Evidence 14905.794\n",
      "\n",
      "Epoch: 223, Evidence: 14905.79395, Convergence: 0.005584\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.58e-01\n",
      "Epoch: 223, Loss: 2460.09709, Residuals: -0.93187, Convergence:   inf\n",
      "Epoch: 224, Loss: 2456.14404, Residuals: -0.92962, Convergence: 0.001609\n",
      "Epoch: 225, Loss: 2452.86500, Residuals: -0.92792, Convergence: 0.001337\n",
      "Epoch: 226, Loss: 2450.05542, Residuals: -0.92662, Convergence: 0.001147\n",
      "Epoch: 227, Loss: 2447.59781, Residuals: -0.92562, Convergence: 0.001004\n",
      "Epoch: 228, Loss: 2445.41379, Residuals: -0.92485, Convergence: 0.000893\n",
      "Evidence 14935.910\n",
      "\n",
      "Epoch: 228, Evidence: 14935.91016, Convergence: 0.002016\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 2.06e-01\n",
      "Epoch: 228, Loss: 2461.14411, Residuals: -0.92485, Convergence:   inf\n",
      "Epoch: 229, Loss: 2458.29382, Residuals: -0.92321, Convergence: 0.001159\n",
      "Epoch: 230, Loss: 2455.89982, Residuals: -0.92204, Convergence: 0.000975\n",
      "Evidence 14948.602\n",
      "\n",
      "Epoch: 230, Evidence: 14948.60156, Convergence: 0.000849\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.70e-01\n",
      "Epoch: 230, Loss: 2461.91740, Residuals: -0.92204, Convergence:   inf\n",
      "Epoch: 231, Loss: 2457.32546, Residuals: -0.91987, Convergence: 0.001869\n",
      "Epoch: 232, Loss: 2453.88366, Residuals: -0.91870, Convergence: 0.001403\n",
      "Epoch: 233, Loss: 2451.10590, Residuals: -0.91814, Convergence: 0.001133\n",
      "Epoch: 234, Loss: 2448.75951, Residuals: -0.91802, Convergence: 0.000958\n",
      "Evidence 14966.182\n",
      "\n",
      "Epoch: 234, Evidence: 14966.18164, Convergence: 0.002023\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.40e-01\n",
      "Epoch: 234, Loss: 2462.17087, Residuals: -0.91802, Convergence:   inf\n",
      "Epoch: 235, Loss: 2459.12079, Residuals: -0.91618, Convergence: 0.001240\n",
      "Epoch: 236, Loss: 2456.74613, Residuals: -0.91556, Convergence: 0.000967\n",
      "Evidence 14977.230\n",
      "\n",
      "Epoch: 236, Evidence: 14977.23047, Convergence: 0.000738\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.18e-01\n",
      "Epoch: 236, Loss: 2462.43682, Residuals: -0.91556, Convergence:   inf\n",
      "Epoch: 237, Loss: 2458.11674, Residuals: -0.91330, Convergence: 0.001757\n",
      "Epoch: 238, Loss: 2455.01989, Residuals: -0.91516, Convergence: 0.001261\n",
      "Epoch: 239, Loss: 2452.43642, Residuals: -0.91601, Convergence: 0.001053\n",
      "Epoch: 240, Loss: 2450.22499, Residuals: -0.91881, Convergence: 0.000903\n",
      "Evidence 14992.570\n",
      "\n",
      "Epoch: 240, Evidence: 14992.57031, Convergence: 0.001760\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 1.05e-01\n",
      "Epoch: 240, Loss: 2461.78051, Residuals: -0.91881, Convergence:   inf\n",
      "Epoch: 241, Loss: 2460.27758, Residuals: -0.91694, Convergence: 0.000611\n",
      "Evidence 14999.359\n",
      "\n",
      "Epoch: 241, Evidence: 14999.35938, Convergence: 0.000453\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 8.54e-02\n",
      "Epoch: 241, Loss: 2462.83087, Residuals: -0.91694, Convergence:   inf\n",
      "Epoch: 242, Loss: 2505.04550, Residuals: -0.96100, Convergence: -0.016852\n",
      "Epoch: 242, Loss: 2460.90845, Residuals: -0.91552, Convergence: 0.000781\n",
      "Evidence 15003.354\n",
      "\n",
      "Epoch: 242, Evidence: 15003.35352, Convergence: 0.000719\n",
      "Updating hyper-parameters...\n",
      "Total samples: 182, Updated regularization: 7.87e-02\n",
      "Epoch: 242, Loss: 2462.21819, Residuals: -0.91552, Convergence:   inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 243, Loss: 2468.84853, Residuals: -0.91874, Convergence: -0.002686\n",
      "Epoch: 243, Loss: 2462.67263, Residuals: -0.91406, Convergence: -0.000185\n",
      "Evidence 15004.677\n",
      "\n",
      "Epoch: 243, Evidence: 15004.67676, Convergence: 0.000807\n",
      "Total samples: 185, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 383.50796, Residuals: -4.50028, Convergence:   inf\n",
      "Epoch: 1, Loss: 357.67400, Residuals: -4.37994, Convergence: 0.072228\n",
      "Epoch: 2, Loss: 336.55078, Residuals: -4.21617, Convergence: 0.062764\n",
      "Epoch: 3, Loss: 320.43636, Residuals: -4.05245, Convergence: 0.050289\n",
      "Epoch: 4, Loss: 308.12975, Residuals: -3.90787, Convergence: 0.039940\n",
      "Epoch: 5, Loss: 298.36249, Residuals: -3.78000, Convergence: 0.032736\n",
      "Epoch: 6, Loss: 290.42835, Residuals: -3.66864, Convergence: 0.027319\n",
      "Epoch: 7, Loss: 283.83799, Residuals: -3.57334, Convergence: 0.023219\n",
      "Epoch: 8, Loss: 278.22858, Residuals: -3.49204, Convergence: 0.020161\n",
      "Epoch: 9, Loss: 273.34369, Residuals: -3.42241, Convergence: 0.017871\n",
      "Epoch: 10, Loss: 269.00075, Residuals: -3.36233, Convergence: 0.016145\n",
      "Epoch: 11, Loss: 265.06732, Residuals: -3.31001, Convergence: 0.014839\n",
      "Epoch: 12, Loss: 261.44664, Residuals: -3.26390, Convergence: 0.013849\n",
      "Epoch: 13, Loss: 258.06829, Residuals: -3.22267, Convergence: 0.013091\n",
      "Epoch: 14, Loss: 254.88178, Residuals: -3.18514, Convergence: 0.012502\n",
      "Epoch: 15, Loss: 251.85318, Residuals: -3.15036, Convergence: 0.012025\n",
      "Epoch: 16, Loss: 248.96305, Residuals: -3.11765, Convergence: 0.011609\n",
      "Epoch: 17, Loss: 246.19858, Residuals: -3.08661, Convergence: 0.011229\n",
      "Epoch: 18, Loss: 243.54001, Residuals: -3.05686, Convergence: 0.010916\n",
      "Epoch: 19, Loss: 240.95596, Residuals: -3.02791, Convergence: 0.010724\n",
      "Epoch: 20, Loss: 238.40981, Residuals: -2.99917, Convergence: 0.010680\n",
      "Epoch: 21, Loss: 235.86889, Residuals: -2.97010, Convergence: 0.010773\n",
      "Epoch: 22, Loss: 233.30930, Residuals: -2.94032, Convergence: 0.010971\n",
      "Epoch: 23, Loss: 230.70811, Residuals: -2.90955, Convergence: 0.011275\n",
      "Epoch: 24, Loss: 228.02651, Residuals: -2.87734, Convergence: 0.011760\n",
      "Epoch: 25, Loss: 225.20933, Residuals: -2.84305, Convergence: 0.012509\n",
      "Epoch: 26, Loss: 222.24050, Residuals: -2.80638, Convergence: 0.013359\n",
      "Epoch: 27, Loss: 219.22291, Residuals: -2.76835, Convergence: 0.013765\n",
      "Epoch: 28, Loss: 216.27781, Residuals: -2.73035, Convergence: 0.013617\n",
      "Epoch: 29, Loss: 213.43623, Residuals: -2.69286, Convergence: 0.013313\n",
      "Epoch: 30, Loss: 210.68719, Residuals: -2.65586, Convergence: 0.013048\n",
      "Epoch: 31, Loss: 208.01539, Residuals: -2.61925, Convergence: 0.012844\n",
      "Epoch: 32, Loss: 205.40938, Residuals: -2.58296, Convergence: 0.012687\n",
      "Epoch: 33, Loss: 202.86192, Residuals: -2.54692, Convergence: 0.012558\n",
      "Epoch: 34, Loss: 200.36901, Residuals: -2.51110, Convergence: 0.012442\n",
      "Epoch: 35, Loss: 197.92892, Residuals: -2.47548, Convergence: 0.012328\n",
      "Epoch: 36, Loss: 195.54143, Residuals: -2.44004, Convergence: 0.012210\n",
      "Epoch: 37, Loss: 193.20724, Residuals: -2.40479, Convergence: 0.012081\n",
      "Epoch: 38, Loss: 190.92750, Residuals: -2.36971, Convergence: 0.011940\n",
      "Epoch: 39, Loss: 188.70350, Residuals: -2.33482, Convergence: 0.011786\n",
      "Epoch: 40, Loss: 186.53640, Residuals: -2.30012, Convergence: 0.011618\n",
      "Epoch: 41, Loss: 184.42717, Residuals: -2.26563, Convergence: 0.011437\n",
      "Epoch: 42, Loss: 182.37651, Residuals: -2.23136, Convergence: 0.011244\n",
      "Epoch: 43, Loss: 180.38494, Residuals: -2.19732, Convergence: 0.011041\n",
      "Epoch: 44, Loss: 178.45292, Residuals: -2.16353, Convergence: 0.010826\n",
      "Epoch: 45, Loss: 176.58112, Residuals: -2.13003, Convergence: 0.010600\n",
      "Epoch: 46, Loss: 174.77057, Residuals: -2.09685, Convergence: 0.010360\n",
      "Epoch: 47, Loss: 173.02274, Residuals: -2.06403, Convergence: 0.010102\n",
      "Epoch: 48, Loss: 171.33937, Residuals: -2.03162, Convergence: 0.009825\n",
      "Epoch: 49, Loss: 169.72221, Residuals: -1.99969, Convergence: 0.009528\n",
      "Epoch: 50, Loss: 168.17265, Residuals: -1.96828, Convergence: 0.009214\n",
      "Epoch: 51, Loss: 166.69143, Residuals: -1.93746, Convergence: 0.008886\n",
      "Epoch: 52, Loss: 165.27856, Residuals: -1.90728, Convergence: 0.008548\n",
      "Epoch: 53, Loss: 163.93339, Residuals: -1.87779, Convergence: 0.008206\n",
      "Epoch: 54, Loss: 162.65467, Residuals: -1.84901, Convergence: 0.007862\n",
      "Epoch: 55, Loss: 161.44071, Residuals: -1.82098, Convergence: 0.007520\n",
      "Epoch: 56, Loss: 160.28950, Residuals: -1.79372, Convergence: 0.007182\n",
      "Epoch: 57, Loss: 159.19880, Residuals: -1.76726, Convergence: 0.006851\n",
      "Epoch: 58, Loss: 158.16617, Residuals: -1.74161, Convergence: 0.006529\n",
      "Epoch: 59, Loss: 157.18900, Residuals: -1.71678, Convergence: 0.006217\n",
      "Epoch: 60, Loss: 156.26453, Residuals: -1.69277, Convergence: 0.005916\n",
      "Epoch: 61, Loss: 155.38987, Residuals: -1.66958, Convergence: 0.005629\n",
      "Epoch: 62, Loss: 154.56201, Residuals: -1.64721, Convergence: 0.005356\n",
      "Epoch: 63, Loss: 153.77784, Residuals: -1.62562, Convergence: 0.005099\n",
      "Epoch: 64, Loss: 153.03426, Residuals: -1.60481, Convergence: 0.004859\n",
      "Epoch: 65, Loss: 152.32816, Residuals: -1.58474, Convergence: 0.004635\n",
      "Epoch: 66, Loss: 151.65659, Residuals: -1.56539, Convergence: 0.004428\n",
      "Epoch: 67, Loss: 151.01677, Residuals: -1.54671, Convergence: 0.004237\n",
      "Epoch: 68, Loss: 150.40622, Residuals: -1.52869, Convergence: 0.004059\n",
      "Epoch: 69, Loss: 149.82277, Residuals: -1.51127, Convergence: 0.003894\n",
      "Epoch: 70, Loss: 149.26462, Residuals: -1.49445, Convergence: 0.003739\n",
      "Epoch: 71, Loss: 148.73030, Residuals: -1.47819, Convergence: 0.003593\n",
      "Epoch: 72, Loss: 148.21863, Residuals: -1.46247, Convergence: 0.003452\n",
      "Epoch: 73, Loss: 147.72866, Residuals: -1.44729, Convergence: 0.003317\n",
      "Epoch: 74, Loss: 147.25962, Residuals: -1.43263, Convergence: 0.003185\n",
      "Epoch: 75, Loss: 146.81083, Residuals: -1.41847, Convergence: 0.003057\n",
      "Epoch: 76, Loss: 146.38168, Residuals: -1.40482, Convergence: 0.002932\n",
      "Epoch: 77, Loss: 145.97158, Residuals: -1.39165, Convergence: 0.002809\n",
      "Epoch: 78, Loss: 145.57992, Residuals: -1.37897, Convergence: 0.002690\n",
      "Epoch: 79, Loss: 145.20611, Residuals: -1.36677, Convergence: 0.002574\n",
      "Epoch: 80, Loss: 144.84953, Residuals: -1.35502, Convergence: 0.002462\n",
      "Epoch: 81, Loss: 144.50956, Residuals: -1.34373, Convergence: 0.002353\n",
      "Epoch: 82, Loss: 144.18557, Residuals: -1.33288, Convergence: 0.002247\n",
      "Epoch: 83, Loss: 143.87691, Residuals: -1.32245, Convergence: 0.002145\n",
      "Epoch: 84, Loss: 143.58299, Residuals: -1.31244, Convergence: 0.002047\n",
      "Epoch: 85, Loss: 143.30317, Residuals: -1.30283, Convergence: 0.001953\n",
      "Epoch: 86, Loss: 143.03689, Residuals: -1.29361, Convergence: 0.001862\n",
      "Epoch: 87, Loss: 142.78357, Residuals: -1.28477, Convergence: 0.001774\n",
      "Epoch: 88, Loss: 142.54266, Residuals: -1.27628, Convergence: 0.001690\n",
      "Epoch: 89, Loss: 142.31368, Residuals: -1.26814, Convergence: 0.001609\n",
      "Epoch: 90, Loss: 142.09612, Residuals: -1.26034, Convergence: 0.001531\n",
      "Epoch: 91, Loss: 141.88955, Residuals: -1.25286, Convergence: 0.001456\n",
      "Epoch: 92, Loss: 141.69354, Residuals: -1.24569, Convergence: 0.001383\n",
      "Epoch: 93, Loss: 141.50772, Residuals: -1.23881, Convergence: 0.001313\n",
      "Epoch: 94, Loss: 141.33171, Residuals: -1.23223, Convergence: 0.001245\n",
      "Epoch: 95, Loss: 141.16519, Residuals: -1.22591, Convergence: 0.001180\n",
      "Epoch: 96, Loss: 141.00785, Residuals: -1.21987, Convergence: 0.001116\n",
      "Epoch: 97, Loss: 140.85942, Residuals: -1.21408, Convergence: 0.001054\n",
      "Epoch: 98, Loss: 140.71962, Residuals: -1.20853, Convergence: 0.000993\n",
      "Evidence -182.173\n",
      "\n",
      "Epoch: 98, Evidence: -182.17288, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 7.24e-01\n",
      "Epoch: 98, Loss: 1387.74017, Residuals: -1.20853, Convergence:   inf\n",
      "Epoch: 99, Loss: 1325.50818, Residuals: -1.23749, Convergence: 0.046950\n",
      "Epoch: 100, Loss: 1278.27575, Residuals: -1.26026, Convergence: 0.036950\n",
      "Epoch: 101, Loss: 1242.63083, Residuals: -1.27658, Convergence: 0.028685\n",
      "Epoch: 102, Loss: 1214.90922, Residuals: -1.28795, Convergence: 0.022818\n",
      "Epoch: 103, Loss: 1192.50303, Residuals: -1.29626, Convergence: 0.018789\n",
      "Epoch: 104, Loss: 1173.91758, Residuals: -1.30250, Convergence: 0.015832\n",
      "Epoch: 105, Loss: 1158.25556, Residuals: -1.30715, Convergence: 0.013522\n",
      "Epoch: 106, Loss: 1144.90657, Residuals: -1.31045, Convergence: 0.011659\n",
      "Epoch: 107, Loss: 1133.41702, Residuals: -1.31255, Convergence: 0.010137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 108, Loss: 1123.43211, Residuals: -1.31360, Convergence: 0.008888\n",
      "Epoch: 109, Loss: 1114.66609, Residuals: -1.31370, Convergence: 0.007864\n",
      "Epoch: 110, Loss: 1106.88158, Residuals: -1.31296, Convergence: 0.007033\n",
      "Epoch: 111, Loss: 1099.87969, Residuals: -1.31146, Convergence: 0.006366\n",
      "Epoch: 112, Loss: 1093.48695, Residuals: -1.30924, Convergence: 0.005846\n",
      "Epoch: 113, Loss: 1087.54968, Residuals: -1.30637, Convergence: 0.005459\n",
      "Epoch: 114, Loss: 1081.92635, Residuals: -1.30285, Convergence: 0.005198\n",
      "Epoch: 115, Loss: 1076.48546, Residuals: -1.29870, Convergence: 0.005054\n",
      "Epoch: 116, Loss: 1071.10530, Residuals: -1.29391, Convergence: 0.005023\n",
      "Epoch: 117, Loss: 1065.67922, Residuals: -1.28848, Convergence: 0.005092\n",
      "Epoch: 118, Loss: 1060.13122, Residuals: -1.28243, Convergence: 0.005233\n",
      "Epoch: 119, Loss: 1054.43385, Residuals: -1.27579, Convergence: 0.005403\n",
      "Epoch: 120, Loss: 1048.62649, Residuals: -1.26867, Convergence: 0.005538\n",
      "Epoch: 121, Loss: 1042.81263, Residuals: -1.26114, Convergence: 0.005575\n",
      "Epoch: 122, Loss: 1037.12893, Residuals: -1.25331, Convergence: 0.005480\n",
      "Epoch: 123, Loss: 1031.69732, Residuals: -1.24527, Convergence: 0.005265\n",
      "Epoch: 124, Loss: 1026.59432, Residuals: -1.23709, Convergence: 0.004971\n",
      "Epoch: 125, Loss: 1021.84694, Residuals: -1.22885, Convergence: 0.004646\n",
      "Epoch: 126, Loss: 1017.44804, Residuals: -1.22063, Convergence: 0.004323\n",
      "Epoch: 127, Loss: 1013.37233, Residuals: -1.21247, Convergence: 0.004022\n",
      "Epoch: 128, Loss: 1009.58919, Residuals: -1.20442, Convergence: 0.003747\n",
      "Epoch: 129, Loss: 1006.06783, Residuals: -1.19653, Convergence: 0.003500\n",
      "Epoch: 130, Loss: 1002.78102, Residuals: -1.18883, Convergence: 0.003278\n",
      "Epoch: 131, Loss: 999.70531, Residuals: -1.18134, Convergence: 0.003077\n",
      "Epoch: 132, Loss: 996.82092, Residuals: -1.17409, Convergence: 0.002894\n",
      "Epoch: 133, Loss: 994.11166, Residuals: -1.16709, Convergence: 0.002725\n",
      "Epoch: 134, Loss: 991.56272, Residuals: -1.16035, Convergence: 0.002571\n",
      "Epoch: 135, Loss: 989.16213, Residuals: -1.15388, Convergence: 0.002427\n",
      "Epoch: 136, Loss: 986.89834, Residuals: -1.14769, Convergence: 0.002294\n",
      "Epoch: 137, Loss: 984.76144, Residuals: -1.14178, Convergence: 0.002170\n",
      "Epoch: 138, Loss: 982.74164, Residuals: -1.13614, Convergence: 0.002055\n",
      "Epoch: 139, Loss: 980.83003, Residuals: -1.13077, Convergence: 0.001949\n",
      "Epoch: 140, Loss: 979.01830, Residuals: -1.12566, Convergence: 0.001851\n",
      "Epoch: 141, Loss: 977.29834, Residuals: -1.12080, Convergence: 0.001760\n",
      "Epoch: 142, Loss: 975.66249, Residuals: -1.11618, Convergence: 0.001677\n",
      "Epoch: 143, Loss: 974.10412, Residuals: -1.11179, Convergence: 0.001600\n",
      "Epoch: 144, Loss: 972.61620, Residuals: -1.10762, Convergence: 0.001530\n",
      "Epoch: 145, Loss: 971.19273, Residuals: -1.10366, Convergence: 0.001466\n",
      "Epoch: 146, Loss: 969.82827, Residuals: -1.09990, Convergence: 0.001407\n",
      "Epoch: 147, Loss: 968.51720, Residuals: -1.09632, Convergence: 0.001354\n",
      "Epoch: 148, Loss: 967.25519, Residuals: -1.09291, Convergence: 0.001305\n",
      "Epoch: 149, Loss: 966.03769, Residuals: -1.08967, Convergence: 0.001260\n",
      "Epoch: 150, Loss: 964.86069, Residuals: -1.08658, Convergence: 0.001220\n",
      "Epoch: 151, Loss: 963.72072, Residuals: -1.08363, Convergence: 0.001183\n",
      "Epoch: 152, Loss: 962.61387, Residuals: -1.08081, Convergence: 0.001150\n",
      "Epoch: 153, Loss: 961.53722, Residuals: -1.07811, Convergence: 0.001120\n",
      "Epoch: 154, Loss: 960.48819, Residuals: -1.07553, Convergence: 0.001092\n",
      "Epoch: 155, Loss: 959.46308, Residuals: -1.07305, Convergence: 0.001068\n",
      "Epoch: 156, Loss: 958.45952, Residuals: -1.07067, Convergence: 0.001047\n",
      "Epoch: 157, Loss: 957.47500, Residuals: -1.06837, Convergence: 0.001028\n",
      "Epoch: 158, Loss: 956.50662, Residuals: -1.06616, Convergence: 0.001012\n",
      "Epoch: 159, Loss: 955.55172, Residuals: -1.06401, Convergence: 0.000999\n",
      "Evidence 11306.080\n",
      "\n",
      "Epoch: 159, Evidence: 11306.08008, Convergence: 1.016113\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 5.76e-01\n",
      "Epoch: 159, Loss: 2380.49897, Residuals: -1.06401, Convergence:   inf\n",
      "Epoch: 160, Loss: 2338.50300, Residuals: -1.07273, Convergence: 0.017958\n",
      "Epoch: 161, Loss: 2309.86292, Residuals: -1.07297, Convergence: 0.012399\n",
      "Epoch: 162, Loss: 2285.73368, Residuals: -1.07194, Convergence: 0.010556\n",
      "Epoch: 163, Loss: 2265.04772, Residuals: -1.07033, Convergence: 0.009133\n",
      "Epoch: 164, Loss: 2247.17639, Residuals: -1.06832, Convergence: 0.007953\n",
      "Epoch: 165, Loss: 2231.64095, Residuals: -1.06598, Convergence: 0.006961\n",
      "Epoch: 166, Loss: 2218.05419, Residuals: -1.06339, Convergence: 0.006126\n",
      "Epoch: 167, Loss: 2206.09434, Residuals: -1.06061, Convergence: 0.005421\n",
      "Epoch: 168, Loss: 2195.48618, Residuals: -1.05769, Convergence: 0.004832\n",
      "Epoch: 169, Loss: 2185.99374, Residuals: -1.05465, Convergence: 0.004342\n",
      "Epoch: 170, Loss: 2177.41101, Residuals: -1.05151, Convergence: 0.003942\n",
      "Epoch: 171, Loss: 2169.56479, Residuals: -1.04825, Convergence: 0.003616\n",
      "Epoch: 172, Loss: 2162.32437, Residuals: -1.04487, Convergence: 0.003348\n",
      "Epoch: 173, Loss: 2155.60167, Residuals: -1.04140, Convergence: 0.003119\n",
      "Epoch: 174, Loss: 2149.34695, Residuals: -1.03785, Convergence: 0.002910\n",
      "Epoch: 175, Loss: 2143.53570, Residuals: -1.03428, Convergence: 0.002711\n",
      "Epoch: 176, Loss: 2138.14527, Residuals: -1.03072, Convergence: 0.002521\n",
      "Epoch: 177, Loss: 2133.15271, Residuals: -1.02723, Convergence: 0.002340\n",
      "Epoch: 178, Loss: 2128.52929, Residuals: -1.02382, Convergence: 0.002172\n",
      "Epoch: 179, Loss: 2124.24374, Residuals: -1.02054, Convergence: 0.002017\n",
      "Epoch: 180, Loss: 2120.26558, Residuals: -1.01738, Convergence: 0.001876\n",
      "Epoch: 181, Loss: 2116.56524, Residuals: -1.01436, Convergence: 0.001748\n",
      "Epoch: 182, Loss: 2113.11702, Residuals: -1.01150, Convergence: 0.001632\n",
      "Epoch: 183, Loss: 2109.89781, Residuals: -1.00878, Convergence: 0.001526\n",
      "Epoch: 184, Loss: 2106.88793, Residuals: -1.00620, Convergence: 0.001429\n",
      "Epoch: 185, Loss: 2104.07033, Residuals: -1.00378, Convergence: 0.001339\n",
      "Epoch: 186, Loss: 2101.42982, Residuals: -1.00150, Convergence: 0.001257\n",
      "Epoch: 187, Loss: 2098.95371, Residuals: -0.99935, Convergence: 0.001180\n",
      "Epoch: 188, Loss: 2096.62991, Residuals: -0.99734, Convergence: 0.001108\n",
      "Epoch: 189, Loss: 2094.44812, Residuals: -0.99545, Convergence: 0.001042\n",
      "Epoch: 190, Loss: 2092.39788, Residuals: -0.99368, Convergence: 0.000980\n",
      "Evidence 14498.297\n",
      "\n",
      "Epoch: 190, Evidence: 14498.29688, Convergence: 0.220179\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 4.39e-01\n",
      "Epoch: 190, Loss: 2508.09049, Residuals: -0.99368, Convergence:   inf\n",
      "Epoch: 191, Loss: 2492.91090, Residuals: -0.99134, Convergence: 0.006089\n",
      "Epoch: 192, Loss: 2480.57512, Residuals: -0.98838, Convergence: 0.004973\n",
      "Epoch: 193, Loss: 2469.98508, Residuals: -0.98538, Convergence: 0.004287\n",
      "Epoch: 194, Loss: 2460.82658, Residuals: -0.98245, Convergence: 0.003722\n",
      "Epoch: 195, Loss: 2452.85751, Residuals: -0.97964, Convergence: 0.003249\n",
      "Epoch: 196, Loss: 2445.88806, Residuals: -0.97700, Convergence: 0.002849\n",
      "Epoch: 197, Loss: 2439.75939, Residuals: -0.97452, Convergence: 0.002512\n",
      "Epoch: 198, Loss: 2434.34277, Residuals: -0.97222, Convergence: 0.002225\n",
      "Epoch: 199, Loss: 2429.52859, Residuals: -0.97008, Convergence: 0.001982\n",
      "Epoch: 200, Loss: 2425.22579, Residuals: -0.96809, Convergence: 0.001774\n",
      "Epoch: 201, Loss: 2421.35881, Residuals: -0.96624, Convergence: 0.001597\n",
      "Epoch: 202, Loss: 2417.86230, Residuals: -0.96452, Convergence: 0.001446\n",
      "Epoch: 203, Loss: 2414.68525, Residuals: -0.96292, Convergence: 0.001316\n",
      "Epoch: 204, Loss: 2411.78125, Residuals: -0.96141, Convergence: 0.001204\n",
      "Epoch: 205, Loss: 2409.11426, Residuals: -0.96001, Convergence: 0.001107\n",
      "Epoch: 206, Loss: 2406.65386, Residuals: -0.95869, Convergence: 0.001022\n",
      "Epoch: 207, Loss: 2404.37378, Residuals: -0.95745, Convergence: 0.000948\n",
      "Evidence 14934.573\n",
      "\n",
      "Epoch: 207, Evidence: 14934.57324, Convergence: 0.029213\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 3.35e-01\n",
      "Epoch: 207, Loss: 2513.30004, Residuals: -0.95745, Convergence:   inf\n",
      "Epoch: 208, Loss: 2506.47677, Residuals: -0.95460, Convergence: 0.002722\n",
      "Epoch: 209, Loss: 2500.84138, Residuals: -0.95187, Convergence: 0.002253\n",
      "Epoch: 210, Loss: 2496.05845, Residuals: -0.94939, Convergence: 0.001916\n",
      "Epoch: 211, Loss: 2491.93769, Residuals: -0.94716, Convergence: 0.001654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 212, Loss: 2488.34147, Residuals: -0.94517, Convergence: 0.001445\n",
      "Epoch: 213, Loss: 2485.16387, Residuals: -0.94339, Convergence: 0.001279\n",
      "Epoch: 214, Loss: 2482.32608, Residuals: -0.94181, Convergence: 0.001143\n",
      "Epoch: 215, Loss: 2479.76612, Residuals: -0.94041, Convergence: 0.001032\n",
      "Epoch: 216, Loss: 2477.43546, Residuals: -0.93915, Convergence: 0.000941\n",
      "Evidence 15026.350\n",
      "\n",
      "Epoch: 216, Evidence: 15026.34961, Convergence: 0.006108\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 2.64e-01\n",
      "Epoch: 216, Loss: 2514.82067, Residuals: -0.93915, Convergence:   inf\n",
      "Epoch: 217, Loss: 2510.92875, Residuals: -0.93670, Convergence: 0.001550\n",
      "Epoch: 218, Loss: 2507.67520, Residuals: -0.93452, Convergence: 0.001297\n",
      "Epoch: 219, Loss: 2504.87009, Residuals: -0.93262, Convergence: 0.001120\n",
      "Epoch: 220, Loss: 2502.40328, Residuals: -0.93097, Convergence: 0.000986\n",
      "Evidence 15055.456\n",
      "\n",
      "Epoch: 220, Evidence: 15055.45605, Convergence: 0.001933\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 2.14e-01\n",
      "Epoch: 220, Loss: 2515.85257, Residuals: -0.93097, Convergence:   inf\n",
      "Epoch: 221, Loss: 2512.96790, Residuals: -0.92878, Convergence: 0.001148\n",
      "Epoch: 222, Loss: 2510.52081, Residuals: -0.92687, Convergence: 0.000975\n",
      "Evidence 15067.440\n",
      "\n",
      "Epoch: 222, Evidence: 15067.44043, Convergence: 0.000795\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 1.79e-01\n",
      "Epoch: 222, Loss: 2516.60992, Residuals: -0.92687, Convergence:   inf\n",
      "Epoch: 223, Loss: 2512.01002, Residuals: -0.92350, Convergence: 0.001831\n",
      "Epoch: 224, Loss: 2508.45010, Residuals: -0.92068, Convergence: 0.001419\n",
      "Epoch: 225, Loss: 2505.53850, Residuals: -0.91868, Convergence: 0.001162\n",
      "Epoch: 226, Loss: 2503.05705, Residuals: -0.91739, Convergence: 0.000991\n",
      "Evidence 15085.889\n",
      "\n",
      "Epoch: 226, Evidence: 15085.88867, Convergence: 0.002017\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 1.49e-01\n",
      "Epoch: 226, Loss: 2516.84136, Residuals: -0.91739, Convergence:   inf\n",
      "Epoch: 227, Loss: 2513.80639, Residuals: -0.91439, Convergence: 0.001207\n",
      "Epoch: 228, Loss: 2511.38734, Residuals: -0.91261, Convergence: 0.000963\n",
      "Evidence 15096.948\n",
      "\n",
      "Epoch: 228, Evidence: 15096.94824, Convergence: 0.000733\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 1.27e-01\n",
      "Epoch: 228, Loss: 2517.09000, Residuals: -0.91261, Convergence:   inf\n",
      "Epoch: 229, Loss: 2512.65475, Residuals: -0.90851, Convergence: 0.001765\n",
      "Epoch: 230, Loss: 2509.49198, Residuals: -0.90929, Convergence: 0.001260\n",
      "Epoch: 231, Loss: 2506.82268, Residuals: -0.91051, Convergence: 0.001065\n",
      "Epoch: 232, Loss: 2504.54234, Residuals: -0.91342, Convergence: 0.000910\n",
      "Evidence 15113.204\n",
      "\n",
      "Epoch: 232, Evidence: 15113.20410, Convergence: 0.001807\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 1.12e-01\n",
      "Epoch: 232, Loss: 2516.71500, Residuals: -0.91342, Convergence:   inf\n",
      "Epoch: 233, Loss: 2515.16032, Residuals: -0.91136, Convergence: 0.000618\n",
      "Evidence 15119.646\n",
      "\n",
      "Epoch: 233, Evidence: 15119.64648, Convergence: 0.000426\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 9.23e-02\n",
      "Epoch: 233, Loss: 2517.58909, Residuals: -0.91136, Convergence:   inf\n",
      "Epoch: 234, Loss: 2563.42197, Residuals: -0.94230, Convergence: -0.017880\n",
      "Epoch: 234, Loss: 2515.52103, Residuals: -0.90853, Convergence: 0.000822\n",
      "Evidence 15123.879\n",
      "\n",
      "Epoch: 234, Evidence: 15123.87891, Convergence: 0.000706\n",
      "Updating hyper-parameters...\n",
      "Total samples: 185, Updated regularization: 8.43e-02\n",
      "Epoch: 234, Loss: 2517.18336, Residuals: -0.90853, Convergence:   inf\n",
      "Epoch: 235, Loss: 2521.94791, Residuals: -0.90868, Convergence: -0.001889\n",
      "Epoch: 235, Loss: 2517.34849, Residuals: -0.90702, Convergence: -0.000066\n",
      "Evidence 15125.292\n",
      "\n",
      "Epoch: 235, Evidence: 15125.29199, Convergence: 0.000799\n",
      "Total samples: 184, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 383.76569, Residuals: -4.51915, Convergence:   inf\n",
      "Epoch: 1, Loss: 357.98745, Residuals: -4.39902, Convergence: 0.072009\n",
      "Epoch: 2, Loss: 336.80572, Residuals: -4.23434, Convergence: 0.062890\n",
      "Epoch: 3, Loss: 320.64871, Residuals: -4.06956, Convergence: 0.050388\n",
      "Epoch: 4, Loss: 308.31324, Residuals: -3.92423, Convergence: 0.040010\n",
      "Epoch: 5, Loss: 298.51605, Residuals: -3.79558, Convergence: 0.032820\n",
      "Epoch: 6, Loss: 290.55038, Residuals: -3.68344, Convergence: 0.027416\n",
      "Epoch: 7, Loss: 283.92483, Residuals: -3.58734, Convergence: 0.023336\n",
      "Epoch: 8, Loss: 278.27477, Residuals: -3.50522, Convergence: 0.020304\n",
      "Epoch: 9, Loss: 273.34283, Residuals: -3.43474, Convergence: 0.018043\n",
      "Epoch: 10, Loss: 268.94600, Residuals: -3.37380, Convergence: 0.016348\n",
      "Epoch: 11, Loss: 264.95164, Residuals: -3.32062, Convergence: 0.015076\n",
      "Epoch: 12, Loss: 261.26301, Residuals: -3.27367, Convergence: 0.014118\n",
      "Epoch: 13, Loss: 257.81017, Residuals: -3.23163, Convergence: 0.013393\n",
      "Epoch: 14, Loss: 254.54419, Residuals: -3.19336, Convergence: 0.012831\n",
      "Epoch: 15, Loss: 251.43436, Residuals: -3.15791, Convergence: 0.012368\n",
      "Epoch: 16, Loss: 248.46517, Residuals: -3.12464, Convergence: 0.011950\n",
      "Epoch: 17, Loss: 245.62533, Residuals: -3.09313, Convergence: 0.011562\n",
      "Epoch: 18, Loss: 242.89395, Residuals: -3.06293, Convergence: 0.011245\n",
      "Epoch: 19, Loss: 240.23878, Residuals: -3.03345, Convergence: 0.011052\n",
      "Epoch: 20, Loss: 237.62435, Residuals: -3.00410, Convergence: 0.011002\n",
      "Epoch: 21, Loss: 235.02141, Residuals: -2.97438, Convergence: 0.011075\n",
      "Epoch: 22, Loss: 232.41017, Residuals: -2.94401, Convergence: 0.011235\n",
      "Epoch: 23, Loss: 229.76914, Residuals: -2.91275, Convergence: 0.011494\n",
      "Epoch: 24, Loss: 227.05789, Residuals: -2.88017, Convergence: 0.011941\n",
      "Epoch: 25, Loss: 224.21886, Residuals: -2.84560, Convergence: 0.012662\n",
      "Epoch: 26, Loss: 221.23216, Residuals: -2.80871, Convergence: 0.013500\n",
      "Epoch: 27, Loss: 218.19766, Residuals: -2.77049, Convergence: 0.013907\n",
      "Epoch: 28, Loss: 215.23937, Residuals: -2.73235, Convergence: 0.013744\n",
      "Epoch: 29, Loss: 212.39051, Residuals: -2.69479, Convergence: 0.013413\n",
      "Epoch: 30, Loss: 209.64007, Residuals: -2.65780, Convergence: 0.013120\n",
      "Epoch: 31, Loss: 206.97234, Residuals: -2.62126, Convergence: 0.012889\n",
      "Epoch: 32, Loss: 204.37568, Residuals: -2.58510, Convergence: 0.012705\n",
      "Epoch: 33, Loss: 201.84287, Residuals: -2.54923, Convergence: 0.012548\n",
      "Epoch: 34, Loss: 199.37003, Residuals: -2.51363, Convergence: 0.012403\n",
      "Epoch: 35, Loss: 196.95551, Residuals: -2.47826, Convergence: 0.012259\n",
      "Epoch: 36, Loss: 194.59898, Residuals: -2.44312, Convergence: 0.012110\n",
      "Epoch: 37, Loss: 192.30087, Residuals: -2.40821, Convergence: 0.011951\n",
      "Epoch: 38, Loss: 190.06195, Residuals: -2.37353, Convergence: 0.011780\n",
      "Epoch: 39, Loss: 187.88314, Residuals: -2.33909, Convergence: 0.011597\n",
      "Epoch: 40, Loss: 185.76538, Residuals: -2.30490, Convergence: 0.011400\n",
      "Epoch: 41, Loss: 183.70957, Residuals: -2.27098, Convergence: 0.011191\n",
      "Epoch: 42, Loss: 181.71657, Residuals: -2.23734, Convergence: 0.010968\n",
      "Epoch: 43, Loss: 179.78711, Residuals: -2.20402, Convergence: 0.010732\n",
      "Epoch: 44, Loss: 177.92181, Residuals: -2.17103, Convergence: 0.010484\n",
      "Epoch: 45, Loss: 176.12112, Residuals: -2.13842, Convergence: 0.010224\n",
      "Epoch: 46, Loss: 174.38531, Residuals: -2.10622, Convergence: 0.009954\n",
      "Epoch: 47, Loss: 172.71444, Residuals: -2.07444, Convergence: 0.009674\n",
      "Epoch: 48, Loss: 171.10834, Residuals: -2.04314, Convergence: 0.009387\n",
      "Epoch: 49, Loss: 169.56654, Residuals: -2.01234, Convergence: 0.009093\n",
      "Epoch: 50, Loss: 168.08833, Residuals: -1.98206, Convergence: 0.008794\n",
      "Epoch: 51, Loss: 166.67272, Residuals: -1.95234, Convergence: 0.008493\n",
      "Epoch: 52, Loss: 165.31844, Residuals: -1.92319, Convergence: 0.008192\n",
      "Epoch: 53, Loss: 164.02398, Residuals: -1.89464, Convergence: 0.007892\n",
      "Epoch: 54, Loss: 162.78765, Residuals: -1.86669, Convergence: 0.007595\n",
      "Epoch: 55, Loss: 161.60760, Residuals: -1.83934, Convergence: 0.007302\n",
      "Epoch: 56, Loss: 160.48192, Residuals: -1.81262, Convergence: 0.007014\n",
      "Epoch: 57, Loss: 159.40873, Residuals: -1.78651, Convergence: 0.006732\n",
      "Epoch: 58, Loss: 158.38620, Residuals: -1.76102, Convergence: 0.006456\n",
      "Epoch: 59, Loss: 157.41258, Residuals: -1.73615, Convergence: 0.006185\n",
      "Epoch: 60, Loss: 156.48623, Residuals: -1.71190, Convergence: 0.005920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61, Loss: 155.60560, Residuals: -1.68828, Convergence: 0.005659\n",
      "Epoch: 62, Loss: 154.76920, Residuals: -1.66530, Convergence: 0.005404\n",
      "Epoch: 63, Loss: 153.97554, Residuals: -1.64296, Convergence: 0.005154\n",
      "Epoch: 64, Loss: 153.22320, Residuals: -1.62126, Convergence: 0.004910\n",
      "Epoch: 65, Loss: 152.51074, Residuals: -1.60023, Convergence: 0.004672\n",
      "Epoch: 66, Loss: 151.83673, Residuals: -1.57987, Convergence: 0.004439\n",
      "Epoch: 67, Loss: 151.19972, Residuals: -1.56017, Convergence: 0.004213\n",
      "Epoch: 68, Loss: 150.59830, Residuals: -1.54116, Convergence: 0.003994\n",
      "Epoch: 69, Loss: 150.03099, Residuals: -1.52283, Convergence: 0.003781\n",
      "Epoch: 70, Loss: 149.49634, Residuals: -1.50517, Convergence: 0.003576\n",
      "Epoch: 71, Loss: 148.99283, Residuals: -1.48820, Convergence: 0.003379\n",
      "Epoch: 72, Loss: 148.51892, Residuals: -1.47190, Convergence: 0.003191\n",
      "Epoch: 73, Loss: 148.07298, Residuals: -1.45627, Convergence: 0.003012\n",
      "Epoch: 74, Loss: 147.65328, Residuals: -1.44129, Convergence: 0.002842\n",
      "Epoch: 75, Loss: 147.25802, Residuals: -1.42695, Convergence: 0.002684\n",
      "Epoch: 76, Loss: 146.88529, Residuals: -1.41324, Convergence: 0.002538\n",
      "Epoch: 77, Loss: 146.53314, Residuals: -1.40011, Convergence: 0.002403\n",
      "Epoch: 78, Loss: 146.19960, Residuals: -1.38755, Convergence: 0.002281\n",
      "Epoch: 79, Loss: 145.88282, Residuals: -1.37552, Convergence: 0.002171\n",
      "Epoch: 80, Loss: 145.58110, Residuals: -1.36398, Convergence: 0.002073\n",
      "Epoch: 81, Loss: 145.29297, Residuals: -1.35290, Convergence: 0.001983\n",
      "Epoch: 82, Loss: 145.01724, Residuals: -1.34226, Convergence: 0.001901\n",
      "Epoch: 83, Loss: 144.75300, Residuals: -1.33203, Convergence: 0.001825\n",
      "Epoch: 84, Loss: 144.49957, Residuals: -1.32219, Convergence: 0.001754\n",
      "Epoch: 85, Loss: 144.25647, Residuals: -1.31272, Convergence: 0.001685\n",
      "Epoch: 86, Loss: 144.02336, Residuals: -1.30362, Convergence: 0.001619\n",
      "Epoch: 87, Loss: 143.79999, Residuals: -1.29487, Convergence: 0.001553\n",
      "Epoch: 88, Loss: 143.58611, Residuals: -1.28648, Convergence: 0.001490\n",
      "Epoch: 89, Loss: 143.38148, Residuals: -1.27844, Convergence: 0.001427\n",
      "Epoch: 90, Loss: 143.18574, Residuals: -1.27075, Convergence: 0.001367\n",
      "Epoch: 91, Loss: 142.99846, Residuals: -1.26340, Convergence: 0.001310\n",
      "Epoch: 92, Loss: 142.81906, Residuals: -1.25638, Convergence: 0.001256\n",
      "Epoch: 93, Loss: 142.64684, Residuals: -1.24969, Convergence: 0.001207\n",
      "Epoch: 94, Loss: 142.48092, Residuals: -1.24331, Convergence: 0.001165\n",
      "Epoch: 95, Loss: 142.32036, Residuals: -1.23723, Convergence: 0.001128\n",
      "Epoch: 96, Loss: 142.16417, Residuals: -1.23142, Convergence: 0.001099\n",
      "Epoch: 97, Loss: 142.01137, Residuals: -1.22586, Convergence: 0.001076\n",
      "Epoch: 98, Loss: 141.86109, Residuals: -1.22052, Convergence: 0.001059\n",
      "Epoch: 99, Loss: 141.71260, Residuals: -1.21537, Convergence: 0.001048\n",
      "Epoch: 100, Loss: 141.56539, Residuals: -1.21038, Convergence: 0.001040\n",
      "Epoch: 101, Loss: 141.41911, Residuals: -1.20555, Convergence: 0.001034\n",
      "Epoch: 102, Loss: 141.27363, Residuals: -1.20085, Convergence: 0.001030\n",
      "Epoch: 103, Loss: 141.12898, Residuals: -1.19626, Convergence: 0.001025\n",
      "Epoch: 104, Loss: 140.98524, Residuals: -1.19178, Convergence: 0.001020\n",
      "Epoch: 105, Loss: 140.84264, Residuals: -1.18741, Convergence: 0.001013\n",
      "Epoch: 106, Loss: 140.70141, Residuals: -1.18313, Convergence: 0.001004\n",
      "Epoch: 107, Loss: 140.56179, Residuals: -1.17895, Convergence: 0.000993\n",
      "Evidence -181.361\n",
      "\n",
      "Epoch: 107, Evidence: -181.36066, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 7.24e-01\n",
      "Epoch: 107, Loss: 1384.19215, Residuals: -1.17895, Convergence:   inf\n",
      "Epoch: 108, Loss: 1327.17420, Residuals: -1.20562, Convergence: 0.042962\n",
      "Epoch: 109, Loss: 1283.11191, Residuals: -1.22847, Convergence: 0.034340\n",
      "Epoch: 110, Loss: 1249.16490, Residuals: -1.24690, Convergence: 0.027176\n",
      "Epoch: 111, Loss: 1222.25804, Residuals: -1.26156, Convergence: 0.022014\n",
      "Epoch: 112, Loss: 1200.24021, Residuals: -1.27352, Convergence: 0.018345\n",
      "Epoch: 113, Loss: 1181.87454, Residuals: -1.28329, Convergence: 0.015539\n",
      "Epoch: 114, Loss: 1166.37415, Residuals: -1.29111, Convergence: 0.013289\n",
      "Epoch: 115, Loss: 1153.16636, Residuals: -1.29716, Convergence: 0.011453\n",
      "Epoch: 116, Loss: 1141.80409, Residuals: -1.30164, Convergence: 0.009951\n",
      "Epoch: 117, Loss: 1131.93076, Residuals: -1.30475, Convergence: 0.008723\n",
      "Epoch: 118, Loss: 1123.25721, Residuals: -1.30667, Convergence: 0.007722\n",
      "Epoch: 119, Loss: 1115.54459, Residuals: -1.30753, Convergence: 0.006914\n",
      "Epoch: 120, Loss: 1108.59212, Residuals: -1.30745, Convergence: 0.006271\n",
      "Epoch: 121, Loss: 1102.22575, Residuals: -1.30652, Convergence: 0.005776\n",
      "Epoch: 122, Loss: 1096.29140, Residuals: -1.30481, Convergence: 0.005413\n",
      "Epoch: 123, Loss: 1090.64915, Residuals: -1.30235, Convergence: 0.005173\n",
      "Epoch: 124, Loss: 1085.16811, Residuals: -1.29915, Convergence: 0.005051\n",
      "Epoch: 125, Loss: 1079.72725, Residuals: -1.29523, Convergence: 0.005039\n",
      "Epoch: 126, Loss: 1074.22044, Residuals: -1.29058, Convergence: 0.005126\n",
      "Epoch: 127, Loss: 1068.56956, Residuals: -1.28522, Convergence: 0.005288\n",
      "Epoch: 128, Loss: 1062.74816, Residuals: -1.27921, Convergence: 0.005478\n",
      "Epoch: 129, Loss: 1056.80566, Residuals: -1.27263, Convergence: 0.005623\n",
      "Epoch: 130, Loss: 1050.86592, Residuals: -1.26558, Convergence: 0.005652\n",
      "Epoch: 131, Loss: 1045.08353, Residuals: -1.25817, Convergence: 0.005533\n",
      "Epoch: 132, Loss: 1039.58187, Residuals: -1.25051, Convergence: 0.005292\n",
      "Epoch: 133, Loss: 1034.42437, Residuals: -1.24269, Convergence: 0.004986\n",
      "Epoch: 134, Loss: 1029.62050, Residuals: -1.23480, Convergence: 0.004666\n",
      "Epoch: 135, Loss: 1025.14940, Residuals: -1.22690, Convergence: 0.004361\n",
      "Epoch: 136, Loss: 1020.97831, Residuals: -1.21907, Convergence: 0.004085\n",
      "Epoch: 137, Loss: 1017.07261, Residuals: -1.21134, Convergence: 0.003840\n",
      "Epoch: 138, Loss: 1013.40211, Residuals: -1.20376, Convergence: 0.003622\n",
      "Epoch: 139, Loss: 1009.94072, Residuals: -1.19636, Convergence: 0.003427\n",
      "Epoch: 140, Loss: 1006.66818, Residuals: -1.18916, Convergence: 0.003251\n",
      "Epoch: 141, Loss: 1003.56775, Residuals: -1.18218, Convergence: 0.003089\n",
      "Epoch: 142, Loss: 1000.62716, Residuals: -1.17544, Convergence: 0.002939\n",
      "Epoch: 143, Loss: 997.83542, Residuals: -1.16895, Convergence: 0.002798\n",
      "Epoch: 144, Loss: 995.18440, Residuals: -1.16272, Convergence: 0.002664\n",
      "Epoch: 145, Loss: 992.66625, Residuals: -1.15675, Convergence: 0.002537\n",
      "Epoch: 146, Loss: 990.27407, Residuals: -1.15105, Convergence: 0.002416\n",
      "Epoch: 147, Loss: 988.00125, Residuals: -1.14561, Convergence: 0.002300\n",
      "Epoch: 148, Loss: 985.84166, Residuals: -1.14043, Convergence: 0.002191\n",
      "Epoch: 149, Loss: 983.78889, Residuals: -1.13550, Convergence: 0.002087\n",
      "Epoch: 150, Loss: 981.83691, Residuals: -1.13080, Convergence: 0.001988\n",
      "Epoch: 151, Loss: 979.97994, Residuals: -1.12634, Convergence: 0.001895\n",
      "Epoch: 152, Loss: 978.21229, Residuals: -1.12209, Convergence: 0.001807\n",
      "Epoch: 153, Loss: 976.52835, Residuals: -1.11805, Convergence: 0.001724\n",
      "Epoch: 154, Loss: 974.92257, Residuals: -1.11421, Convergence: 0.001647\n",
      "Epoch: 155, Loss: 973.38930, Residuals: -1.11054, Convergence: 0.001575\n",
      "Epoch: 156, Loss: 971.92300, Residuals: -1.10705, Convergence: 0.001509\n",
      "Epoch: 157, Loss: 970.51787, Residuals: -1.10372, Convergence: 0.001448\n",
      "Epoch: 158, Loss: 969.16797, Residuals: -1.10052, Convergence: 0.001393\n",
      "Epoch: 159, Loss: 967.86783, Residuals: -1.09746, Convergence: 0.001343\n",
      "Epoch: 160, Loss: 966.61096, Residuals: -1.09452, Convergence: 0.001300\n",
      "Epoch: 161, Loss: 965.39144, Residuals: -1.09168, Convergence: 0.001263\n",
      "Epoch: 162, Loss: 964.20295, Residuals: -1.08894, Convergence: 0.001233\n",
      "Epoch: 163, Loss: 963.03933, Residuals: -1.08627, Convergence: 0.001208\n",
      "Epoch: 164, Loss: 961.89416, Residuals: -1.08367, Convergence: 0.001191\n",
      "Epoch: 165, Loss: 960.76213, Residuals: -1.08111, Convergence: 0.001178\n",
      "Epoch: 166, Loss: 959.63796, Residuals: -1.07860, Convergence: 0.001171\n",
      "Epoch: 167, Loss: 958.51827, Residuals: -1.07611, Convergence: 0.001168\n",
      "Epoch: 168, Loss: 957.40085, Residuals: -1.07364, Convergence: 0.001167\n",
      "Epoch: 169, Loss: 956.28578, Residuals: -1.07118, Convergence: 0.001166\n",
      "Epoch: 170, Loss: 955.17526, Residuals: -1.06875, Convergence: 0.001163\n",
      "Epoch: 171, Loss: 954.07335, Residuals: -1.06634, Convergence: 0.001155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 172, Loss: 952.98560, Residuals: -1.06395, Convergence: 0.001141\n",
      "Epoch: 173, Loss: 951.91729, Residuals: -1.06161, Convergence: 0.001122\n",
      "Epoch: 174, Loss: 950.87460, Residuals: -1.05931, Convergence: 0.001097\n",
      "Epoch: 175, Loss: 949.86206, Residuals: -1.05707, Convergence: 0.001066\n",
      "Epoch: 176, Loss: 948.88414, Residuals: -1.05490, Convergence: 0.001031\n",
      "Epoch: 177, Loss: 947.94380, Residuals: -1.05280, Convergence: 0.000992\n",
      "Evidence 11507.858\n",
      "\n",
      "Epoch: 177, Evidence: 11507.85840, Convergence: 1.015760\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 5.74e-01\n",
      "Epoch: 177, Loss: 2372.62244, Residuals: -1.05280, Convergence:   inf\n",
      "Epoch: 178, Loss: 2336.73127, Residuals: -1.06189, Convergence: 0.015360\n",
      "Epoch: 179, Loss: 2311.52192, Residuals: -1.06076, Convergence: 0.010906\n",
      "Epoch: 180, Loss: 2290.43884, Residuals: -1.05905, Convergence: 0.009205\n",
      "Epoch: 181, Loss: 2272.70019, Residuals: -1.05716, Convergence: 0.007805\n",
      "Epoch: 182, Loss: 2257.64493, Residuals: -1.05516, Convergence: 0.006669\n",
      "Epoch: 183, Loss: 2244.73402, Residuals: -1.05307, Convergence: 0.005752\n",
      "Epoch: 184, Loss: 2233.52518, Residuals: -1.05091, Convergence: 0.005018\n",
      "Epoch: 185, Loss: 2223.65338, Residuals: -1.04866, Convergence: 0.004439\n",
      "Epoch: 186, Loss: 2214.82089, Residuals: -1.04630, Convergence: 0.003988\n",
      "Epoch: 187, Loss: 2206.79736, Residuals: -1.04380, Convergence: 0.003636\n",
      "Epoch: 188, Loss: 2199.42220, Residuals: -1.04115, Convergence: 0.003353\n",
      "Epoch: 189, Loss: 2192.59721, Residuals: -1.03836, Convergence: 0.003113\n",
      "Epoch: 190, Loss: 2186.27332, Residuals: -1.03547, Convergence: 0.002893\n",
      "Epoch: 191, Loss: 2180.42453, Residuals: -1.03254, Convergence: 0.002682\n",
      "Epoch: 192, Loss: 2175.03085, Residuals: -1.02961, Convergence: 0.002480\n",
      "Epoch: 193, Loss: 2170.06801, Residuals: -1.02673, Convergence: 0.002287\n",
      "Epoch: 194, Loss: 2165.50871, Residuals: -1.02392, Convergence: 0.002105\n",
      "Epoch: 195, Loss: 2161.32153, Residuals: -1.02121, Convergence: 0.001937\n",
      "Epoch: 196, Loss: 2157.47225, Residuals: -1.01862, Convergence: 0.001784\n",
      "Epoch: 197, Loss: 2153.92872, Residuals: -1.01614, Convergence: 0.001645\n",
      "Epoch: 198, Loss: 2150.65900, Residuals: -1.01379, Convergence: 0.001520\n",
      "Epoch: 199, Loss: 2147.63270, Residuals: -1.01155, Convergence: 0.001409\n",
      "Epoch: 200, Loss: 2144.82259, Residuals: -1.00943, Convergence: 0.001310\n",
      "Epoch: 201, Loss: 2142.20280, Residuals: -1.00741, Convergence: 0.001223\n",
      "Epoch: 202, Loss: 2139.75180, Residuals: -1.00550, Convergence: 0.001145\n",
      "Epoch: 203, Loss: 2137.45033, Residuals: -1.00369, Convergence: 0.001077\n",
      "Epoch: 204, Loss: 2135.28128, Residuals: -1.00197, Convergence: 0.001016\n",
      "Epoch: 205, Loss: 2133.23098, Residuals: -1.00034, Convergence: 0.000961\n",
      "Evidence 14589.973\n",
      "\n",
      "Epoch: 205, Evidence: 14589.97266, Convergence: 0.211249\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 4.37e-01\n",
      "Epoch: 205, Loss: 2493.89908, Residuals: -1.00034, Convergence:   inf\n",
      "Epoch: 206, Loss: 2480.73488, Residuals: -0.99725, Convergence: 0.005307\n",
      "Epoch: 207, Loss: 2470.11611, Residuals: -0.99382, Convergence: 0.004299\n",
      "Epoch: 208, Loss: 2461.04750, Residuals: -0.99062, Convergence: 0.003685\n",
      "Epoch: 209, Loss: 2453.22282, Residuals: -0.98770, Convergence: 0.003190\n",
      "Epoch: 210, Loss: 2446.41517, Residuals: -0.98508, Convergence: 0.002783\n",
      "Epoch: 211, Loss: 2440.45097, Residuals: -0.98273, Convergence: 0.002444\n",
      "Epoch: 212, Loss: 2435.19168, Residuals: -0.98062, Convergence: 0.002160\n",
      "Epoch: 213, Loss: 2430.52698, Residuals: -0.97872, Convergence: 0.001919\n",
      "Epoch: 214, Loss: 2426.36483, Residuals: -0.97701, Convergence: 0.001715\n",
      "Epoch: 215, Loss: 2422.62992, Residuals: -0.97547, Convergence: 0.001542\n",
      "Epoch: 216, Loss: 2419.25983, Residuals: -0.97407, Convergence: 0.001393\n",
      "Epoch: 217, Loss: 2416.20297, Residuals: -0.97280, Convergence: 0.001265\n",
      "Epoch: 218, Loss: 2413.41605, Residuals: -0.97165, Convergence: 0.001155\n",
      "Epoch: 219, Loss: 2410.86466, Residuals: -0.97059, Convergence: 0.001058\n",
      "Epoch: 220, Loss: 2408.51834, Residuals: -0.96962, Convergence: 0.000974\n",
      "Evidence 14929.661\n",
      "\n",
      "Epoch: 220, Evidence: 14929.66113, Convergence: 0.022753\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 3.35e-01\n",
      "Epoch: 220, Loss: 2499.51365, Residuals: -0.96962, Convergence:   inf\n",
      "Epoch: 221, Loss: 2493.17925, Residuals: -0.96652, Convergence: 0.002541\n",
      "Epoch: 222, Loss: 2487.97674, Residuals: -0.96391, Convergence: 0.002091\n",
      "Epoch: 223, Loss: 2483.55543, Residuals: -0.96173, Convergence: 0.001780\n",
      "Epoch: 224, Loss: 2479.73905, Residuals: -0.95989, Convergence: 0.001539\n",
      "Epoch: 225, Loss: 2476.40245, Residuals: -0.95833, Convergence: 0.001347\n",
      "Epoch: 226, Loss: 2473.45162, Residuals: -0.95698, Convergence: 0.001193\n",
      "Epoch: 227, Loss: 2470.81472, Residuals: -0.95583, Convergence: 0.001067\n",
      "Epoch: 228, Loss: 2468.43546, Residuals: -0.95482, Convergence: 0.000964\n",
      "Evidence 15006.344\n",
      "\n",
      "Epoch: 228, Evidence: 15006.34375, Convergence: 0.005110\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 2.63e-01\n",
      "Epoch: 228, Loss: 2501.19886, Residuals: -0.95482, Convergence:   inf\n",
      "Epoch: 229, Loss: 2497.36826, Residuals: -0.95239, Convergence: 0.001534\n",
      "Epoch: 230, Loss: 2494.17202, Residuals: -0.95048, Convergence: 0.001281\n",
      "Epoch: 231, Loss: 2491.42216, Residuals: -0.94892, Convergence: 0.001104\n",
      "Epoch: 232, Loss: 2489.01263, Residuals: -0.94763, Convergence: 0.000968\n",
      "Evidence 15033.591\n",
      "\n",
      "Epoch: 232, Evidence: 15033.59082, Convergence: 0.001812\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 2.12e-01\n",
      "Epoch: 232, Loss: 2502.20450, Residuals: -0.94763, Convergence:   inf\n",
      "Epoch: 233, Loss: 2499.33754, Residuals: -0.94561, Convergence: 0.001147\n",
      "Epoch: 234, Loss: 2496.91938, Residuals: -0.94402, Convergence: 0.000968\n",
      "Evidence 15045.439\n",
      "\n",
      "Epoch: 234, Evidence: 15045.43945, Convergence: 0.000788\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.76e-01\n",
      "Epoch: 234, Loss: 2502.93159, Residuals: -0.94402, Convergence:   inf\n",
      "Epoch: 235, Loss: 2498.38900, Residuals: -0.94111, Convergence: 0.001818\n",
      "Epoch: 236, Loss: 2494.92365, Residuals: -0.93900, Convergence: 0.001389\n",
      "Epoch: 237, Loss: 2492.11783, Residuals: -0.93744, Convergence: 0.001126\n",
      "Epoch: 238, Loss: 2489.73548, Residuals: -0.93639, Convergence: 0.000957\n",
      "Evidence 15063.272\n",
      "\n",
      "Epoch: 238, Evidence: 15063.27246, Convergence: 0.001970\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.46e-01\n",
      "Epoch: 238, Loss: 2503.12730, Residuals: -0.93639, Convergence:   inf\n",
      "Epoch: 239, Loss: 2500.14942, Residuals: -0.93354, Convergence: 0.001191\n",
      "Epoch: 240, Loss: 2497.78242, Residuals: -0.93188, Convergence: 0.000948\n",
      "Evidence 15074.149\n",
      "\n",
      "Epoch: 240, Evidence: 15074.14941, Convergence: 0.000722\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.24e-01\n",
      "Epoch: 240, Loss: 2503.30738, Residuals: -0.93188, Convergence:   inf\n",
      "Epoch: 241, Loss: 2498.96073, Residuals: -0.92752, Convergence: 0.001739\n",
      "Epoch: 242, Loss: 2495.82542, Residuals: -0.92759, Convergence: 0.001256\n",
      "Epoch: 243, Loss: 2493.23485, Residuals: -0.92778, Convergence: 0.001039\n",
      "Epoch: 244, Loss: 2490.93446, Residuals: -0.92974, Convergence: 0.000924\n",
      "Evidence 15089.847\n",
      "\n",
      "Epoch: 244, Evidence: 15089.84668, Convergence: 0.001761\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.10e-01\n",
      "Epoch: 244, Loss: 2502.71391, Residuals: -0.92974, Convergence:   inf\n",
      "Epoch: 245, Loss: 2501.48231, Residuals: -0.92594, Convergence: 0.000492\n",
      "Evidence 15095.895\n",
      "\n",
      "Epoch: 245, Evidence: 15095.89453, Convergence: 0.000401\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 9.04e-02\n",
      "Epoch: 245, Loss: 2503.52855, Residuals: -0.92594, Convergence:   inf\n",
      "Epoch: 246, Loss: 2553.64759, Residuals: -0.96001, Convergence: -0.019626\n",
      "Epoch: 246, Loss: 2501.31848, Residuals: -0.92251, Convergence: 0.000884\n",
      "Evidence 15100.150\n",
      "\n",
      "Epoch: 246, Evidence: 15100.15039, Convergence: 0.000682\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 8.34e-02\n",
      "Epoch: 246, Loss: 2503.01177, Residuals: -0.92251, Convergence:   inf\n",
      "Epoch: 247, Loss: 2510.04894, Residuals: -0.92094, Convergence: -0.002804\n",
      "Epoch: 247, Loss: 2503.65581, Residuals: -0.91974, Convergence: -0.000257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence 15101.029\n",
      "\n",
      "Epoch: 247, Evidence: 15101.02930, Convergence: 0.000741\n",
      "Total samples: 184, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 383.99836, Residuals: -4.54153, Convergence:   inf\n",
      "Epoch: 1, Loss: 358.19579, Residuals: -4.42093, Convergence: 0.072035\n",
      "Epoch: 2, Loss: 337.02580, Residuals: -4.25576, Convergence: 0.062814\n",
      "Epoch: 3, Loss: 320.85743, Residuals: -4.09028, Convergence: 0.050391\n",
      "Epoch: 4, Loss: 308.51307, Residuals: -3.94452, Convergence: 0.040012\n",
      "Epoch: 5, Loss: 298.71017, Residuals: -3.81558, Convergence: 0.032817\n",
      "Epoch: 6, Loss: 290.74476, Residuals: -3.70333, Convergence: 0.027397\n",
      "Epoch: 7, Loss: 284.12585, Residuals: -3.60727, Convergence: 0.023296\n",
      "Epoch: 8, Loss: 278.49012, Residuals: -3.52524, Convergence: 0.020237\n",
      "Epoch: 9, Loss: 273.58145, Residuals: -3.45488, Convergence: 0.017942\n",
      "Epoch: 10, Loss: 269.21743, Residuals: -3.39407, Convergence: 0.016210\n",
      "Epoch: 11, Loss: 265.26556, Residuals: -3.34102, Convergence: 0.014898\n",
      "Epoch: 12, Loss: 261.62907, Residuals: -3.29421, Convergence: 0.013899\n",
      "Epoch: 13, Loss: 258.23788, Residuals: -3.25233, Convergence: 0.013132\n",
      "Epoch: 14, Loss: 255.04259, Residuals: -3.21423, Convergence: 0.012528\n",
      "Epoch: 15, Loss: 252.01136, Residuals: -3.17897, Convergence: 0.012028\n",
      "Epoch: 16, Loss: 249.12725, Residuals: -3.14588, Convergence: 0.011577\n",
      "Epoch: 17, Loss: 246.37864, Residuals: -3.11454, Convergence: 0.011156\n",
      "Epoch: 18, Loss: 243.74558, Residuals: -3.08452, Convergence: 0.010802\n",
      "Epoch: 19, Loss: 241.19682, Residuals: -3.05532, Convergence: 0.010567\n",
      "Epoch: 20, Loss: 238.69661, Residuals: -3.02637, Convergence: 0.010474\n",
      "Epoch: 21, Loss: 236.21271, Residuals: -2.99716, Convergence: 0.010516\n",
      "Epoch: 22, Loss: 233.71988, Residuals: -2.96734, Convergence: 0.010666\n",
      "Epoch: 23, Loss: 231.19249, Residuals: -2.93660, Convergence: 0.010932\n",
      "Epoch: 24, Loss: 228.58883, Residuals: -2.90450, Convergence: 0.011390\n",
      "Epoch: 25, Loss: 225.85232, Residuals: -2.87036, Convergence: 0.012116\n",
      "Epoch: 26, Loss: 222.97431, Residuals: -2.83401, Convergence: 0.012907\n",
      "Epoch: 27, Loss: 220.07185, Residuals: -2.79662, Convergence: 0.013189\n",
      "Epoch: 28, Loss: 217.26315, Residuals: -2.75957, Convergence: 0.012928\n",
      "Epoch: 29, Loss: 214.56928, Residuals: -2.72327, Convergence: 0.012555\n",
      "Epoch: 30, Loss: 211.97293, Residuals: -2.68762, Convergence: 0.012248\n",
      "Epoch: 31, Loss: 209.45460, Residuals: -2.65249, Convergence: 0.012023\n",
      "Epoch: 32, Loss: 206.99963, Residuals: -2.61774, Convergence: 0.011860\n",
      "Epoch: 33, Loss: 204.59838, Residuals: -2.58328, Convergence: 0.011736\n",
      "Epoch: 34, Loss: 202.24514, Residuals: -2.54906, Convergence: 0.011636\n",
      "Epoch: 35, Loss: 199.93715, Residuals: -2.51503, Convergence: 0.011544\n",
      "Epoch: 36, Loss: 197.67371, Residuals: -2.48116, Convergence: 0.011450\n",
      "Epoch: 37, Loss: 195.45549, Residuals: -2.44745, Convergence: 0.011349\n",
      "Epoch: 38, Loss: 193.28392, Residuals: -2.41388, Convergence: 0.011235\n",
      "Epoch: 39, Loss: 191.16079, Residuals: -2.38047, Convergence: 0.011107\n",
      "Epoch: 40, Loss: 189.08788, Residuals: -2.34722, Convergence: 0.010963\n",
      "Epoch: 41, Loss: 187.06674, Residuals: -2.31416, Convergence: 0.010804\n",
      "Epoch: 42, Loss: 185.09853, Residuals: -2.28128, Convergence: 0.010633\n",
      "Epoch: 43, Loss: 183.18405, Residuals: -2.24863, Convergence: 0.010451\n",
      "Epoch: 44, Loss: 181.32370, Residuals: -2.21621, Convergence: 0.010260\n",
      "Epoch: 45, Loss: 179.51762, Residuals: -2.18404, Convergence: 0.010061\n",
      "Epoch: 46, Loss: 177.76591, Residuals: -2.15215, Convergence: 0.009854\n",
      "Epoch: 47, Loss: 176.06879, Residuals: -2.12054, Convergence: 0.009639\n",
      "Epoch: 48, Loss: 174.42671, Residuals: -2.08926, Convergence: 0.009414\n",
      "Epoch: 49, Loss: 172.84039, Residuals: -2.05832, Convergence: 0.009178\n",
      "Epoch: 50, Loss: 171.31064, Residuals: -2.02777, Convergence: 0.008930\n",
      "Epoch: 51, Loss: 169.83820, Residuals: -1.99765, Convergence: 0.008670\n",
      "Epoch: 52, Loss: 168.42352, Residuals: -1.96799, Convergence: 0.008400\n",
      "Epoch: 53, Loss: 167.06666, Residuals: -1.93884, Convergence: 0.008122\n",
      "Epoch: 54, Loss: 165.76723, Residuals: -1.91025, Convergence: 0.007839\n",
      "Epoch: 55, Loss: 164.52439, Residuals: -1.88224, Convergence: 0.007554\n",
      "Epoch: 56, Loss: 163.33694, Residuals: -1.85485, Convergence: 0.007270\n",
      "Epoch: 57, Loss: 162.20331, Residuals: -1.82811, Convergence: 0.006989\n",
      "Epoch: 58, Loss: 161.12168, Residuals: -1.80204, Convergence: 0.006713\n",
      "Epoch: 59, Loss: 160.08995, Residuals: -1.77666, Convergence: 0.006445\n",
      "Epoch: 60, Loss: 159.10586, Residuals: -1.75198, Convergence: 0.006185\n",
      "Epoch: 61, Loss: 158.16705, Residuals: -1.72799, Convergence: 0.005936\n",
      "Epoch: 62, Loss: 157.27111, Residuals: -1.70469, Convergence: 0.005697\n",
      "Epoch: 63, Loss: 156.41575, Residuals: -1.68208, Convergence: 0.005469\n",
      "Epoch: 64, Loss: 155.59882, Residuals: -1.66013, Convergence: 0.005250\n",
      "Epoch: 65, Loss: 154.81844, Residuals: -1.63885, Convergence: 0.005041\n",
      "Epoch: 66, Loss: 154.07299, Residuals: -1.61822, Convergence: 0.004838\n",
      "Epoch: 67, Loss: 153.36108, Residuals: -1.59822, Convergence: 0.004642\n",
      "Epoch: 68, Loss: 152.68150, Residuals: -1.57887, Convergence: 0.004451\n",
      "Epoch: 69, Loss: 152.03322, Residuals: -1.56015, Convergence: 0.004264\n",
      "Epoch: 70, Loss: 151.41524, Residuals: -1.54206, Convergence: 0.004081\n",
      "Epoch: 71, Loss: 150.82661, Residuals: -1.52459, Convergence: 0.003903\n",
      "Epoch: 72, Loss: 150.26640, Residuals: -1.50775, Convergence: 0.003728\n",
      "Epoch: 73, Loss: 149.73366, Residuals: -1.49152, Convergence: 0.003558\n",
      "Epoch: 74, Loss: 149.22741, Residuals: -1.47591, Convergence: 0.003392\n",
      "Epoch: 75, Loss: 148.74665, Residuals: -1.46089, Convergence: 0.003232\n",
      "Epoch: 76, Loss: 148.29037, Residuals: -1.44647, Convergence: 0.003077\n",
      "Epoch: 77, Loss: 147.85754, Residuals: -1.43262, Convergence: 0.002927\n",
      "Epoch: 78, Loss: 147.44714, Residuals: -1.41934, Convergence: 0.002783\n",
      "Epoch: 79, Loss: 147.05815, Residuals: -1.40661, Convergence: 0.002645\n",
      "Epoch: 80, Loss: 146.68958, Residuals: -1.39441, Convergence: 0.002513\n",
      "Epoch: 81, Loss: 146.34047, Residuals: -1.38273, Convergence: 0.002386\n",
      "Epoch: 82, Loss: 146.00987, Residuals: -1.37155, Convergence: 0.002264\n",
      "Epoch: 83, Loss: 145.69689, Residuals: -1.36085, Convergence: 0.002148\n",
      "Epoch: 84, Loss: 145.40066, Residuals: -1.35062, Convergence: 0.002037\n",
      "Epoch: 85, Loss: 145.12036, Residuals: -1.34083, Convergence: 0.001931\n",
      "Epoch: 86, Loss: 144.85524, Residuals: -1.33148, Convergence: 0.001830\n",
      "Epoch: 87, Loss: 144.60456, Residuals: -1.32254, Convergence: 0.001734\n",
      "Epoch: 88, Loss: 144.36765, Residuals: -1.31400, Convergence: 0.001641\n",
      "Epoch: 89, Loss: 144.14387, Residuals: -1.30584, Convergence: 0.001552\n",
      "Epoch: 90, Loss: 143.93262, Residuals: -1.29805, Convergence: 0.001468\n",
      "Epoch: 91, Loss: 143.73336, Residuals: -1.29061, Convergence: 0.001386\n",
      "Epoch: 92, Loss: 143.54557, Residuals: -1.28352, Convergence: 0.001308\n",
      "Epoch: 93, Loss: 143.36878, Residuals: -1.27675, Convergence: 0.001233\n",
      "Epoch: 94, Loss: 143.20252, Residuals: -1.27030, Convergence: 0.001161\n",
      "Epoch: 95, Loss: 143.04635, Residuals: -1.26416, Convergence: 0.001092\n",
      "Epoch: 96, Loss: 142.89983, Residuals: -1.25832, Convergence: 0.001025\n",
      "Epoch: 97, Loss: 142.76251, Residuals: -1.25277, Convergence: 0.000962\n",
      "Evidence -184.520\n",
      "\n",
      "Epoch: 97, Evidence: -184.52011, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 7.24e-01\n",
      "Epoch: 97, Loss: 1380.26358, Residuals: -1.25277, Convergence:   inf\n",
      "Epoch: 98, Loss: 1316.82702, Residuals: -1.28372, Convergence: 0.048174\n",
      "Epoch: 99, Loss: 1268.92563, Residuals: -1.30775, Convergence: 0.037750\n",
      "Epoch: 100, Loss: 1232.98111, Residuals: -1.32464, Convergence: 0.029153\n",
      "Epoch: 101, Loss: 1205.10473, Residuals: -1.33619, Convergence: 0.023132\n",
      "Epoch: 102, Loss: 1182.57551, Residuals: -1.34451, Convergence: 0.019051\n",
      "Epoch: 103, Loss: 1163.87231, Residuals: -1.35072, Convergence: 0.016070\n",
      "Epoch: 104, Loss: 1148.08677, Residuals: -1.35530, Convergence: 0.013749\n",
      "Epoch: 105, Loss: 1134.59771, Residuals: -1.35849, Convergence: 0.011889\n",
      "Epoch: 106, Loss: 1122.94168, Residuals: -1.36045, Convergence: 0.010380\n",
      "Epoch: 107, Loss: 1112.75344, Residuals: -1.36130, Convergence: 0.009156\n",
      "Epoch: 108, Loss: 1103.73609, Residuals: -1.36113, Convergence: 0.008170\n",
      "Epoch: 109, Loss: 1095.64280, Residuals: -1.36004, Convergence: 0.007387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 110, Loss: 1088.26215, Residuals: -1.35806, Convergence: 0.006782\n",
      "Epoch: 111, Loss: 1081.40979, Residuals: -1.35525, Convergence: 0.006337\n",
      "Epoch: 112, Loss: 1074.92282, Residuals: -1.35162, Convergence: 0.006035\n",
      "Epoch: 113, Loss: 1068.65726, Residuals: -1.34717, Convergence: 0.005863\n",
      "Epoch: 114, Loss: 1062.49169, Residuals: -1.34193, Convergence: 0.005803\n",
      "Epoch: 115, Loss: 1056.33808, Residuals: -1.33592, Convergence: 0.005825\n",
      "Epoch: 116, Loss: 1050.15858, Residuals: -1.32922, Convergence: 0.005884\n",
      "Epoch: 117, Loss: 1043.98016, Residuals: -1.32190, Convergence: 0.005918\n",
      "Epoch: 118, Loss: 1037.88716, Residuals: -1.31410, Convergence: 0.005871\n",
      "Epoch: 119, Loss: 1031.98884, Residuals: -1.30593, Convergence: 0.005715\n",
      "Epoch: 120, Loss: 1026.37932, Residuals: -1.29750, Convergence: 0.005465\n",
      "Epoch: 121, Loss: 1021.11172, Residuals: -1.28890, Convergence: 0.005159\n",
      "Epoch: 122, Loss: 1016.19880, Residuals: -1.28022, Convergence: 0.004835\n",
      "Epoch: 123, Loss: 1011.62669, Residuals: -1.27153, Convergence: 0.004520\n",
      "Epoch: 124, Loss: 1007.36737, Residuals: -1.26288, Convergence: 0.004228\n",
      "Epoch: 125, Loss: 1003.38944, Residuals: -1.25434, Convergence: 0.003965\n",
      "Epoch: 126, Loss: 999.66282, Residuals: -1.24593, Convergence: 0.003728\n",
      "Epoch: 127, Loss: 996.16060, Residuals: -1.23770, Convergence: 0.003516\n",
      "Epoch: 128, Loss: 992.86111, Residuals: -1.22966, Convergence: 0.003323\n",
      "Epoch: 129, Loss: 989.74567, Residuals: -1.22185, Convergence: 0.003148\n",
      "Epoch: 130, Loss: 986.79966, Residuals: -1.21428, Convergence: 0.002985\n",
      "Epoch: 131, Loss: 984.01171, Residuals: -1.20697, Convergence: 0.002833\n",
      "Epoch: 132, Loss: 981.37181, Residuals: -1.19993, Convergence: 0.002690\n",
      "Epoch: 133, Loss: 978.87168, Residuals: -1.19317, Convergence: 0.002554\n",
      "Epoch: 134, Loss: 976.50394, Residuals: -1.18670, Convergence: 0.002425\n",
      "Epoch: 135, Loss: 974.26152, Residuals: -1.18052, Convergence: 0.002302\n",
      "Epoch: 136, Loss: 972.13748, Residuals: -1.17463, Convergence: 0.002185\n",
      "Epoch: 137, Loss: 970.12509, Residuals: -1.16902, Convergence: 0.002074\n",
      "Epoch: 138, Loss: 968.21737, Residuals: -1.16368, Convergence: 0.001970\n",
      "Epoch: 139, Loss: 966.40741, Residuals: -1.15862, Convergence: 0.001873\n",
      "Epoch: 140, Loss: 964.68849, Residuals: -1.15381, Convergence: 0.001782\n",
      "Epoch: 141, Loss: 963.05368, Residuals: -1.14925, Convergence: 0.001698\n",
      "Epoch: 142, Loss: 961.49710, Residuals: -1.14493, Convergence: 0.001619\n",
      "Epoch: 143, Loss: 960.01250, Residuals: -1.14082, Convergence: 0.001546\n",
      "Epoch: 144, Loss: 958.59444, Residuals: -1.13693, Convergence: 0.001479\n",
      "Epoch: 145, Loss: 957.23762, Residuals: -1.13323, Convergence: 0.001417\n",
      "Epoch: 146, Loss: 955.93703, Residuals: -1.12972, Convergence: 0.001361\n",
      "Epoch: 147, Loss: 954.68850, Residuals: -1.12638, Convergence: 0.001308\n",
      "Epoch: 148, Loss: 953.48777, Residuals: -1.12321, Convergence: 0.001259\n",
      "Epoch: 149, Loss: 952.33132, Residuals: -1.12018, Convergence: 0.001214\n",
      "Epoch: 150, Loss: 951.21578, Residuals: -1.11729, Convergence: 0.001173\n",
      "Epoch: 151, Loss: 950.13828, Residuals: -1.11454, Convergence: 0.001134\n",
      "Epoch: 152, Loss: 949.09541, Residuals: -1.11191, Convergence: 0.001099\n",
      "Epoch: 153, Loss: 948.08519, Residuals: -1.10939, Convergence: 0.001066\n",
      "Epoch: 154, Loss: 947.10511, Residuals: -1.10698, Convergence: 0.001035\n",
      "Epoch: 155, Loss: 946.15291, Residuals: -1.10466, Convergence: 0.001006\n",
      "Epoch: 156, Loss: 945.22686, Residuals: -1.10244, Convergence: 0.000980\n",
      "Evidence 11177.677\n",
      "\n",
      "Epoch: 156, Evidence: 11177.67676, Convergence: 1.016508\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 5.77e-01\n",
      "Epoch: 156, Loss: 2367.03341, Residuals: -1.10244, Convergence:   inf\n",
      "Epoch: 157, Loss: 2328.00238, Residuals: -1.11330, Convergence: 0.016766\n",
      "Epoch: 158, Loss: 2299.52813, Residuals: -1.11252, Convergence: 0.012383\n",
      "Epoch: 159, Loss: 2275.55810, Residuals: -1.11069, Convergence: 0.010534\n",
      "Epoch: 160, Loss: 2255.14764, Residuals: -1.10847, Convergence: 0.009051\n",
      "Epoch: 161, Loss: 2237.61489, Residuals: -1.10596, Convergence: 0.007835\n",
      "Epoch: 162, Loss: 2222.42845, Residuals: -1.10322, Convergence: 0.006833\n",
      "Epoch: 163, Loss: 2209.15803, Residuals: -1.10027, Convergence: 0.006007\n",
      "Epoch: 164, Loss: 2197.44988, Residuals: -1.09714, Convergence: 0.005328\n",
      "Epoch: 165, Loss: 2187.01093, Residuals: -1.09385, Convergence: 0.004773\n",
      "Epoch: 166, Loss: 2177.59934, Residuals: -1.09042, Convergence: 0.004322\n",
      "Epoch: 167, Loss: 2169.01563, Residuals: -1.08684, Convergence: 0.003957\n",
      "Epoch: 168, Loss: 2161.10340, Residuals: -1.08313, Convergence: 0.003661\n",
      "Epoch: 169, Loss: 2153.75394, Residuals: -1.07931, Convergence: 0.003412\n",
      "Epoch: 170, Loss: 2146.90339, Residuals: -1.07539, Convergence: 0.003191\n",
      "Epoch: 171, Loss: 2140.52309, Residuals: -1.07143, Convergence: 0.002981\n",
      "Epoch: 172, Loss: 2134.60351, Residuals: -1.06749, Convergence: 0.002773\n",
      "Epoch: 173, Loss: 2129.13611, Residuals: -1.06360, Convergence: 0.002568\n",
      "Epoch: 174, Loss: 2124.10715, Residuals: -1.05981, Convergence: 0.002368\n",
      "Epoch: 175, Loss: 2119.49280, Residuals: -1.05616, Convergence: 0.002177\n",
      "Epoch: 176, Loss: 2115.26573, Residuals: -1.05266, Convergence: 0.001998\n",
      "Epoch: 177, Loss: 2111.39243, Residuals: -1.04932, Convergence: 0.001834\n",
      "Epoch: 178, Loss: 2107.84059, Residuals: -1.04615, Convergence: 0.001685\n",
      "Epoch: 179, Loss: 2104.57794, Residuals: -1.04315, Convergence: 0.001550\n",
      "Epoch: 180, Loss: 2101.57247, Residuals: -1.04031, Convergence: 0.001430\n",
      "Epoch: 181, Loss: 2098.79612, Residuals: -1.03763, Convergence: 0.001323\n",
      "Epoch: 182, Loss: 2096.22205, Residuals: -1.03509, Convergence: 0.001228\n",
      "Epoch: 183, Loss: 2093.82706, Residuals: -1.03270, Convergence: 0.001144\n",
      "Epoch: 184, Loss: 2091.58985, Residuals: -1.03044, Convergence: 0.001070\n",
      "Epoch: 185, Loss: 2089.49288, Residuals: -1.02831, Convergence: 0.001004\n",
      "Epoch: 186, Loss: 2087.51902, Residuals: -1.02629, Convergence: 0.000946\n",
      "Evidence 14393.988\n",
      "\n",
      "Epoch: 186, Evidence: 14393.98828, Convergence: 0.223448\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 4.40e-01\n",
      "Epoch: 186, Loss: 2493.75383, Residuals: -1.02629, Convergence:   inf\n",
      "Epoch: 187, Loss: 2479.17133, Residuals: -1.02281, Convergence: 0.005882\n",
      "Epoch: 188, Loss: 2467.29759, Residuals: -1.01882, Convergence: 0.004812\n",
      "Epoch: 189, Loss: 2457.12863, Residuals: -1.01500, Convergence: 0.004139\n",
      "Epoch: 190, Loss: 2448.34995, Residuals: -1.01142, Convergence: 0.003586\n",
      "Epoch: 191, Loss: 2440.72356, Residuals: -1.00810, Convergence: 0.003125\n",
      "Epoch: 192, Loss: 2434.05659, Residuals: -1.00504, Convergence: 0.002739\n",
      "Epoch: 193, Loss: 2428.19216, Residuals: -1.00223, Convergence: 0.002415\n",
      "Epoch: 194, Loss: 2422.99840, Residuals: -0.99964, Convergence: 0.002144\n",
      "Epoch: 195, Loss: 2418.36673, Residuals: -0.99727, Convergence: 0.001915\n",
      "Epoch: 196, Loss: 2414.20825, Residuals: -0.99510, Convergence: 0.001722\n",
      "Epoch: 197, Loss: 2410.44842, Residuals: -0.99310, Convergence: 0.001560\n",
      "Epoch: 198, Loss: 2407.02842, Residuals: -0.99128, Convergence: 0.001421\n",
      "Epoch: 199, Loss: 2403.89763, Residuals: -0.98961, Convergence: 0.001302\n",
      "Epoch: 200, Loss: 2401.01675, Residuals: -0.98808, Convergence: 0.001200\n",
      "Epoch: 201, Loss: 2398.35248, Residuals: -0.98668, Convergence: 0.001111\n",
      "Epoch: 202, Loss: 2395.87761, Residuals: -0.98539, Convergence: 0.001033\n",
      "Epoch: 203, Loss: 2393.56900, Residuals: -0.98422, Convergence: 0.000965\n",
      "Evidence 14823.151\n",
      "\n",
      "Epoch: 203, Evidence: 14823.15137, Convergence: 0.028952\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 3.35e-01\n",
      "Epoch: 203, Loss: 2498.81702, Residuals: -0.98422, Convergence:   inf\n",
      "Epoch: 204, Loss: 2492.16384, Residuals: -0.98083, Convergence: 0.002670\n",
      "Epoch: 205, Loss: 2486.62615, Residuals: -0.97788, Convergence: 0.002227\n",
      "Epoch: 206, Loss: 2481.90611, Residuals: -0.97537, Convergence: 0.001902\n",
      "Epoch: 207, Loss: 2477.82827, Residuals: -0.97324, Convergence: 0.001646\n",
      "Epoch: 208, Loss: 2474.26047, Residuals: -0.97143, Convergence: 0.001442\n",
      "Epoch: 209, Loss: 2471.09946, Residuals: -0.96989, Convergence: 0.001279\n",
      "Epoch: 210, Loss: 2468.26703, Residuals: -0.96857, Convergence: 0.001148\n",
      "Epoch: 211, Loss: 2465.70322, Residuals: -0.96745, Convergence: 0.001040\n",
      "Epoch: 212, Loss: 2463.36108, Residuals: -0.96648, Convergence: 0.000951\n",
      "Evidence 14911.729\n",
      "\n",
      "Epoch: 212, Evidence: 14911.72852, Convergence: 0.005940\n",
      "Updating hyper-parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 184, Updated regularization: 2.61e-01\n",
      "Epoch: 212, Loss: 2500.32738, Residuals: -0.96648, Convergence:   inf\n",
      "Epoch: 213, Loss: 2496.38075, Residuals: -0.96406, Convergence: 0.001581\n",
      "Epoch: 214, Loss: 2493.07503, Residuals: -0.96212, Convergence: 0.001326\n",
      "Epoch: 215, Loss: 2490.22816, Residuals: -0.96057, Convergence: 0.001143\n",
      "Epoch: 216, Loss: 2487.72830, Residuals: -0.95933, Convergence: 0.001005\n",
      "Epoch: 217, Loss: 2485.49833, Residuals: -0.95834, Convergence: 0.000897\n",
      "Evidence 14943.176\n",
      "\n",
      "Epoch: 217, Evidence: 14943.17578, Convergence: 0.002104\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 2.09e-01\n",
      "Epoch: 217, Loss: 2501.27120, Residuals: -0.95834, Convergence:   inf\n",
      "Epoch: 218, Loss: 2498.44000, Residuals: -0.95648, Convergence: 0.001133\n",
      "Epoch: 219, Loss: 2496.04556, Residuals: -0.95506, Convergence: 0.000959\n",
      "Evidence 14955.802\n",
      "\n",
      "Epoch: 219, Evidence: 14955.80176, Convergence: 0.000844\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.73e-01\n",
      "Epoch: 219, Loss: 2502.02423, Residuals: -0.95506, Convergence:   inf\n",
      "Epoch: 220, Loss: 2497.48099, Residuals: -0.95250, Convergence: 0.001819\n",
      "Epoch: 221, Loss: 2494.02488, Residuals: -0.95093, Convergence: 0.001386\n",
      "Epoch: 222, Loss: 2491.17806, Residuals: -0.94993, Convergence: 0.001143\n",
      "Epoch: 223, Loss: 2488.72900, Residuals: -0.94936, Convergence: 0.000984\n",
      "Evidence 14973.621\n",
      "\n",
      "Epoch: 223, Evidence: 14973.62109, Convergence: 0.002033\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.43e-01\n",
      "Epoch: 223, Loss: 2502.30877, Residuals: -0.94936, Convergence:   inf\n",
      "Epoch: 224, Loss: 2499.30060, Residuals: -0.94725, Convergence: 0.001204\n",
      "Epoch: 225, Loss: 2496.88014, Residuals: -0.94627, Convergence: 0.000969\n",
      "Evidence 14984.374\n",
      "\n",
      "Epoch: 225, Evidence: 14984.37402, Convergence: 0.000718\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.21e-01\n",
      "Epoch: 225, Loss: 2502.60552, Residuals: -0.94627, Convergence:   inf\n",
      "Epoch: 226, Loss: 2498.15516, Residuals: -0.94354, Convergence: 0.001781\n",
      "Epoch: 227, Loss: 2494.84760, Residuals: -0.94481, Convergence: 0.001326\n",
      "Epoch: 228, Loss: 2492.10618, Residuals: -0.94549, Convergence: 0.001100\n",
      "Epoch: 229, Loss: 2489.67716, Residuals: -0.94810, Convergence: 0.000976\n",
      "Evidence 15000.363\n",
      "\n",
      "Epoch: 229, Evidence: 15000.36328, Convergence: 0.001783\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.08e-01\n",
      "Epoch: 229, Loss: 2502.07390, Residuals: -0.94810, Convergence:   inf\n",
      "Epoch: 230, Loss: 2500.65063, Residuals: -0.94751, Convergence: 0.000569\n",
      "Evidence 15006.424\n",
      "\n",
      "Epoch: 230, Evidence: 15006.42383, Convergence: 0.000404\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 8.80e-02\n",
      "Epoch: 230, Loss: 2503.09788, Residuals: -0.94751, Convergence:   inf\n",
      "Epoch: 231, Loss: 2552.43285, Residuals: -0.99190, Convergence: -0.019329\n",
      "Epoch: 231, Loss: 2500.67315, Residuals: -0.94635, Convergence: 0.000970\n",
      "Evidence 15010.890\n",
      "\n",
      "Epoch: 231, Evidence: 15010.88965, Convergence: 0.000701\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 8.11e-02\n",
      "Epoch: 231, Loss: 2502.59700, Residuals: -0.94635, Convergence:   inf\n",
      "Epoch: 232, Loss: 2509.19455, Residuals: -0.95277, Convergence: -0.002629\n",
      "Epoch: 232, Loss: 2503.05737, Residuals: -0.94727, Convergence: -0.000184\n",
      "Evidence 15011.970\n",
      "\n",
      "Epoch: 232, Evidence: 15011.96973, Convergence: 0.000773\n",
      "Total samples: 184, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 383.68459, Residuals: -4.51596, Convergence:   inf\n",
      "Epoch: 1, Loss: 357.91305, Residuals: -4.39608, Convergence: 0.072005\n",
      "Epoch: 2, Loss: 336.76915, Residuals: -4.23155, Convergence: 0.062785\n",
      "Epoch: 3, Loss: 320.66437, Residuals: -4.06717, Convergence: 0.050223\n",
      "Epoch: 4, Loss: 308.37689, Residuals: -3.92208, Convergence: 0.039846\n",
      "Epoch: 5, Loss: 298.62972, Residuals: -3.79360, Convergence: 0.032640\n",
      "Epoch: 6, Loss: 290.71663, Residuals: -3.68169, Convergence: 0.027219\n",
      "Epoch: 7, Loss: 284.14500, Residuals: -3.58588, Convergence: 0.023128\n",
      "Epoch: 8, Loss: 278.55045, Residuals: -3.50408, Convergence: 0.020084\n",
      "Epoch: 9, Loss: 273.67651, Residuals: -3.43395, Convergence: 0.017809\n",
      "Epoch: 10, Loss: 269.34112, Residuals: -3.37338, Convergence: 0.016096\n",
      "Epoch: 11, Loss: 265.41278, Residuals: -3.32057, Convergence: 0.014801\n",
      "Epoch: 12, Loss: 261.79598, Residuals: -3.27401, Convergence: 0.013815\n",
      "Epoch: 13, Loss: 258.42183, Residuals: -3.23235, Convergence: 0.013057\n",
      "Epoch: 14, Loss: 255.24152, Residuals: -3.19444, Convergence: 0.012460\n",
      "Epoch: 15, Loss: 252.22280, Residuals: -3.15933, Convergence: 0.011968\n",
      "Epoch: 16, Loss: 249.34774, Residuals: -3.12634, Convergence: 0.011530\n",
      "Epoch: 17, Loss: 246.60439, Residuals: -3.09509, Convergence: 0.011124\n",
      "Epoch: 18, Loss: 243.97344, Residuals: -3.06522, Convergence: 0.010784\n",
      "Epoch: 19, Loss: 241.42394, Residuals: -3.03623, Convergence: 0.010560\n",
      "Epoch: 20, Loss: 238.92031, Residuals: -3.00755, Convergence: 0.010479\n",
      "Epoch: 21, Loss: 236.43175, Residuals: -2.97869, Convergence: 0.010525\n",
      "Epoch: 22, Loss: 233.93746, Residuals: -2.94931, Convergence: 0.010662\n",
      "Epoch: 23, Loss: 231.41928, Residuals: -2.91918, Convergence: 0.010881\n",
      "Epoch: 24, Loss: 228.84451, Residuals: -2.88792, Convergence: 0.011251\n",
      "Epoch: 25, Loss: 226.15982, Residuals: -2.85492, Convergence: 0.011871\n",
      "Epoch: 26, Loss: 223.32467, Residuals: -2.81963, Convergence: 0.012695\n",
      "Epoch: 27, Loss: 220.39917, Residuals: -2.78262, Convergence: 0.013274\n",
      "Epoch: 28, Loss: 217.51788, Residuals: -2.74534, Convergence: 0.013246\n",
      "Epoch: 29, Loss: 214.74051, Residuals: -2.70861, Convergence: 0.012934\n",
      "Epoch: 30, Loss: 212.06272, Residuals: -2.67250, Convergence: 0.012627\n",
      "Epoch: 31, Loss: 209.46772, Residuals: -2.63691, Convergence: 0.012389\n",
      "Epoch: 32, Loss: 206.94158, Residuals: -2.60173, Convergence: 0.012207\n",
      "Epoch: 33, Loss: 204.47518, Residuals: -2.56688, Convergence: 0.012062\n",
      "Epoch: 34, Loss: 202.06334, Residuals: -2.53229, Convergence: 0.011936\n",
      "Epoch: 35, Loss: 199.70365, Residuals: -2.49792, Convergence: 0.011816\n",
      "Epoch: 36, Loss: 197.39547, Residuals: -2.46374, Convergence: 0.011693\n",
      "Epoch: 37, Loss: 195.13919, Residuals: -2.42973, Convergence: 0.011562\n",
      "Epoch: 38, Loss: 192.93558, Residuals: -2.39590, Convergence: 0.011421\n",
      "Epoch: 39, Loss: 190.78557, Residuals: -2.36222, Convergence: 0.011269\n",
      "Epoch: 40, Loss: 188.68998, Residuals: -2.32871, Convergence: 0.011106\n",
      "Epoch: 41, Loss: 186.64958, Residuals: -2.29537, Convergence: 0.010932\n",
      "Epoch: 42, Loss: 184.66512, Residuals: -2.26222, Convergence: 0.010746\n",
      "Epoch: 43, Loss: 182.73751, Residuals: -2.22927, Convergence: 0.010549\n",
      "Epoch: 44, Loss: 180.86793, Residuals: -2.19656, Convergence: 0.010337\n",
      "Epoch: 45, Loss: 179.05784, Residuals: -2.16414, Convergence: 0.010109\n",
      "Epoch: 46, Loss: 177.30878, Residuals: -2.13204, Convergence: 0.009864\n",
      "Epoch: 47, Loss: 175.62213, Residuals: -2.10032, Convergence: 0.009604\n",
      "Epoch: 48, Loss: 173.99883, Residuals: -2.06903, Convergence: 0.009329\n",
      "Epoch: 49, Loss: 172.43919, Residuals: -2.03820, Convergence: 0.009045\n",
      "Epoch: 50, Loss: 170.94293, Residuals: -2.00787, Convergence: 0.008753\n",
      "Epoch: 51, Loss: 169.50925, Residuals: -1.97807, Convergence: 0.008458\n",
      "Epoch: 52, Loss: 168.13702, Residuals: -1.94883, Convergence: 0.008161\n",
      "Epoch: 53, Loss: 166.82487, Residuals: -1.92015, Convergence: 0.007865\n",
      "Epoch: 54, Loss: 165.57130, Residuals: -1.89208, Convergence: 0.007571\n",
      "Epoch: 55, Loss: 164.37467, Residuals: -1.86462, Convergence: 0.007280\n",
      "Epoch: 56, Loss: 163.23320, Residuals: -1.83780, Convergence: 0.006993\n",
      "Epoch: 57, Loss: 162.14494, Residuals: -1.81163, Convergence: 0.006712\n",
      "Epoch: 58, Loss: 161.10779, Residuals: -1.78615, Convergence: 0.006438\n",
      "Epoch: 59, Loss: 160.11945, Residuals: -1.76134, Convergence: 0.006173\n",
      "Epoch: 60, Loss: 159.17745, Residuals: -1.73722, Convergence: 0.005918\n",
      "Epoch: 61, Loss: 158.27923, Residuals: -1.71379, Convergence: 0.005675\n",
      "Epoch: 62, Loss: 157.42217, Residuals: -1.69104, Convergence: 0.005444\n",
      "Epoch: 63, Loss: 156.60368, Residuals: -1.66896, Convergence: 0.005227\n",
      "Epoch: 64, Loss: 155.82128, Residuals: -1.64753, Convergence: 0.005021\n",
      "Epoch: 65, Loss: 155.07270, Residuals: -1.62673, Convergence: 0.004827\n",
      "Epoch: 66, Loss: 154.35592, Residuals: -1.60656, Convergence: 0.004644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67, Loss: 153.66919, Residuals: -1.58699, Convergence: 0.004469\n",
      "Epoch: 68, Loss: 153.01102, Residuals: -1.56801, Convergence: 0.004302\n",
      "Epoch: 69, Loss: 152.38013, Residuals: -1.54962, Convergence: 0.004140\n",
      "Epoch: 70, Loss: 151.77547, Residuals: -1.53180, Convergence: 0.003984\n",
      "Epoch: 71, Loss: 151.19611, Residuals: -1.51454, Convergence: 0.003832\n",
      "Epoch: 72, Loss: 150.64124, Residuals: -1.49785, Convergence: 0.003683\n",
      "Epoch: 73, Loss: 150.11010, Residuals: -1.48171, Convergence: 0.003538\n",
      "Epoch: 74, Loss: 149.60198, Residuals: -1.46612, Convergence: 0.003396\n",
      "Epoch: 75, Loss: 149.11617, Residuals: -1.45107, Convergence: 0.003258\n",
      "Epoch: 76, Loss: 148.65197, Residuals: -1.43655, Convergence: 0.003123\n",
      "Epoch: 77, Loss: 148.20867, Residuals: -1.42256, Convergence: 0.002991\n",
      "Epoch: 78, Loss: 147.78553, Residuals: -1.40909, Convergence: 0.002863\n",
      "Epoch: 79, Loss: 147.38184, Residuals: -1.39612, Convergence: 0.002739\n",
      "Epoch: 80, Loss: 146.99686, Residuals: -1.38365, Convergence: 0.002619\n",
      "Epoch: 81, Loss: 146.62984, Residuals: -1.37167, Convergence: 0.002503\n",
      "Epoch: 82, Loss: 146.28006, Residuals: -1.36015, Convergence: 0.002391\n",
      "Epoch: 83, Loss: 145.94678, Residuals: -1.34909, Convergence: 0.002284\n",
      "Epoch: 84, Loss: 145.62932, Residuals: -1.33848, Convergence: 0.002180\n",
      "Epoch: 85, Loss: 145.32699, Residuals: -1.32830, Convergence: 0.002080\n",
      "Epoch: 86, Loss: 145.03914, Residuals: -1.31854, Convergence: 0.001985\n",
      "Epoch: 87, Loss: 144.76516, Residuals: -1.30918, Convergence: 0.001893\n",
      "Epoch: 88, Loss: 144.50444, Residuals: -1.30022, Convergence: 0.001804\n",
      "Epoch: 89, Loss: 144.25645, Residuals: -1.29162, Convergence: 0.001719\n",
      "Epoch: 90, Loss: 144.02065, Residuals: -1.28339, Convergence: 0.001637\n",
      "Epoch: 91, Loss: 143.79658, Residuals: -1.27551, Convergence: 0.001558\n",
      "Epoch: 92, Loss: 143.58378, Residuals: -1.26797, Convergence: 0.001482\n",
      "Epoch: 93, Loss: 143.38183, Residuals: -1.26075, Convergence: 0.001408\n",
      "Epoch: 94, Loss: 143.19035, Residuals: -1.25384, Convergence: 0.001337\n",
      "Epoch: 95, Loss: 143.00897, Residuals: -1.24724, Convergence: 0.001268\n",
      "Epoch: 96, Loss: 142.83739, Residuals: -1.24092, Convergence: 0.001201\n",
      "Epoch: 97, Loss: 142.67529, Residuals: -1.23489, Convergence: 0.001136\n",
      "Epoch: 98, Loss: 142.52238, Residuals: -1.22913, Convergence: 0.001073\n",
      "Epoch: 99, Loss: 142.37840, Residuals: -1.22364, Convergence: 0.001011\n",
      "Epoch: 100, Loss: 142.24308, Residuals: -1.21841, Convergence: 0.000951\n",
      "Evidence -184.062\n",
      "\n",
      "Epoch: 100, Evidence: -184.06229, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 7.25e-01\n",
      "Epoch: 100, Loss: 1385.96317, Residuals: -1.21841, Convergence:   inf\n",
      "Epoch: 101, Loss: 1320.82683, Residuals: -1.25058, Convergence: 0.049315\n",
      "Epoch: 102, Loss: 1271.31585, Residuals: -1.27579, Convergence: 0.038945\n",
      "Epoch: 103, Loss: 1234.09785, Residuals: -1.29379, Convergence: 0.030158\n",
      "Epoch: 104, Loss: 1205.30362, Residuals: -1.30635, Convergence: 0.023890\n",
      "Epoch: 105, Loss: 1182.10762, Residuals: -1.31561, Convergence: 0.019623\n",
      "Epoch: 106, Loss: 1162.89686, Residuals: -1.32270, Convergence: 0.016520\n",
      "Epoch: 107, Loss: 1146.70697, Residuals: -1.32808, Convergence: 0.014119\n",
      "Epoch: 108, Loss: 1132.88283, Residuals: -1.33201, Convergence: 0.012203\n",
      "Epoch: 109, Loss: 1120.94016, Residuals: -1.33464, Convergence: 0.010654\n",
      "Epoch: 110, Loss: 1110.50086, Residuals: -1.33610, Convergence: 0.009401\n",
      "Epoch: 111, Loss: 1101.26152, Residuals: -1.33649, Convergence: 0.008390\n",
      "Epoch: 112, Loss: 1092.97438, Residuals: -1.33590, Convergence: 0.007582\n",
      "Epoch: 113, Loss: 1085.43258, Residuals: -1.33439, Convergence: 0.006948\n",
      "Epoch: 114, Loss: 1078.46279, Residuals: -1.33203, Convergence: 0.006463\n",
      "Epoch: 115, Loss: 1071.91761, Residuals: -1.32885, Convergence: 0.006106\n",
      "Epoch: 116, Loss: 1065.67067, Residuals: -1.32490, Convergence: 0.005862\n",
      "Epoch: 117, Loss: 1059.61585, Residuals: -1.32019, Convergence: 0.005714\n",
      "Epoch: 118, Loss: 1053.66694, Residuals: -1.31476, Convergence: 0.005646\n",
      "Epoch: 119, Loss: 1047.76530, Residuals: -1.30868, Convergence: 0.005633\n",
      "Epoch: 120, Loss: 1041.89114, Residuals: -1.30201, Convergence: 0.005638\n",
      "Epoch: 121, Loss: 1036.07311, Residuals: -1.29487, Convergence: 0.005615\n",
      "Epoch: 122, Loss: 1030.38072, Residuals: -1.28737, Convergence: 0.005525\n",
      "Epoch: 123, Loss: 1024.89913, Residuals: -1.27960, Convergence: 0.005348\n",
      "Epoch: 124, Loss: 1019.69774, Residuals: -1.27166, Convergence: 0.005101\n",
      "Epoch: 125, Loss: 1014.81256, Residuals: -1.26361, Convergence: 0.004814\n",
      "Epoch: 126, Loss: 1010.24792, Residuals: -1.25551, Convergence: 0.004518\n",
      "Epoch: 127, Loss: 1005.98774, Residuals: -1.24744, Convergence: 0.004235\n",
      "Epoch: 128, Loss: 1002.00553, Residuals: -1.23942, Convergence: 0.003974\n",
      "Epoch: 129, Loss: 998.27305, Residuals: -1.23150, Convergence: 0.003739\n",
      "Epoch: 130, Loss: 994.76298, Residuals: -1.22371, Convergence: 0.003529\n",
      "Epoch: 131, Loss: 991.45220, Residuals: -1.21607, Convergence: 0.003339\n",
      "Epoch: 132, Loss: 988.32078, Residuals: -1.20862, Convergence: 0.003168\n",
      "Epoch: 133, Loss: 985.35151, Residuals: -1.20136, Convergence: 0.003013\n",
      "Epoch: 134, Loss: 982.53108, Residuals: -1.19432, Convergence: 0.002871\n",
      "Epoch: 135, Loss: 979.84798, Residuals: -1.18751, Convergence: 0.002738\n",
      "Epoch: 136, Loss: 977.29252, Residuals: -1.18092, Convergence: 0.002615\n",
      "Epoch: 137, Loss: 974.85594, Residuals: -1.17458, Convergence: 0.002499\n",
      "Epoch: 138, Loss: 972.53108, Residuals: -1.16847, Convergence: 0.002391\n",
      "Epoch: 139, Loss: 970.31087, Residuals: -1.16261, Convergence: 0.002288\n",
      "Epoch: 140, Loss: 968.18940, Residuals: -1.15699, Convergence: 0.002191\n",
      "Epoch: 141, Loss: 966.15993, Residuals: -1.15160, Convergence: 0.002101\n",
      "Epoch: 142, Loss: 964.21729, Residuals: -1.14644, Convergence: 0.002015\n",
      "Epoch: 143, Loss: 962.35554, Residuals: -1.14151, Convergence: 0.001935\n",
      "Epoch: 144, Loss: 960.56893, Residuals: -1.13679, Convergence: 0.001860\n",
      "Epoch: 145, Loss: 958.85276, Residuals: -1.13228, Convergence: 0.001790\n",
      "Epoch: 146, Loss: 957.20112, Residuals: -1.12796, Convergence: 0.001725\n",
      "Epoch: 147, Loss: 955.60952, Residuals: -1.12382, Convergence: 0.001666\n",
      "Epoch: 148, Loss: 954.07216, Residuals: -1.11986, Convergence: 0.001611\n",
      "Epoch: 149, Loss: 952.58445, Residuals: -1.11606, Convergence: 0.001562\n",
      "Epoch: 150, Loss: 951.14072, Residuals: -1.11241, Convergence: 0.001518\n",
      "Epoch: 151, Loss: 949.73582, Residuals: -1.10890, Convergence: 0.001479\n",
      "Epoch: 152, Loss: 948.36466, Residuals: -1.10552, Convergence: 0.001446\n",
      "Epoch: 153, Loss: 947.02111, Residuals: -1.10225, Convergence: 0.001419\n",
      "Epoch: 154, Loss: 945.70091, Residuals: -1.09907, Convergence: 0.001396\n",
      "Epoch: 155, Loss: 944.39887, Residuals: -1.09598, Convergence: 0.001379\n",
      "Epoch: 156, Loss: 943.11175, Residuals: -1.09297, Convergence: 0.001365\n",
      "Epoch: 157, Loss: 941.83714, Residuals: -1.09002, Convergence: 0.001353\n",
      "Epoch: 158, Loss: 940.57504, Residuals: -1.08713, Convergence: 0.001342\n",
      "Epoch: 159, Loss: 939.32680, Residuals: -1.08431, Convergence: 0.001329\n",
      "Epoch: 160, Loss: 938.09610, Residuals: -1.08154, Convergence: 0.001312\n",
      "Epoch: 161, Loss: 936.88779, Residuals: -1.07884, Convergence: 0.001290\n",
      "Epoch: 162, Loss: 935.70664, Residuals: -1.07621, Convergence: 0.001262\n",
      "Epoch: 163, Loss: 934.55787, Residuals: -1.07366, Convergence: 0.001229\n",
      "Epoch: 164, Loss: 933.44517, Residuals: -1.07120, Convergence: 0.001192\n",
      "Epoch: 165, Loss: 932.37167, Residuals: -1.06882, Convergence: 0.001151\n",
      "Epoch: 166, Loss: 931.33893, Residuals: -1.06653, Convergence: 0.001109\n",
      "Epoch: 167, Loss: 930.34746, Residuals: -1.06433, Convergence: 0.001066\n",
      "Epoch: 168, Loss: 929.39739, Residuals: -1.06221, Convergence: 0.001022\n",
      "Epoch: 169, Loss: 928.48798, Residuals: -1.06019, Convergence: 0.000979\n",
      "Evidence 11298.766\n",
      "\n",
      "Epoch: 169, Evidence: 11298.76562, Convergence: 1.016291\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 5.75e-01\n",
      "Epoch: 169, Loss: 2371.50948, Residuals: -1.06019, Convergence:   inf\n",
      "Epoch: 170, Loss: 2331.94789, Residuals: -1.06930, Convergence: 0.016965\n",
      "Epoch: 171, Loss: 2304.33365, Residuals: -1.06812, Convergence: 0.011984\n",
      "Epoch: 172, Loss: 2281.50953, Residuals: -1.06607, Convergence: 0.010004\n",
      "Epoch: 173, Loss: 2262.38081, Residuals: -1.06376, Convergence: 0.008455\n",
      "Epoch: 174, Loss: 2246.18716, Residuals: -1.06132, Convergence: 0.007209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 175, Loss: 2232.34660, Residuals: -1.05879, Convergence: 0.006200\n",
      "Epoch: 176, Loss: 2220.39204, Residuals: -1.05621, Convergence: 0.005384\n",
      "Epoch: 177, Loss: 2209.94284, Residuals: -1.05358, Convergence: 0.004728\n",
      "Epoch: 178, Loss: 2200.68746, Residuals: -1.05089, Convergence: 0.004206\n",
      "Epoch: 179, Loss: 2192.37107, Residuals: -1.04812, Convergence: 0.003793\n",
      "Epoch: 180, Loss: 2184.79986, Residuals: -1.04525, Convergence: 0.003465\n",
      "Epoch: 181, Loss: 2177.83351, Residuals: -1.04227, Convergence: 0.003199\n",
      "Epoch: 182, Loss: 2171.38390, Residuals: -1.03921, Convergence: 0.002970\n",
      "Epoch: 183, Loss: 2165.39518, Residuals: -1.03609, Convergence: 0.002766\n",
      "Epoch: 184, Loss: 2159.83081, Residuals: -1.03296, Convergence: 0.002576\n",
      "Epoch: 185, Loss: 2154.66034, Residuals: -1.02985, Convergence: 0.002400\n",
      "Epoch: 186, Loss: 2149.85438, Residuals: -1.02680, Convergence: 0.002235\n",
      "Epoch: 187, Loss: 2145.38556, Residuals: -1.02384, Convergence: 0.002083\n",
      "Epoch: 188, Loss: 2141.22633, Residuals: -1.02097, Convergence: 0.001942\n",
      "Epoch: 189, Loss: 2137.35381, Residuals: -1.01822, Convergence: 0.001812\n",
      "Epoch: 190, Loss: 2133.74618, Residuals: -1.01559, Convergence: 0.001691\n",
      "Epoch: 191, Loss: 2130.38466, Residuals: -1.01308, Convergence: 0.001578\n",
      "Epoch: 192, Loss: 2127.25122, Residuals: -1.01070, Convergence: 0.001473\n",
      "Epoch: 193, Loss: 2124.32992, Residuals: -1.00845, Convergence: 0.001375\n",
      "Epoch: 194, Loss: 2121.60543, Residuals: -1.00631, Convergence: 0.001284\n",
      "Epoch: 195, Loss: 2119.06369, Residuals: -1.00428, Convergence: 0.001199\n",
      "Epoch: 196, Loss: 2116.69017, Residuals: -1.00237, Convergence: 0.001121\n",
      "Epoch: 197, Loss: 2114.47212, Residuals: -1.00055, Convergence: 0.001049\n",
      "Epoch: 198, Loss: 2112.39839, Residuals: -0.99883, Convergence: 0.000982\n",
      "Evidence 14474.387\n",
      "\n",
      "Epoch: 198, Evidence: 14474.38672, Convergence: 0.219396\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 4.36e-01\n",
      "Epoch: 198, Loss: 2496.06681, Residuals: -0.99883, Convergence:   inf\n",
      "Epoch: 199, Loss: 2482.41742, Residuals: -0.99609, Convergence: 0.005498\n",
      "Epoch: 200, Loss: 2471.27941, Residuals: -0.99278, Convergence: 0.004507\n",
      "Epoch: 201, Loss: 2461.69823, Residuals: -0.98958, Convergence: 0.003892\n",
      "Epoch: 202, Loss: 2453.40007, Residuals: -0.98664, Convergence: 0.003382\n",
      "Epoch: 203, Loss: 2446.18036, Residuals: -0.98399, Convergence: 0.002951\n",
      "Epoch: 204, Loss: 2439.87083, Residuals: -0.98161, Convergence: 0.002586\n",
      "Epoch: 205, Loss: 2434.32937, Residuals: -0.97947, Convergence: 0.002276\n",
      "Epoch: 206, Loss: 2429.43652, Residuals: -0.97756, Convergence: 0.002014\n",
      "Epoch: 207, Loss: 2425.09143, Residuals: -0.97584, Convergence: 0.001792\n",
      "Epoch: 208, Loss: 2421.21066, Residuals: -0.97429, Convergence: 0.001603\n",
      "Epoch: 209, Loss: 2417.72281, Residuals: -0.97290, Convergence: 0.001443\n",
      "Epoch: 210, Loss: 2414.57058, Residuals: -0.97165, Convergence: 0.001306\n",
      "Epoch: 211, Loss: 2411.70568, Residuals: -0.97051, Convergence: 0.001188\n",
      "Epoch: 212, Loss: 2409.08767, Residuals: -0.96949, Convergence: 0.001087\n",
      "Epoch: 213, Loss: 2406.68386, Residuals: -0.96855, Convergence: 0.000999\n",
      "Evidence 14849.086\n",
      "\n",
      "Epoch: 213, Evidence: 14849.08594, Convergence: 0.025234\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 3.33e-01\n",
      "Epoch: 213, Loss: 2501.60042, Residuals: -0.96855, Convergence:   inf\n",
      "Epoch: 214, Loss: 2494.96295, Residuals: -0.96556, Convergence: 0.002660\n",
      "Epoch: 215, Loss: 2489.47668, Residuals: -0.96300, Convergence: 0.002204\n",
      "Epoch: 216, Loss: 2484.82491, Residuals: -0.96092, Convergence: 0.001872\n",
      "Epoch: 217, Loss: 2480.82637, Residuals: -0.95923, Convergence: 0.001612\n",
      "Epoch: 218, Loss: 2477.34722, Residuals: -0.95785, Convergence: 0.001404\n",
      "Epoch: 219, Loss: 2474.28152, Residuals: -0.95670, Convergence: 0.001239\n",
      "Epoch: 220, Loss: 2471.55019, Residuals: -0.95575, Convergence: 0.001105\n",
      "Epoch: 221, Loss: 2469.09080, Residuals: -0.95495, Convergence: 0.000996\n",
      "Evidence 14929.890\n",
      "\n",
      "Epoch: 221, Evidence: 14929.88965, Convergence: 0.005412\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 2.61e-01\n",
      "Epoch: 221, Loss: 2503.24312, Residuals: -0.95495, Convergence:   inf\n",
      "Epoch: 222, Loss: 2499.20021, Residuals: -0.95277, Convergence: 0.001618\n",
      "Epoch: 223, Loss: 2495.84287, Residuals: -0.95111, Convergence: 0.001345\n",
      "Epoch: 224, Loss: 2492.97394, Residuals: -0.94985, Convergence: 0.001151\n",
      "Epoch: 225, Loss: 2490.47539, Residuals: -0.94888, Convergence: 0.001003\n",
      "Epoch: 226, Loss: 2488.26395, Residuals: -0.94812, Convergence: 0.000889\n",
      "Evidence 14960.791\n",
      "\n",
      "Epoch: 226, Evidence: 14960.79102, Convergence: 0.002065\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 2.09e-01\n",
      "Epoch: 226, Loss: 2504.14034, Residuals: -0.94812, Convergence:   inf\n",
      "Epoch: 227, Loss: 2501.27844, Residuals: -0.94652, Convergence: 0.001144\n",
      "Epoch: 228, Loss: 2498.87887, Residuals: -0.94537, Convergence: 0.000960\n",
      "Evidence 14973.654\n",
      "\n",
      "Epoch: 228, Evidence: 14973.65430, Convergence: 0.000859\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.74e-01\n",
      "Epoch: 228, Loss: 2504.84598, Residuals: -0.94537, Convergence:   inf\n",
      "Epoch: 229, Loss: 2500.33450, Residuals: -0.94339, Convergence: 0.001804\n",
      "Epoch: 230, Loss: 2496.91424, Residuals: -0.94231, Convergence: 0.001370\n",
      "Epoch: 231, Loss: 2494.13572, Residuals: -0.94168, Convergence: 0.001114\n",
      "Epoch: 232, Loss: 2491.77347, Residuals: -0.94141, Convergence: 0.000948\n",
      "Evidence 14991.367\n",
      "\n",
      "Epoch: 232, Evidence: 14991.36719, Convergence: 0.002040\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.44e-01\n",
      "Epoch: 232, Loss: 2505.04708, Residuals: -0.94141, Convergence:   inf\n",
      "Epoch: 233, Loss: 2502.08252, Residuals: -0.93958, Convergence: 0.001185\n",
      "Epoch: 234, Loss: 2499.73115, Residuals: -0.93884, Convergence: 0.000941\n",
      "Evidence 15002.235\n",
      "\n",
      "Epoch: 234, Evidence: 15002.23535, Convergence: 0.000724\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.21e-01\n",
      "Epoch: 234, Loss: 2505.27609, Residuals: -0.93884, Convergence:   inf\n",
      "Epoch: 235, Loss: 2500.96950, Residuals: -0.93638, Convergence: 0.001722\n",
      "Epoch: 236, Loss: 2497.83307, Residuals: -0.93780, Convergence: 0.001256\n",
      "Epoch: 237, Loss: 2495.22414, Residuals: -0.93818, Convergence: 0.001046\n",
      "Epoch: 238, Loss: 2492.89842, Residuals: -0.94061, Convergence: 0.000933\n",
      "Evidence 15017.787\n",
      "\n",
      "Epoch: 238, Evidence: 15017.78711, Convergence: 0.001759\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 1.08e-01\n",
      "Epoch: 238, Loss: 2504.60291, Residuals: -0.94061, Convergence:   inf\n",
      "Epoch: 239, Loss: 2502.82625, Residuals: -0.93878, Convergence: 0.000710\n",
      "Evidence 15024.305\n",
      "\n",
      "Epoch: 239, Evidence: 15024.30469, Convergence: 0.000434\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 8.85e-02\n",
      "Epoch: 239, Loss: 2505.58272, Residuals: -0.93878, Convergence:   inf\n",
      "Epoch: 240, Loss: 2548.31938, Residuals: -0.98107, Convergence: -0.016771\n",
      "Epoch: 240, Loss: 2503.39191, Residuals: -0.93732, Convergence: 0.000875\n",
      "Evidence 15028.339\n",
      "\n",
      "Epoch: 240, Evidence: 15028.33887, Convergence: 0.000702\n",
      "Updating hyper-parameters...\n",
      "Total samples: 184, Updated regularization: 8.15e-02\n",
      "Epoch: 240, Loss: 2505.04234, Residuals: -0.93732, Convergence:   inf\n",
      "Epoch: 241, Loss: 2508.91758, Residuals: -0.94212, Convergence: -0.001545\n",
      "Epoch: 241, Loss: 2504.76265, Residuals: -0.93724, Convergence: 0.000112\n",
      "Evidence 15030.241\n",
      "\n",
      "Epoch: 241, Evidence: 15030.24121, Convergence: 0.000829\n",
      "Total samples: 183, Number of parameters: 300, Initial regularization: 1.00e+00\n",
      "Epoch: 0, Loss: 384.35709, Residuals: -4.55192, Convergence:   inf\n",
      "Epoch: 1, Loss: 358.45088, Residuals: -4.43119, Convergence: 0.072273\n",
      "Epoch: 2, Loss: 337.16037, Residuals: -4.26531, Convergence: 0.063147\n",
      "Epoch: 3, Loss: 320.94058, Residuals: -4.09977, Convergence: 0.050538\n",
      "Epoch: 4, Loss: 308.56226, Residuals: -3.95400, Convergence: 0.040116\n",
      "Epoch: 5, Loss: 298.73352, Residuals: -3.82499, Convergence: 0.032901\n",
      "Epoch: 6, Loss: 290.74673, Residuals: -3.71254, Convergence: 0.027470\n",
      "Epoch: 7, Loss: 284.11078, Residuals: -3.61620, Convergence: 0.023357\n",
      "Epoch: 8, Loss: 278.46236, Residuals: -3.53390, Convergence: 0.020284\n",
      "Epoch: 9, Loss: 273.54532, Residuals: -3.46328, Convergence: 0.017975\n",
      "Epoch: 10, Loss: 269.17751, Residuals: -3.40220, Convergence: 0.016226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Loss: 265.22675, Residuals: -3.34887, Convergence: 0.014896\n",
      "Epoch: 12, Loss: 261.59617, Residuals: -3.30174, Convergence: 0.013879\n",
      "Epoch: 13, Loss: 258.21470, Residuals: -3.25950, Convergence: 0.013096\n",
      "Epoch: 14, Loss: 255.03100, Residuals: -3.22100, Convergence: 0.012484\n",
      "Epoch: 15, Loss: 252.01058, Residuals: -3.18528, Convergence: 0.011985\n",
      "Epoch: 16, Loss: 249.13405, Residuals: -3.15170, Convergence: 0.011546\n",
      "Epoch: 17, Loss: 246.38878, Residuals: -3.11986, Convergence: 0.011142\n",
      "Epoch: 18, Loss: 243.75528, Residuals: -3.08938, Convergence: 0.010804\n",
      "Epoch: 19, Loss: 241.20265, Residuals: -3.05978, Convergence: 0.010583\n",
      "Epoch: 20, Loss: 238.69412, Residuals: -3.03048, Convergence: 0.010509\n",
      "Epoch: 21, Loss: 236.19428, Residuals: -3.00090, Convergence: 0.010584\n",
      "Epoch: 22, Loss: 233.67295, Residuals: -2.97059, Convergence: 0.010790\n",
      "Epoch: 23, Loss: 231.10019, Residuals: -2.93917, Convergence: 0.011133\n",
      "Epoch: 24, Loss: 228.43163, Residuals: -2.90613, Convergence: 0.011682\n",
      "Epoch: 25, Loss: 225.61095, Residuals: -2.87084, Convergence: 0.012502\n",
      "Epoch: 26, Loss: 222.64265, Residuals: -2.83324, Convergence: 0.013332\n",
      "Epoch: 27, Loss: 219.65892, Residuals: -2.79473, Convergence: 0.013583\n",
      "Epoch: 28, Loss: 216.77098, Residuals: -2.75660, Convergence: 0.013323\n",
      "Epoch: 29, Loss: 213.99418, Residuals: -2.71921, Convergence: 0.012976\n",
      "Epoch: 30, Loss: 211.31199, Residuals: -2.68247, Convergence: 0.012693\n",
      "Epoch: 31, Loss: 208.70729, Residuals: -2.64625, Convergence: 0.012480\n",
      "Epoch: 32, Loss: 206.16777, Residuals: -2.61043, Convergence: 0.012318\n",
      "Epoch: 33, Loss: 203.68557, Residuals: -2.57494, Convergence: 0.012186\n",
      "Epoch: 34, Loss: 201.25606, Residuals: -2.53972, Convergence: 0.012072\n",
      "Epoch: 35, Loss: 198.87679, Residuals: -2.50472, Convergence: 0.011964\n",
      "Epoch: 36, Loss: 196.54678, Residuals: -2.46991, Convergence: 0.011855\n",
      "Epoch: 37, Loss: 194.26597, Residuals: -2.43529, Convergence: 0.011741\n",
      "Epoch: 38, Loss: 192.03492, Residuals: -2.40083, Convergence: 0.011618\n",
      "Epoch: 39, Loss: 189.85457, Residuals: -2.36655, Convergence: 0.011484\n",
      "Epoch: 40, Loss: 187.72606, Residuals: -2.33244, Convergence: 0.011338\n",
      "Epoch: 41, Loss: 185.65058, Residuals: -2.29853, Convergence: 0.011179\n",
      "Epoch: 42, Loss: 183.62926, Residuals: -2.26484, Convergence: 0.011008\n",
      "Epoch: 43, Loss: 181.66313, Residuals: -2.23137, Convergence: 0.010823\n",
      "Epoch: 44, Loss: 179.75299, Residuals: -2.19817, Convergence: 0.010626\n",
      "Epoch: 45, Loss: 177.89957, Residuals: -2.16526, Convergence: 0.010418\n",
      "Epoch: 46, Loss: 176.10360, Residuals: -2.13267, Convergence: 0.010198\n",
      "Epoch: 47, Loss: 174.36602, Residuals: -2.10043, Convergence: 0.009965\n",
      "Epoch: 48, Loss: 172.68810, Residuals: -2.06857, Convergence: 0.009716\n",
      "Epoch: 49, Loss: 171.07155, Residuals: -2.03715, Convergence: 0.009450\n",
      "Epoch: 50, Loss: 169.51822, Residuals: -2.00620, Convergence: 0.009163\n",
      "Epoch: 51, Loss: 168.02976, Residuals: -1.97577, Convergence: 0.008858\n",
      "Epoch: 52, Loss: 166.60722, Residuals: -1.94592, Convergence: 0.008538\n",
      "Epoch: 53, Loss: 165.25076, Residuals: -1.91669, Convergence: 0.008209\n",
      "Epoch: 54, Loss: 163.95959, Residuals: -1.88810, Convergence: 0.007875\n",
      "Epoch: 55, Loss: 162.73217, Residuals: -1.86020, Convergence: 0.007543\n",
      "Epoch: 56, Loss: 161.56629, Residuals: -1.83299, Convergence: 0.007216\n",
      "Epoch: 57, Loss: 160.45938, Residuals: -1.80649, Convergence: 0.006898\n",
      "Epoch: 58, Loss: 159.40866, Residuals: -1.78070, Convergence: 0.006591\n",
      "Epoch: 59, Loss: 158.41127, Residuals: -1.75562, Convergence: 0.006296\n",
      "Epoch: 60, Loss: 157.46445, Residuals: -1.73125, Convergence: 0.006013\n",
      "Epoch: 61, Loss: 156.56560, Residuals: -1.70758, Convergence: 0.005741\n",
      "Epoch: 62, Loss: 155.71231, Residuals: -1.68462, Convergence: 0.005480\n",
      "Epoch: 63, Loss: 154.90239, Residuals: -1.66236, Convergence: 0.005229\n",
      "Epoch: 64, Loss: 154.13378, Residuals: -1.64080, Convergence: 0.004987\n",
      "Epoch: 65, Loss: 153.40461, Residuals: -1.61995, Convergence: 0.004753\n",
      "Epoch: 66, Loss: 152.71306, Residuals: -1.59980, Convergence: 0.004528\n",
      "Epoch: 67, Loss: 152.05732, Residuals: -1.58036, Convergence: 0.004312\n",
      "Epoch: 68, Loss: 151.43558, Residuals: -1.56162, Convergence: 0.004106\n",
      "Epoch: 69, Loss: 150.84600, Residuals: -1.54358, Convergence: 0.003909\n",
      "Epoch: 70, Loss: 150.28666, Residuals: -1.52622, Convergence: 0.003722\n",
      "Epoch: 71, Loss: 149.75563, Residuals: -1.50953, Convergence: 0.003546\n",
      "Epoch: 72, Loss: 149.25100, Residuals: -1.49348, Convergence: 0.003381\n",
      "Epoch: 73, Loss: 148.77096, Residuals: -1.47806, Convergence: 0.003227\n",
      "Epoch: 74, Loss: 148.31384, Residuals: -1.46323, Convergence: 0.003082\n",
      "Epoch: 75, Loss: 147.87818, Residuals: -1.44898, Convergence: 0.002946\n",
      "Epoch: 76, Loss: 147.46275, Residuals: -1.43526, Convergence: 0.002817\n",
      "Epoch: 77, Loss: 147.06651, Residuals: -1.42207, Convergence: 0.002694\n",
      "Epoch: 78, Loss: 146.68858, Residuals: -1.40939, Convergence: 0.002576\n",
      "Epoch: 79, Loss: 146.32825, Residuals: -1.39718, Convergence: 0.002462\n",
      "Epoch: 80, Loss: 145.98487, Residuals: -1.38545, Convergence: 0.002352\n",
      "Epoch: 81, Loss: 145.65787, Residuals: -1.37417, Convergence: 0.002245\n",
      "Epoch: 82, Loss: 145.34670, Residuals: -1.36333, Convergence: 0.002141\n",
      "Epoch: 83, Loss: 145.05083, Residuals: -1.35291, Convergence: 0.002040\n",
      "Epoch: 84, Loss: 144.76974, Residuals: -1.34290, Convergence: 0.001942\n",
      "Epoch: 85, Loss: 144.50291, Residuals: -1.33329, Convergence: 0.001847\n",
      "Epoch: 86, Loss: 144.24981, Residuals: -1.32406, Convergence: 0.001755\n",
      "Epoch: 87, Loss: 144.00995, Residuals: -1.31520, Convergence: 0.001666\n",
      "Epoch: 88, Loss: 143.78282, Residuals: -1.30669, Convergence: 0.001580\n",
      "Epoch: 89, Loss: 143.56794, Residuals: -1.29853, Convergence: 0.001497\n",
      "Epoch: 90, Loss: 143.36485, Residuals: -1.29070, Convergence: 0.001417\n",
      "Epoch: 91, Loss: 143.17310, Residuals: -1.28319, Convergence: 0.001339\n",
      "Epoch: 92, Loss: 142.99224, Residuals: -1.27600, Convergence: 0.001265\n",
      "Epoch: 93, Loss: 142.82186, Residuals: -1.26910, Convergence: 0.001193\n",
      "Epoch: 94, Loss: 142.66155, Residuals: -1.26250, Convergence: 0.001124\n",
      "Epoch: 95, Loss: 142.51088, Residuals: -1.25618, Convergence: 0.001057\n",
      "Epoch: 96, Loss: 142.36942, Residuals: -1.25014, Convergence: 0.000994\n",
      "Evidence -184.009\n",
      "\n",
      "Epoch: 96, Evidence: -184.00909, Convergence:   inf\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.24e-01\n",
      "Epoch: 96, Loss: 1372.28903, Residuals: -1.25014, Convergence:   inf\n",
      "Epoch: 97, Loss: 1309.59504, Residuals: -1.28241, Convergence: 0.047873\n",
      "Epoch: 98, Loss: 1262.65189, Residuals: -1.30688, Convergence: 0.037178\n",
      "Epoch: 99, Loss: 1227.52358, Residuals: -1.32344, Convergence: 0.028617\n",
      "Epoch: 100, Loss: 1200.13567, Residuals: -1.33437, Convergence: 0.022821\n",
      "Epoch: 101, Loss: 1177.85808, Residuals: -1.34203, Convergence: 0.018914\n",
      "Epoch: 102, Loss: 1159.27389, Residuals: -1.34758, Convergence: 0.016031\n",
      "Epoch: 103, Loss: 1143.53479, Residuals: -1.35153, Convergence: 0.013764\n",
      "Epoch: 104, Loss: 1130.05589, Residuals: -1.35413, Convergence: 0.011928\n",
      "Epoch: 105, Loss: 1118.39817, Residuals: -1.35558, Convergence: 0.010424\n",
      "Epoch: 106, Loss: 1108.21706, Residuals: -1.35600, Convergence: 0.009187\n",
      "Epoch: 107, Loss: 1099.23259, Residuals: -1.35552, Convergence: 0.008173\n",
      "Epoch: 108, Loss: 1091.21495, Residuals: -1.35423, Convergence: 0.007347\n",
      "Epoch: 109, Loss: 1083.96986, Residuals: -1.35220, Convergence: 0.006684\n",
      "Epoch: 110, Loss: 1077.33132, Residuals: -1.34950, Convergence: 0.006162\n",
      "Epoch: 111, Loss: 1071.15535, Residuals: -1.34615, Convergence: 0.005766\n",
      "Epoch: 112, Loss: 1065.31440, Residuals: -1.34219, Convergence: 0.005483\n",
      "Epoch: 113, Loss: 1059.69662, Residuals: -1.33761, Convergence: 0.005301\n",
      "Epoch: 114, Loss: 1054.20428, Residuals: -1.33242, Convergence: 0.005210\n",
      "Epoch: 115, Loss: 1048.75590, Residuals: -1.32662, Convergence: 0.005195\n",
      "Epoch: 116, Loss: 1043.29254, Residuals: -1.32022, Convergence: 0.005237\n",
      "Epoch: 117, Loss: 1037.78895, Residuals: -1.31327, Convergence: 0.005303\n",
      "Epoch: 118, Loss: 1032.26558, Residuals: -1.30583, Convergence: 0.005351\n",
      "Epoch: 119, Loss: 1026.78991, Residuals: -1.29800, Convergence: 0.005333\n",
      "Epoch: 120, Loss: 1021.45860, Residuals: -1.28987, Convergence: 0.005219\n",
      "Epoch: 121, Loss: 1016.36207, Residuals: -1.28156, Convergence: 0.005014\n",
      "Epoch: 122, Loss: 1011.55912, Residuals: -1.27313, Convergence: 0.004748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 123, Loss: 1007.07085, Residuals: -1.26467, Convergence: 0.004457\n",
      "Epoch: 124, Loss: 1002.89090, Residuals: -1.25624, Convergence: 0.004168\n",
      "Epoch: 125, Loss: 998.99710, Residuals: -1.24790, Convergence: 0.003898\n",
      "Epoch: 126, Loss: 995.36212, Residuals: -1.23968, Convergence: 0.003652\n",
      "Epoch: 127, Loss: 991.95935, Residuals: -1.23163, Convergence: 0.003430\n",
      "Epoch: 128, Loss: 988.76491, Residuals: -1.22377, Convergence: 0.003231\n",
      "Epoch: 129, Loss: 985.75814, Residuals: -1.21612, Convergence: 0.003050\n",
      "Epoch: 130, Loss: 982.92245, Residuals: -1.20871, Convergence: 0.002885\n",
      "Epoch: 131, Loss: 980.24372, Residuals: -1.20154, Convergence: 0.002733\n",
      "Epoch: 132, Loss: 977.71066, Residuals: -1.19463, Convergence: 0.002591\n",
      "Epoch: 133, Loss: 975.31390, Residuals: -1.18798, Convergence: 0.002457\n",
      "Epoch: 134, Loss: 973.04503, Residuals: -1.18161, Convergence: 0.002332\n",
      "Epoch: 135, Loss: 970.89639, Residuals: -1.17552, Convergence: 0.002213\n",
      "Epoch: 136, Loss: 968.86083, Residuals: -1.16970, Convergence: 0.002101\n",
      "Epoch: 137, Loss: 966.93184, Residuals: -1.16415, Convergence: 0.001995\n",
      "Epoch: 138, Loss: 965.10287, Residuals: -1.15886, Convergence: 0.001895\n",
      "Epoch: 139, Loss: 963.36748, Residuals: -1.15384, Convergence: 0.001801\n",
      "Epoch: 140, Loss: 961.71925, Residuals: -1.14907, Convergence: 0.001714\n",
      "Epoch: 141, Loss: 960.15219, Residuals: -1.14454, Convergence: 0.001632\n",
      "Epoch: 142, Loss: 958.66016, Residuals: -1.14024, Convergence: 0.001556\n",
      "Epoch: 143, Loss: 957.23784, Residuals: -1.13616, Convergence: 0.001486\n",
      "Epoch: 144, Loss: 955.87961, Residuals: -1.13229, Convergence: 0.001421\n",
      "Epoch: 145, Loss: 954.57998, Residuals: -1.12862, Convergence: 0.001361\n",
      "Epoch: 146, Loss: 953.33448, Residuals: -1.12513, Convergence: 0.001306\n",
      "Epoch: 147, Loss: 952.13851, Residuals: -1.12181, Convergence: 0.001256\n",
      "Epoch: 148, Loss: 950.98781, Residuals: -1.11866, Convergence: 0.001210\n",
      "Epoch: 149, Loss: 949.87871, Residuals: -1.11566, Convergence: 0.001168\n",
      "Epoch: 150, Loss: 948.80707, Residuals: -1.11279, Convergence: 0.001129\n",
      "Epoch: 151, Loss: 947.76996, Residuals: -1.11006, Convergence: 0.001094\n",
      "Epoch: 152, Loss: 946.76414, Residuals: -1.10745, Convergence: 0.001062\n",
      "Epoch: 153, Loss: 945.78680, Residuals: -1.10495, Convergence: 0.001033\n",
      "Epoch: 154, Loss: 944.83532, Residuals: -1.10255, Convergence: 0.001007\n",
      "Epoch: 155, Loss: 943.90684, Residuals: -1.10025, Convergence: 0.000984\n",
      "Evidence 11089.814\n",
      "\n",
      "Epoch: 155, Evidence: 11089.81445, Convergence: 1.016593\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 5.78e-01\n",
      "Epoch: 155, Loss: 2360.04130, Residuals: -1.10025, Convergence:   inf\n",
      "Epoch: 156, Loss: 2319.55949, Residuals: -1.11054, Convergence: 0.017452\n",
      "Epoch: 157, Loss: 2291.43472, Residuals: -1.10995, Convergence: 0.012274\n",
      "Epoch: 158, Loss: 2267.74386, Residuals: -1.10817, Convergence: 0.010447\n",
      "Epoch: 159, Loss: 2247.37533, Residuals: -1.10599, Convergence: 0.009063\n",
      "Epoch: 160, Loss: 2229.62624, Residuals: -1.10352, Convergence: 0.007961\n",
      "Epoch: 161, Loss: 2213.98559, Residuals: -1.10083, Convergence: 0.007064\n",
      "Epoch: 162, Loss: 2200.07332, Residuals: -1.09793, Convergence: 0.006324\n",
      "Epoch: 163, Loss: 2187.60003, Residuals: -1.09484, Convergence: 0.005702\n",
      "Epoch: 164, Loss: 2176.33381, Residuals: -1.09158, Convergence: 0.005177\n",
      "Epoch: 165, Loss: 2166.07881, Residuals: -1.08817, Convergence: 0.004734\n",
      "Epoch: 166, Loss: 2156.66335, Residuals: -1.08460, Convergence: 0.004366\n",
      "Epoch: 167, Loss: 2147.94014, Residuals: -1.08087, Convergence: 0.004061\n",
      "Epoch: 168, Loss: 2139.79196, Residuals: -1.07700, Convergence: 0.003808\n",
      "Epoch: 169, Loss: 2132.14002, Residuals: -1.07299, Convergence: 0.003589\n",
      "Epoch: 170, Loss: 2124.94780, Residuals: -1.06888, Convergence: 0.003385\n",
      "Epoch: 171, Loss: 2118.20777, Residuals: -1.06472, Convergence: 0.003182\n",
      "Epoch: 172, Loss: 2111.92467, Residuals: -1.06056, Convergence: 0.002975\n",
      "Epoch: 173, Loss: 2106.09955, Residuals: -1.05648, Convergence: 0.002766\n",
      "Epoch: 174, Loss: 2100.72350, Residuals: -1.05250, Convergence: 0.002559\n",
      "Epoch: 175, Loss: 2095.77850, Residuals: -1.04868, Convergence: 0.002360\n",
      "Epoch: 176, Loss: 2091.23962, Residuals: -1.04502, Convergence: 0.002170\n",
      "Epoch: 177, Loss: 2087.07824, Residuals: -1.04154, Convergence: 0.001994\n",
      "Epoch: 178, Loss: 2083.26456, Residuals: -1.03824, Convergence: 0.001831\n",
      "Epoch: 179, Loss: 2079.76854, Residuals: -1.03511, Convergence: 0.001681\n",
      "Epoch: 180, Loss: 2076.56045, Residuals: -1.03217, Convergence: 0.001545\n",
      "Epoch: 181, Loss: 2073.61273, Residuals: -1.02939, Convergence: 0.001422\n",
      "Epoch: 182, Loss: 2070.89874, Residuals: -1.02676, Convergence: 0.001311\n",
      "Epoch: 183, Loss: 2068.39492, Residuals: -1.02429, Convergence: 0.001211\n",
      "Epoch: 184, Loss: 2066.07898, Residuals: -1.02196, Convergence: 0.001121\n",
      "Epoch: 185, Loss: 2063.93178, Residuals: -1.01975, Convergence: 0.001040\n",
      "Epoch: 186, Loss: 2061.93493, Residuals: -1.01767, Convergence: 0.000968\n",
      "Evidence 14285.443\n",
      "\n",
      "Epoch: 186, Evidence: 14285.44336, Convergence: 0.223698\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 4.40e-01\n",
      "Epoch: 186, Loss: 2489.01265, Residuals: -1.01767, Convergence:   inf\n",
      "Epoch: 187, Loss: 2473.33143, Residuals: -1.01486, Convergence: 0.006340\n",
      "Epoch: 188, Loss: 2460.66547, Residuals: -1.01124, Convergence: 0.005147\n",
      "Epoch: 189, Loss: 2449.87436, Residuals: -1.00767, Convergence: 0.004405\n",
      "Epoch: 190, Loss: 2440.58670, Residuals: -1.00428, Convergence: 0.003806\n",
      "Epoch: 191, Loss: 2432.53203, Residuals: -1.00114, Convergence: 0.003311\n",
      "Epoch: 192, Loss: 2425.50353, Residuals: -0.99824, Convergence: 0.002898\n",
      "Epoch: 193, Loss: 2419.33469, Residuals: -0.99556, Convergence: 0.002550\n",
      "Epoch: 194, Loss: 2413.88797, Residuals: -0.99309, Convergence: 0.002256\n",
      "Epoch: 195, Loss: 2409.05039, Residuals: -0.99079, Convergence: 0.002008\n",
      "Epoch: 196, Loss: 2404.72831, Residuals: -0.98865, Convergence: 0.001797\n",
      "Epoch: 197, Loss: 2400.84248, Residuals: -0.98666, Convergence: 0.001619\n",
      "Epoch: 198, Loss: 2397.32852, Residuals: -0.98481, Convergence: 0.001466\n",
      "Epoch: 199, Loss: 2394.13180, Residuals: -0.98308, Convergence: 0.001335\n",
      "Epoch: 200, Loss: 2391.20829, Residuals: -0.98146, Convergence: 0.001223\n",
      "Epoch: 201, Loss: 2388.52059, Residuals: -0.97994, Convergence: 0.001125\n",
      "Epoch: 202, Loss: 2386.03724, Residuals: -0.97853, Convergence: 0.001041\n",
      "Epoch: 203, Loss: 2383.73311, Residuals: -0.97720, Convergence: 0.000967\n",
      "Evidence 14751.253\n",
      "\n",
      "Epoch: 203, Evidence: 14751.25293, Convergence: 0.031578\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 3.37e-01\n",
      "Epoch: 203, Loss: 2494.53967, Residuals: -0.97720, Convergence:   inf\n",
      "Epoch: 204, Loss: 2487.52514, Residuals: -0.97386, Convergence: 0.002820\n",
      "Epoch: 205, Loss: 2481.75359, Residuals: -0.97077, Convergence: 0.002326\n",
      "Epoch: 206, Loss: 2476.85855, Residuals: -0.96807, Convergence: 0.001976\n",
      "Epoch: 207, Loss: 2472.64181, Residuals: -0.96574, Convergence: 0.001705\n",
      "Epoch: 208, Loss: 2468.95965, Residuals: -0.96372, Convergence: 0.001491\n",
      "Epoch: 209, Loss: 2465.70470, Residuals: -0.96196, Convergence: 0.001320\n",
      "Epoch: 210, Loss: 2462.79366, Residuals: -0.96042, Convergence: 0.001182\n",
      "Epoch: 211, Loss: 2460.16323, Residuals: -0.95906, Convergence: 0.001069\n",
      "Epoch: 212, Loss: 2457.76293, Residuals: -0.95786, Convergence: 0.000977\n",
      "Evidence 14844.884\n",
      "\n",
      "Epoch: 212, Evidence: 14844.88379, Convergence: 0.006307\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.65e-01\n",
      "Epoch: 212, Loss: 2495.99901, Residuals: -0.95786, Convergence:   inf\n",
      "Epoch: 213, Loss: 2491.94457, Residuals: -0.95523, Convergence: 0.001627\n",
      "Epoch: 214, Loss: 2488.56485, Residuals: -0.95305, Convergence: 0.001358\n",
      "Epoch: 215, Loss: 2485.65188, Residuals: -0.95126, Convergence: 0.001172\n",
      "Epoch: 216, Loss: 2483.08987, Residuals: -0.94978, Convergence: 0.001032\n",
      "Epoch: 217, Loss: 2480.79882, Residuals: -0.94852, Convergence: 0.000924\n",
      "Evidence 14877.154\n",
      "\n",
      "Epoch: 217, Evidence: 14877.15430, Convergence: 0.002169\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 2.14e-01\n",
      "Epoch: 217, Loss: 2496.81548, Residuals: -0.94852, Convergence:   inf\n",
      "Epoch: 218, Loss: 2493.94875, Residuals: -0.94640, Convergence: 0.001149\n",
      "Epoch: 219, Loss: 2491.51426, Residuals: -0.94473, Convergence: 0.000977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence 14889.920\n",
      "\n",
      "Epoch: 219, Evidence: 14889.91992, Convergence: 0.000857\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.78e-01\n",
      "Epoch: 219, Loss: 2497.46910, Residuals: -0.94473, Convergence:   inf\n",
      "Epoch: 220, Loss: 2492.90647, Residuals: -0.94158, Convergence: 0.001830\n",
      "Epoch: 221, Loss: 2489.31904, Residuals: -0.93956, Convergence: 0.001441\n",
      "Epoch: 222, Loss: 2486.35146, Residuals: -0.93804, Convergence: 0.001194\n",
      "Epoch: 223, Loss: 2483.79393, Residuals: -0.93703, Convergence: 0.001030\n",
      "Epoch: 224, Loss: 2481.52428, Residuals: -0.93635, Convergence: 0.000915\n",
      "Evidence 14910.696\n",
      "\n",
      "Epoch: 224, Evidence: 14910.69629, Convergence: 0.002250\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.47e-01\n",
      "Epoch: 224, Loss: 2497.53050, Residuals: -0.93635, Convergence:   inf\n",
      "Epoch: 225, Loss: 2494.62349, Residuals: -0.93365, Convergence: 0.001165\n",
      "Epoch: 226, Loss: 2492.24858, Residuals: -0.93219, Convergence: 0.000953\n",
      "Evidence 14922.049\n",
      "\n",
      "Epoch: 226, Evidence: 14922.04883, Convergence: 0.000761\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.25e-01\n",
      "Epoch: 226, Loss: 2497.77226, Residuals: -0.93219, Convergence:   inf\n",
      "Epoch: 227, Loss: 2493.41173, Residuals: -0.92840, Convergence: 0.001749\n",
      "Epoch: 228, Loss: 2490.11406, Residuals: -0.92879, Convergence: 0.001324\n",
      "Epoch: 229, Loss: 2487.29266, Residuals: -0.92892, Convergence: 0.001134\n",
      "Epoch: 230, Loss: 2484.83924, Residuals: -0.93137, Convergence: 0.000987\n",
      "Evidence 14938.260\n",
      "\n",
      "Epoch: 230, Evidence: 14938.25977, Convergence: 0.001845\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 1.11e-01\n",
      "Epoch: 230, Loss: 2497.10638, Residuals: -0.93137, Convergence:   inf\n",
      "Epoch: 231, Loss: 2495.31396, Residuals: -0.92975, Convergence: 0.000718\n",
      "Evidence 14944.590\n",
      "\n",
      "Epoch: 231, Evidence: 14944.58984, Convergence: 0.000424\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 9.07e-02\n",
      "Epoch: 231, Loss: 2498.12185, Residuals: -0.92975, Convergence:   inf\n",
      "Epoch: 232, Loss: 2545.41690, Residuals: -0.97259, Convergence: -0.018580\n",
      "Epoch: 232, Loss: 2495.61045, Residuals: -0.92745, Convergence: 0.001006\n",
      "Epoch: 233, Loss: 2495.06616, Residuals: -0.92873, Convergence: 0.000218\n",
      "Evidence 14950.045\n",
      "\n",
      "Epoch: 233, Evidence: 14950.04492, Convergence: 0.000788\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.55e-02\n",
      "Epoch: 233, Loss: 2498.07797, Residuals: -0.92873, Convergence:   inf\n",
      "Epoch: 234, Loss: 2555.08290, Residuals: -0.97864, Convergence: -0.022310\n",
      "Epoch: 234, Loss: 2495.62393, Residuals: -0.92548, Convergence: 0.000983\n",
      "Evidence 14954.585\n",
      "\n",
      "Epoch: 234, Evidence: 14954.58496, Convergence: 0.001092\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 7.03e-02\n",
      "Epoch: 234, Loss: 2497.70146, Residuals: -0.92548, Convergence:   inf\n",
      "Epoch: 235, Loss: 2497.23609, Residuals: -0.92477, Convergence: 0.000186\n",
      "Evidence 14956.451\n",
      "\n",
      "Epoch: 235, Evidence: 14956.45117, Convergence: 0.000125\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 5.89e-02\n",
      "Epoch: 235, Loss: 2498.29541, Residuals: -0.92477, Convergence:   inf\n",
      "Epoch: 236, Loss: 2550.94379, Residuals: -0.97672, Convergence: -0.020639\n",
      "Epoch: 236, Loss: 2496.22592, Residuals: -0.92130, Convergence: 0.000829\n",
      "Evidence 14960.007\n",
      "\n",
      "Epoch: 236, Evidence: 14960.00684, Convergence: 0.000362\n",
      "Updating hyper-parameters...\n",
      "Total samples: 183, Updated regularization: 5.54e-02\n",
      "Epoch: 236, Loss: 2497.82952, Residuals: -0.92130, Convergence:   inf\n",
      "Epoch: 237, Loss: 2501.75465, Residuals: -0.92541, Convergence: -0.001569\n",
      "Epoch: 237, Loss: 2497.88398, Residuals: -0.92049, Convergence: -0.000022\n",
      "Evidence 14960.993\n",
      "\n",
      "Epoch: 237, Evidence: 14960.99316, Convergence: 0.000428\n"
     ]
    }
   ],
   "source": [
    "# perform k-fold x-validation several times (n_trials) with randomized partitions of data\n",
    "for k in range(4, n_trials):\n",
    "    \n",
    "    # initialize dataframe to save predictions\n",
    "    df_kfold = pd.DataFrame()\n",
    "\n",
    "    # object to get indices of kfold splits\n",
    "    kf = KFold(n_splits=n_splits, random_state=k, shuffle=True)\n",
    "\n",
    "    # loop over k-fold splits\n",
    "    for train_index, test_index in kf.split(unique_experiments):\n",
    "\n",
    "        # update index values to pull trajectories \n",
    "        train_index = np.in1d(all_experiments, unique_experiments[train_index])\n",
    "        test_index = np.in1d(all_experiments, unique_experiments[test_index])\n",
    "\n",
    "        # pull train and test indices\n",
    "        train_df  = df.iloc[train_index].copy() \n",
    "        test_df   = df.iloc[test_index].copy() \n",
    "\n",
    "        # scale metabolites based on training data values \n",
    "        train_df_scaled = train_df.copy()\n",
    "        test_df_scaled  = test_df.copy()\n",
    "        train_df_scaled[metabolites] /= train_df[metabolites].max()\n",
    "        test_df_scaled[metabolites]  /= train_df[metabolites].max()\n",
    "        \n",
    "        # init model \n",
    "        model = CR(dataframe=train_df_scaled, species=species, resources=metabolites, r0=1.)\n",
    "\n",
    "        # fit to data \n",
    "        model.fit(lr=1e-1, map_tol=1e-3, evd_tol=1e-3)\n",
    "\n",
    "        # make predictions\n",
    "        pred_df_kfold = model.predict_df(test_df_scaled, species, metabolites)\n",
    "        \n",
    "        # scale back resources\n",
    "        for i, variable in enumerate(metabolites):\n",
    "            pred_df_kfold[variable + \" true\"] *= train_df[variable].max()\n",
    "            pred_df_kfold[variable + \" pred\"] *= train_df[variable].max()\n",
    "            pred_df_kfold[variable + \" stdv\"] *= train_df[variable].max()\n",
    "            \n",
    "        # save kfold predictions\n",
    "        df_kfold = pd.concat((df_kfold, pred_df_kfold))\n",
    "        df_kfold.to_csv(f\"Fig2/Kfold/MCR_{n_splits}_fold_{k+1}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
